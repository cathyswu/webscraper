url,title,content,content_length,code_block_count,headings_count,code_blocks,timestamp
https://pytorch.org/docs/stable/notes/cpu_threading_torchscript_inference.html,CPU threading and TorchScript inference — PyTorch 2.7 documentation,"CPU threading and TorchScript inference ¶ PyTorch allows using multiple CPU threads during TorchScript model inference.
The following figure shows different levels of parallelism one would find in a
typical application: One or more inference threads execute a model’s forward pass on the given inputs.
Each inference thread invokes a JIT interpreter that executes the ops
of a model inline, one by one. A model can utilize a fork TorchScript
primitive to launch an asynchronous task. Forking several operations at once
results in a task that is executed in parallel. The fork operator returns a Future object which can be used to synchronize on later, for example: @torch . jit . script def compute_z ( x ): return torch . mm ( x , self . w_z ) @torch . jit . script def forward ( x ): # launch compute_z asynchronously: fut = torch . jit . _fork ( compute_z , x ) # execute the next operation in parallel to compute_z: y = torch . mm ( x , self . w_y ) # wait for the result of compute_z: z = torch . jit . _wait ( fut ) return y + z PyTorch uses a single thread pool for the inter-op parallelism, this thread pool
is shared by all inference tasks that are forked within the application process. In addition to the inter-op parallelism, PyTorch can also utilize multiple threads
within the ops ( intra-op parallelism ). This can be useful in many cases,
including element-wise ops on large tensors, convolutions, GEMMs, embedding
lookups and others. Build options ¶ PyTorch uses an internal ATen library to implement ops. In addition to that,
PyTorch can also be built with support of external libraries, such as MKL and MKL-DNN ,
to speed up computations on CPU. ATen, MKL and MKL-DNN support intra-op parallelism and depend on the
following parallelization libraries to implement it: OpenMP - a standard (and a library, usually shipped with a compiler), widely used in external libraries; TBB - a newer parallelization library optimized for task-based parallelism and concurrent environments. OpenMP historically has been used by a large number of libraries. It is known
for a relative ease of use and support for loop-based parallelism and other primitives. TBB is used to a lesser extent in external libraries, but, at the same time,
is optimized for the concurrent environments. PyTorch’s TBB backend guarantees that
there’s a separate, single, per-process intra-op thread pool used by all of the
ops running in the application. Depending of the use case, one might find one or another parallelization
library a better choice in their application. PyTorch allows selecting of the parallelization backend used by ATen and other
libraries at the build time with the following build options: Library Build Option Values Notes ATen ATEN_THREADING OMP (default), TBB MKL MKL_THREADING (same) To enable MKL use BLAS=MKL MKL-DNN MKLDNN_CPU_RUNTIME (same) To enable MKL-DNN use USE_MKLDNN=1 It is recommended not to mix OpenMP and TBB within one build. Any of the TBB values above require USE_TBB=1 build setting (default: OFF).
A separate setting USE_OPENMP=1 (default: ON) is required for OpenMP parallelism. Runtime API ¶ The following API is used to control thread settings: Type of parallelism Settings Notes Inter-op parallelism at::set_num_interop_threads , at::get_num_interop_threads (C++) set_num_interop_threads , get_num_interop_threads (Python, torch module) Default number of threads: number of CPU cores. Intra-op parallelism at::set_num_threads , at::get_num_threads (C++) set_num_threads , get_num_threads (Python, torch module) Environment variables: OMP_NUM_THREADS and MKL_NUM_THREADS For the intra-op parallelism settings, at::set_num_threads , torch.set_num_threads always take precedence
over environment variables, MKL_NUM_THREADS variable takes precedence over OMP_NUM_THREADS . Tuning the number of threads ¶ The following simple script shows how a runtime of matrix multiplication changes with the number of threads: import timeit runtimes = [] threads = [ 1 ] + [ t for t in range ( 2 , 49 , 2 )] for t in threads : torch . set_num_threads ( t ) r = timeit . timeit ( setup = ""import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)"" , stmt = ""torch.mm(x, y)"" , number = 100 ) runtimes . append ( r ) # ... plotting (threads, runtimes) ... Running the script on a system with 24 physical CPU cores (Xeon E5-2680, MKL and OpenMP based build) results in the following runtimes: The following considerations should be taken into account when tuning the number of intra- and inter-op threads: When choosing the number of threads one needs to avoid oversubscription (using too many threads, leads to performance degradation). For example, in an application that uses a large application thread pool or heavily relies on
inter-op parallelism, one might find disabling intra-op parallelism as a possible option (i.e. by calling set_num_threads(1) ); In a typical application one might encounter a trade off between latency (time spent on processing an inference request) and throughput (amount of work done per unit of time). Tuning the number of threads can be a useful
tool to adjust this trade off in one way or another. For example, in latency critical applications one might want to increase the number of intra-op threads to process each request as fast as possible. At the same time, parallel implementations
of ops may add an extra overhead that increases amount work done per single request and thus reduces the overall throughput. Warning OpenMP does not guarantee that a single per-process intra-op thread
pool is going to be used in the application. On the contrary, two different application or inter-op
threads may use different OpenMP thread pools for intra-op work.
This might result in a large number of threads used by the application.
Extra care in tuning the number of threads is needed to avoid
oversubscription in multi-threaded applications in OpenMP case. Note Pre-built PyTorch releases are compiled with OpenMP support. Note parallel_info utility prints information about thread settings and can be used for debugging.
Similar output can be also obtained in Python with torch.__config__.parallel_info() call.",6168,2,7,"@torch.jit.script
def compute_z(x):
    return torch.mm(x, self.w_z)

@torch.jit.script
def forward(x):
    # launch compute_z asynchronously:
    fut = torch.jit._fork(compute_z, x)
    # execute the next operation in parallel to compute_z:
    y = torch.mm(x, self.w_y)
    # wait for the result of compute_z:
    z = torch.jit._wait(fut)
    return y + z
---
import timeit
runtimes = []
threads = [1] + [t for t in range(2, 49, 2)]
for t in threads:
    torch.set_num_threads(t)
    r = timeit.timeit(setup = ""import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)"", stmt=""torch.mm(x, y)"", number=100)
    runtimes.append(r)
# ... plotting (threads, runtimes) ...",1752175578.7478187
https://pytorch.org/docs/stable/library.html,torch.library — PyTorch 2.7 documentation,"torch.library ¶ torch.library is a collection of APIs for extending PyTorch’s core library
of operators. It contains utilities for testing custom operators, creating new
custom operators, and extending operators defined with PyTorch’s C++ operator
registration APIs (e.g. aten operators). For a detailed guide on effectively using these APIs, please see PyTorch Custom Operators Landing Page for more details on how to effectively use these APIs. Testing custom ops ¶ Use torch.library.opcheck() to test custom ops for incorrect usage of the
Python torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports
training, use torch.autograd.gradcheck() to test that the gradients are
mathematically correct. torch.library. opcheck ( op , args , kwargs = None , * , test_utils = ('test_schema', 'test_autograd_registration', 'test_faketensor', 'test_aot_dispatch_dynamic') , raise_exception = True , atol = None , rtol = None ) [source] [source] ¶ Given an operator and some sample arguments, tests if the operator is
registered correctly. That is, when you use the torch.library/TORCH_LIBRARY APIs to create a
custom op, you specified metadata (e.g. mutability info) about the custom op
and these APIs require that the functions you pass them satisfy certain
properties (e.g. no data pointer access in the fake/meta/abstract kernel) opcheck tests these metadata and properties. Concretely, we test the following: test_schema: If the schema matches the implementation of
the operator. For example: if the schema specifies a Tensor is mutated,
then we check the implementation mutates the Tensor. If the schema
specifies that we return a new Tensor, then we check that the
implementation returns a new Tensor (instead of an existing one or
a view of an existing one). test_autograd_registration: If the operator supports training
(autograd): we check that its autograd formula is registered via
torch.library.register_autograd or a manual registration to one
or more DispatchKey::Autograd keys. Any other DispatchKey-based
registrations may lead to undefined behavior. test_faketensor: If the operator has a FakeTensor kernel
(and if it is correct). The FakeTensor kernel is necessary (
but not sufficient) for the operator to work with PyTorch compilation
APIs (torch.compile/export/FX). We check that a FakeTensor kernel
(also sometimes known as a meta kernel) was registered for the
operator and that it is correct. This test takes the result of
running the operator on real tensors and the result of running
the operator on FakeTensors and checks that they have the same
Tensor metadata (sizes/strides/dtype/device/etc). test_aot_dispatch_dynamic: If the operator has correct behavior
with PyTorch compilation APIs (torch.compile/export/FX).
This checks that the outputs (and gradients, if applicable) are the
same under eager-mode PyTorch and torch.compile.
This test is a superset of test_faketensor and is an e2e test;
other things it tests are that the operator supports
functionalization and that the backward pass (if it exists) also
supports FakeTensor and functionalization. For best results, please call opcheck multiple times with a
representative set of inputs. If your operator supports
autograd, please use opcheck with inputs with requires_grad = True ;
if your operator supports multiple devices (e.g. CPU and CUDA), please
use opcheck with inputs on all supported devices. Parameters op ( Union [ OpOverload , OpOverloadPacket , CustomOpDef ] ) – The operator. Must either be a function decorated with torch.library.custom_op() or an OpOverload/OpOverloadPacket
found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo) args ( tuple [ Any , ... ] ) – The args to the operator kwargs ( Optional [ dict [ str , Any ] ] ) – The kwargs to the operator test_utils ( Union [ str , Sequence [ str ] ] ) – Tests that we should run. Default: all of them.
Example: (“test_schema”, “test_faketensor”) raise_exception ( bool ) – If we should raise an exception on the first
error. If False, we will return a dict with information
on if each test passed or not. rtol ( Optional [ float ] ) – Relative tolerance for floating point comparisons.
If specified atol must also be specified.
If omitted, default values based on the dtype are selected
(see the table in torch.testing.assert_close() ). atol ( Optional [ float ] ) – Absolute tolerance for floating point comparisons.
If specified rtol must also be specified.
If omitted, default values based on the dtype are selected
(see the table in torch.testing.assert_close() ). Return type dict [ str , str ] Warning opcheck and torch.autograd.gradcheck() test different things;
opcheck tests if your usage of torch.library APIs is correct while torch.autograd.gradcheck() tests if your autograd formula is
mathematically correct. Use both to test custom ops that support
gradient computation. Example >>> @torch . library . custom_op ( ""mylib::numpy_mul"" , mutates_args = ()) >>> def numpy_mul ( x : Tensor , y : float ) -> Tensor : >>> x_np = x . numpy ( force = True ) >>> z_np = x_np * y >>> return torch . from_numpy ( z_np ) . to ( x . device ) >>> >>> @numpy_mul . register_fake >>> def _ ( x , y ): >>> return torch . empty_like ( x ) >>> >>> def setup_context ( ctx , inputs , output ): >>> y , = inputs >>> ctx . y = y >>> >>> def backward ( ctx , grad ): >>> return grad * ctx . y , None >>> >>> numpy_mul . register_autograd ( backward , setup_context = setup_context ) >>> >>> sample_inputs = [ >>> ( torch . randn ( 3 ), 3.14 ), >>> ( torch . randn ( 2 , 3 , device = 'cuda' ), 2.718 ), >>> ( torch . randn ( 1 , 10 , requires_grad = True ), 1.234 ), >>> ( torch . randn ( 64 , 64 , device = 'cuda' , requires_grad = True ), 90.18 ), >>> ] >>> >>> for args in sample_inputs : >>> torch . library . opcheck ( numpy_mul , args ) Creating new custom ops in Python ¶ Use torch.library.custom_op() to create new custom ops. torch.library. custom_op ( name , fn = None , / , * , mutates_args , device_types = None , schema = None ) [source] ¶ Wraps a function into custom operator. Reasons why you may want to create a custom op include:
- Wrapping a third-party library or custom kernel to work with PyTorch
subsystems like Autograd.
- Preventing torch.compile/export/FX tracing from peeking inside your function. This API is used as a decorator around a function (please see examples).
The provided function must have type hints; these are needed to interface
with PyTorch’s various subsystems. Parameters name ( str ) – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace. mutates_args ( Iterable [ str ] or ""unknown"" ) – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated. device_types ( None | str | Sequence [ str ] ) – The device type(s) the function
is valid for. If no device type is provided, then the function
is used as the default implementation for all device types.
Examples: “cpu”, “cuda”.
When registering a device-specific implementation for an operator that accepts no Tensors,
we require the operator to have a “device: torch.device argument”. schema ( None | str ) – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”. Return type Union [ Callable [[ Callable [[…], object ]], CustomOpDef ], CustomOpDef ] Note We recommend not passing in a schema arg and instead letting us infer
it from the type annotations. It is error-prone to write your own schema.
You may wish to provide your own schema if our interpretation of
the type annotation is not what you want.
For more info on how to write a schema string, see here Examples:: >>> import torch >>> from torch import Tensor >>> from torch.library import custom_op >>> import numpy as np >>> >>> @custom_op ( ""mylib::numpy_sin"" , mutates_args = ()) >>> def numpy_sin ( x : Tensor ) -> Tensor : >>> x_np = x . cpu () . numpy () >>> y_np = np . sin ( x_np ) >>> return torch . from_numpy ( y_np ) . to ( device = x . device ) >>> >>> x = torch . randn ( 3 ) >>> y = numpy_sin ( x ) >>> assert torch . allclose ( y , x . sin ()) >>> >>> # Example of a custom op that only works for one device type. >>> @custom_op ( ""mylib::numpy_sin_cpu"" , mutates_args = (), device_types = ""cpu"" ) >>> def numpy_sin_cpu ( x : Tensor ) -> Tensor : >>> x_np = x . numpy () >>> y_np = np . sin ( x_np ) >>> return torch . from_numpy ( y_np ) >>> >>> x = torch . randn ( 3 ) >>> y = numpy_sin_cpu ( x ) >>> assert torch . allclose ( y , x . sin ()) >>> >>> # Example of a custom op that mutates an input >>> @custom_op ( ""mylib::numpy_sin_inplace"" , mutates_args = { ""x"" }, device_types = ""cpu"" ) >>> def numpy_sin_inplace ( x : Tensor ) -> None : >>> x_np = x . numpy () >>> np . sin ( x_np , out = x_np ) >>> >>> x = torch . randn ( 3 ) >>> expected = x . sin () >>> numpy_sin_inplace ( x ) >>> assert torch . allclose ( x , expected ) >>> >>> # Example of a factory function >>> @torch . library . custom_op ( ""mylib::bar"" , mutates_args = {}, device_types = ""cpu"" ) >>> def bar ( device : torch . device ) -> Tensor : >>> return torch . ones ( 3 ) >>> >>> bar ( ""cpu"" ) torch.library. triton_op ( name , fn = None , / , * , mutates_args , schema = None ) [source] ¶ Create a custom operator whose implementation is backed by 1+ triton kernels. This is a more structured way of using triton kernels with PyTorch.
Prefer using triton kernels with no torch.library custom operator wrappers
(like torch.library.custom_op() , torch.library.triton_op() ) because
that is simpler;
only use torch.library.custom_op() / torch.library.triton_op() if you
want to create an operator that behaves like PyTorch built-in operators.
For example, you may use a torch.library wrapper API to define the
behavior of the triton kernel when passed a tensor subclass or under
a TorchDispatchMode. Use torch.library.triton_op() instead of torch.library.custom_op() when the implementation
consists of 1+ triton kernels. torch.library.custom_op() treats
custom operators as opaque ( torch.compile() and torch.export.export() will never trace into them), but triton_op makes the implementation visible to these subsystems, allowing them
to optimize the triton kernel(s). Note that fn must only consist of calls to PyTorch-understood
operators and triton kernels. Any triton kernels called inside fn must be wrapped in a call to torch.library.wrap_triton() . Parameters name ( str ) – A name for the custom op that looks like “{namespace}::{name}”,
e.g. “mylib::my_linear”. The name is used as the op’s stable identifier
in PyTorch subsystems (e.g. torch.export, FX graphs).
To avoid name collisions, please use your project name as the namespace;
e.g. all custom ops in pytorch/fbgemm use “fbgemm” as the namespace. mutates_args ( Iterable [ str ] or ""unknown"" ) – The names of args that the function mutates.
This MUST be accurate, otherwise, the behavior is undefined. If “unknown”,
it pessimistically assumes that all inputs to the operator are being mutated. schema ( None | str ) – A schema string for the operator. If None
(recommended) we’ll infer a schema for the operator from its type
annotations. We recommend letting us infer a schema unless you
have a specific reason not to.
Example: “(Tensor x, int y) -> (Tensor, Tensor)”. Return type Callable Example: >>> import torch >>> from torch.library import triton_op , wrap_triton >>> >>> import triton >>> from triton import language as tl >>> >>> @triton . jit >>> def add_kernel ( >>> in_ptr0 , >>> in_ptr1 , >>> out_ptr , >>> n_elements , >>> BLOCK_SIZE : ""tl.constexpr"" , >>> ): >>> pid = tl . program_id ( axis = 0 ) >>> block_start = pid * BLOCK_SIZE >>> offsets = block_start + tl . arange ( 0 , BLOCK_SIZE ) >>> mask = offsets < n_elements >>> x = tl . load ( in_ptr0 + offsets , mask = mask ) >>> y = tl . load ( in_ptr1 + offsets , mask = mask ) >>> output = x + y >>> tl . store ( out_ptr + offsets , output , mask = mask ) >>> >>> @triton_op ( ""mylib::add"" , mutates_args = {}) >>> def add ( x : torch . Tensor , y : torch . Tensor ) -> torch . Tensor : >>> output = torch . empty_like ( x ) >>> n_elements = output . numel () >>> >>> def grid ( meta ): >>> return ( triton . cdiv ( n_elements , meta [ ""BLOCK_SIZE"" ]),) >>> >>> # NB: we need to wrap the triton kernel in a call to wrap_triton >>> wrap_triton ( add_kernel )[ grid ]( x , y , output , n_elements , 16 ) >>> return output >>> >>> @torch . compile >>> def f ( x , y ): >>> return add ( x , y ) >>> >>> x = torch . randn ( 3 , device = ""cuda"" ) >>> y = torch . randn ( 3 , device = ""cuda"" ) >>> >>> z = f ( x , y ) >>> assert torch . allclose ( z , x + y ) torch.library. wrap_triton ( triton_kernel , / ) [source] ¶ Allows capture of a triton kernel into a graph via make_fx or
non-strict torch.export . These technologies perform Dispatcher-based tracing (via __torch_dispatch__ ) and cannot see calls to raw triton kernels.
The wrap_triton API wraps a triton kernel into a callable that
can actually be traced into a graph. Please use this API together with torch.library.triton_op() . Examples >>> import torch >>> import triton >>> from triton import language as tl >>> from torch.fx.experimental.proxy_tensor import make_fx >>> from torch.library import wrap_triton >>> >>> @triton . jit >>> def add_kernel ( >>> in_ptr0 , >>> in_ptr1 , >>> out_ptr , >>> n_elements , >>> BLOCK_SIZE : ""tl.constexpr"" , >>> ): >>> pid = tl . program_id ( axis = 0 ) >>> block_start = pid * BLOCK_SIZE >>> offsets = block_start + tl . arange ( 0 , BLOCK_SIZE ) >>> mask = offsets < n_elements >>> x = tl . load ( in_ptr0 + offsets , mask = mask ) >>> y = tl . load ( in_ptr1 + offsets , mask = mask ) >>> output = x + y >>> tl . store ( out_ptr + offsets , output , mask = mask ) >>> >>> def add ( x , y ): >>> output = torch . empty_like ( x ) >>> n_elements = output . numel () >>> >>> def grid_fn ( meta ): >>> return ( triton . cdiv ( n_elements , meta [ ""BLOCK_SIZE"" ]),) >>> >>> wrap_triton ( add_kernel )[ grid_fn ]( x , y , output , n_elements , 16 ) >>> return output >>> >>> x = torch . randn ( 3 , device = ""cuda"" ) >>> y = torch . randn ( 3 , device = ""cuda"" ) >>> gm = make_fx ( add )( x , y ) >>> print ( gm . code ) >>> # def forward(self, x_1, y_1): >>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False) >>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation( >>> #         kernel_idx = 0, constant_args_idx = 0, >>> #         grid = [(1, 1, 1)], kwargs = { >>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like, >>> #             'n_elements': 3, 'BLOCK_SIZE': 16 >>> #         }) >>> #     return empty_like Return type Any Extending custom ops (created from Python or C++) ¶ Use the register.* methods, such as torch.library.register_kernel() and torch.library.register_fake() , to add implementations
for any operators (they may have been created using torch.library.custom_op() or
via PyTorch’s C++ operator registration APIs). torch.library. register_kernel ( op , device_types , func = None , / , * , lib = None ) [source] [source] ¶ Register an implementation for a device type for this operator. Some valid device_types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.
This API may be used as a decorator. Parameters op ( str | OpOverload ) – The operator to register an impl to. device_types ( None | str | Sequence [ str ] ) – The device_types to register an impl to.
If None, we will register to all device types – please only use
this option if your implementation is truly device-type-agnostic. func ( Callable ) – The function to register as the implementation for
the given device types. lib ( Optional [ Library ] ) – If provided, the lifetime of this registration Examples:: >>> import torch >>> from torch import Tensor >>> from torch.library import custom_op >>> import numpy as np >>> >>> # Create a custom op that works on cpu >>> @custom_op ( ""mylib::numpy_sin"" , mutates_args = (), device_types = ""cpu"" ) >>> def numpy_sin ( x : Tensor ) -> Tensor : >>> x_np = x . numpy () >>> y_np = np . sin ( x_np ) >>> return torch . from_numpy ( y_np ) >>> >>> # Add implementations for the cuda device >>> @torch . library . register_kernel ( ""mylib::numpy_sin"" , ""cuda"" ) >>> def _ ( x ): >>> x_np = x . cpu () . numpy () >>> y_np = np . sin ( x_np ) >>> return torch . from_numpy ( y_np ) . to ( device = x . device ) >>> >>> x_cpu = torch . randn ( 3 ) >>> x_cuda = x_cpu . cuda () >>> assert torch . allclose ( numpy_sin ( x_cpu ), x_cpu . sin ()) >>> assert torch . allclose ( numpy_sin ( x_cuda ), x_cuda . sin ()) torch.library. register_autocast ( op , device_type , cast_inputs , / , * , lib = None ) [source] [source] ¶ Register an autocast dispatch rule for this custom op. Valid device_type include: “cpu” and “cuda”. Parameters op ( str | OpOverload ) – The operator to register an autocast dispatch rule to. device_type ( str ) – Device type to use. ‘cuda’ or ‘cpu’.
The type is the same as the type attribute of a torch.device .
Thus, you may obtain the device type of a tensor using Tensor.device.type . cast_inputs ( torch.dtype ) – When custom op runs in an autocast-enabled region,
casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors
are not affected), then executes custom op with autocast disabled. lib ( Optional [ Library ] ) – If provided, the lifetime of this registration Examples:: >>> import torch >>> from torch import Tensor >>> from torch.library import custom_op >>> >>> # Create a custom op that works on cuda >>> @torch . library . custom_op ( ""mylib::my_sin"" , mutates_args = ()) >>> def my_sin ( x : Tensor ) -> Tensor : >>> return torch . sin ( x ) >>> >>> # Register autocast dispatch rule for the cuda device >>> torch . library . register_autocast ( ""mylib::my_sin"" , ""cuda"" , torch . float16 ) >>> >>> x = torch . randn ( 3 , dtype = torch . float32 , device = ""cuda"" ) >>> with torch . autocast ( ""cuda"" , dtype = torch . float16 ): >>> y = torch . ops . mylib . my_sin ( x ) >>> assert y . dtype == torch . float16 torch.library. register_autograd ( op , backward , / , * , setup_context = None , lib = None ) [source] [source] ¶ Register a backward formula for this custom op. In order for an operator to work with autograd, you need to register
a backward formula:
1. You must tell us how to compute gradients during the backward pass
by providing us a “backward” function.
2. If you need any values from the forward to compute gradients, you can
use setup_context to save values for backward. backward runs during the backward pass. It accepts (ctx, *grads) :
- grads is one or more gradients. The number of gradients matches
the number of outputs of the operator.
The ctx object is the same ctx object used by torch.autograd.Function . The semantics of backward_fn are the
same as torch.autograd.Function.backward() . setup_context(ctx, inputs, output) runs during the forward pass.
Please save quantities needed for backward onto the ctx object via
either torch.autograd.function.FunctionCtx.save_for_backward() or assigning them as attributes of ctx . If your custom op has
kwarg-only arguments, we expect the signature of setup_context to be setup_context(ctx, inputs, keyword_only_inputs, output) . Both setup_context_fn and backward_fn must be traceable. That is,
they may not directly access torch.Tensor.data_ptr() and they must
not depend on or mutate global state. If you need a non-traceable backward,
you can make it a separate custom_op that you call inside backward_fn . If you need different autograd behavior on different devices, then we
recommend creating two different custom operators, one for each device
that needs different behavior, and switching between them at runtime. Examples >>> import torch >>> import numpy as np >>> from torch import Tensor >>> >>> @torch . library . custom_op ( ""mylib::numpy_sin"" , mutates_args = ()) >>> def numpy_sin ( x : Tensor ) -> Tensor : >>> x_np = x . cpu () . numpy () >>> y_np = np . sin ( x_np ) >>> return torch . from_numpy ( y_np ) . to ( device = x . device ) >>> >>> def setup_context ( ctx , inputs , output ) -> Tensor : >>> x , = inputs >>> ctx . save_for_backward ( x ) >>> >>> def backward ( ctx , grad ): >>> x , = ctx . saved_tensors >>> return grad * x . cos () >>> >>> torch . library . register_autograd ( ... ""mylib::numpy_sin"" , backward , setup_context = setup_context ... ) >>> >>> x = torch . randn ( 3 , requires_grad = True ) >>> y = numpy_sin ( x ) >>> ( grad_x ,) = torch . autograd . grad ( y , x , torch . ones_like ( y )) >>> assert torch . allclose ( grad_x , x . cos ()) >>> >>> # Example with a keyword-only arg >>> @torch . library . custom_op ( ""mylib::numpy_mul"" , mutates_args = ()) >>> def numpy_mul ( x : Tensor , * , val : float ) -> Tensor : >>> x_np = x . cpu () . numpy () >>> y_np = x_np * val >>> return torch . from_numpy ( y_np ) . to ( device = x . device ) >>> >>> def setup_context ( ctx , inputs , keyword_only_inputs , output ) -> Tensor : >>> ctx . val = keyword_only_inputs [ ""val"" ] >>> >>> def backward ( ctx , grad ): >>> return grad * ctx . val >>> >>> torch . library . register_autograd ( ... ""mylib::numpy_mul"" , backward , setup_context = setup_context ... ) >>> >>> x = torch . randn ( 3 , requires_grad = True ) >>> y = numpy_mul ( x , val = 3.14 ) >>> ( grad_x ,) = torch . autograd . grad ( y , x , torch . ones_like ( y )) >>> assert torch . allclose ( grad_x , torch . full_like ( x , 3.14 )) torch.library. register_fake ( op , func = None , / , * , lib = None , _stacklevel = 1 ) [source] [source] ¶ Register a FakeTensor implementation (“fake impl”) for this operator. Also sometimes known as a “meta kernel”, “abstract impl”. An “FakeTensor implementation” specifies the behavior of this operator on
Tensors that carry no data (“FakeTensor”). Given some input Tensors with
certain properties (sizes/strides/storage_offset/device), it specifies
what the properties of the output Tensors are. The FakeTensor implementation has the same signature as the operator.
It is run for both FakeTensors and meta tensors. To write a FakeTensor
implementation, assume that all Tensor inputs to the operator are
regular CPU/CUDA/Meta tensors, but they do not have storage, and
you are trying to return regular CPU/CUDA/Meta tensor(s) as output.
The FakeTensor implementation must consist of only PyTorch operations
(and may not directly access the storage or data of any input or
intermediate Tensors). This API may be used as a decorator (see examples). For a detailed guide on custom ops, please see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html Examples >>> import torch >>> import numpy as np >>> from torch import Tensor >>> >>> # Example 1: an operator without data-dependent output shape >>> @torch . library . custom_op ( ""mylib::custom_linear"" , mutates_args = ()) >>> def custom_linear ( x : Tensor , weight : Tensor , bias : Tensor ) -> Tensor : >>> raise NotImplementedError ( ""Implementation goes here"" ) >>> >>> @torch . library . register_fake ( ""mylib::custom_linear"" ) >>> def _ ( x , weight , bias ): >>> assert x . dim () == 2 >>> assert weight . dim () == 2 >>> assert bias . dim () == 1 >>> assert x . shape [ 1 ] == weight . shape [ 1 ] >>> assert weight . shape [ 0 ] == bias . shape [ 0 ] >>> assert x . device == weight . device >>> >>> return ( x @ weight . t ()) + bias >>> >>> with torch . _subclasses . fake_tensor . FakeTensorMode (): >>> x = torch . randn ( 2 , 3 ) >>> w = torch . randn ( 3 , 3 ) >>> b = torch . randn ( 3 ) >>> y = torch . ops . mylib . custom_linear ( x , w , b ) >>> >>> assert y . shape == ( 2 , 3 ) >>> >>> # Example 2: an operator with data-dependent output shape >>> @torch . library . custom_op ( ""mylib::custom_nonzero"" , mutates_args = ()) >>> def custom_nonzero ( x : Tensor ) -> Tensor : >>> x_np = x . numpy ( force = True ) >>> res = np . stack ( np . nonzero ( x_np ), axis = 1 ) >>> return torch . tensor ( res , device = x . device ) >>> >>> @torch . library . register_fake ( ""mylib::custom_nonzero"" ) >>> def _ ( x ): >>> # Number of nonzero-elements is data-dependent. >>> # Since we cannot peek at the data in an fake impl, >>> # we use the ctx object to construct a new symint that >>> # represents the data-dependent size. >>> ctx = torch . library . get_ctx () >>> nnz = ctx . new_dynamic_size () >>> shape = [ nnz , x . dim ()] >>> result = x . new_empty ( shape , dtype = torch . int64 ) >>> return result >>> >>> from torch.fx.experimental.proxy_tensor import make_fx >>> >>> x = torch . tensor ([ 0 , 1 , 2 , 3 , 4 , 0 ]) >>> trace = make_fx ( torch . ops . mylib . custom_nonzero , tracing_mode = ""symbolic"" )( x ) >>> trace . print_readable () >>> >>> assert torch . allclose ( trace ( x ), torch . ops . mylib . custom_nonzero ( x )) torch.library. register_vmap ( op , func = None , / , * , lib = None ) [source] [source] ¶ Register a vmap implementation to support torch.vmap() for this custom op. This API may be used as a decorator (see examples). In order for an operator to work with torch.vmap() , you may need to register a
vmap implementation in the following signature: vmap_func(info, in_dims: Tuple[Optional[int]], *args, **kwargs) , where *args and **kwargs are the arguments and kwargs for op .
We do not support kwarg-only Tensor args. It specifies how do we compute the batched version of op given inputs with an additional
dimension (specified by in_dims ). For each arg in args , in_dims has a corresponding Optional[int] . It is None if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over. info is a collection of additional metadata that may be helpful: info.batch_size specifies the size of the dimension being vmapped over, while info.randomness is the randomness option that was passed to torch.vmap() . The return of the function func is a tuple of (output, out_dims) . Similar to in_dims , out_dims should be of the same structure as output and contain one out_dim per output that specifies if the output has the vmapped dimension and what index it is in. Examples >>> import torch >>> import numpy as np >>> from torch import Tensor >>> from typing import Tuple >>> >>> def to_numpy ( tensor ): >>> return tensor . cpu () . numpy () >>> >>> lib = torch . library . Library ( ""mylib"" , ""FRAGMENT"" ) >>> @torch . library . custom_op ( ""mylib::numpy_cube"" , mutates_args = ()) >>> def numpy_cube ( x : Tensor ) -> Tuple [ Tensor , Tensor ]: >>> x_np = to_numpy ( x ) >>> dx = torch . tensor ( 3 * x_np ** 2 , device = x . device ) >>> return torch . tensor ( x_np ** 3 , device = x . device ), dx >>> >>> def numpy_cube_vmap ( info , in_dims , x ): >>> result = numpy_cube ( x ) >>> return result , ( in_dims [ 0 ], in_dims [ 0 ]) >>> >>> torch . library . register_vmap ( numpy_cube , numpy_cube_vmap ) >>> >>> x = torch . randn ( 3 ) >>> torch . vmap ( numpy_cube )( x ) >>> >>> @torch . library . custom_op ( ""mylib::numpy_mul"" , mutates_args = ()) >>> def numpy_mul ( x : Tensor , y : Tensor ) -> Tensor : >>> return torch . tensor ( to_numpy ( x ) * to_numpy ( y ), device = x . device ) >>> >>> @torch . library . register_vmap ( ""mylib::numpy_mul"" ) >>> def numpy_mul_vmap ( info , in_dims , x , y ): >>> x_bdim , y_bdim = in_dims >>> x = x . movedim ( x_bdim , - 1 ) if x_bdim is not None else x . unsqueeze ( - 1 ) >>> y = y . movedim ( y_bdim , - 1 ) if y_bdim is not None else y . unsqueeze ( - 1 ) >>> result = x * y >>> result = result . movedim ( - 1 , 0 ) >>> return result , 0 >>> >>> >>> x = torch . randn ( 3 ) >>> y = torch . randn ( 3 ) >>> torch . vmap ( numpy_mul )( x , y ) Note The vmap function should aim to preserve the semantics of the entire custom operator.
That is, grad(vmap(op)) should be replaceable with a grad(map(op)) . If your custom operator has any custom behavior in the backward pass, please
keep this in mind. torch.library. impl_abstract ( qualname , func = None , * , lib = None , _stacklevel = 1 ) [source] [source] ¶ This API was renamed to torch.library.register_fake() in PyTorch 2.4.
Please use that instead. torch.library. get_ctx ( ) [source] [source] ¶ get_ctx() returns the current AbstractImplCtx object. Calling get_ctx() is only valid inside of an fake impl
(see torch.library.register_fake() for more usage details. Return type FakeImplCtx torch.library. register_torch_dispatch ( op , torch_dispatch_class , func = None , / , * , lib = None ) [source] [source] ¶ Registers a torch_dispatch rule for the given operator and torch_dispatch_class . This allows for open registration to specify the behavior between the operator
and the torch_dispatch_class without needing to modify the torch_dispatch_class or the operator directly. The torch_dispatch_class is either a Tensor subclass with __torch_dispatch__ or a
TorchDispatchMode. If it is a Tensor subclass, we expect func to have the following signature: (cls, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any If it is a TorchDispatchMode, we expect func to have the following signature: (mode, func: OpOverload, types: Tuple[type, ...], args, kwargs) -> Any args and kwargs will have been normalized the same way they are
in __torch_dispatch__ (see __torch_dispatch__ calling convention ). Examples >>> import torch >>> >>> @torch . library . custom_op ( ""mylib::foo"" , mutates_args = {}) >>> def foo ( x : torch . Tensor ) -> torch . Tensor : >>> return x . clone () >>> >>> class MyMode ( torch . utils . _python_dispatch . TorchDispatchMode ): >>> def __torch_dispatch__ ( self , func , types , args = (), kwargs = None ): >>> return func ( * args , ** kwargs ) >>> >>> @torch . library . register_torch_dispatch ( ""mylib::foo"" , MyMode ) >>> def _ ( mode , func , types , args , kwargs ): >>> x , = args >>> return x + 1 >>> >>> x = torch . randn ( 3 ) >>> y = foo ( x ) >>> assert torch . allclose ( y , x ) >>> >>> with MyMode (): >>> y = foo ( x ) >>> assert torch . allclose ( y , x + 1 ) torch.library. infer_schema ( prototype_function , / , * , mutates_args , op_name = None ) [source] ¶ Parses the schema of a given function with type hints. The schema is inferred from the
function’s type hints, and can be used to define a new operator. We make the following assumptions: None of the outputs alias any of the inputs or each other. String type annotations “device, dtype, Tensor, types” without library specification are assumed to be torch.*. Similarly, string type annotations “Optional, List, Sequence, Union” without library specification are assumed to be typing.*. Only the args listed in mutates_args are being mutated. If mutates_args is “unknown”, it assumes that all inputs to the operator are being mutates. Callers (e.g. the custom ops API) are responsible for checking these assumptions. Parameters prototype_function ( Callable ) – The function from which to infer a schema for from its type annotations. op_name ( Optional [ str ] ) – The name of the operator in the schema. If name is None, then the
name is not included in the inferred schema. Note that the input schema to torch.library.Library.define requires a operator name. mutates_args ( ""unknown"" | Iterable [ str ] ) – The arguments that are mutated in the function. Returns The inferred schema. Return type str Example >>> def foo_impl ( x : torch . Tensor ) -> torch . Tensor : >>> return x . sin () >>> >>> infer_schema ( foo_impl , op_name = ""foo"" , mutates_args = {}) foo(Tensor x) -> Tensor >>> >>> infer_schema ( foo_impl , mutates_args = {}) (Tensor x) -> Tensor class torch._library.custom_ops. CustomOpDef ( namespace , name , schema , fn ) [source] [source] ¶ CustomOpDef is a wrapper around a function that turns it into a custom op. It has various methods for registering additional behavior for this
custom op. You should not instantiate CustomOpDef directly; instead, use the torch.library.custom_op() API. set_kernel_enabled ( device_type , enabled = True ) [source] [source] ¶ Disable or re-enable an already registered kernel for this custom operator. If the kernel is already disabled/enabled, this is a no-op. Note If a kernel is first disabled and then registered, it is disabled until enabled again. Parameters device_type ( str ) – The device type to disable/enable the kernel for. disable ( bool ) – Whether to disable or enable the kernel. Example >>> inp = torch . randn ( 1 ) >>> >>> # define custom op `f`. >>> @custom_op ( ""mylib::f"" , mutates_args = ()) >>> def f ( x : Tensor ) -> Tensor : >>> return torch . zeros ( 1 ) >>> >>> print ( f ( inp )) # tensor([0.]), default kernel >>> >>> @f . register_kernel ( ""cpu"" ) >>> def _ ( x ): >>> return torch . ones ( 1 ) >>> >>> print ( f ( inp )) # tensor([1.]), CPU kernel >>> >>> # temporarily disable the CPU kernel >>> with f . set_kernel_enabled ( ""cpu"" , enabled = False ): >>> print ( f ( inp )) # tensor([0.]) with CPU kernel disabled Low-level APIs ¶ The following APIs are direct bindings to PyTorch’s C++ low-level
operator registration APIs. Warning The low-level operator registration APIs and the PyTorch Dispatcher are a
complicated PyTorch concept. We recommend you use the higher level APIs above
(that do not require a torch.library.Library object) when possible.
This blog post < http://blog.ezyang.com/2020/09/lets-talk-about-the-pytorch-dispatcher/ >`_
is a good starting point to learn about the PyTorch Dispatcher. A tutorial that walks you through some examples on how to use this API is available on Google Colab . class torch.library. Library ( ns , kind , dispatch_key = '' ) [source] [source] ¶ A class to create libraries that can be used to register new operators or
override operators in existing libraries from Python.
A user can optionally pass in a dispatch keyname if they only want to register
kernels corresponding to only one specific dispatch key. To create a library to override operators in an existing library (with name ns), set the kind to “IMPL”.
To create a new library (with name ns) to register new operators, set the kind to “DEF”.
To create a fragment of a possibly existing library to register operators (and bypass
the limitation that there is only one library for a given namespace), set the kind to
“FRAGMENT”. Parameters ns – library name kind – “DEF”, “IMPL” (default: “IMPL”), “FRAGMENT” dispatch_key – PyTorch dispatch key (default: “”) define ( schema , alias_analysis = '' , * , tags = () ) [source] [source] ¶ Defines a new operator and its semantics in the ns namespace. Parameters schema – function schema to define a new operator. alias_analysis ( optional ) – Indicates if the aliasing properties of the operator arguments can be
inferred from the schema (default behavior) or not (“CONSERVATIVE”). tags ( Tag | Sequence [ Tag ] ) – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it. Returns name of the operator as inferred from the schema. Example:: >>> my_lib = Library ( ""mylib"" , ""DEF"" ) >>> my_lib . define ( ""sum(Tensor self) -> Tensor"" ) fallback ( fn , dispatch_key = '' , * , with_keyset = False ) [source] [source] ¶ Registers the function implementation as the fallback for the given key. This function only works for a library with global namespace (“_”). Parameters fn – function used as fallback for the given dispatch key or fallthrough_kernel() to register a fallthrough. dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with. with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to fn when calling. This should be used to create the appropriate keyset for redispatch calls. Example:: >>> my_lib = Library ( ""_"" , ""IMPL"" ) >>> def fallback_kernel ( op , * args , ** kwargs ): >>> # Handle all autocast ops generically >>> # ... >>> my_lib . fallback ( fallback_kernel , ""Autocast"" ) impl ( op_name , fn , dispatch_key = '' , * , with_keyset = False ) [source] [source] ¶ Registers the function implementation for an operator defined in the library. Parameters op_name – operator name (along with the overload) or OpOverload object. fn – function that’s the operator implementation for the input dispatch key or fallthrough_kernel() to register a fallthrough. dispatch_key – dispatch key that the input function should be registered for. By default, it uses
the dispatch key that the library was created with. with_keyset – flag controlling if the current dispatcher call keyset should be passed as the first argument
to fn when calling. This should be used to create the appropriate keyset for redispatch calls. Example:: >>> my_lib = Library ( ""aten"" , ""IMPL"" ) >>> def div_cpu ( self , other ): >>> return self * ( 1 / other ) >>> my_lib . impl ( ""div.Tensor"" , div_cpu , ""CPU"" ) torch.library. fallthrough_kernel ( ) [source] [source] ¶ A dummy function to pass to Library.impl in order to register a fallthrough. torch.library. define ( qualname , schema , * , lib = None , tags = () ) [source] [source] ¶ torch.library. define ( lib , schema , alias_analysis = '' ) Defines a new operator. In PyTorch, defining an op (short for “operator”) is a two step-process:
- we need to define the op (by providing an operator name and schema)
- we need to implement behavior for how the operator interacts with
various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc. This entrypoint defines the custom operator (the first step)
you must then perform the second step by calling various impl_* APIs, like torch.library.impl() or torch.library.register_fake() . Parameters qualname ( str ) – The qualified name for the operator. Should be
a string that looks like “namespace::name”, e.g. “aten::sin”.
Operators in PyTorch need a namespace to
avoid name collisions; a given operator may only be created once.
If you are writing a Python library, we recommend the namespace to
be the name of your top-level module. schema ( str ) – The schema of the operator. E.g. “(Tensor x) -> Tensor”
for an op that accepts one Tensor and returns one Tensor. It does
not contain the operator name (that is passed in qualname ). lib ( Optional [ Library ] ) – If provided, the lifetime of this operator
will be tied to the lifetime of the Library object. tags ( Tag | Sequence [ Tag ] ) – one or more torch.Tag to apply to this
operator. Tagging an operator changes the operator’s behavior
under various PyTorch subsystems; please read the docs for the
torch.Tag carefully before applying it. Example:: >>> import torch >>> import numpy as np >>> >>> # Define the operator >>> torch . library . define ( ""mylib::sin"" , ""(Tensor x) -> Tensor"" ) >>> >>> # Add implementations for the operator >>> @torch . library . impl ( ""mylib::sin"" , ""cpu"" ) >>> def f ( x ): >>> return torch . from_numpy ( np . sin ( x . numpy ())) >>> >>> # Call the new operator from torch.ops. >>> x = torch . randn ( 3 ) >>> y = torch . ops . mylib . sin ( x ) >>> assert torch . allclose ( y , x . sin ()) torch.library. impl ( lib , name , dispatch_key = '' ) [source] [source] ¶ torch.library. impl ( qualname : str , types : Union [ str , Sequence [ str ] ] , func : Literal [ None ] = None , * , lib : Optional [ Library ] = None ) → Callable [ [ Callable [ ... , object ] ] , None ] torch.library. impl ( qualname : str , types : Union [ str , Sequence [ str ] ] , func : Callable [ ... , object ] , * , lib : Optional [ Library ] = None ) → None torch.library. impl ( lib : Library , name : str , dispatch_key : str = '' ) → Callable [ [ Callable [ _P , _T ] ] , Callable [ _P , _T ] ] Register an implementation for a device type for this operator. You may pass “default” for types to register this implementation as the
default implementation for ALL device types.
Please only use this if the implementation truly supports all device types;
for example, this is true if it is a composition of built-in PyTorch operators. This API may be used as a decorator. You can use nested decorators
with this API provided they return a function and are placed inside
this API (see Example 2). Some valid types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”. Parameters qualname ( str ) – Should be a string that looks like “namespace::operator_name”. types ( str | Sequence [ str ] ) – The device types to register an impl to. lib ( Optional [ Library ] ) – If provided, the lifetime of this registration
will be tied to the lifetime of the Library object. Examples >>> import torch >>> import numpy as np >>> # Example 1: Register function. >>> # Define the operator >>> torch . library . define ( ""mylib::mysin"" , ""(Tensor x) -> Tensor"" ) >>> >>> # Add implementations for the cpu device >>> @torch . library . impl ( ""mylib::mysin"" , ""cpu"" ) >>> def f ( x ): >>> return torch . from_numpy ( np . sin ( x . numpy ())) >>> >>> x = torch . randn ( 3 ) >>> y = torch . ops . mylib . mysin ( x ) >>> assert torch . allclose ( y , x . sin ()) >>> >>> # Example 2: Register function with decorator. >>> def custom_decorator ( func ): >>> def wrapper ( * args , ** kwargs ): >>> return func ( * args , ** kwargs ) + 1 >>> return wrapper >>> >>> # Define the operator >>> torch . library . define ( ""mylib::sin_plus_one"" , ""(Tensor x) -> Tensor"" ) >>> >>> # Add implementations for the operator >>> @torch . library . impl ( ""mylib::sin_plus_one"" , ""cpu"" ) >>> @custom_decorator >>> def f ( x ): >>> return torch . from_numpy ( np . sin ( x . numpy ())) >>> >>> # Call the new operator from torch.ops. >>> x = torch . randn ( 3 ) >>> >>> y1 = torch . ops . mylib . sin_plus_one ( x ) >>> y2 = torch . sin ( x ) + 1 >>> assert torch . allclose ( y1 , y2 )",42268,17,8,">>> @torch.library.custom_op(""mylib::numpy_mul"", mutates_args=())
>>> def numpy_mul(x: Tensor, y: float) -> Tensor:
>>>     x_np = x.numpy(force=True)
>>>     z_np = x_np * y
>>>     return torch.from_numpy(z_np).to(x.device)
>>>
>>> @numpy_mul.register_fake
>>> def _(x, y):
>>>     return torch.empty_like(x)
>>>
>>> def setup_context(ctx, inputs, output):
>>>     y, = inputs
>>>     ctx.y = y
>>>
>>> def backward(ctx, grad):
>>>     return grad * ctx.y, None
>>>
>>> numpy_mul.register_autograd(backward, setup_context=setup_context)
>>>
>>> sample_inputs = [
>>>     (torch.randn(3), 3.14),
>>>     (torch.randn(2, 3, device='cuda'), 2.718),
>>>     (torch.randn(1, 10, requires_grad=True), 1.234),
>>>     (torch.randn(64, 64, device='cuda', requires_grad=True), 90.18),
>>> ]
>>>
>>> for args in sample_inputs:
>>>     torch.library.opcheck(numpy_mul, args)
---
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>> import numpy as np
>>>
>>> @custom_op(""mylib::numpy_sin"", mutates_args=())
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>     x_np = x.cpu().numpy()
>>>     y_np = np.sin(x_np)
>>>     return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> x = torch.randn(3)
>>> y = numpy_sin(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example of a custom op that only works for one device type.
>>> @custom_op(""mylib::numpy_sin_cpu"", mutates_args=(), device_types=""cpu"")
>>> def numpy_sin_cpu(x: Tensor) -> Tensor:
>>>     x_np = x.numpy()
>>>     y_np = np.sin(x_np)
>>>     return torch.from_numpy(y_np)
>>>
>>> x = torch.randn(3)
>>> y = numpy_sin_cpu(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example of a custom op that mutates an input
>>> @custom_op(""mylib::numpy_sin_inplace"", mutates_args={""x""}, device_types=""cpu"")
>>> def numpy_sin_inplace(x: Tensor) -> None:
>>>     x_np = x.numpy()
>>>     np.sin(x_np, out=x_np)
>>>
>>> x = torch.randn(3)
>>> expected = x.sin()
>>> numpy_sin_inplace(x)
>>> assert torch.allclose(x, expected)
>>>
>>> # Example of a factory function
>>> @torch.library.custom_op(""mylib::bar"", mutates_args={}, device_types=""cpu"")
>>> def bar(device: torch.device) -> Tensor:
>>>     return torch.ones(3)
>>>
>>> bar(""cpu"")
---
>>> import torch
>>> from torch.library import triton_op, wrap_triton
>>>
>>> import triton
>>> from triton import language as tl
>>>
>>> @triton.jit
>>> def add_kernel(
>>>     in_ptr0,
>>>     in_ptr1,
>>>     out_ptr,
>>>     n_elements,
>>>     BLOCK_SIZE: ""tl.constexpr"",
>>> ):
>>>     pid = tl.program_id(axis=0)
>>>     block_start = pid * BLOCK_SIZE
>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)
>>>     mask = offsets < n_elements
>>>     x = tl.load(in_ptr0 + offsets, mask=mask)
>>>     y = tl.load(in_ptr1 + offsets, mask=mask)
>>>     output = x + y
>>>     tl.store(out_ptr + offsets, output, mask=mask)
>>>
>>> @triton_op(""mylib::add"", mutates_args={})
>>> def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
>>>     output = torch.empty_like(x)
>>>     n_elements = output.numel()
>>>
>>>     def grid(meta):
>>>         return (triton.cdiv(n_elements, meta[""BLOCK_SIZE""]),)
>>>
>>>     # NB: we need to wrap the triton kernel in a call to wrap_triton
>>>     wrap_triton(add_kernel)[grid](x, y, output, n_elements, 16)
>>>     return output
>>>
>>> @torch.compile
>>> def f(x, y):
>>>     return add(x, y)
>>>
>>> x = torch.randn(3, device=""cuda"")
>>> y = torch.randn(3, device=""cuda"")
>>>
>>> z = f(x, y)
>>> assert torch.allclose(z, x + y)
---
>>> import torch
>>> import triton
>>> from triton import language as tl
>>> from torch.fx.experimental.proxy_tensor import make_fx
>>> from torch.library import wrap_triton
>>>
>>> @triton.jit
>>> def add_kernel(
>>>     in_ptr0,
>>>     in_ptr1,
>>>     out_ptr,
>>>     n_elements,
>>>     BLOCK_SIZE: ""tl.constexpr"",
>>> ):
>>>     pid = tl.program_id(axis=0)
>>>     block_start = pid * BLOCK_SIZE
>>>     offsets = block_start + tl.arange(0, BLOCK_SIZE)
>>>     mask = offsets < n_elements
>>>     x = tl.load(in_ptr0 + offsets, mask=mask)
>>>     y = tl.load(in_ptr1 + offsets, mask=mask)
>>>     output = x + y
>>>     tl.store(out_ptr + offsets, output, mask=mask)
>>>
>>> def add(x, y):
>>>     output = torch.empty_like(x)
>>>     n_elements = output.numel()
>>>
>>>     def grid_fn(meta):
>>>         return (triton.cdiv(n_elements, meta[""BLOCK_SIZE""]),)
>>>
>>>     wrap_triton(add_kernel)[grid_fn](x, y, output, n_elements, 16)
>>>     return output
>>>
>>> x = torch.randn(3, device=""cuda"")
>>> y = torch.randn(3, device=""cuda"")
>>> gm = make_fx(add)(x, y)
>>> print(gm.code)
>>> # def forward(self, x_1, y_1):
>>> #     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)
>>> #     triton_kernel_wrapper_mutation_proxy = triton_kernel_wrapper_mutation(
>>> #         kernel_idx = 0, constant_args_idx = 0,
>>> #         grid = [(1, 1, 1)], kwargs = {
>>> #             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,
>>> #             'n_elements': 3, 'BLOCK_SIZE': 16
>>> #         })
>>> #     return empty_like
---
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>> import numpy as np
>>>
>>> # Create a custom op that works on cpu
>>> @custom_op(""mylib::numpy_sin"", mutates_args=(), device_types=""cpu"")
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>     x_np = x.numpy()
>>>     y_np = np.sin(x_np)
>>>     return torch.from_numpy(y_np)
>>>
>>> # Add implementations for the cuda device
>>> @torch.library.register_kernel(""mylib::numpy_sin"", ""cuda"")
>>> def _(x):
>>>     x_np = x.cpu().numpy()
>>>     y_np = np.sin(x_np)
>>>     return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> x_cpu = torch.randn(3)
>>> x_cuda = x_cpu.cuda()
>>> assert torch.allclose(numpy_sin(x_cpu), x_cpu.sin())
>>> assert torch.allclose(numpy_sin(x_cuda), x_cuda.sin())
---
>>> import torch
>>> from torch import Tensor
>>> from torch.library import custom_op
>>>
>>> # Create a custom op that works on cuda
>>> @torch.library.custom_op(""mylib::my_sin"", mutates_args=())
>>> def my_sin(x: Tensor) -> Tensor:
>>>     return torch.sin(x)
>>>
>>> # Register autocast dispatch rule for the cuda device
>>> torch.library.register_autocast(""mylib::my_sin"", ""cuda"", torch.float16)
>>>
>>> x = torch.randn(3, dtype=torch.float32, device=""cuda"")
>>> with torch.autocast(""cuda"", dtype=torch.float16):
>>>     y = torch.ops.mylib.my_sin(x)
>>> assert y.dtype == torch.float16
---
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>>
>>> @torch.library.custom_op(""mylib::numpy_sin"", mutates_args=())
>>> def numpy_sin(x: Tensor) -> Tensor:
>>>     x_np = x.cpu().numpy()
>>>     y_np = np.sin(x_np)
>>>     return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> def setup_context(ctx, inputs, output) -> Tensor:
>>>     x, = inputs
>>>     ctx.save_for_backward(x)
>>>
>>> def backward(ctx, grad):
>>>     x, = ctx.saved_tensors
>>>     return grad * x.cos()
>>>
>>> torch.library.register_autograd(
...     ""mylib::numpy_sin"", backward, setup_context=setup_context
... )
>>>
>>> x = torch.randn(3, requires_grad=True)
>>> y = numpy_sin(x)
>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))
>>> assert torch.allclose(grad_x, x.cos())
>>>
>>> # Example with a keyword-only arg
>>> @torch.library.custom_op(""mylib::numpy_mul"", mutates_args=())
>>> def numpy_mul(x: Tensor, *, val: float) -> Tensor:
>>>     x_np = x.cpu().numpy()
>>>     y_np = x_np * val
>>>     return torch.from_numpy(y_np).to(device=x.device)
>>>
>>> def setup_context(ctx, inputs, keyword_only_inputs, output) -> Tensor:
>>>     ctx.val = keyword_only_inputs[""val""]
>>>
>>> def backward(ctx, grad):
>>>     return grad * ctx.val
>>>
>>> torch.library.register_autograd(
...     ""mylib::numpy_mul"", backward, setup_context=setup_context
... )
>>>
>>> x = torch.randn(3, requires_grad=True)
>>> y = numpy_mul(x, val=3.14)
>>> (grad_x,) = torch.autograd.grad(y, x, torch.ones_like(y))
>>> assert torch.allclose(grad_x, torch.full_like(x, 3.14))
---
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>>
>>> # Example 1: an operator without data-dependent output shape
>>> @torch.library.custom_op(""mylib::custom_linear"", mutates_args=())
>>> def custom_linear(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor:
>>>     raise NotImplementedError(""Implementation goes here"")
>>>
>>> @torch.library.register_fake(""mylib::custom_linear"")
>>> def _(x, weight, bias):
>>>     assert x.dim() == 2
>>>     assert weight.dim() == 2
>>>     assert bias.dim() == 1
>>>     assert x.shape[1] == weight.shape[1]
>>>     assert weight.shape[0] == bias.shape[0]
>>>     assert x.device == weight.device
>>>
>>>     return (x @ weight.t()) + bias
>>>
>>> with torch._subclasses.fake_tensor.FakeTensorMode():
>>>     x = torch.randn(2, 3)
>>>     w = torch.randn(3, 3)
>>>     b = torch.randn(3)
>>>     y = torch.ops.mylib.custom_linear(x, w, b)
>>>
>>> assert y.shape == (2, 3)
>>>
>>> # Example 2: an operator with data-dependent output shape
>>> @torch.library.custom_op(""mylib::custom_nonzero"", mutates_args=())
>>> def custom_nonzero(x: Tensor) -> Tensor:
>>>     x_np = x.numpy(force=True)
>>>     res = np.stack(np.nonzero(x_np), axis=1)
>>>     return torch.tensor(res, device=x.device)
>>>
>>> @torch.library.register_fake(""mylib::custom_nonzero"")
>>> def _(x):
>>> # Number of nonzero-elements is data-dependent.
>>> # Since we cannot peek at the data in an fake impl,
>>> # we use the ctx object to construct a new symint that
>>> # represents the data-dependent size.
>>>     ctx = torch.library.get_ctx()
>>>     nnz = ctx.new_dynamic_size()
>>>     shape = [nnz, x.dim()]
>>>     result = x.new_empty(shape, dtype=torch.int64)
>>>     return result
>>>
>>> from torch.fx.experimental.proxy_tensor import make_fx
>>>
>>> x = torch.tensor([0, 1, 2, 3, 4, 0])
>>> trace = make_fx(torch.ops.mylib.custom_nonzero, tracing_mode=""symbolic"")(x)
>>> trace.print_readable()
>>>
>>> assert torch.allclose(trace(x), torch.ops.mylib.custom_nonzero(x))
---
>>> import torch
>>> import numpy as np
>>> from torch import Tensor
>>> from typing import Tuple
>>>
>>> def to_numpy(tensor):
>>>     return tensor.cpu().numpy()
>>>
>>> lib = torch.library.Library(""mylib"", ""FRAGMENT"")
>>> @torch.library.custom_op(""mylib::numpy_cube"", mutates_args=())
>>> def numpy_cube(x: Tensor) -> Tuple[Tensor, Tensor]:
>>>     x_np = to_numpy(x)
>>>     dx = torch.tensor(3 * x_np ** 2, device=x.device)
>>>     return torch.tensor(x_np ** 3, device=x.device), dx
>>>
>>> def numpy_cube_vmap(info, in_dims, x):
>>>     result = numpy_cube(x)
>>>     return result, (in_dims[0], in_dims[0])
>>>
>>> torch.library.register_vmap(numpy_cube, numpy_cube_vmap)
>>>
>>> x = torch.randn(3)
>>> torch.vmap(numpy_cube)(x)
>>>
>>> @torch.library.custom_op(""mylib::numpy_mul"", mutates_args=())
>>> def numpy_mul(x: Tensor, y: Tensor) -> Tensor:
>>>     return torch.tensor(to_numpy(x) * to_numpy(y), device=x.device)
>>>
>>> @torch.library.register_vmap(""mylib::numpy_mul"")
>>> def numpy_mul_vmap(info, in_dims, x, y):
>>>     x_bdim, y_bdim = in_dims
>>>     x = x.movedim(x_bdim, -1) if x_bdim is not None else x.unsqueeze(-1)
>>>     y = y.movedim(y_bdim, -1) if y_bdim is not None else y.unsqueeze(-1)
>>>     result = x * y
>>>     result = result.movedim(-1, 0)
>>>     return result, 0
>>>
>>>
>>> x = torch.randn(3)
>>> y = torch.randn(3)
>>> torch.vmap(numpy_mul)(x, y)
---
>>> import torch
>>>
>>> @torch.library.custom_op(""mylib::foo"", mutates_args={})
>>> def foo(x: torch.Tensor) -> torch.Tensor:
>>>     return x.clone()
>>>
>>> class MyMode(torch.utils._python_dispatch.TorchDispatchMode):
>>>     def __torch_dispatch__(self, func, types, args=(), kwargs=None):
>>>         return func(*args, **kwargs)
>>>
>>> @torch.library.register_torch_dispatch(""mylib::foo"", MyMode)
>>> def _(mode, func, types, args, kwargs):
>>>     x, = args
>>>     return x + 1
>>>
>>> x = torch.randn(3)
>>> y = foo(x)
>>> assert torch.allclose(y, x)
>>>
>>> with MyMode():
>>>     y = foo(x)
>>> assert torch.allclose(y, x + 1)
---
>>> def foo_impl(x: torch.Tensor) -> torch.Tensor:
>>>     return x.sin()
>>>
>>> infer_schema(foo_impl, op_name=""foo"", mutates_args={})
foo(Tensor x) -> Tensor
>>>
>>> infer_schema(foo_impl, mutates_args={})
(Tensor x) -> Tensor
---
>>> inp = torch.randn(1)
>>>
>>> # define custom op `f`.
>>> @custom_op(""mylib::f"", mutates_args=())
>>> def f(x: Tensor) -> Tensor:
>>>     return torch.zeros(1)
>>>
>>> print(f(inp))  # tensor([0.]), default kernel
>>>
>>> @f.register_kernel(""cpu"")
>>> def _(x):
>>>     return torch.ones(1)
>>>
>>> print(f(inp))  # tensor([1.]), CPU kernel
>>>
>>> # temporarily disable the CPU kernel
>>> with f.set_kernel_enabled(""cpu"", enabled = False):
>>>     print(f(inp))  # tensor([0.]) with CPU kernel disabled
---
>>> my_lib = Library(""mylib"", ""DEF"")
>>> my_lib.define(""sum(Tensor self) -> Tensor"")
---
>>> my_lib = Library(""_"", ""IMPL"")
>>> def fallback_kernel(op, *args, **kwargs):
>>>     # Handle all autocast ops generically
>>>     # ...
>>> my_lib.fallback(fallback_kernel, ""Autocast"")
---
>>> my_lib = Library(""aten"", ""IMPL"")
>>> def div_cpu(self, other):
>>>     return self * (1 / other)
>>> my_lib.impl(""div.Tensor"", div_cpu, ""CPU"")
---
>>> import torch
>>> import numpy as np
>>>
>>> # Define the operator
>>> torch.library.define(""mylib::sin"", ""(Tensor x) -> Tensor"")
>>>
>>> # Add implementations for the operator
>>> @torch.library.impl(""mylib::sin"", ""cpu"")
>>> def f(x):
>>>     return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> # Call the new operator from torch.ops.
>>> x = torch.randn(3)
>>> y = torch.ops.mylib.sin(x)
>>> assert torch.allclose(y, x.sin())
---
>>> import torch
>>> import numpy as np
>>> # Example 1: Register function.
>>> # Define the operator
>>> torch.library.define(""mylib::mysin"", ""(Tensor x) -> Tensor"")
>>>
>>> # Add implementations for the cpu device
>>> @torch.library.impl(""mylib::mysin"", ""cpu"")
>>> def f(x):
>>>     return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> x = torch.randn(3)
>>> y = torch.ops.mylib.mysin(x)
>>> assert torch.allclose(y, x.sin())
>>>
>>> # Example 2: Register function with decorator.
>>> def custom_decorator(func):
>>>     def wrapper(*args, **kwargs):
>>>         return func(*args, **kwargs) + 1
>>>     return wrapper
>>>
>>> # Define the operator
>>> torch.library.define(""mylib::sin_plus_one"", ""(Tensor x) -> Tensor"")
>>>
>>> # Add implementations for the operator
>>> @torch.library.impl(""mylib::sin_plus_one"", ""cpu"")
>>> @custom_decorator
>>> def f(x):
>>>     return torch.from_numpy(np.sin(x.numpy()))
>>>
>>> # Call the new operator from torch.ops.
>>> x = torch.randn(3)
>>>
>>> y1 = torch.ops.mylib.sin_plus_one(x)
>>> y2 = torch.sin(x) + 1
>>> assert torch.allclose(y1, y2)",1752175579.9579487
https://pytorch.org/docs/stable/meta.html,Meta device — PyTorch 2.7 documentation,"Meta device ¶ The “meta” device is an abstract device which denotes a tensor which records
only metadata, but no actual data.  Meta tensors have two primary use cases: Models can be loaded on the meta device, allowing you to load a
representation of the model without actually loading the actual parameters
into memory.  This can be helpful if you need to make transformations on
the model before you load the actual data. Most operations can be performed on meta tensors, producing new meta
tensors that describe what the result would have been if you performed
the operation on a real tensor.  You can use this to perform abstract
analysis without needing to spend time on compute or space to represent
the actual tensors.  Because meta tensors do not have real data, you cannot
perform data-dependent operations like torch.nonzero() or item() .  In some cases, not all device types (e.g., CPU
and CUDA) have exactly the same output metadata for an operation; we
typically prefer representing the CUDA behavior faithfully in this
situation. Warning Although in principle meta tensor computation should always be faster than
an equivalent CPU/CUDA computation, many meta tensor implementations are
implemented in Python and have not been ported to C++ for speed, so you
may find that you get lower absolute framework latency with small CPU tensors. Idioms for working with meta tensors ¶ An object can be loaded with torch.load() onto meta device by specifying map_location='meta' : >>> torch . save ( torch . randn ( 2 ), 'foo.pt' ) >>> torch . load ( 'foo.pt' , map_location = 'meta' ) tensor(..., device='meta', size=(2,)) If you have some arbitrary code which performs some tensor construction without
explicitly specifying a device, you can override it to instead construct on meta device by using
the torch.device() context manager: >>> with torch . device ( 'meta' ): ... print ( torch . randn ( 30 , 30 )) ... tensor(..., device='meta', size=(30, 30)) This is especially helpful NN module construction, where you often are not
able to explicitly pass in a device for initialization: >>> from torch.nn.modules import Linear >>> with torch . device ( 'meta' ): ... print ( Linear ( 20 , 30 )) ... Linear(in_features=20, out_features=30, bias=True) You cannot convert a meta tensor directly to a CPU/CUDA tensor, because the
meta tensor stores no data and we do not know what the correct data values for
your new tensor are: >>> torch . ones ( 5 , device = 'meta' ) . to ( ""cpu"" ) Traceback (most recent call last): File ""<stdin>"" , line 1 , in <module> NotImplementedError : Cannot copy out of meta tensor; no data! Use a factory function like torch.empty_like() to explicitly specify how
you would like the missing data to be filled in. NN modules have a convenience method torch.nn.Module.to_empty() that
allow you to the module to another device, leaving all parameters
uninitialized.  You are expected to explicitly reinitialize the parameters
manually: >>> from torch.nn.modules import Linear >>> with torch . device ( 'meta' ): ... m = Linear ( 20 , 30 ) >>> m . to_empty ( device = ""cpu"" ) Linear(in_features=20, out_features=30, bias=True) torch._subclasses.meta_utils contains undocumented utilities for taking
an arbitrary Tensor and constructing an equivalent meta Tensor with high
fidelity.  These APIs are experimental and may be changed in a BC breaking way
at any time.",3391,5,5,">>> torch.save(torch.randn(2), 'foo.pt')
>>> torch.load('foo.pt', map_location='meta')
tensor(..., device='meta', size=(2,))
---
>>> with torch.device('meta'):
...     print(torch.randn(30, 30))
...
tensor(..., device='meta', size=(30, 30))
---
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...     print(Linear(20, 30))
...
Linear(in_features=20, out_features=30, bias=True)
---
>>> torch.ones(5, device='meta').to(""cpu"")
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NotImplementedError: Cannot copy out of meta tensor; no data!
---
>>> from torch.nn.modules import Linear
>>> with torch.device('meta'):
...     m = Linear(20, 30)
>>> m.to_empty(device=""cpu"")
Linear(in_features=20, out_features=30, bias=True)",1752175581.0115218
https://pytorch.org/docs/stable/notes/mps.html,MPS backend — PyTorch 2.7 documentation,"MPS backend ¶ mps device enables high-performance
training on GPU for MacOS devices with Metal programming framework.  It
introduces a new device to map Machine Learning computational graphs and
primitives on highly efficient Metal Performance Shaders Graph framework and
tuned kernels provided by Metal Performance Shaders framework respectively. The new MPS backend extends the PyTorch ecosystem and provides existing scripts
capabilities to setup and run operations on GPU. To get started, simply move your Tensor and Module to the mps device: # Check that MPS is available if not torch . backends . mps . is_available (): if not torch . backends . mps . is_built (): print ( ""MPS not available because the current PyTorch install was not "" ""built with MPS enabled."" ) else : print ( ""MPS not available because the current MacOS version is not 12.3+ "" ""and/or you do not have an MPS-enabled device on this machine."" ) else : mps_device = torch . device ( ""mps"" ) # Create a Tensor directly on the mps device x = torch . ones ( 5 , device = mps_device ) # Or x = torch . ones ( 5 , device = ""mps"" ) # Any operation happens on the GPU y = x * 2 # Move your model to mps just like any other device model = YourFavoriteNet () model . to ( mps_device ) # Now every call runs on the GPU pred = model ( x )",1302,1,4,"# Check that MPS is available
if not torch.backends.mps.is_available():
    if not torch.backends.mps.is_built():
        print(""MPS not available because the current PyTorch install was not ""
              ""built with MPS enabled."")
    else:
        print(""MPS not available because the current MacOS version is not 12.3+ ""
              ""and/or you do not have an MPS-enabled device on this machine."")

else:
    mps_device = torch.device(""mps"")

    # Create a Tensor directly on the mps device
    x = torch.ones(5, device=mps_device)
    # Or
    x = torch.ones(5, device=""mps"")

    # Any operation happens on the GPU
    y = x * 2

    # Move your model to mps just like any other device
    model = YourFavoriteNet()
    model.to(mps_device)

    # Now every call runs on the GPU
    pred = model(x)",1752175582.0742545
https://pytorch.org/docs/stable/deploy.html,torch::deploy has been moved to pytorch/multipy — PyTorch 2.7 documentation,torch::deploy has been moved to pytorch/multipy ¶ torch::deploy has been moved to its new home at https://github.com/pytorch/multipy .,134,0,4,,1752175583.151761
https://pytorch.org/docs/stable/torch_cuda_memory.html,Understanding CUDA Memory Usage — PyTorch 2.7 documentation,"Understanding CUDA Memory Usage ¶ To debug CUDA memory use, PyTorch provides a way to generate memory snapshots that record the state of allocated CUDA memory
at any point in time, and optionally record the history of allocation events that led up to that snapshot. The generated snapshots can then be drag and dropped onto the interactiver viewer hosted at pytorch.org/memory_viz which
can be used to explore the snapshot. Generating a Snapshot ¶ The common pattern for recording a snapshot is to enable memory history, run the code to be observed, and then save a file with a pickled snapshot: # enable memory history, which will # add tracebacks and event history to snapshots torch . cuda . memory . _record_memory_history () run_your_code () torch . cuda . memory . _dump_snapshot ( ""my_snapshot.pickle"" ) Using the visualizer ¶ Open pytorch.org/memory_viz and drag/drop the pickled snapshot file into the visualizer.
The visualizer is a javascript application that runs locally on your computer. It does not upload any snapshot data. Active Memory Timeline ¶ The Active Memory Timeline shows all the live tensors over time in the snapshot on a particular GPU. Pan/Zoom over the plot to look at smaller allocations.
Mouse over allocated blocks to see a stack trace for when that block was allocated, and details like its address. The detail slider can be adjusted to
render fewer allocations and improve performance when there is a lot of data. Allocator State History ¶ The Allocator State History shows individual allocator events in a timeline on the left. Select an event in the timeline to see a visual summary of the
allocator state at that event. This summary shows each individual segment returned from cudaMalloc and how it is split up into blocks of individual allocations
or free space. Mouse over segments and blocks to see the stack trace when the memory was allocated. Mouse over events to see the stack trace when the event occurred,
such as when a tensor was freed. Out of memory errors are reported as OOM events. Looking at the state of memory during an OOM may provide insight into why
an allocation failed even though reserved memory still exists. The stack trace information also reports the address at which an allocation occurred.
The address b7f064c000000_0 refers to the (b)lock at address 7f064c000000 which is the “_0”th time this address was allocated.
This unique string can be looked up in the Active Memory Timeline and searched
in the Active State History to examine the memory state when a tensor was allocated or freed. Snapshot API Reference ¶ torch.cuda.memory. _record_memory_history ( enabled = 'all' , context = 'all' , stacks = 'all' , max_entries = 9223372036854775807 , device = None ) [source] [source] ¶ Enable recording of stack traces associated with memory
allocations, so you can tell what allocated any piece of memory in torch.cuda.memory._snapshot() . In addition too keeping stack traces with each current allocation and free,
this will also enable recording of a history of all alloc/free events. Use torch.cuda.memory._snapshot() to retrieve this information,
and the tools in _memory_viz.py to visualize snapshots. The Python trace collection is fast (2us per trace), so you may consider
enabling this on production jobs if you anticipate ever having to debug
memory issues. C++ trace collection is also fast (~50ns/frame), which for many typical programs
works out to ~2us per trace, but can vary depending on stack depth. Parameters enabled ( Literal [ None , ""state"" , ""all"" ] , optional ) – None , disable recording memory history. “state” , keep information for currenly allocated memory. “all” , additionally keep a history of all alloc/free calls.
Defaults to “all”. context ( Literal [ None , ""state"" , ""alloc"" , ""all"" ] , optional ) – None , Do not record any tracebacks. “state” , Record tracebacks for currently allocated memory. “alloc” , additionally keep tracebacks for alloc calls. “all” , additionally keep tracebacks for free calls.
Defaults to “all”. stacks ( Literal [ ""python"" , ""all"" ] , optional ) – “python” , include Python, TorchScript, and inductor frames in tracebacks “all” , additionally include C++ frames
Defaults to “all”. max_entries ( int , optional ) – Keep a maximum of max_entries alloc/free events in the recorded history recorded. torch.cuda.memory. _snapshot ( device = None ) [source] [source] ¶ Save a snapshot of CUDA memory state at the time it was called. The state is represented as a dictionary with the following structure. class Snapshot ( TypedDict ): segments : List [ Segment ] device_traces : List [ List [ TraceEntry ]] class Segment ( TypedDict ): # Segments are memory returned from a cudaMalloc call. # The size of reserved memory is the sum of all Segments. # Segments are cached and reused for future allocations. # If the reuse is smaller than the segment, the segment # is split into more then one Block. # empty_cache() frees Segments that are entirely inactive. address : int total_size : int #  cudaMalloc'd size of segment stream : int segment_type : Literal [ 'small' , 'large' ] # 'large' (>1MB) allocated_size : int # size of memory in use active_size : int # size of memory in use or in active_awaiting_free state blocks : List [ Block ] class Block ( TypedDict ): # A piece of memory returned from the allocator, or # current cached but inactive. size : int requested_size : int # size requested during malloc, may be smaller than # size due to rounding address : int state : Literal [ 'active_allocated' , # used by a tensor 'active_awaiting_free' , # waiting for another stream to finish using # this, then it will become free 'inactive' ,] # free for reuse frames : List [ Frame ] # stack trace from where the allocation occurred class Frame ( TypedDict ): filename : str line : int name : str class TraceEntry ( TypedDict ): # When `torch.cuda.memory._record_memory_history()` is enabled, # the snapshot will contain TraceEntry objects that record each # action the allocator took. action : Literal [ 'alloc' # memory allocated 'free_requested' , # the allocated received a call to free memory 'free_completed' , # the memory that was requested to be freed is now # able to be used in future allocation calls 'segment_alloc' , # the caching allocator ask cudaMalloc for more memory # and added it as a segment in its cache 'segment_free' , # the caching allocator called cudaFree to return memory # to cuda possibly trying free up memory to # allocate more segments or because empty_caches was called 'oom' , # the allocator threw an OOM exception. 'size' is # the requested number of bytes that did not succeed 'snapshot' # the allocator generated a memory snapshot # useful to coorelate a previously taken # snapshot with this trace ] addr : int # not present for OOM frames : List [ Frame ] size : int stream : int device_free : int # only present for OOM, the amount of # memory cuda still reports to be free Returns The Snapshot dictionary object torch.cuda.memory. _dump_snapshot ( filename = 'dump_snapshot.pickle' ) [source] [source] ¶ Save a pickled version of the torch.memory._snapshot() dictionary to a file. This file can be opened by the interactive snapshot viewer at pytorch.org/memory_viz Parameters filename ( str , optional ) – Name of the file to create. Defaults to “dump_snapshot.pickle”.",7344,2,9,"# enable memory history, which will
# add tracebacks and event history to snapshots
torch.cuda.memory._record_memory_history()

run_your_code()
torch.cuda.memory._dump_snapshot(""my_snapshot.pickle"")
---
class Snapshot(TypedDict):
    segments : List[Segment]
    device_traces: List[List[TraceEntry]]

class Segment(TypedDict):
    # Segments are memory returned from a cudaMalloc call.
    # The size of reserved memory is the sum of all Segments.
    # Segments are cached and reused for future allocations.
    # If the reuse is smaller than the segment, the segment
    # is split into more then one Block.
    # empty_cache() frees Segments that are entirely inactive.
    address: int
    total_size: int #  cudaMalloc'd size of segment
    stream: int
    segment_type: Literal['small', 'large'] # 'large' (>1MB)
    allocated_size: int # size of memory in use
    active_size: int # size of memory in use or in active_awaiting_free state
    blocks : List[Block]

class Block(TypedDict):
    # A piece of memory returned from the allocator, or
    # current cached but inactive.
    size: int
    requested_size: int # size requested during malloc, may be smaller than
                        # size due to rounding
    address: int
    state: Literal['active_allocated', # used by a tensor
                'active_awaiting_free', # waiting for another stream to finish using
                                        # this, then it will become free
                'inactive',] # free for reuse
    frames: List[Frame] # stack trace from where the allocation occurred

class Frame(TypedDict):
        filename: str
        line: int
        name: str

class TraceEntry(TypedDict):
    # When `torch.cuda.memory._record_memory_history()` is enabled,
    # the snapshot will contain TraceEntry objects that record each
    # action the allocator took.
    action: Literal[
    'alloc'  # memory allocated
    'free_requested', # the allocated received a call to free memory
    'free_completed', # the memory that was requested to be freed is now
                    # able to be used in future allocation calls
    'segment_alloc', # the caching allocator ask cudaMalloc for more memory
                    # and added it as a segment in its cache
    'segment_free',  # the caching allocator called cudaFree to return memory
                    # to cuda possibly trying free up memory to
                    # allocate more segments or because empty_caches was called
    'oom',          # the allocator threw an OOM exception. 'size' is
                    # the requested number of bytes that did not succeed
    'snapshot'      # the allocator generated a memory snapshot
                    # useful to coorelate a previously taken
                    # snapshot with this trace
    ]
    addr: int # not present for OOM
    frames: List[Frame]
    size: int
    stream: int
    device_free: int # only present for OOM, the amount of
                    # memory cuda still reports to be free",1752175584.2201152
https://pytorch.org/docs/stable/cuda.html,torch.cuda — PyTorch 2.7 documentation,"torch.cuda ¶ This package adds support for CUDA tensor types. It implements the same function as CPU tensors, but they utilize
GPUs for computation. It is lazily initialized, so you can always import it, and use is_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA. StreamContext Context-manager that selects a given stream. can_device_access_peer Check if peer access between two devices is possible. current_blas_handle Return cublasHandle_t pointer to current cuBLAS handle current_device Return the index of a currently selected device. current_stream Return the currently selected Stream for a given device. cudart Retrieves the CUDA runtime API module. default_stream Return the default Stream for a given device. device Context-manager that changes the selected device. device_count Return the number of GPUs available. device_memory_used Return used global (device) memory in bytes as given by nvidia-smi or amd-smi . device_of Context-manager that changes the current device to that of given object. get_arch_list Return list CUDA architectures this library was compiled for. get_device_capability Get the cuda capability of a device. get_device_name Get the name of a device. get_device_properties Get the properties of a device. get_gencode_flags Return NVCC gencode flags this library was compiled with. get_stream_from_external Return a Stream from an externally allocated CUDA stream. get_sync_debug_mode Return current value of debug mode for cuda synchronizing operations. init Initialize PyTorch's CUDA state. ipc_collect Force collects GPU memory after it has been released by CUDA IPC. is_available Return a bool indicating if CUDA is currently available. is_initialized Return whether PyTorch's CUDA state has been initialized. is_tf32_supported Return a bool indicating if the current CUDA/ROCm device supports dtype tf32. memory_usage Return the percent of time over the past sample period during which global (device) memory was being read or written as given by nvidia-smi . set_device Set the current device. set_stream Set the current stream.This is a wrapper API to set the stream. set_sync_debug_mode Set the debug mode for cuda synchronizing operations. stream Wrap around the Context-manager StreamContext that selects a given stream. synchronize Wait for all kernels in all streams on a CUDA device to complete. utilization Return the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by nvidia-smi . temperature Return the average temperature of the GPU sensor in Degrees C (Centigrades). power_draw Return the average power draw of the GPU sensor in mW (MilliWatts) clock_rate Return the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by nvidia-smi . OutOfMemoryError Exception raised when device is out of memory Random Number Generator ¶ get_rng_state Return the random number generator state of the specified GPU as a ByteTensor. get_rng_state_all Return a list of ByteTensor representing the random number states of all devices. set_rng_state Set the random number generator state of the specified GPU. set_rng_state_all Set the random number generator state of all devices. manual_seed Set the seed for generating random numbers for the current GPU. manual_seed_all Set the seed for generating random numbers on all GPUs. seed Set the seed for generating random numbers to a random number for the current GPU. seed_all Set the seed for generating random numbers to a random number on all GPUs. initial_seed Return the current random seed of the current GPU. Communication collectives ¶ comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcast a sequence of tensors to the specified GPUs. comm.reduce_add Sum tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. Streams and events ¶ Stream Wrapper around a CUDA stream. ExternalStream Wrapper around an externally allocated CUDA stream. Event Wrapper around a CUDA event. Graphs (beta) ¶ is_current_stream_capturing Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise. graph_pool_handle Return an opaque token representing the id of a graph memory pool. CUDAGraph Wrapper around a CUDA graph. graph Context-manager that captures CUDA work into a torch.cuda.CUDAGraph object for later replay. make_graphed_callables Accept callables (functions or nn.Module s) and returns graphed versions. Memory management ¶ empty_cache Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi . get_per_process_memory_fraction Get memory fraction for a process. list_gpu_processes Return a human-readable printout of the running processes and their GPU memory use for a given device. mem_get_info Return the global free and total GPU memory for a given device using cudaMemGetInfo. memory_stats Return a dictionary of CUDA memory allocator statistics for a given device. host_memory_stats Return a dictionary of CUDA memory allocator statistics for a given device. memory_summary Return a human-readable printout of the current memory allocator statistics for a given device. memory_snapshot Return a snapshot of the CUDA memory allocator state across all devices. memory_allocated Return the current GPU memory occupied by tensors in bytes for a given device. max_memory_allocated Return the maximum GPU memory occupied by tensors in bytes for a given device. reset_max_memory_allocated Reset the starting point in tracking maximum GPU memory occupied by tensors for a given device. memory_reserved Return the current GPU memory managed by the caching allocator in bytes for a given device. max_memory_reserved Return the maximum GPU memory managed by the caching allocator in bytes for a given device. set_per_process_memory_fraction Set memory fraction for a process. memory_cached Deprecated; see memory_reserved() . max_memory_cached Deprecated; see max_memory_reserved() . reset_max_memory_cached Reset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device. reset_peak_memory_stats Reset the ""peak"" stats tracked by the CUDA memory allocator. reset_peak_host_memory_stats Reset the ""peak"" stats tracked by the host memory allocator. caching_allocator_alloc Perform a memory allocation using the CUDA memory allocator. caching_allocator_delete Delete memory allocated using the CUDA memory allocator. get_allocator_backend Return a string describing the active allocator backend as set by PYTORCH_CUDA_ALLOC_CONF . CUDAPluggableAllocator CUDA memory allocator loaded from a so file. change_current_allocator Change the currently used memory allocator to be the one provided. MemPool MemPool represents a pool of memory in a caching allocator. MemPoolContext MemPoolContext holds the currently active pool and stashes the previous pool. caching_allocator_enable Enable or disable the CUDA memory allocator. class torch.cuda. use_mem_pool ( pool , device = None ) [source] [source] ¶ A context manager that routes allocations to a given pool. Parameters pool ( torch.cuda.MemPool ) – a MemPool object to be made active so that
allocations route to this pool. device ( torch.device or int , optional ) – selected device. Uses MemPool on
the current device, given by current_device() ,
if device is None (default). NVIDIA Tools Extension (NVTX) ¶ nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Push a range onto a stack of nested range span. nvtx.range_pop Pop a range off of a stack of nested range spans. nvtx.range Context manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end. Jiterator (beta) ¶ jiterator._create_jit_fn Create a jiterator-generated cuda kernel for an elementwise op. jiterator._create_multi_output_jit_fn Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs. TunableOp ¶ Some operations could be implemented using more than one library or more than
one technique. For example, a GEMM could be implemented for CUDA or ROCm using
either the cublas/cublasLt libraries or hipblas/hipblasLt libraries,
respectively. How does one know which implementation is the fastest and should
be chosen? That’s what TunableOp provides. Certain operators have been
implemented using multiple strategies as Tunable Operators. At runtime, all
strategies are profiled and the fastest is selected for all subsequent
operations. See the documentation for information on how to use it. Stream Sanitizer (prototype) ¶ CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch.
See the documentation for information on how to use it. GPUDirect Storage (prototype) ¶ The APIs in torch.cuda.gds provide thin wrappers around certain cuFile APIs that allow
direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the cufile api documentation for more details. These APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must
ensure that their system is appropriately configured to use GPUDirect Storage per the GPUDirect Storage documentation . See the docs for GdsFile for an example of how to use these. gds_register_buffer Registers a storage on a CUDA device as a cufile buffer. gds_deregister_buffer Deregisters a previously registered storage on a CUDA device as a cufile buffer. GdsFile Wrapper around cuFile.",9713,0,14,,1752175585.2978811
https://pytorch.org/docs/stable/notes/cuda.html,CUDA semantics — PyTorch 2.7 documentation,"CUDA semantics ¶ torch.cuda is used to set up and run CUDA operations. It keeps track of
the currently selected GPU, and all CUDA tensors you allocate will by default be
created on that device. The selected device can be changed with a torch.cuda.device context manager. However, once a tensor is allocated, you can do operations on it irrespective
of the selected device, and the results will be always placed on the same
device as the tensor. Cross-GPU operations are not allowed by default, with the exception of copy_() and other methods with copy-like functionality
such as to() and cuda() .
Unless you enable peer-to-peer memory access, any attempts to launch ops on
tensors spread across different devices will raise an error. Below you can find a small example showcasing this: cuda = torch . device ( 'cuda' ) # Default CUDA device cuda0 = torch . device ( 'cuda:0' ) cuda2 = torch . device ( 'cuda:2' ) # GPU 2 (these are 0-indexed) x = torch . tensor ([ 1. , 2. ], device = cuda0 ) # x.device is device(type='cuda', index=0) y = torch . tensor ([ 1. , 2. ]) . cuda () # y.device is device(type='cuda', index=0) with torch . cuda . device ( 1 ): # allocates a tensor on GPU 1 a = torch . tensor ([ 1. , 2. ], device = cuda ) # transfers a tensor from CPU to GPU 1 b = torch . tensor ([ 1. , 2. ]) . cuda () # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch . tensor ([ 1. , 2. ]) . to ( device = cuda ) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch . randn ( 2 , device = cuda2 ) e = torch . randn ( 2 ) . to ( cuda2 ) f = torch . randn ( 2 ) . cuda ( cuda2 ) # d.device, e.device, and f.device are all device(type='cuda', index=2) TensorFloat-32 (TF32) on Ampere (and later) devices ¶ Starting in PyTorch 1.7, there is a new flag called allow_tf32 . This flag
defaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later.
This flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores,
available on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies
and batched matrix multiplies) and convolutions. TF32 tensor cores are designed to achieve better performance on matmul and convolutions on torch.float32 tensors by rounding input data to have 10 bits of mantissa, and accumulating
results with FP32 precision, maintaining FP32 dynamic range. matmuls and convolutions are controlled separately, and their corresponding flags can be accessed at: # The flag below controls whether to allow TF32 on matmul. This flag defaults to False # in PyTorch 1.12 and later. torch . backends . cuda . matmul . allow_tf32 = True # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True. torch . backends . cudnn . allow_tf32 = True The precision of matmuls can also be set more broadly (limited not just to CUDA) via set_float_32_matmul_precision() .
Note that besides matmuls and convolutions themselves, functions and nn modules that internally uses
matmuls or convolutions are also affected. These include nn.Linear , nn.Conv* , cdist, tensordot,
affine grid and grid sample, adaptive log softmax, GRU and LSTM. To get an idea of the precision and speed, see the example code and benchmark data (on A100) below: a_full = torch . randn ( 10240 , 10240 , dtype = torch . double , device = 'cuda' ) b_full = torch . randn ( 10240 , 10240 , dtype = torch . double , device = 'cuda' ) ab_full = a_full @ b_full mean = ab_full . abs () . mean () # 80.7277 a = a_full . float () b = b_full . float () # Do matmul at TF32 mode. torch . backends . cuda . matmul . allow_tf32 = True ab_tf32 = a @ b # takes 0.016s on GA100 error = ( ab_tf32 - ab_full ) . abs () . max () # 0.1747 relative_error = error / mean # 0.0022 # Do matmul with TF32 disabled. torch . backends . cuda . matmul . allow_tf32 = False ab_fp32 = a @ b # takes 0.11s on GA100 error = ( ab_fp32 - ab_full ) . abs () . max () # 0.0031 relative_error = error / mean # 0.000039 From the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that
relative error compared to double precision is approximately 2 orders of magnitude larger. Note that
the exact ratio of TF32 to single precision speed depends on the hardware generation, as properties
such as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput
may vary from generation to generation or model to model.
If full FP32 precision is needed, users can disable TF32 by: torch . backends . cuda . matmul . allow_tf32 = False torch . backends . cudnn . allow_tf32 = False To toggle the TF32 flags off in C++, you can do at :: globalContext (). setAllowTF32CuBLAS ( false ); at :: globalContext (). setAllowTF32CuDNN ( false ); For more information about TF32, see: TensorFloat-32 CUDA 11 Ampere architecture Reduced Precision Reduction in FP16 GEMMs ¶ (Distinct from full FP16 accumulation that is intended for hardware that has higher throughput
with FP16 accumulation than FP32 accumulation, see Full FP16 accumulation ) fp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large k dimension) and GPU architectures at the cost of numerical precision and potential for overflow. Some example benchmark data on V100: [ --------------------------- bench_gemm_transformer -------------------------- ] [ m , k , n ] | allow_fp16_reduc = True | allow_fp16_reduc = False 1 threads : -------------------------------------------------------------------- [ 4096 , 4048 , 4096 ] | 1634.6 | 1639.8 [ 4096 , 4056 , 4096 ] | 1670.8 | 1661.9 [ 4096 , 4080 , 4096 ] | 1664.2 | 1658.3 [ 4096 , 4096 , 4096 ] | 1639.4 | 1651.0 [ 4096 , 4104 , 4096 ] | 1677.4 | 1674.9 [ 4096 , 4128 , 4096 ] | 1655.7 | 1646.0 [ 4096 , 4144 , 4096 ] | 1796.8 | 2519.6 [ 4096 , 5096 , 4096 ] | 2094.6 | 3190.0 [ 4096 , 5104 , 4096 ] | 2144.0 | 2663.5 [ 4096 , 5112 , 4096 ] | 2149.1 | 2766.9 [ 4096 , 5120 , 4096 ] | 2142.8 | 2631.0 [ 4096 , 9728 , 4096 ] | 3875.1 | 5779.8 [ 4096 , 16384 , 4096 ] | 6182.9 | 9656.5 ( times in microseconds ) . If full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with: torch . backends . cuda . matmul . allow_fp16_reduced_precision_reduction = False To toggle the reduced precision reduction flags in C++, one can do at :: globalContext (). setAllowFP16ReductionCuBLAS ( false ); Reduced Precision Reduction in BF16 GEMMs ¶ A similar flag (as above) exists for BFloat16 GEMMs.
Note that this switch is set to True by default for BF16, if you observe
numerical instability in your workload, you may wish to set it to False . If reduced precision reductions are not desired, users can disable reduced
precision reductions in bf16 GEMMs with: torch . backends . cuda . matmul . allow_bf16_reduced_precision_reduction = False To toggle the reduced precision reduction flags in C++, one can do at :: globalContext (). setAllowBF16ReductionCuBLAS ( true ); Full FP16 Accmumulation in FP16 GEMMs ¶ Certain GPUs have increased performance when doing _all_ FP16 GEMM accumulation
in FP16, at the cost of numerical precision and greater likelihood of overflow.
Note that this setting only has an effect on GPUs of compute capability 7.0 (Volta)
or newer. This behavior can be enabled via: torch . backends . cuda . matmul . allow_fp16_accumulation = True To toggle the reduced precision reduction flags in C++, one can do at :: globalContext (). setAllowFP16AccumulationCuBLAS ( true ); Asynchronous execution ¶ By default, GPU operations are asynchronous.  When you call a function that
uses the GPU, the operations are enqueued to the particular device, but not
necessarily executed until later.  This allows us to execute more computations
in parallel, including operations on CPU or other GPUs. In general, the effect of asynchronous computation is invisible to the caller,
because (1) each device executes operations in the order they are queued, and
(2) PyTorch automatically performs necessary synchronization when copying data
between CPU and GPU or between two GPUs.  Hence, computation will proceed as if
every operation was executed synchronously. You can force synchronous computation by setting environment variable CUDA_LAUNCH_BLOCKING=1 .  This can be handy when an error occurs on the GPU.
(With asynchronous execution, such an error isn’t reported until after the
operation is actually executed, so the stack trace does not show where it was
requested.) A consequence of the asynchronous computation is that time measurements without
synchronizations are not accurate. To get precise measurements, one should either
call torch.cuda.synchronize() before measuring, or use torch.cuda.Event to record times as following: start_event = torch . cuda . Event ( enable_timing = True ) end_event = torch . cuda . Event ( enable_timing = True ) start_event . record () # Run some things here end_event . record () torch . cuda . synchronize () # Wait for the events to be recorded! elapsed_time_ms = start_event . elapsed_time ( end_event ) As an exception, several functions such as to() and copy_() admit an explicit non_blocking argument,
which lets the caller bypass synchronization when it is unnecessary.
Another exception is CUDA streams, explained below. CUDA streams ¶ A CUDA stream is a linear sequence of execution that belongs to a specific
device.  You normally do not need to create one explicitly: by default, each
device uses its own “default” stream. Operations inside each stream are serialized in the order they are created,
but operations from different streams can execute concurrently in any
relative order, unless explicit synchronization functions (such as synchronize() or wait_stream() ) are
used.  For example, the following code is incorrect: cuda = torch . device ( 'cuda' ) s = torch . cuda . Stream () # Create a new stream. A = torch . empty (( 100 , 100 ), device = cuda ) . normal_ ( 0.0 , 1.0 ) with torch . cuda . stream ( s ): # sum() may start execution before normal_() finishes! B = torch . sum ( A ) When the “current stream” is the default stream, PyTorch automatically performs
necessary synchronization when data is moved around, as explained above.
However, when using non-default streams, it is the user’s responsibility to
ensure proper synchronization.  The fixed version of this example is: cuda = torch . device ( 'cuda' ) s = torch . cuda . Stream () # Create a new stream. A = torch . empty (( 100 , 100 ), device = cuda ) . normal_ ( 0.0 , 1.0 ) s . wait_stream ( torch . cuda . default_stream ( cuda )) # NEW! with torch . cuda . stream ( s ): B = torch . sum ( A ) A . record_stream ( s ) # NEW! There are two new additions.  The torch.cuda.Stream.wait_stream() call
ensures that the normal_() execution has finished before we start running sum(A) on a side stream.  The torch.Tensor.record_stream() (see for
more details) ensures that we do not deallocate A before sum(A) has
completed.  You can also manually wait on the stream at some later point in
time with torch.cuda.default_stream(cuda).wait_stream(s) (note that it
is pointless to wait immediately, since that will prevent the stream execution
from running in parallel with other work on the default stream.)  See the
documentation for torch.Tensor.record_stream() on more details on when
to use one or another. Note that this synchronization is necessary even when there is no
read dependency, e.g., as seen in this example: cuda = torch . device ( 'cuda' ) s = torch . cuda . Stream () # Create a new stream. A = torch . empty (( 100 , 100 ), device = cuda ) s . wait_stream ( torch . cuda . default_stream ( cuda )) # STILL REQUIRED! with torch . cuda . stream ( s ): A . normal_ ( 0.0 , 1.0 ) A . record_stream ( s ) Despite the computation on s not reading the contents of A and no
other uses of A , it is still necessary to synchronize, because A may correspond to memory reallocated by the CUDA caching allocator, with
pending operations from the old (deallocated) memory. Stream semantics of backward passes ¶ Each backward CUDA op runs on the same stream that was used for its corresponding forward op.
If your forward pass runs independent ops in parallel on different streams,
this helps the backward pass exploit that same parallelism. The stream semantics of a backward call with respect to surrounding ops are the same
as for any other call. The backward pass inserts internal syncs to ensure this even when
backward ops run on multiple streams as described in the previous paragraph.
More concretely, when calling autograd.backward , autograd.grad , or tensor.backward ,
and optionally supplying CUDA tensor(s) as the  initial gradient(s) (e.g., autograd.backward(..., grad_tensors=initial_grads) , autograd.grad(..., grad_outputs=initial_grads) , or tensor.backward(..., gradient=initial_grad) ),
the acts of optionally populating initial gradient(s), invoking the backward pass, and using the gradients have the same stream-semantics relationship as any group of ops: s = torch . cuda . Stream () # Safe, grads are used in the same stream context as backward() with torch . cuda . stream ( s ): loss . backward () use grads # Unsafe with torch . cuda . stream ( s ): loss . backward () use grads # Safe, with synchronization with torch . cuda . stream ( s ): loss . backward () torch . cuda . current_stream () . wait_stream ( s ) use grads # Safe, populating initial grad and invoking backward are in the same stream context with torch . cuda . stream ( s ): loss . backward ( gradient = torch . ones_like ( loss )) # Unsafe, populating initial_grad and invoking backward are in different stream contexts, # without synchronization initial_grad = torch . ones_like ( loss ) with torch . cuda . stream ( s ): loss . backward ( gradient = initial_grad ) # Safe, with synchronization initial_grad = torch . ones_like ( loss ) s . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( s ): initial_grad . record_stream ( s ) loss . backward ( gradient = initial_grad ) BC note: Using grads on the default stream ¶ In prior versions of PyTorch (1.9 and earlier), the autograd engine always synced
the default stream with all backward ops, so the following pattern: with torch . cuda . stream ( s ): loss . backward () use grads was safe as long as use grads happened on the default stream.
In present PyTorch, that pattern is no longer safe. If backward() and use grads are in different stream contexts, you must sync the streams: with torch . cuda . stream ( s ): loss . backward () torch . cuda . current_stream () . wait_stream ( s ) use grads even if use grads is on the default stream. Memory management ¶ PyTorch uses a caching memory allocator to speed up memory allocations. This
allows fast memory deallocation without device synchronizations. However, the
unused memory managed by the allocator will still show as if used in nvidia-smi . You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by
tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory
managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used
by other GPU applications. However, the occupied GPU memory by tensors will not
be freed so it can not increase the amount of GPU memory available for PyTorch. To better understand how CUDA memory is being used over time, Understanding CUDA Memory Usage describes tools for capturing and visualizing traces of memory use. For more advanced users, we offer more comprehensive memory benchmarking via memory_stats() . We also offer the capability to capture a
complete snapshot of the memory allocator state via memory_snapshot() , which can help you understand the
underlying allocation patterns produced by your code. Optimizing memory usage  with PYTORCH_CUDA_ALLOC_CONF ¶ Use of a caching allocator can interfere with memory checking tools such as cuda-memcheck .  To debug memory errors using cuda-memcheck , set PYTORCH_NO_CUDA_MEMORY_CACHING=1 in your environment to disable caching. The behavior of the caching allocator can be controlled via the environment variable PYTORCH_CUDA_ALLOC_CONF .
The format is PYTORCH_CUDA_ALLOC_CONF=<option>:<value>,<option2>:<value2>... Available options: backend allows selecting the underlying allocator implementation.
Currently, valid options are native , which uses PyTorch’s native
implementation, and cudaMallocAsync , which uses CUDA’s built-in asynchronous allocator . cudaMallocAsync requires CUDA 11.4 or newer. The default is native . backend applies to all devices used by the process, and can’t be
specified on a per-device basis. max_split_size_mb prevents the native allocator
from splitting blocks larger than this size (in MB). This can reduce
fragmentation and may allow some borderline workloads to complete without
running out of memory. Performance cost can range from ‘zero’ to ‘substantial’
depending on allocation patterns.  Default value is unlimited, i.e. all blocks
can be split. The memory_stats() and memory_summary() methods are useful for tuning.  This
option should be used as a last resort for a workload that is aborting
due to ‘out of memory’ and showing a large amount of inactive split blocks. max_split_size_mb is only meaningful with backend:native .
With backend:cudaMallocAsync , max_split_size_mb is ignored. roundup_power2_divisions helps with rounding the requested allocation
size to nearest power-2 division and making better use of the blocks. In
the native CUDACachingAllocator, the sizes are rounded up in multiple
of blocks size of 512, so this works fine for smaller sizes. However, this
can be inefficient for large near-by allocations as each will go to different
size of blocks and re-use of those blocks are minimized. This might create
lots of unused blocks and will waste GPU memory capacity. This option enables
the rounding of allocation size to nearest power-2 division. For example, if
we need to round-up size of 1200 and if number of divisions is 4,
the size 1200 lies between 1024 and 2048 and if we do 4 divisions between
them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200
will be rounded to 1280 as the nearest ceiling of power-2 division.
Specify a single value to apply for all allocation sizes or specify an
array of key value pairs to set power-2 division individually for each
power of two interval. For example to set 1 division for all allocations
under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions
for allocations between 512MB and 1GB and 8 divisions for any larger allocations,
set the knob value to: [256:1,512:2,1024:4,>:8]. roundup_power2_divisions is only meaningful with backend:native .
With backend:cudaMallocAsync , roundup_power2_divisions is ignored. max_non_split_rounding_mb will allow non-split blocks for better reuse, eg, a 1024MB cached block can be re-used for a 512MB allocation request. In the default
case, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block
can only be served with between 512-532 MB size block. If we set the value of this
option to 1024, it will alow 512-1536 MB size blocks to be used for a 512MB block
which increases reuse of larger blocks. This will also help in reducing the stalls
in avoiding expensive cudaMalloc calls. garbage_collection_threshold helps actively reclaiming unused GPU memory to
avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks),
which can be unfavorable to latency-critical GPU applications (e.g., servers).
Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming
GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e.,
80% of the total memory allocated to the GPU application). The algorithm prefers
to free old & unused blocks first to avoid freeing blocks that are actively being
reused. The threshold value should be between greater than 0.0 and less than 1.0. garbage_collection_threshold is only meaningful with backend:native .
With backend:cudaMallocAsync , garbage_collection_threshold is ignored. expandable_segments (experimental, default: False ) If set to True , this setting instructs
the allocator to create CUDA allocations that can later be expanded to better handle cases
where a job changing allocation sizes frequently, such as having a changing batch size.
Normally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations
that are the same size as what the user requests. In the future, parts of these
allocations can be reused for other requests if they are free. This works well
when the program makes many requests of exactly the same size or of sizes that
even multiples of that size. Many deep learning models follow this behavior.
However, one common exception is when the batch size changes slightly from one
iteration to the next, e.g. in batched inference. When the program runs
initially with batch size N , it will make allocations appropriate for that size.
If in the future, it runs at size N - 1 , the existing allocations will still be
big enough. However, if it runs at size N + 1 , then it will have to make new
allocations that are slightly larger. Not all the tensors are the same size.
Some might be (N + 1)*A and others (N + 1)*A*B where A and B are some non-batch
dimensions in the model. Because the allocator reuses existing allocations when
they are big enough, some number of (N + 1)*A allocations will actually fit in
the already existing N*B*A segments, though not perfectly. As the model runs it
will partially fill up all of these segments leaving unusable free slices of
memory at the end of these segments. The allocator at some point will need to cudaMalloc a new (N + 1)*A*B segment. If there is not enough memory, there is
now no way to recover the slices of memory that are free at the end of existing
segments. With models 50+ layers deep, this pattern might repeat 50+ times
creating many slivers. expandable_segments allows the allocator to create a segment initially and then
expand its size later when more memory is needed. Instead of making one segment
per allocation, it tries to make one segment (per stream) that grows as
necessary. Now when the N + 1 case runs, the allocations will tile nicely into
the one large segment until it fills up. Then more memory is requested and
appended to the end of the segment. This process does not create as many slivers
of unusable memory, so it is more likely to succeed at finding this memory. pinned_use_cuda_host_register option is a boolean flag that determines whether to
use the CUDA API’s cudaHostRegister function for allocating pinned memory instead
of the default cudaHostAlloc. When set to True, the memory is allocated using regular
malloc and then pages are mapped to the memory before calling cudaHostRegister.
This pre-mapping of pages helps reduce the lock time during the execution
of cudaHostRegister. pinned_num_register_threads option is only valid when pinned_use_cuda_host_register
is set to True. By default, one thread is used to map the pages. This option allows
using more threads to parallelize the page mapping operations to reduce the overall
allocation time of pinned memory. A good value for this option is 8 based on
benchmarking results. pinned_use_background_threads option is a boolean flag to enable background thread
for processing events. This avoids any slow path associated with querying/processing of
events in the fast allocation path. This feature is disabled by default. Note Some stats reported by the CUDA memory management API are specific to backend:native , and are not meaningful with backend:cudaMallocAsync .
See each function’s docstring for details. Using custom memory allocators for CUDA ¶ It is possible to define allocators as simple functions in C/C++ and compile
them as a shared library, the code below shows a basic allocator that just
traces all the memory operations. #include <sys/types.h> #include <cuda_runtime_api.h> #include <iostream> // Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC extern ""C"" { void * my_malloc ( ssize_t size , int device , cudaStream_t stream ) { void * ptr ; cudaMalloc ( & ptr , size ); std :: cout << ""alloc "" << ptr << size << std :: endl ; return ptr ; } void my_free ( void * ptr , ssize_t size , int device , cudaStream_t stream ) { std :: cout << ""free "" << ptr << "" "" << stream << std :: endl ; cudaFree ( ptr ); } } This can be used in python through the torch.cuda.memory.CUDAPluggableAllocator .
The user is responsible for supplying the path to the .so file and the name
of the alloc/free functions that match the signatures specified above. import torch # Load the allocator new_alloc = torch . cuda . memory . CUDAPluggableAllocator ( 'alloc.so' , 'my_malloc' , 'my_free' ) # Swap the current allocator torch . cuda . memory . change_current_allocator ( new_alloc ) # This will allocate memory in the device using the new allocator b = torch . zeros ( 10 , device = 'cuda' ) import torch # Do an initial memory allocator b = torch . zeros ( 10 , device = 'cuda' ) # Load the allocator new_alloc = torch . cuda . memory . CUDAPluggableAllocator ( 'alloc.so' , 'my_malloc' , 'my_free' ) # This will error since the current allocator was already instantiated torch . cuda . memory . change_current_allocator ( new_alloc ) Mixing different CUDA system allocators in the same program ¶ Depending on your use case, change_current_allocator() may not be what you
want to use, since it swaps the CUDA allocator for the entire program (similar to PYTORCH_CUDA_ALLOC_CONF=backend:cudaMallocAsync ). For instance, if the swapped allocator doesn’t
have caching mechanism, you will lose all the benefits of PyTorch’s CUDACachingAllocator. Instead,
you can selectively mark a region of PyTorch code to use a custom allocator using torch.cuda.MemPool . This will let you use multiple CUDA system allocators in the same
PyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching).
Using torch.cuda.MemPool , you can utilize custom allocators that enable several features,
such as: Allocating output buffers for an all-reduce using ncclMemAlloc allocator can enable NVLink
Switch Reductions (NVLS). This can reduce contention between overlapping compute and communication
kernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads. For Grace CPU based systems, allocating host outputs buffers for an all-gather using cuMemCreate and specifying CU_MEM_LOCATION_TYPE_HOST_NUMA can enable Extended GPU Memory (EGM) based memory transfers
from source GPUs to the destination CPU. This accelerates the all-gather since the transfer
happens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface
Card (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing. If you are crafting a model and don’t want to think about the optimal memory placements of a memory
intensive module at first (e.g. an embedding table), or perhaps you have a module which is not
performance sensitive and doesn’t fit in the GPU, then you could just allocate that module with cudaMallocManaged with preferred CPU location and get your model working first. Note While cudaMallocManaged offers convenient automatic memory management using CUDA Unified Virtual Memory (UVM),
it is not recommended for DL workloads. For DL workloads that fit in GPU memory, explicit placement consistently
outperforms UVM, since there are no page faults and access patterns remain predictable. When GPU memory gets
saturated, UVM has to perform costly double transfers, evicting pages to CPU before bringing in new ones. The code below shows ncclMemAlloc wrapped in a torch.cuda.memory.CUDAPluggableAllocator . import os import torch import torch.distributed as dist from torch.cuda.memory import CUDAPluggableAllocator from torch.distributed.distributed_c10d import _get_default_group from torch.utils import cpp_extension # create allocator nccl_allocator_source = """""" #include <nccl.h> #include <iostream> extern ""C"" { void* nccl_alloc_plug(size_t size, int device, void* stream) { std::cout << ""Using ncclMemAlloc"" << std::endl; void* ptr; ncclResult_t err = ncclMemAlloc(&ptr, size); return ptr; } void nccl_free_plug(void* ptr, size_t size, int device, void* stream) { std::cout << ""Using ncclMemFree"" << std::endl; ncclResult_t err = ncclMemFree(ptr); } } """""" nccl_allocator_libname = ""nccl_allocator"" nccl_allocator = torch . utils . cpp_extension . load_inline ( name = nccl_allocator_libname , cpp_sources = nccl_allocator_source , with_cuda = True , extra_ldflags = [ ""-lnccl"" ], verbose = True , is_python_module = False , build_directory = ""./"" , ) allocator = CUDAPluggableAllocator ( f ""./ { nccl_allocator_libname } .so"" , ""nccl_alloc_plug"" , ""nccl_free_plug"" ) . allocator () # setup distributed rank = int ( os . getenv ( ""RANK"" )) local_rank = int ( os . getenv ( ""LOCAL_RANK"" )) world_size = int ( os . getenv ( ""WORLD_SIZE"" )) torch . cuda . set_device ( local_rank ) dist . init_process_group ( backend = ""nccl"" ) device = torch . device ( f ""cuda: { local_rank } "" ) default_pg = _get_default_group () backend = default_pg . _get_backend ( device ) # Note: for convenience, ProcessGroupNCCL backend provides # the ncclMemAlloc allocator as backend.mem_allocator allocator = backend . mem_allocator You can now define a new memory pool by passing this allocator to torch.cuda.MemPool : pool = torch . cuda . MemPool ( allocator ) The pool can then be used with the torch.cuda.use_mem_pool context manager to
allocate tensors into that pool: with torch . cuda . use_mem_pool ( pool ): # tensor gets allocated with ncclMemAlloc passed in the pool tensor = torch . arange ( 1024 * 1024 * 2 , device = device ) print ( f ""tensor ptr on rank { rank } is { hex ( tensor . data_ptr ()) } "" ) # register user buffers using ncclCommRegister (called under the hood) backend . register_mem_pool ( pool ) # Collective uses Zero Copy NVLS dist . all_reduce ( tensor [ 0 : 4 ]) torch . cuda . synchronize () print ( tensor [ 0 : 4 ]) Note the usage of register_mem_pool in the above example. This is an extra step for
NVLS reductions, where the user buffers need to be registered with NCCL. A user can
de-register the buffers with a similar deregister_mem_pool call. To reclaim memory, users will first need to ensure nothing is using the pool. When none
of the tensors are holding a reference to the pool, empty_cache() will
be called internally on deletion of the pool, hence returning all the memory to the system. del tensor , del pool The following torch.cuda.MemPool.use_count() and torch.cuda.MemPool.snapshot() APIs can be used for debugging purposes: pool = torch . cuda . MemPool ( allocator ) # pool's use count should be 1 at this point as MemPool object # holds a reference assert pool . use_count () == 1 nelem_1mb = 1024 * 1024 // 4 with torch . cuda . use_mem_pool ( pool ): out_0 = torch . randn ( nelem_1mb , device = ""cuda"" ) # pool's use count should be 2 at this point as use_mem_pool # holds a reference assert pool . use_count () == 2 # pool's use count should be back to 1 at this point as use_mem_pool # released its reference assert pool . use_count () == 1 with torch . cuda . use_mem_pool ( pool ): # pool should have 1 segment since we made a small allocation (1 MB) # above and so the CUDACachingAllocator packed it into a 2 MB buffer assert len ( pool . snapshot ()) == 1 out_1 = torch . randn ( nelem_1mb , device = ""cuda"" ) # pool should still have 1 segment since we made another small allocation # (1 MB) that got packed into the existing 2 MB buffer assert len ( pool . snapshot ()) == 1 out_2 = torch . randn ( nelem_1mb , device = ""cuda"" ) # pool now should have 2 segments since the CUDACachingAllocator had # to make a new 2 MB buffer to accomodate out_2 assert len ( pool . snapshot ()) == 2 Note torch.cuda.MemPool holds a reference to the pool. When you use the torch.cuda.use_mem_pool context manager, it will also acquire another reference
to the pool. On exit of the context manager, it will release its reference. After that,
ideally it should only be tensors holding references to the pool. Once the tensors release
their references, the use count of the pool will be 1, reflecting that only the torch.cuda.MemPool object is holding a reference. Only at that point, can the memory
held by the pool be returned to the system when the pool’s destructor is called using del . torch.cuda.MemPool doesn’t currently support expandable_segments mode of
CUDACachingAllocator. NCCL has specific requirements for a buffer to be compatible with NVLS reductions.
These requirements can be broken in a dynamic workload, for instance, the buffer being
sent to NCCL by the CUDACachingAllocator might be split and hence, not correctly aligned.
In those cases, NCCL can use a fallback algorithm instead of NVLS. Allocators like ncclMemAlloc can use more memory than requested, due to alignment
requirements ( CU_MULTICAST_GRANULARITY_RECOMMENDED , CU_MULTICAST_GRANULARITY_MINIMUM ),
and can cause your workload to run out of memory. cuBLAS workspaces ¶ For each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated
if that handle and stream combination executes a cuBLAS kernel that requires a workspace.
In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless torch._C._cuda_clearCublasWorkspaces() is called. The workspace size per allocation can be
specified via the environment variable CUBLAS_WORKSPACE_CONFIG with the format :[SIZE]:[COUNT] .
As an example, the default workspace size per allocation is CUBLAS_WORKSPACE_CONFIG=:4096:2:16:8 which specifies a total size of 2 * 4096 + 8 * 16 KiB . To force cuBLAS to avoid using workspaces,
set CUBLAS_WORKSPACE_CONFIG=:0:0 . cuFFT plan cache ¶ For each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly
running FFT methods (e.g., torch.fft.fft() ) on CUDA tensors of same geometry
with same configuration. Because some cuFFT plans may allocate GPU memory,
these caches have a maximum capacity. You may control and query the properties of the cache of current device with
the following APIs: torch.backends.cuda.cufft_plan_cache.max_size gives the capacity of the
cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions).
Setting this value directly modifies the capacity. torch.backends.cuda.cufft_plan_cache.size gives the number of plans
currently residing in the cache. torch.backends.cuda.cufft_plan_cache.clear() clears the cache. To control and query plan caches of a non-default device, you can index the torch.backends.cuda.cufft_plan_cache object with either a torch.device object or a device index, and access one of the above attributes. E.g., to set
the capacity of the cache for device 1 , one can write torch.backends.cuda.cufft_plan_cache[1].max_size = 10 . Just-in-Time Compilation ¶ PyTorch just-in-time compiles some operations, like torch.special.zeta, when
performed on CUDA tensors. This compilation can be time consuming
(up to a few seconds depending on your hardware and software)
and may occur multiple times for a single operator since many PyTorch operators actually
select from a variety of kernels, each of which must be compiled once, depending on their input.
This compilation occurs once per process, or just once if a kernel cache is used. By default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if
XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except on Windows,
where the kernel cache is not yet supported). The caching behavior can be directly
controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no
cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used
as a kernel cache instead of the default location. Best practices ¶ Device-agnostic code ¶ Due to the structure of PyTorch, you may need to explicitly write
device-agnostic (CPU or GPU) code; an example may be creating a new tensor as
the initial hidden state of a recurrent neural network. The first step is to determine whether the GPU should be used or not. A common
pattern is to use Python’s argparse module to read in user arguments, and
have a flag that can be used to disable CUDA, in combination with is_available() . In the following, args.device results in a torch.device object that can be used to move tensors to CPU or CUDA. import argparse import torch parser = argparse . ArgumentParser ( description = 'PyTorch Example' ) parser . add_argument ( '--disable-cuda' , action = 'store_true' , help = 'Disable CUDA' ) args = parser . parse_args () args . device = None if not args . disable_cuda and torch . cuda . is_available (): args . device = torch . device ( 'cuda' ) else : args . device = torch . device ( 'cpu' ) Note When assessing the availability of CUDA in a given environment ( is_available() ), PyTorch’s default
behavior is to call the CUDA Runtime API method cudaGetDeviceCount . Because this call in turn initializes the
CUDA Driver API (via cuInit ) if it is not already initialized, subsequent forks of a process that has run is_available() will fail with a CUDA initialization error. One can set PYTORCH_NVML_BASED_CUDA_CHECK=1 in your environment before importing PyTorch modules that execute is_available() (or before executing it directly) in order to direct is_available() to attempt an NVML-based assessment ( nvmlDeviceGetCount_v2 ). If the
NVML-based assessment is successful (i.e. NVML discovery/initialization does not fail), is_available() calls will not poison subsequent forks. If NVML discovery/initialization fails, is_available() will fallback to the standard CUDA Runtime
API assessment and the aforementioned fork constraint will apply. Note that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA
Runtime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check
may succeed while later CUDA initialization fails. Now that we have args.device , we can use it to create a Tensor on the
desired device. x = torch . empty (( 8 , 42 ), device = args . device ) net = Network () . to ( device = args . device ) This can be used in a number of cases to produce device agnostic code. Below
is an example when using a dataloader: cuda0 = torch . device ( 'cuda:0' ) # CUDA GPU 0 for i , x in enumerate ( train_loader ): x = x . to ( cuda0 ) When working with multiple GPUs on a system, you can use the CUDA_VISIBLE_DEVICES environment flag to manage which GPUs are available to
PyTorch. As mentioned above, to manually control which GPU a tensor is created
on, the best practice is to use a torch.cuda.device context manager. print ( ""Outside device is 0"" ) # On device 0 (default in most scenarios) with torch . cuda . device ( 1 ): print ( ""Inside device is 1"" ) # On device 1 print ( ""Outside device is still 0"" ) # On device 0 If you have a tensor and would like to create a new tensor of the same type on
the same device, then you can use a torch.Tensor.new_* method
(see torch.Tensor ).
Whilst the previously mentioned torch.* factory functions
( Creation Ops ) depend on the current GPU context and
the attributes arguments you pass in, torch.Tensor.new_* methods preserve
the device and other attributes of the tensor. This is the recommended practice when creating modules in which new
tensors need to be created internally during the forward pass. cuda = torch . device ( 'cuda' ) x_cpu = torch . empty ( 2 ) x_gpu = torch . empty ( 2 , device = cuda ) x_cpu_long = torch . empty ( 2 , dtype = torch . int64 ) y_cpu = x_cpu . new_full ([ 3 , 2 ], fill_value = 0.3 ) print ( y_cpu ) tensor ([[ 0.3000 , 0.3000 ], [ 0.3000 , 0.3000 ], [ 0.3000 , 0.3000 ]]) y_gpu = x_gpu . new_full ([ 3 , 2 ], fill_value =- 5 ) print ( y_gpu ) tensor ([[ - 5.0000 , - 5.0000 ], [ - 5.0000 , - 5.0000 ], [ - 5.0000 , - 5.0000 ]], device = 'cuda:0' ) y_cpu_long = x_cpu_long . new_tensor ([[ 1 , 2 , 3 ]]) print ( y_cpu_long ) tensor ([[ 1 , 2 , 3 ]]) If you want to create a tensor of the same type and size of another tensor, and
fill it with either ones or zeros, ones_like() or zeros_like() are provided as convenient helper functions (which
also preserve torch.device and torch.dtype of a Tensor). x_cpu = torch . empty ( 2 , 3 ) x_gpu = torch . empty ( 2 , 3 ) y_cpu = torch . ones_like ( x_cpu ) y_gpu = torch . zeros_like ( x_gpu ) Use pinned memory buffers ¶ Warning This is an advanced tip. If you overuse pinned memory, it can cause serious
problems when running low on RAM, and you should be aware that pinning is
often an expensive operation. Host to GPU copies are much faster when they originate from pinned (page-locked)
memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region. Also, once you pin a tensor or storage, you can use asynchronous GPU copies.
Just pass an additional non_blocking=True argument to a to() or a cuda() call. This can be used
to overlap data transfers with computation. You can make the DataLoader return batches placed in
pinned memory by passing pin_memory=True to its constructor. Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel ¶ Most use cases involving batched inputs and multiple GPUs should default to
using DistributedDataParallel to utilize more
than one GPU. There are significant caveats to using CUDA models with multiprocessing ; unless care is taken to meet the data handling
requirements exactly, it is likely that your program will have incorrect or
undefined behavior. It is recommended to use DistributedDataParallel ,
instead of DataParallel to do multi-GPU training, even if
there is only a single node. The difference between DistributedDataParallel and DataParallel is: DistributedDataParallel uses multiprocessing where a process is created for each GPU, while DataParallel uses multithreading. By using multiprocessing,
each GPU has its dedicated process, this avoids the performance overhead caused
by GIL of Python interpreter. If you use DistributedDataParallel , you could use torch.distributed.launch utility to launch your program, see Third-party backends . CUDA Graphs ¶ A CUDA graph is a record of the work (mostly kernels and their arguments) that a
CUDA stream and its dependent streams perform.
For general principles and details on the underlying CUDA API, see Getting Started with CUDA Graphs and the Graphs section of the CUDA C Programming Guide. PyTorch supports the construction of CUDA graphs using stream capture , which puts a
CUDA stream in capture mode . CUDA work issued to a capturing stream doesn’t actually
run on the GPU. Instead, the work is recorded in a graph. After capture, the graph can be launched to run the GPU work as many times as needed.
Each replay runs the same kernels with the same arguments. For pointer arguments this
means the same memory addresses are used.
By filling input memory with new data (e.g., from a new batch) before each replay,
you can rerun the same work on new data. Why CUDA Graphs? ¶ Replaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for greatly reduced CPU overhead . A graph’s arguments and kernels are fixed, so a graph replay
skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver
overheads. Under the hood, a replay submits the entire graph’s work to the GPU with
a single call to cudaGraphLaunch .  Kernels in a replay also execute slightly faster
on the GPU, but eliding CPU overhead is the main benefit. You should try CUDA graphs if all or part of your network is graph-safe (usually this means
static shapes and static control flow, but see the other constraints )
and you suspect its runtime is at least somewhat CPU-limited. PyTorch API ¶ Warning This API is in beta and may change in future releases. PyTorch exposes graphs via a raw torch.cuda.CUDAGraph class
and two convenience wrappers, torch.cuda.graph and torch.cuda.make_graphed_callables . torch.cuda.graph is a simple, versatile context manager that
captures CUDA work in its context.
Before capture, warm up the workload to be captured by running
a few eager iterations. Warmup must occur on a side stream.
Because the graph reads from and writes to the same memory addresses in every
replay, you must maintain long-lived references to tensors that hold
input and output data during capture.
To run the graph on new input data, copy new data to the capture’s input tensor(s),
replay the graph, then read the new output from the capture’s output tensor(s).
Example: g = torch . cuda . CUDAGraph () # Placeholder input used for capture static_input = torch . empty (( 5 ,), device = ""cuda"" ) # Warmup before capture s = torch . cuda . Stream () s . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( s ): for _ in range ( 3 ): static_output = static_input * 2 torch . cuda . current_stream () . wait_stream ( s ) # Captures the graph # To allow capture, automatically sets a side stream as the current stream in the context with torch . cuda . graph ( g ): static_output = static_input * 2 # Fills the graph's input memory with new data to compute on static_input . copy_ ( torch . full (( 5 ,), 3 , device = ""cuda"" )) g . replay () # static_output holds the results print ( static_output ) # full of 3 * 2 = 6 # Fills the graph's input memory with more data to compute on static_input . copy_ ( torch . full (( 5 ,), 4 , device = ""cuda"" )) g . replay () print ( static_output ) # full of 4 * 2 = 8 See Whole-network capture , Usage with torch.cuda.amp , and Usage with multiple streams for realistic and advanced patterns. make_graphed_callables is more sophisticated. make_graphed_callables accepts Python functions and torch.nn.Module s. For each passed function or Module,
it creates separate graphs of the forward-pass and backward-pass work. See Partial-network capture . Constraints ¶ A set of ops is capturable if it doesn’t violate any of the following constraints. Constraints apply to all work in a torch.cuda.graph context and all work in the forward and backward passes
of any callable you pass to torch.cuda.make_graphed_callables() . Violating any of these will likely cause a runtime error: Capture must occur on a non-default stream. (This is only a concern if you use the raw CUDAGraph.capture_begin and CUDAGraph.capture_end calls. graph and make_graphed_callables() set a side stream for you.) Ops that synchronize the CPU with the GPU (e.g., .item() calls) are prohibited. CUDA RNG operations are permitted, and when using multiple torch.Generator instances within a graph,
they must be registered using CUDAGraph.register_generator_state before graph capture.
Avoid using Generator.get_state and Generator.set_state during capture;
instead, utilize Generator.graphsafe_set_state and Generator.graphsafe_get_state for managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs. Violating any of these will likely cause silent numerical errors or undefined behavior: Within a process, only one capture may be underway at a time. No non-captured CUDA work may run in this process (on any thread) while capture is underway. CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay. Every replay reads from and writes to the same (virtual) memory addresses. Dynamic control flow (based on CPU or GPU data) is prohibited. Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence
has the same size and layout in every replay. Using multiple streams in a capture is allowed, but there are restrictions . Non-constraints ¶ Once captured, the graph may be replayed on any stream. Whole-network capture ¶ If your entire network is capturable, you can capture and replay an entire iteration: N , D_in , H , D_out = 640 , 4096 , 2048 , 1024 model = torch . nn . Sequential ( torch . nn . Linear ( D_in , H ), torch . nn . Dropout ( p = 0.2 ), torch . nn . Linear ( H , D_out ), torch . nn . Dropout ( p = 0.1 )) . cuda () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . SGD ( model . parameters (), lr = 0.1 ) # Placeholders used for capture static_input = torch . randn ( N , D_in , device = 'cuda' ) static_target = torch . randn ( N , D_out , device = 'cuda' ) # warmup # Uses static_input and static_target here for convenience, # but in a real setting, because the warmup includes optimizer.step() # you must use a few batches of real data. s = torch . cuda . Stream () s . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( s ): for i in range ( 3 ): optimizer . zero_grad ( set_to_none = True ) y_pred = model ( static_input ) loss = loss_fn ( y_pred , static_target ) loss . backward () optimizer . step () torch . cuda . current_stream () . wait_stream ( s ) # capture g = torch . cuda . CUDAGraph () # Sets grads to None before capture, so backward() will create # .grad attributes with allocations from the graph's private pool optimizer . zero_grad ( set_to_none = True ) with torch . cuda . graph ( g ): static_y_pred = model ( static_input ) static_loss = loss_fn ( static_y_pred , static_target ) static_loss . backward () optimizer . step () real_inputs = [ torch . rand_like ( static_input ) for _ in range ( 10 )] real_targets = [ torch . rand_like ( static_target ) for _ in range ( 10 )] for data , target in zip ( real_inputs , real_targets ): # Fills the graph's input memory with new data to compute on static_input . copy_ ( data ) static_target . copy_ ( target ) # replay() includes forward, backward, and step. # You don't even need to call optimizer.zero_grad() between iterations # because the captured backward refills static .grad tensors in place. g . replay () # Params have been updated. static_y_pred, static_loss, and .grad # attributes hold values from computing on this iteration's data. Partial-network capture ¶ If some of your network is unsafe to capture (e.g., due to dynamic control flow,
dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe
part(s) eagerly and use torch.cuda.make_graphed_callables() to graph only
the capture-safe part(s). By default, callables returned by make_graphed_callables() are autograd-aware, and can be used in the training loop as direct replacements
for the functions or nn.Module s you passed. make_graphed_callables() internally creates CUDAGraph objects, runs warmup iterations, and maintains
static inputs and outputs as needed.  Therefore (unlike with torch.cuda.graph ) you don’t need to handle those manually. In the following example, data-dependent dynamic control flow means the
network isn’t capturable end-to-end, but make_graphed_callables() lets us capture and run graph-safe sections as graphs regardless: N , D_in , H , D_out = 640 , 4096 , 2048 , 1024 module1 = torch . nn . Linear ( D_in , H ) . cuda () module2 = torch . nn . Linear ( H , D_out ) . cuda () module3 = torch . nn . Linear ( H , D_out ) . cuda () loss_fn = torch . nn . MSELoss () optimizer = torch . optim . SGD ( chain ( module1 . parameters (), module2 . parameters (), module3 . parameters ()), lr = 0.1 ) # Sample inputs used for capture # requires_grad state of sample inputs must match # requires_grad state of real inputs each callable will see. x = torch . randn ( N , D_in , device = 'cuda' ) h = torch . randn ( N , H , device = 'cuda' , requires_grad = True ) module1 = torch . cuda . make_graphed_callables ( module1 , ( x ,)) module2 = torch . cuda . make_graphed_callables ( module2 , ( h ,)) module3 = torch . cuda . make_graphed_callables ( module3 , ( h ,)) real_inputs = [ torch . rand_like ( x ) for _ in range ( 10 )] real_targets = [ torch . randn ( N , D_out , device = ""cuda"" ) for _ in range ( 10 )] for data , target in zip ( real_inputs , real_targets ): optimizer . zero_grad ( set_to_none = True ) tmp = module1 ( data ) # forward ops run as a graph if tmp . sum () . item () > 0 : tmp = module2 ( tmp ) # forward ops run as a graph else : tmp = module3 ( tmp ) # forward ops run as a graph loss = loss_fn ( tmp , target ) # module2's or module3's (whichever was chosen) backward ops, # as well as module1's backward ops, run as graphs loss . backward () optimizer . step () Usage with torch.cuda.amp ¶ For typical optimizers, GradScaler.step syncs
the CPU with the GPU, which is prohibited during capture. To avoid errors, either use partial-network capture , or (if forward, loss,
and backward are capture-safe) capture forward, loss, and backward but not the
optimizer step: # warmup # In a real setting, use a few batches of real data. s = torch . cuda . Stream () s . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( s ): for i in range ( 3 ): optimizer . zero_grad ( set_to_none = True ) with torch . cuda . amp . autocast (): y_pred = model ( static_input ) loss = loss_fn ( y_pred , static_target ) scaler . scale ( loss ) . backward () scaler . step ( optimizer ) scaler . update () torch . cuda . current_stream () . wait_stream ( s ) # capture g = torch . cuda . CUDAGraph () optimizer . zero_grad ( set_to_none = True ) with torch . cuda . graph ( g ): with torch . cuda . amp . autocast (): static_y_pred = model ( static_input ) static_loss = loss_fn ( static_y_pred , static_target ) scaler . scale ( static_loss ) . backward () # don't capture scaler.step(optimizer) or scaler.update() real_inputs = [ torch . rand_like ( static_input ) for _ in range ( 10 )] real_targets = [ torch . rand_like ( static_target ) for _ in range ( 10 )] for data , target in zip ( real_inputs , real_targets ): static_input . copy_ ( data ) static_target . copy_ ( target ) g . replay () # Runs scaler.step and scaler.update eagerly scaler . step ( optimizer ) scaler . update () Usage with multiple streams ¶ Capture mode automatically propagates to any streams that sync with a capturing stream.
Within capture, you may expose parallelism by issuing calls to different streams,
but the overall stream dependency DAG must branch out from the
initial capturing stream after capture begins and rejoin the initial stream
before capture ends: with torch . cuda . graph ( g ): # at context manager entrance, torch.cuda.current_stream() # is the initial capturing stream # INCORRECT (does not branch out from or rejoin initial stream) with torch . cuda . stream ( s ): cuda_work () # CORRECT: # branches out from initial stream s . wait_stream ( torch . cuda . current_stream ()) with torch . cuda . stream ( s ): cuda_work () # rejoins initial stream before capture ends torch . cuda . current_stream () . wait_stream ( s ) Note To avoid confusion for power users looking at replays in nsight systems or nvprof:
Unlike eager execution, the graph interprets a nontrivial stream DAG in capture
as a hint, not a command. During replay, the graph may reorganize independent ops
onto different streams or enqueue them in a different order (while respecting your
original DAG’s overall dependencies). Usage with DistributedDataParallel ¶ NCCL < 2.9.6 ¶ NCCL versions earlier than 2.9.6 don’t allow collectives to be captured.
You must use partial-network capture ,
which defers allreduces to happen outside graphed sections of backward. Call make_graphed_callables() on graphable network sections before wrapping the network with DDP. NCCL >= 2.9.6 ¶ NCCL versions 2.9.6 or later allow collectives in the graph.
Approaches that capture an entire backward pass are a viable option, but need three setup steps. Disable DDP’s internal async error handling: os . environ [ ""NCCL_ASYNC_ERROR_HANDLING"" ] = ""0"" torch . distributed . init_process_group ( ... ) Before full-backward capture, DDP must be constructed in a side-stream context: with torch . cuda . stream ( s ): model = DistributedDataParallel ( model ) Your warmup must run at least 11 DDP-enabled eager iterations before capture. Graph memory management ¶ A captured graph acts on the same virtual addresses every time it replays.
If PyTorch frees the memory, a later replay can hit an illegal memory access.
If PyTorch reassigns the memory to new tensors, the replay can corrupt the values
seen by those tensors.  Therefore, the virtual addresses used by the graph must be
reserved for the graph across replays. The PyTorch caching allocator achieves this
by detecting when capture is underway and satisfying the capture’s allocations
from a graph-private memory pool. The private pool stays alive until its CUDAGraph object and all tensors created during capture
go out of scope. Private pools are maintained automatically. By default, the allocator creates a
separate private pool for each capture. If you capture multiple graphs,
this conservative approach ensures graph replays never corrupt each other’s values,
but sometimes needlessly wastes memory. Sharing memory across captures ¶ To economize the memory stashed in private pools, torch.cuda.graph and torch.cuda.make_graphed_callables() optionally allow different
captures to share the same private pool.
It’s safe for a set of graphs to share a private pool if you know they’ll always
be replayed in the same order they were captured,
and never be replayed concurrently. torch.cuda.graph ’s pool argument is a hint to use a particular private pool,
and can be used to share memory across graphs as shown: g1 = torch . cuda . CUDAGraph () g2 = torch . cuda . CUDAGraph () # (create static inputs for g1 and g2, run warmups of their workloads...) # Captures g1 with torch . cuda . graph ( g1 ): static_out_1 = g1_workload ( static_in_1 ) # Captures g2, hinting that g2 may share a memory pool with g1 with torch . cuda . graph ( g2 , pool = g1 . pool ()): static_out_2 = g2_workload ( static_in_2 ) static_in_1 . copy_ ( real_data_1 ) static_in_2 . copy_ ( real_data_2 ) g1 . replay () g2 . replay () With torch.cuda.make_graphed_callables() , if you want to graph several
callables and you know they’ll always run in the same order (and never concurrently)
pass them as a tuple in the same order they’ll run in the live workload, and make_graphed_callables() will capture their graphs using a shared
private pool. If, in the live workload, your callables will run in an order that occasionally changes,
or if they’ll run concurrently, passing them as a tuple to a single invocation of make_graphed_callables() is not allowed. Instead, you must call make_graphed_callables() separately for each one.",59204,41,37,"cuda = torch.device('cuda')     # Default CUDA device
cuda0 = torch.device('cuda:0')
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)
---
# The flag below controls whether to allow TF32 on matmul. This flag defaults to False
# in PyTorch 1.12 and later.
torch.backends.cuda.matmul.allow_tf32 = True

# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.
torch.backends.cudnn.allow_tf32 = True
---
a_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
b_full = torch.randn(10240, 10240, dtype=torch.double, device='cuda')
ab_full = a_full @ b_full
mean = ab_full.abs().mean()  # 80.7277

a = a_full.float()
b = b_full.float()

# Do matmul at TF32 mode.
torch.backends.cuda.matmul.allow_tf32 = True
ab_tf32 = a @ b  # takes 0.016s on GA100
error = (ab_tf32 - ab_full).abs().max()  # 0.1747
relative_error = error / mean  # 0.0022

# Do matmul with TF32 disabled.
torch.backends.cuda.matmul.allow_tf32 = False
ab_fp32 = a @ b  # takes 0.11s on GA100
error = (ab_fp32 - ab_full).abs().max()  # 0.0031
relative_error = error / mean  # 0.000039
---
torch.backends.cuda.matmul.allow_tf32 = False
torch.backends.cudnn.allow_tf32 = False
---
at::globalContext().setAllowTF32CuBLAS(false);
at::globalContext().setAllowTF32CuDNN(false);
---
[--------------------------- bench_gemm_transformer --------------------------]
      [  m ,  k  ,  n  ]    |  allow_fp16_reduc=True  |  allow_fp16_reduc=False
1 threads: --------------------------------------------------------------------
      [4096, 4048, 4096]    |           1634.6        |           1639.8
      [4096, 4056, 4096]    |           1670.8        |           1661.9
      [4096, 4080, 4096]    |           1664.2        |           1658.3
      [4096, 4096, 4096]    |           1639.4        |           1651.0
      [4096, 4104, 4096]    |           1677.4        |           1674.9
      [4096, 4128, 4096]    |           1655.7        |           1646.0
      [4096, 4144, 4096]    |           1796.8        |           2519.6
      [4096, 5096, 4096]    |           2094.6        |           3190.0
      [4096, 5104, 4096]    |           2144.0        |           2663.5
      [4096, 5112, 4096]    |           2149.1        |           2766.9
      [4096, 5120, 4096]    |           2142.8        |           2631.0
      [4096, 9728, 4096]    |           3875.1        |           5779.8
      [4096, 16384, 4096]   |           6182.9        |           9656.5
(times in microseconds).
---
torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False
---
at::globalContext().setAllowFP16ReductionCuBLAS(false);
---
torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False
---
at::globalContext().setAllowBF16ReductionCuBLAS(true);
---
torch.backends.cuda.matmul.allow_fp16_accumulation = True
---
at::globalContext().setAllowFP16AccumulationCuBLAS(true);
---
start_event = torch.cuda.Event(enable_timing=True)
end_event = torch.cuda.Event(enable_timing=True)
start_event.record()

# Run some things here

end_event.record()
torch.cuda.synchronize()  # Wait for the events to be recorded!
elapsed_time_ms = start_event.elapsed_time(end_event)
---
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
with torch.cuda.stream(s):
    # sum() may start execution before normal_() finishes!
    B = torch.sum(A)
---
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda).normal_(0.0, 1.0)
s.wait_stream(torch.cuda.default_stream(cuda))  # NEW!
with torch.cuda.stream(s):
    B = torch.sum(A)
A.record_stream(s)  # NEW!
---
cuda = torch.device('cuda')
s = torch.cuda.Stream()  # Create a new stream.
A = torch.empty((100, 100), device=cuda)
s.wait_stream(torch.cuda.default_stream(cuda))  # STILL REQUIRED!
with torch.cuda.stream(s):
    A.normal_(0.0, 1.0)
    A.record_stream(s)
---
s = torch.cuda.Stream()

# Safe, grads are used in the same stream context as backward()
with torch.cuda.stream(s):
    loss.backward()
    use grads

# Unsafe
with torch.cuda.stream(s):
    loss.backward()
use grads

# Safe, with synchronization
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads

# Safe, populating initial grad and invoking backward are in the same stream context
with torch.cuda.stream(s):
    loss.backward(gradient=torch.ones_like(loss))

# Unsafe, populating initial_grad and invoking backward are in different stream contexts,
# without synchronization
initial_grad = torch.ones_like(loss)
with torch.cuda.stream(s):
    loss.backward(gradient=initial_grad)

# Safe, with synchronization
initial_grad = torch.ones_like(loss)
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    initial_grad.record_stream(s)
    loss.backward(gradient=initial_grad)
---
with torch.cuda.stream(s):
    loss.backward()
use grads
---
with torch.cuda.stream(s):
    loss.backward()
torch.cuda.current_stream().wait_stream(s)
use grads
---
#include <sys/types.h>
#include <cuda_runtime_api.h>
#include <iostream>
// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC
extern ""C"" {
void* my_malloc(ssize_t size, int device, cudaStream_t stream) {
   void *ptr;
   cudaMalloc(&ptr, size);
   std::cout<<""alloc ""<<ptr<<size<<std::endl;
   return ptr;
}

void my_free(void* ptr, ssize_t size, int device, cudaStream_t stream) {
   std::cout<<""free ""<<ptr<< "" ""<<stream<<std::endl;
   cudaFree(ptr);
}
}
---
import torch

# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# Swap the current allocator
torch.cuda.memory.change_current_allocator(new_alloc)
# This will allocate memory in the device using the new allocator
b = torch.zeros(10, device='cuda')
---
import torch

# Do an initial memory allocator
b = torch.zeros(10, device='cuda')
# Load the allocator
new_alloc = torch.cuda.memory.CUDAPluggableAllocator(
    'alloc.so', 'my_malloc', 'my_free')
# This will error since the current allocator was already instantiated
torch.cuda.memory.change_current_allocator(new_alloc)
---
import os

import torch
import torch.distributed as dist
from torch.cuda.memory import CUDAPluggableAllocator
from torch.distributed.distributed_c10d import _get_default_group
from torch.utils import cpp_extension


# create allocator
nccl_allocator_source = """"""
#include <nccl.h>
#include <iostream>
extern ""C"" {

void* nccl_alloc_plug(size_t size, int device, void* stream) {
  std::cout << ""Using ncclMemAlloc"" << std::endl;
  void* ptr;
  ncclResult_t err = ncclMemAlloc(&ptr, size);
  return ptr;

}

void nccl_free_plug(void* ptr, size_t size, int device, void* stream) {
  std::cout << ""Using ncclMemFree"" << std::endl;
  ncclResult_t err = ncclMemFree(ptr);
}

}
""""""
nccl_allocator_libname = ""nccl_allocator""
nccl_allocator = torch.utils.cpp_extension.load_inline(
    name=nccl_allocator_libname,
    cpp_sources=nccl_allocator_source,
    with_cuda=True,
    extra_ldflags=[""-lnccl""],
    verbose=True,
    is_python_module=False,
    build_directory=""./"",
)

allocator = CUDAPluggableAllocator(
    f""./{nccl_allocator_libname}.so"", ""nccl_alloc_plug"", ""nccl_free_plug""
).allocator()

# setup distributed
rank = int(os.getenv(""RANK""))
local_rank = int(os.getenv(""LOCAL_RANK""))
world_size = int(os.getenv(""WORLD_SIZE""))
torch.cuda.set_device(local_rank)
dist.init_process_group(backend=""nccl"")
device = torch.device(f""cuda:{local_rank}"")
default_pg = _get_default_group()
backend = default_pg._get_backend(device)

# Note: for convenience, ProcessGroupNCCL backend provides
# the ncclMemAlloc allocator as backend.mem_allocator
allocator = backend.mem_allocator
---
pool = torch.cuda.MemPool(allocator)
---
with torch.cuda.use_mem_pool(pool):
    # tensor gets allocated with ncclMemAlloc passed in the pool
    tensor = torch.arange(1024 * 1024 * 2, device=device)
    print(f""tensor ptr on rank {rank} is {hex(tensor.data_ptr())}"")

# register user buffers using ncclCommRegister (called under the hood)
backend.register_mem_pool(pool)

# Collective uses Zero Copy NVLS
dist.all_reduce(tensor[0:4])
torch.cuda.synchronize()
print(tensor[0:4])
---
del tensor, del pool
---
pool = torch.cuda.MemPool(allocator)

# pool's use count should be 1 at this point as MemPool object
# holds a reference
assert pool.use_count() == 1

nelem_1mb = 1024 * 1024 // 4

with torch.cuda.use_mem_pool(pool):
    out_0 = torch.randn(nelem_1mb, device=""cuda"")

    # pool's use count should be 2 at this point as use_mem_pool
    # holds a reference
    assert pool.use_count() == 2

# pool's use count should be back to 1 at this point as use_mem_pool
# released its reference
assert pool.use_count() == 1

with torch.cuda.use_mem_pool(pool):
    # pool should have 1 segment since we made a small allocation (1 MB)
    # above and so the CUDACachingAllocator packed it into a 2 MB buffer
    assert len(pool.snapshot()) == 1

    out_1 = torch.randn(nelem_1mb, device=""cuda"")

    # pool should still have 1 segment since we made another small allocation
    # (1 MB) that got packed into the existing 2 MB buffer
    assert len(pool.snapshot()) == 1

    out_2 = torch.randn(nelem_1mb, device=""cuda"")

    # pool now should have 2 segments since the CUDACachingAllocator had
    # to make a new 2 MB buffer to accomodate out_2
    assert len(pool.snapshot()) == 2
---
import argparse
import torch

parser = argparse.ArgumentParser(description='PyTorch Example')
parser.add_argument('--disable-cuda', action='store_true',
                    help='Disable CUDA')
args = parser.parse_args()
args.device = None
if not args.disable_cuda and torch.cuda.is_available():
    args.device = torch.device('cuda')
else:
    args.device = torch.device('cpu')
---
x = torch.empty((8, 42), device=args.device)
net = Network().to(device=args.device)
---
cuda0 = torch.device('cuda:0')  # CUDA GPU 0
for i, x in enumerate(train_loader):
    x = x.to(cuda0)
---
print(""Outside device is 0"")  # On device 0 (default in most scenarios)
with torch.cuda.device(1):
    print(""Inside device is 1"")  # On device 1
print(""Outside device is still 0"")  # On device 0
---
cuda = torch.device('cuda')
x_cpu = torch.empty(2)
x_gpu = torch.empty(2, device=cuda)
x_cpu_long = torch.empty(2, dtype=torch.int64)

y_cpu = x_cpu.new_full([3, 2], fill_value=0.3)
print(y_cpu)

    tensor([[ 0.3000,  0.3000],
            [ 0.3000,  0.3000],
            [ 0.3000,  0.3000]])

y_gpu = x_gpu.new_full([3, 2], fill_value=-5)
print(y_gpu)

    tensor([[-5.0000, -5.0000],
            [-5.0000, -5.0000],
            [-5.0000, -5.0000]], device='cuda:0')

y_cpu_long = x_cpu_long.new_tensor([[1, 2, 3]])
print(y_cpu_long)

    tensor([[ 1,  2,  3]])
---
x_cpu = torch.empty(2, 3)
x_gpu = torch.empty(2, 3)

y_cpu = torch.ones_like(x_cpu)
y_gpu = torch.zeros_like(x_gpu)
---
g = torch.cuda.CUDAGraph()

# Placeholder input used for capture
static_input = torch.empty((5,), device=""cuda"")

# Warmup before capture
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for _ in range(3):
        static_output = static_input * 2
torch.cuda.current_stream().wait_stream(s)

# Captures the graph
# To allow capture, automatically sets a side stream as the current stream in the context
with torch.cuda.graph(g):
    static_output = static_input * 2

# Fills the graph's input memory with new data to compute on
static_input.copy_(torch.full((5,), 3, device=""cuda""))
g.replay()
# static_output holds the results
print(static_output)  # full of 3 * 2 = 6

# Fills the graph's input memory with more data to compute on
static_input.copy_(torch.full((5,), 4, device=""cuda""))
g.replay()
print(static_output)  # full of 4 * 2 = 8
---
N, D_in, H, D_out = 640, 4096, 2048, 1024
model = torch.nn.Sequential(torch.nn.Linear(D_in, H),
                            torch.nn.Dropout(p=0.2),
                            torch.nn.Linear(H, D_out),
                            torch.nn.Dropout(p=0.1)).cuda()
loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)

# Placeholders used for capture
static_input = torch.randn(N, D_in, device='cuda')
static_target = torch.randn(N, D_out, device='cuda')

# warmup
# Uses static_input and static_target here for convenience,
# but in a real setting, because the warmup includes optimizer.step()
# you must use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for i in range(3):
        optimizer.zero_grad(set_to_none=True)
        y_pred = model(static_input)
        loss = loss_fn(y_pred, static_target)
        loss.backward()
        optimizer.step()
torch.cuda.current_stream().wait_stream(s)

# capture
g = torch.cuda.CUDAGraph()
# Sets grads to None before capture, so backward() will create
# .grad attributes with allocations from the graph's private pool
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
    static_y_pred = model(static_input)
    static_loss = loss_fn(static_y_pred, static_target)
    static_loss.backward()
    optimizer.step()

real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    # Fills the graph's input memory with new data to compute on
    static_input.copy_(data)
    static_target.copy_(target)
    # replay() includes forward, backward, and step.
    # You don't even need to call optimizer.zero_grad() between iterations
    # because the captured backward refills static .grad tensors in place.
    g.replay()
    # Params have been updated. static_y_pred, static_loss, and .grad
    # attributes hold values from computing on this iteration's data.
---
N, D_in, H, D_out = 640, 4096, 2048, 1024

module1 = torch.nn.Linear(D_in, H).cuda()
module2 = torch.nn.Linear(H, D_out).cuda()
module3 = torch.nn.Linear(H, D_out).cuda()

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.SGD(chain(module1.parameters(),
                                  module2.parameters(),
                                  module3.parameters()),
                            lr=0.1)

# Sample inputs used for capture
# requires_grad state of sample inputs must match
# requires_grad state of real inputs each callable will see.
x = torch.randn(N, D_in, device='cuda')
h = torch.randn(N, H, device='cuda', requires_grad=True)

module1 = torch.cuda.make_graphed_callables(module1, (x,))
module2 = torch.cuda.make_graphed_callables(module2, (h,))
module3 = torch.cuda.make_graphed_callables(module3, (h,))

real_inputs = [torch.rand_like(x) for _ in range(10)]
real_targets = [torch.randn(N, D_out, device=""cuda"") for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    optimizer.zero_grad(set_to_none=True)

    tmp = module1(data)  # forward ops run as a graph

    if tmp.sum().item() > 0:
        tmp = module2(tmp)  # forward ops run as a graph
    else:
        tmp = module3(tmp)  # forward ops run as a graph

    loss = loss_fn(tmp, target)
    # module2's or module3's (whichever was chosen) backward ops,
    # as well as module1's backward ops, run as graphs
    loss.backward()
    optimizer.step()
---
# warmup
# In a real setting, use a few batches of real data.
s = torch.cuda.Stream()
s.wait_stream(torch.cuda.current_stream())
with torch.cuda.stream(s):
    for i in range(3):
        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast():
            y_pred = model(static_input)
            loss = loss_fn(y_pred, static_target)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
torch.cuda.current_stream().wait_stream(s)

# capture
g = torch.cuda.CUDAGraph()
optimizer.zero_grad(set_to_none=True)
with torch.cuda.graph(g):
    with torch.cuda.amp.autocast():
        static_y_pred = model(static_input)
        static_loss = loss_fn(static_y_pred, static_target)
    scaler.scale(static_loss).backward()
    # don't capture scaler.step(optimizer) or scaler.update()

real_inputs = [torch.rand_like(static_input) for _ in range(10)]
real_targets = [torch.rand_like(static_target) for _ in range(10)]

for data, target in zip(real_inputs, real_targets):
    static_input.copy_(data)
    static_target.copy_(target)
    g.replay()
    # Runs scaler.step and scaler.update eagerly
    scaler.step(optimizer)
    scaler.update()
---
with torch.cuda.graph(g):
    # at context manager entrance, torch.cuda.current_stream()
    # is the initial capturing stream

    # INCORRECT (does not branch out from or rejoin initial stream)
    with torch.cuda.stream(s):
        cuda_work()

    # CORRECT:
    # branches out from initial stream
    s.wait_stream(torch.cuda.current_stream())
    with torch.cuda.stream(s):
        cuda_work()
    # rejoins initial stream before capture ends
    torch.cuda.current_stream().wait_stream(s)
---
os.environ[""NCCL_ASYNC_ERROR_HANDLING""] = ""0""
torch.distributed.init_process_group(...)
---
with torch.cuda.stream(s):
    model = DistributedDataParallel(model)
---
g1 = torch.cuda.CUDAGraph()
g2 = torch.cuda.CUDAGraph()

# (create static inputs for g1 and g2, run warmups of their workloads...)

# Captures g1
with torch.cuda.graph(g1):
    static_out_1 = g1_workload(static_in_1)

# Captures g2, hinting that g2 may share a memory pool with g1
with torch.cuda.graph(g2, pool=g1.pool()):
    static_out_2 = g2_workload(static_in_2)

static_in_1.copy_(real_data_1)
static_in_2.copy_(real_data_2)
g1.replay()
g2.replay()",1752175586.4873989
https://pytorch.org/docs/stable/nn.functional.html,torch.nn.functional — PyTorch 2.7 documentation,"torch.nn.functional ¶ Convolution functions ¶ conv1d Applies a 1D convolution over an input signal composed of several input planes. conv2d Applies a 2D convolution over an input image composed of several input planes. conv3d Applies a 3D convolution over an input image composed of several input planes. conv_transpose1d Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called ""deconvolution"". conv_transpose2d Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called ""deconvolution"". conv_transpose3d Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called ""deconvolution"" unfold Extract sliding local blocks from a batched input tensor. fold Combine an array of sliding local blocks into a large containing tensor. Pooling functions ¶ avg_pool1d Applies a 1D average pooling over an input signal composed of several input planes. avg_pool2d Applies 2D average-pooling operation in k H × k W kH \times kW k H × kW regions by step size s H × s W sH \times sW sH × s W steps. avg_pool3d Applies 3D average-pooling operation in k T × k H × k W kT \times kH \times kW k T × k H × kW regions by step size s T × s H × s W sT \times sH \times sW s T × sH × s W steps. max_pool1d Applies a 1D max pooling over an input signal composed of several input planes. max_pool2d Applies a 2D max pooling over an input signal composed of several input planes. max_pool3d Applies a 3D max pooling over an input signal composed of several input planes. max_unpool1d Compute a partial inverse of MaxPool1d . max_unpool2d Compute a partial inverse of MaxPool2d . max_unpool3d Compute a partial inverse of MaxPool3d . lp_pool1d Apply a 1D power-average pooling over an input signal composed of several input planes. lp_pool2d Apply a 2D power-average pooling over an input signal composed of several input planes. lp_pool3d Apply a 3D power-average pooling over an input signal composed of several input planes. adaptive_max_pool1d Applies a 1D adaptive max pooling over an input signal composed of several input planes. adaptive_max_pool2d Applies a 2D adaptive max pooling over an input signal composed of several input planes. adaptive_max_pool3d Applies a 3D adaptive max pooling over an input signal composed of several input planes. adaptive_avg_pool1d Applies a 1D adaptive average pooling over an input signal composed of several input planes. adaptive_avg_pool2d Apply a 2D adaptive average pooling over an input signal composed of several input planes. adaptive_avg_pool3d Apply a 3D adaptive average pooling over an input signal composed of several input planes. fractional_max_pool2d Applies 2D fractional max pooling over an input signal composed of several input planes. fractional_max_pool3d Applies 3D fractional max pooling over an input signal composed of several input planes. Attention Mechanisms ¶ The torch.nn.attention.bias module contains attention_biases that are designed to be used with
scaled_dot_product_attention. scaled_dot_product_attention scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, Non-linear activation functions ¶ threshold Apply a threshold to each element of the input Tensor. threshold_ In-place version of threshold() . relu Applies the rectified linear unit function element-wise. relu_ In-place version of relu() . hardtanh Applies the HardTanh function element-wise. hardtanh_ In-place version of hardtanh() . hardswish Apply hardswish function, element-wise. relu6 Applies the element-wise function ReLU6 ( x ) = min ⁡ ( max ⁡ ( 0 , x ) , 6 ) \text{ReLU6}(x) = \min(\max(0,x), 6) ReLU6 ( x ) = min ( max ( 0 , x ) , 6 ) . elu Apply the Exponential Linear Unit (ELU) function element-wise. elu_ In-place version of elu() . selu Applies element-wise, SELU ( x ) = s c a l e ∗ ( max ⁡ ( 0 , x ) + min ⁡ ( 0 , α ∗ ( exp ⁡ ( x ) − 1 ) ) ) \text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1))) SELU ( x ) = sc a l e ∗ ( max ( 0 , x ) + min ( 0 , α ∗ ( exp ( x ) − 1 ))) , with α = 1.6732632423543772848170429916717 \alpha=1.6732632423543772848170429916717 α = 1.6732632423543772848170429916717 and s c a l e = 1.0507009873554804934193349852946 scale=1.0507009873554804934193349852946 sc a l e = 1.0507009873554804934193349852946 . celu Applies element-wise, CELU ( x ) = max ⁡ ( 0 , x ) + min ⁡ ( 0 , α ∗ ( exp ⁡ ( x / α ) − 1 ) ) \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1)) CELU ( x ) = max ( 0 , x ) + min ( 0 , α ∗ ( exp ( x / α ) − 1 )) . leaky_relu Applies element-wise, LeakyReLU ( x ) = max ⁡ ( 0 , x ) + negative_slope ∗ min ⁡ ( 0 , x ) \text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x) LeakyReLU ( x ) = max ( 0 , x ) + negative_slope ∗ min ( 0 , x ) leaky_relu_ In-place version of leaky_relu() . prelu Applies element-wise the function PReLU ( x ) = max ⁡ ( 0 , x ) + weight ∗ min ⁡ ( 0 , x ) \text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x) PReLU ( x ) = max ( 0 , x ) + weight ∗ min ( 0 , x ) where weight is a learnable parameter. rrelu Randomized leaky ReLU. rrelu_ In-place version of rrelu() . glu The gated linear unit. gelu When the approximate argument is 'none', it applies element-wise the function GELU ( x ) = x ∗ Φ ( x ) \text{GELU}(x) = x * \Phi(x) GELU ( x ) = x ∗ Φ ( x ) logsigmoid Applies element-wise LogSigmoid ( x i ) = log ⁡ ( 1 1 + exp ⁡ ( − x i ) ) \text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right) LogSigmoid ( x i ​ ) = lo g ( 1 + e x p ( − x i ​ ) 1 ​ ) hardshrink Applies the hard shrinkage function element-wise tanhshrink Applies element-wise, Tanhshrink ( x ) = x − Tanh ( x ) \text{Tanhshrink}(x) = x - \text{Tanh}(x) Tanhshrink ( x ) = x − Tanh ( x ) softsign Applies element-wise, the function SoftSign ( x ) = x 1 + ∣ x ∣ \text{SoftSign}(x) = \frac{x}{1 + |x|} SoftSign ( x ) = 1 + ∣ x ∣ x ​ softplus Applies element-wise, the function Softplus ( x ) = 1 β ∗ log ⁡ ( 1 + exp ⁡ ( β ∗ x ) ) \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x)) Softplus ( x ) = β 1 ​ ∗ lo g ( 1 + exp ( β ∗ x )) . softmin Apply a softmin function. softmax Apply a softmax function. softshrink Applies the soft shrinkage function elementwise gumbel_softmax Sample from the Gumbel-Softmax distribution ( Link 1 Link 2 ) and optionally discretize. log_softmax Apply a softmax followed by a logarithm. tanh Applies element-wise, Tanh ( x ) = tanh ⁡ ( x ) = exp ⁡ ( x ) − exp ⁡ ( − x ) exp ⁡ ( x ) + exp ⁡ ( − x ) \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)} Tanh ( x ) = tanh ( x ) = e x p ( x ) + e x p ( − x ) e x p ( x ) − e x p ( − x ) ​ sigmoid Applies the element-wise function Sigmoid ( x ) = 1 1 + exp ⁡ ( − x ) \text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)} Sigmoid ( x ) = 1 + e x p ( − x ) 1 ​ hardsigmoid Apply the Hardsigmoid function element-wise. silu Apply the Sigmoid Linear Unit (SiLU) function, element-wise. mish Apply the Mish function, element-wise. batch_norm Apply Batch Normalization for each channel across a batch of data. group_norm Apply Group Normalization for last certain number of dimensions. instance_norm Apply Instance Normalization independently for each channel in every data sample within a batch. layer_norm Apply Layer Normalization for last certain number of dimensions. local_response_norm Apply local response normalization over an input signal. rms_norm Apply Root Mean Square Layer Normalization. normalize Perform L p L_p L p ​ normalization of inputs over specified dimension. Linear functions ¶ linear Applies a linear transformation to the incoming data: y = x A T + b y = xA^T + b y = x A T + b . bilinear Applies a bilinear transformation to the incoming data: y = x 1 T A x 2 + b y = x_1^T A x_2 + b y = x 1 T ​ A x 2 ​ + b Dropout functions ¶ dropout During training, randomly zeroes some elements of the input tensor with probability p . alpha_dropout Apply alpha dropout to the input. feature_alpha_dropout Randomly masks out entire channels (a channel is a feature map). dropout1d Randomly zero out entire channels (a channel is a 1D feature map). dropout2d Randomly zero out entire channels (a channel is a 2D feature map). dropout3d Randomly zero out entire channels (a channel is a 3D feature map). Sparse functions ¶ embedding Generate a simple lookup table that looks up embeddings in a fixed dictionary and size. embedding_bag Compute sums, means or maxes of bags of embeddings. one_hot Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. Distance functions ¶ pairwise_distance See torch.nn.PairwiseDistance for details cosine_similarity Returns cosine similarity between x1 and x2 , computed along dim. pdist Computes the p-norm distance between every pair of row vectors in the input. Loss functions ¶ binary_cross_entropy Measure Binary Cross Entropy between the target and input probabilities. binary_cross_entropy_with_logits Calculate Binary Cross Entropy between target and input logits. poisson_nll_loss Poisson negative log likelihood loss. cosine_embedding_loss See CosineEmbeddingLoss for details. cross_entropy Compute the cross entropy loss between input logits and target. ctc_loss Apply the Connectionist Temporal Classification loss. gaussian_nll_loss Gaussian negative log likelihood loss. hinge_embedding_loss See HingeEmbeddingLoss for details. kl_div Compute the KL Divergence loss. l1_loss Function that takes the mean element-wise absolute value difference. mse_loss Measures the element-wise mean squared error, with optional weighting. margin_ranking_loss See MarginRankingLoss for details. multilabel_margin_loss See MultiLabelMarginLoss for details. multilabel_soft_margin_loss See MultiLabelSoftMarginLoss for details. multi_margin_loss See MultiMarginLoss for details. nll_loss Compute the negative log likelihood loss. huber_loss Computes the Huber loss, with optional weighting. smooth_l1_loss Compute the Smooth L1 loss. soft_margin_loss See SoftMarginLoss for details. triplet_margin_loss Compute the triplet loss between given input tensors and a margin greater than 0. triplet_margin_with_distance_loss Compute the triplet margin loss for input tensors using a custom distance function. Vision functions ¶ pixel_shuffle Rearranges elements in a tensor of shape ( ∗ , C × r 2 , H , W ) (*, C \times r^2, H, W) ( ∗ , C × r 2 , H , W ) to a tensor of shape ( ∗ , C , H × r , W × r ) (*, C, H \times r, W \times r) ( ∗ , C , H × r , W × r ) , where r is the upscale_factor . pixel_unshuffle Reverses the PixelShuffle operation by rearranging elements in a tensor of shape ( ∗ , C , H × r , W × r ) (*, C, H \times r, W \times r) ( ∗ , C , H × r , W × r ) to a tensor of shape ( ∗ , C × r 2 , H , W ) (*, C \times r^2, H, W) ( ∗ , C × r 2 , H , W ) , where r is the downscale_factor . pad Pads tensor. interpolate Down/up samples the input. upsample Upsample input. upsample_nearest Upsamples the input, using nearest neighbours' pixel values. upsample_bilinear Upsamples the input, using bilinear upsampling. grid_sample Compute grid sample. affine_grid Generate 2D or 3D flow field (sampling grid), given a batch of affine matrices theta . DataParallel functions (multi-GPU, distributed) ¶ data_parallel ¶ torch.nn.parallel.data_parallel Evaluate module(input) in parallel across the GPUs given in device_ids.",11592,0,16,,1752175587.6024537
https://pytorch.org/docs/stable/notes/modules.html,Modules — PyTorch 2.7 documentation,"Modules ¶ PyTorch uses modules to represent neural networks. Modules are: Building blocks of stateful computation. PyTorch provides a robust library of modules and makes it simple to define new custom modules, allowing for
easy construction of elaborate, multi-layer neural networks. Tightly integrated with PyTorch’s autograd system. Modules make it simple to specify learnable parameters for PyTorch’s Optimizers to update. Easy to work with and transform. Modules are straightforward to save and restore, transfer between
CPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,
many topics in this note are elaborated on in other notes or tutorials, and links to many of those documents
are provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Initialization Module Hooks Advanced Features Distributed Training Profiling Performance Improving Performance with Quantization Improving Memory Usage with Pruning Parametrizations Transforming Modules with FX A Simple Custom Module ¶ To get started, let’s look at a simpler, custom version of PyTorch’s Linear module.
This module applies an affine transformation to its input. import torch from torch import nn class MyLinear ( nn . Module ): def __init__ ( self , in_features , out_features ): super () . __init__ () self . weight = nn . Parameter ( torch . randn ( in_features , out_features )) self . bias = nn . Parameter ( torch . randn ( out_features )) def forward ( self , input ): return ( input @ self . weight ) + self . bias This simple module has the following fundamental characteristics of modules: It inherits from the base Module class. All modules should subclass Module for composability with other modules. It defines some “state” that is used in computation. Here, the state consists of randomly-initialized weight and bias tensors that define the affine
transformation. Because each of these is defined as a Parameter , they are registered for the module and will automatically be tracked and returned from calls
to parameters() . Parameters can be
considered the “learnable” aspects of the module’s computation (more on this later). Note that modules
are not required to have state, and can also be stateless. It defines a forward() function that performs the computation. For this affine transformation module, the input
is matrix-multiplied with the weight parameter (using the @ short-hand notation) and added to the bias parameter to produce the output. More generally, the forward() implementation for a module can perform arbitrary
computation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be
constructed and called: m = MyLinear ( 4 , 3 ) sample_input = torch . randn ( 4 ) m ( sample_input ) : tensor ([ - 0.3037 , - 1.0413 , - 4.2057 ], grad_fn =< AddBackward0 > ) Note that the module itself is callable, and that calling it invokes its forward() function.
This name is in reference to the concepts of “forward pass” and “backward pass”, which apply to each module.
The “forward pass” is responsible for applying the computation represented by the module
to the given input(s) (as shown in the above snippet). The “backward pass” computes gradients of
module outputs with respect to its inputs, which can be used for “training” parameters through gradient
descent methods. PyTorch’s autograd system automatically takes care of this backward pass computation, so it
is not required to manually implement a backward() function for each module. The process of training
module parameters through successive forward / backward passes is covered in detail in Neural Network Training with Modules . The full set of parameters registered by the module can be iterated through via a call to parameters() or named_parameters() ,
where the latter includes each parameter’s name: for parameter in m . named_parameters (): print ( parameter ) : ( 'weight' , Parameter containing : tensor ([[ 1.0597 , 1.1796 , 0.8247 ], [ - 0.5080 , - 1.2635 , - 1.1045 ], [ 0.0593 , 0.2469 , - 1.4299 ], [ - 0.4926 , - 0.5457 , 0.4793 ]], requires_grad = True )) ( 'bias' , Parameter containing : tensor ([ 0.3634 , 0.2015 , - 0.8525 ], requires_grad = True )) In general, the parameters registered by a module are aspects of the module’s computation that should be
“learned”. A later section of this note shows how to update these parameters using one of PyTorch’s Optimizers.
Before we get to that, however, let’s first examine how modules can be composed with one another. Modules as Building Blocks ¶ Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.
The simplest way to do this is using the Sequential module. It allows us to chain together
multiple modules: net = nn . Sequential ( MyLinear ( 4 , 3 ), nn . ReLU (), MyLinear ( 3 , 1 ) ) sample_input = torch . randn ( 4 ) net ( sample_input ) : tensor ([ - 0.6749 ], grad_fn =< AddBackward0 > ) Note that Sequential automatically feeds the output of the first MyLinear module as input
into the ReLU , and the output of that as input into the second MyLinear module. As
shown, it is limited to in-order chaining of modules with a single input and output. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives
full flexibility on how submodules are used for a module’s computation. For example, here’s a simple neural network implemented as a custom module: import torch.nn.functional as F class Net ( nn . Module ): def __init__ ( self ): super () . __init__ () self . l0 = MyLinear ( 4 , 3 ) self . l1 = MyLinear ( 3 , 1 ) def forward ( self , x ): x = self . l0 ( x ) x = F . relu ( x ) x = self . l1 ( x ) return x This module is composed of two “children” or “submodules” ( l0 and l1 ) that define the layers of
the neural network and are utilized for computation within the module’s forward() method. Immediate
children of a module can be iterated through via a call to children() or named_children() : net = Net () for child in net . named_children (): print ( child ) : ( 'l0' , MyLinear ()) ( 'l1' , MyLinear ()) To go deeper than just the immediate children, modules() and named_modules() recursively iterate through a module and its child modules: class BigNet ( nn . Module ): def __init__ ( self ): super () . __init__ () self . l1 = MyLinear ( 5 , 4 ) self . net = Net () def forward ( self , x ): return self . net ( self . l1 ( x )) big_net = BigNet () for module in big_net . named_modules (): print ( module ) : ( '' , BigNet ( ( l1 ): MyLinear () ( net ): Net ( ( l0 ): MyLinear () ( l1 ): MyLinear () ) )) ( 'l1' , MyLinear ()) ( 'net' , Net ( ( l0 ): MyLinear () ( l1 ): MyLinear () )) ( 'net.l0' , MyLinear ()) ( 'net.l1' , MyLinear ()) Sometimes, it’s necessary for a module to dynamically define submodules.
The ModuleList and ModuleDict modules are useful here; they
register submodules from a list or dict: class DynamicNet ( nn . Module ): def __init__ ( self , num_layers ): super () . __init__ () self . linears = nn . ModuleList ( [ MyLinear ( 4 , 4 ) for _ in range ( num_layers )]) self . activations = nn . ModuleDict ({ 'relu' : nn . ReLU (), 'lrelu' : nn . LeakyReLU () }) self . final = MyLinear ( 4 , 1 ) def forward ( self , x , act ): for linear in self . linears : x = linear ( x ) x = self . activations [ act ]( x ) x = self . final ( x ) return x dynamic_net = DynamicNet ( 3 ) sample_input = torch . randn ( 4 ) output = dynamic_net ( sample_input , 'relu' ) For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.
This means that calls to parameters() and named_parameters() will
recursively include child parameters, allowing for convenient optimization of all parameters within the network: for parameter in dynamic_net . named_parameters (): print ( parameter ) : ( 'linears.0.weight' , Parameter containing : tensor ([[ - 1.2051 , 0.7601 , 1.1065 , 0.1963 ], [ 3.0592 , 0.4354 , 1.6598 , 0.9828 ], [ - 0.4446 , 0.4628 , 0.8774 , 1.6848 ], [ - 0.1222 , 1.5458 , 1.1729 , 1.4647 ]], requires_grad = True )) ( 'linears.0.bias' , Parameter containing : tensor ([ 1.5310 , 1.0609 , - 2.0940 , 1.1266 ], requires_grad = True )) ( 'linears.1.weight' , Parameter containing : tensor ([[ 2.1113 , - 0.0623 , - 1.0806 , 0.3508 ], [ - 0.0550 , 1.5317 , 1.1064 , - 0.5562 ], [ - 0.4028 , - 0.6942 , 1.5793 , - 1.0140 ], [ - 0.0329 , 0.1160 , - 1.7183 , - 1.0434 ]], requires_grad = True )) ( 'linears.1.bias' , Parameter containing : tensor ([ 0.0361 , - 0.9768 , - 0.3889 , 1.1613 ], requires_grad = True )) ( 'linears.2.weight' , Parameter containing : tensor ([[ - 2.6340 , - 0.3887 , - 0.9979 , 0.0767 ], [ - 0.3526 , 0.8756 , - 1.5847 , - 0.6016 ], [ - 0.3269 , - 0.1608 , 0.2897 , - 2.0829 ], [ 2.6338 , 0.9239 , 0.6943 , - 1.5034 ]], requires_grad = True )) ( 'linears.2.bias' , Parameter containing : tensor ([ 1.0268 , 0.4489 , - 0.9403 , 0.1571 ], requires_grad = True )) ( 'final.weight' , Parameter containing : tensor ([[ 0.2509 ], [ - 0.5052 ], [ 0.3088 ], [ - 1.4951 ]], requires_grad = True )) ( 'final.bias' , Parameter containing : tensor ([ 0.3381 ], requires_grad = True )) It’s also easy to move all parameters to a different device or change their precision using to() : # Move all parameters to a CUDA device dynamic_net . to ( device = 'cuda' ) # Change precision of all parameters dynamic_net . to ( dtype = torch . float64 ) dynamic_net ( torch . randn ( 5 , device = 'cuda' , dtype = torch . float64 )) : tensor ([ 6.5166 ], device = 'cuda:0' , dtype = torch . float64 , grad_fn =< AddBackward0 > ) More generally, an arbitrary function can be applied to a module and its submodules recursively by
using the apply() function. For example, to apply custom initialization to parameters
of a module and its submodules: # Define a function to initialize Linear weights. # Note that no_grad() is used here to avoid tracking this computation in the autograd graph. @torch . no_grad () def init_weights ( m ): if isinstance ( m , nn . Linear ): nn . init . xavier_normal_ ( m . weight ) m . bias . fill_ ( 0.0 ) # Apply the function recursively on the module and its submodules. dynamic_net . apply ( init_weights ) These examples show how elaborate neural networks can be formed through module composition and conveniently
manipulated. To allow for quick and easy construction of neural networks with minimal boilerplate, PyTorch
provides a large library of performant modules within the torch.nn namespace that perform common neural
network operations like pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Library of PyTorch-provided modules: torch.nn Defining neural net modules: https://pytorch.org/tutorials/beginner/examples_nn/polynomial_module.html Neural Network Training with Modules ¶ Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch’s
Optimizers from torch.optim : # Create the network (from previous section) and optimizer net = Net () optimizer = torch . optim . SGD ( net . parameters (), lr = 1e-4 , weight_decay = 1e-2 , momentum = 0.9 ) # Run a sample training loop that ""teaches"" the network # to output the constant zero function for _ in range ( 10000 ): input = torch . randn ( 4 ) output = net ( input ) loss = torch . abs ( output ) net . zero_grad () loss . backward () optimizer . step () # After training, switch the module to eval mode to do inference, compute performance metrics, etc. # (see discussion below for a description of training and evaluation modes) ... net . eval () ... In this simplified example, the network learns to simply output zero, as any non-zero output is “penalized” according
to its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the
key parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network’s
parameters are associated with it. A training loop… acquires an input, runs the network, computes a loss, zeros the network’s parameters’ gradients, calls loss.backward() to update the parameters’ gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network’s parameters have changed. In particular, examining the
value of l1 ‘s weight parameter shows that its values are now much closer to 0 (as may be expected): print ( net . l1 . weight ) : Parameter containing : tensor ([[ - 0.0013 ], [ 0.0030 ], [ - 0.0008 ]], requires_grad = True ) Note that the above process is done entirely while the network module is in “training mode”. Modules default to
training mode and can be switched between training and evaluation modes using train() and eval() . They can behave differently depending on which mode they are in. For example, the BatchNorm module maintains a running mean and variance during training that are not updated
when the module is in evaluation mode. In general, modules should be in training mode during training
and only switched to evaluation mode for inference or evaluation. Below is an example of a custom module
that behaves differently between the two modes: class ModalModule ( nn . Module ): def __init__ ( self ): super () . __init__ () def forward ( self , x ): if self . training : # Add a constant only in training mode. return x + 1. else : return x m = ModalModule () x = torch . randn ( 4 ) print ( 'training mode output: {} ' . format ( m ( x ))) : tensor ([ 1.6614 , 1.2669 , 1.0617 , 1.6213 , 0.5481 ]) m . eval () print ( 'evaluation mode output: {} ' . format ( m ( x ))) : tensor ([ 0.6614 , 0.2669 , 0.0617 , 0.6213 , - 0.4519 ]) Training neural networks can often be tricky. For more information, check out: Using Optimizers: https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html . Neural network training: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd: https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html Module State ¶ In the previous section, we demonstrated training a module’s “parameters”, or learnable aspects of computation.
Now, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. “state dictionary”): # Save the module torch . save ( net . state_dict (), 'net.pt' ) ... # Load the module later on new_net = Net () new_net . load_state_dict ( torch . load ( 'net.pt' )) : < All keys matched successfully > A module’s state_dict contains state that affects its computation. This includes, but is not limited to, the
module’s parameters. For some modules, it may be useful to have state beyond parameters that affects module
computation but is not learnable. For such cases, PyTorch provides the concept of “buffers”, both “persistent”
and “non-persistent”. Following is an overview of the various types of state a module can have: Parameters : learnable aspects of computation; contained within the state_dict Buffers : non-learnable aspects of computation Persistent buffers: contained within the state_dict (i.e. serialized when saving & loading) Non-persistent buffers: not contained within the state_dict (i.e. left out of serialization) As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want
the current value of the running mean to be considered part of the module’s state_dict so that it will be
restored when loading a serialized form of the module, but we don’t want it to be learnable.
This snippet shows how to use register_buffer() to accomplish this: class RunningMean ( nn . Module ): def __init__ ( self , num_features , momentum = 0.9 ): super () . __init__ () self . momentum = momentum self . register_buffer ( 'mean' , torch . zeros ( num_features )) def forward ( self , x ): self . mean = self . momentum * self . mean + ( 1.0 - self . momentum ) * x return self . mean Now, the current value of the running mean is considered part of the module’s state_dict and will be properly restored when loading the module from disk: m = RunningMean ( 4 ) for _ in range ( 10 ): input = torch . randn ( 4 ) m ( input ) print ( m . state_dict ()) : OrderedDict ([( 'mean' , tensor ([ 0.1041 , - 0.1113 , - 0.0647 , 0.1515 ]))])) # Serialized form will contain the 'mean' tensor torch . save ( m . state_dict (), 'mean.pt' ) m_loaded = RunningMean ( 4 ) m_loaded . load_state_dict ( torch . load ( 'mean.pt' )) assert ( torch . all ( m . mean == m_loaded . mean )) As mentioned previously, buffers can be left out of the module’s state_dict by marking them as non-persistent: self . register_buffer ( 'unserialized_thing' , torch . randn ( 5 ), persistent = False ) Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with to() : # Moves all module parameters and buffers to the specified device / dtype m . to ( device = 'cuda' , dtype = torch . float64 ) Buffers of a module can be iterated over using buffers() or named_buffers() . for buffer in m . named_buffers (): print ( buffer ) The following class demonstrates the various ways of registering parameters and buffers within a module: class StatefulModule ( nn . Module ): def __init__ ( self ): super () . __init__ () # Setting a nn.Parameter as an attribute of the module automatically registers the tensor # as a parameter of the module. self . param1 = nn . Parameter ( torch . randn ( 2 )) # Alternative string-based way to register a parameter. self . register_parameter ( 'param2' , nn . Parameter ( torch . randn ( 3 ))) # Reserves the ""param3"" attribute as a parameter, preventing it from being set to anything # except a parameter. ""None"" entries like this will not be present in the module's state_dict. self . register_parameter ( 'param3' , None ) # Registers a list of parameters. self . param_list = nn . ParameterList ([ nn . Parameter ( torch . randn ( 2 )) for i in range ( 3 )]) # Registers a dictionary of parameters. self . param_dict = nn . ParameterDict ({ 'foo' : nn . Parameter ( torch . randn ( 3 )), 'bar' : nn . Parameter ( torch . randn ( 4 )) }) # Registers a persistent buffer (one that appears in the module's state_dict). self . register_buffer ( 'buffer1' , torch . randn ( 4 ), persistent = True ) # Registers a non-persistent buffer (one that does not appear in the module's state_dict). self . register_buffer ( 'buffer2' , torch . randn ( 5 ), persistent = False ) # Reserves the ""buffer3"" attribute as a buffer, preventing it from being set to anything # except a buffer. ""None"" entries like this will not be present in the module's state_dict. self . register_buffer ( 'buffer3' , None ) # Adding a submodule registers its parameters as parameters of the module. self . linear = nn . Linear ( 2 , 3 ) m = StatefulModule () # Save and load state_dict. torch . save ( m . state_dict (), 'state.pt' ) m_loaded = StatefulModule () m_loaded . load_state_dict ( torch . load ( 'state.pt' )) # Note that non-persistent buffer ""buffer2"" and reserved attributes ""param3"" and ""buffer3"" do # not appear in the state_dict. print ( m_loaded . state_dict ()) : OrderedDict ([( 'param1' , tensor ([ - 0.0322 , 0.9066 ])), ( 'param2' , tensor ([ - 0.4472 , 0.1409 , 0.4852 ])), ( 'buffer1' , tensor ([ 0.6949 , - 0.1944 , 1.2911 , - 2.1044 ])), ( 'param_list.0' , tensor ([ 0.4202 , - 0.1953 ])), ( 'param_list.1' , tensor ([ 1.5299 , - 0.8747 ])), ( 'param_list.2' , tensor ([ - 1.6289 , 1.4898 ])), ( 'param_dict.bar' , tensor ([ - 0.6434 , 1.5187 , 0.0346 , - 0.4077 ])), ( 'param_dict.foo' , tensor ([ - 0.0845 , - 1.4324 , 0.7022 ])), ( 'linear.weight' , tensor ([[ - 0.3915 , - 0.6176 ], [ 0.6062 , - 0.5992 ], [ 0.4452 , - 0.2843 ]])), ( 'linear.bias' , tensor ([ - 0.3710 , - 0.0795 , - 0.3947 ]))]) For more information, check out: Saving and loading: https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics: https://pytorch.org/docs/main/notes/serialization.html What is a state dict? https://pytorch.org/tutorials/recipes/recipes/what_is_state_dict.html Module Initialization ¶ By default, parameters and floating-point buffers for modules provided by torch.nn are initialized during
module instantiation as 32-bit floating point values on the CPU using an initialization scheme determined to
perform well historically for the module type. For certain use cases, it may be desired to initialize with a different
dtype, device (e.g. GPU), or initialization technique. Examples: # Initialize module directly onto GPU. m = nn . Linear ( 5 , 3 , device = 'cuda' ) # Initialize module with 16-bit floating point parameters. m = nn . Linear ( 5 , 3 , dtype = torch . half ) # Skip default parameter initialization and perform custom (e.g. orthogonal) initialization. m = torch . nn . utils . skip_init ( nn . Linear , 5 , 3 ) nn . init . orthogonal_ ( m . weight ) Note that the device and dtype options demonstrated above also apply to any floating-point buffers registered
for the module: m = nn . BatchNorm2d ( 3 , dtype = torch . half ) print ( m . running_mean ) : tensor ([ 0. , 0. , 0. ], dtype = torch . float16 ) While module writers can use any device or dtype to initialize parameters in their custom modules, good practice is
to use dtype=torch.float and device='cpu' by default as well. Optionally, you can provide full flexibility
in these areas for your custom module by conforming to the convention demonstrated above that all torch.nn modules follow: Provide a device constructor kwarg that applies to any parameters / buffers registered by the module. Provide a dtype constructor kwarg that applies to any parameters / floating-point buffers registered by
the module. Only use initialization functions (i.e. functions from torch.nn.init ) on parameters and buffers within the
module’s constructor. Note that this is only required to use skip_init() ; see this page for an explanation. For more information, check out: Skipping module parameter initialization: https://pytorch.org/tutorials/prototype/skip_param_init.html Module Hooks ¶ In Neural Network Training with Modules , we demonstrated the training process for a module, which iteratively
performs forward and backward passes, updating module parameters each iteration. For more control
over this process, PyTorch provides “hooks” that can perform arbitrary computation during a forward or backward
pass, even modifying how the pass is done if desired. Some useful examples for this functionality include
debugging, visualizing activations, examining gradients in-depth, etc. Hooks can be added to modules
you haven’t written yourself, meaning this functionality can be applied to third-party or PyTorch-provided modules. PyTorch provides two types of hooks for modules: Forward hooks are called during the forward pass. They can be installed for a given module with register_forward_pre_hook() and register_forward_hook() .
These hooks will be called respectively just before the forward function is called and just after it is called.
Alternatively, these hooks can be installed globally for all modules with the analogous register_module_forward_pre_hook() and register_module_forward_hook() functions. Backward hooks are called during the backward pass. They can be installed with register_full_backward_pre_hook() and register_full_backward_hook() .
These hooks will be called when the backward for this Module has been computed. register_full_backward_pre_hook() will allow the user to access the gradients for outputs
while register_full_backward_hook() will allow the user to access the gradients
both the inputs and outputs. Alternatively, they can be installed globally for all modules with register_module_full_backward_hook() and register_module_full_backward_pre_hook() . All hooks allow the user to return an updated value that will be used throughout the remaining computation.
Thus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or
modify some inputs/outputs without having to change the module’s forward() function. Below is an example demonstrating usage of forward and backward hooks: torch . manual_seed ( 1 ) def forward_pre_hook ( m , inputs ): # Allows for examination and modification of the input before the forward pass. # Note that inputs are always wrapped in a tuple. input = inputs [ 0 ] return input + 1. def forward_hook ( m , inputs , output ): # Allows for examination of inputs / outputs and modification of the outputs # after the forward pass. Note that inputs are always wrapped in a tuple while outputs # are passed as-is. # Residual computation a la ResNet. return output + inputs [ 0 ] def backward_hook ( m , grad_inputs , grad_outputs ): # Allows for examination of grad_inputs / grad_outputs and modification of # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and # grad_outputs are always wrapped in tuples. new_grad_inputs = [ torch . ones_like ( gi ) * 42. for gi in grad_inputs ] return new_grad_inputs # Create sample module & input. m = nn . Linear ( 3 , 3 ) x = torch . randn ( 2 , 3 , requires_grad = True ) # ==== Demonstrate forward hooks. ==== # Run input through module before and after adding hooks. print ( 'output with no forward hooks: {} ' . format ( m ( x ))) : output with no forward hooks : tensor ([[ - 0.5059 , - 0.8158 , 0.2390 ], [ - 0.0043 , 0.4724 , - 0.1714 ]], grad_fn =< AddmmBackward > ) # Note that the modified input results in a different output. forward_pre_hook_handle = m . register_forward_pre_hook ( forward_pre_hook ) print ( 'output with forward pre hook: {} ' . format ( m ( x ))) : output with forward pre hook : tensor ([[ - 0.5752 , - 0.7421 , 0.4942 ], [ - 0.0736 , 0.5461 , 0.0838 ]], grad_fn =< AddmmBackward > ) # Note the modified output. forward_hook_handle = m . register_forward_hook ( forward_hook ) print ( 'output with both forward hooks: {} ' . format ( m ( x ))) : output with both forward hooks : tensor ([[ - 1.0980 , 0.6396 , 0.4666 ], [ 0.3634 , 0.6538 , 1.0256 ]], grad_fn =< AddBackward0 > ) # Remove hooks; note that the output here matches the output before adding hooks. forward_pre_hook_handle . remove () forward_hook_handle . remove () print ( 'output after removing forward hooks: {} ' . format ( m ( x ))) : output after removing forward hooks : tensor ([[ - 0.5059 , - 0.8158 , 0.2390 ], [ - 0.0043 , 0.4724 , - 0.1714 ]], grad_fn =< AddmmBackward > ) # ==== Demonstrate backward hooks. ==== m ( x ) . sum () . backward () print ( 'x.grad with no backwards hook: {} ' . format ( x . grad )) : x . grad with no backwards hook : tensor ([[ 0.4497 , - 0.5046 , 0.3146 ], [ 0.4497 , - 0.5046 , 0.3146 ]]) # Clear gradients before running backward pass again. m . zero_grad () x . grad . zero_ () m . register_full_backward_hook ( backward_hook ) m ( x ) . sum () . backward () print ( 'x.grad with backwards hook: {} ' . format ( x . grad )) : x . grad with backwards hook : tensor ([[ 42. , 42. , 42. ], [ 42. , 42. , 42. ]]) Advanced Features ¶ PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities
are available for custom-written modules, with the small caveat that certain features may require modules to conform
to particular constraints in order to be supported. In-depth discussion of these features and the corresponding
requirements can be found in the links below. Distributed Training ¶ Various methods for distributed training exist within PyTorch, both for scaling up training using multiple GPUs
as well as training across multiple machines. Check out the distributed training overview page for
detailed information on how to utilize these. Profiling Performance ¶ The PyTorch Profiler can be useful for identifying
performance bottlenecks within your models. It measures and outputs performance characteristics for
both memory usage and time spent. Improving Performance with Quantization ¶ Applying quantization techniques to modules can improve performance and memory usage by utilizing lower
bitwidths than floating-point precision. Check out the various PyTorch-provided mechanisms for quantization here . Improving Memory Usage with Pruning ¶ Large deep learning models are often over-parametrized, resulting in high memory usage. To combat this, PyTorch
provides mechanisms for model pruning, which can help reduce memory usage while maintaining task accuracy. The Pruning tutorial describes how to utilize
the pruning techniques PyTorch provides or define custom pruning techniques as necessary. Parametrizations ¶ For certain applications, it can be beneficial to constrain the parameter space during model training. For example,
enforcing orthogonality of the learned parameters can improve convergence for RNNs. PyTorch provides a mechanism for
applying parametrizations such as this, and
further allows for custom constraints to be defined. Transforming Modules with FX ¶ The FX component of PyTorch provides a flexible way to transform
modules by operating directly on module computation graphs. This can be used to programmatically generate or
manipulate modules for a broad array of use cases. To explore FX, check out these examples of using FX for convolution + batch norm fusion and CPU performance analysis .",29673,24,17,"import torch
from torch import nn

class MyLinear(nn.Module):
  def __init__(self, in_features, out_features):
    super().__init__()
    self.weight = nn.Parameter(torch.randn(in_features, out_features))
    self.bias = nn.Parameter(torch.randn(out_features))

  def forward(self, input):
    return (input @ self.weight) + self.bias
---
m = MyLinear(4, 3)
sample_input = torch.randn(4)
m(sample_input)
: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)
---
for parameter in m.named_parameters():
  print(parameter)
: ('weight', Parameter containing:
tensor([[ 1.0597,  1.1796,  0.8247],
        [-0.5080, -1.2635, -1.1045],
        [ 0.0593,  0.2469, -1.4299],
        [-0.4926, -0.5457,  0.4793]], requires_grad=True))
('bias', Parameter containing:
tensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))
---
net = nn.Sequential(
  MyLinear(4, 3),
  nn.ReLU(),
  MyLinear(3, 1)
)

sample_input = torch.randn(4)
net(sample_input)
: tensor([-0.6749], grad_fn=<AddBackward0>)
---
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super().__init__()
    self.l0 = MyLinear(4, 3)
    self.l1 = MyLinear(3, 1)
  def forward(self, x):
    x = self.l0(x)
    x = F.relu(x)
    x = self.l1(x)
    return x
---
net = Net()
for child in net.named_children():
  print(child)
: ('l0', MyLinear())
('l1', MyLinear())
---
class BigNet(nn.Module):
  def __init__(self):
    super().__init__()
    self.l1 = MyLinear(5, 4)
    self.net = Net()
  def forward(self, x):
    return self.net(self.l1(x))

big_net = BigNet()
for module in big_net.named_modules():
  print(module)
: ('', BigNet(
  (l1): MyLinear()
  (net): Net(
    (l0): MyLinear()
    (l1): MyLinear()
  )
))
('l1', MyLinear())
('net', Net(
  (l0): MyLinear()
  (l1): MyLinear()
))
('net.l0', MyLinear())
('net.l1', MyLinear())
---
class DynamicNet(nn.Module):
  def __init__(self, num_layers):
    super().__init__()
    self.linears = nn.ModuleList(
      [MyLinear(4, 4) for _ in range(num_layers)])
    self.activations = nn.ModuleDict({
      'relu': nn.ReLU(),
      'lrelu': nn.LeakyReLU()
    })
    self.final = MyLinear(4, 1)
  def forward(self, x, act):
    for linear in self.linears:
      x = linear(x)
      x = self.activations[act](x)
    x = self.final(x)
    return x

dynamic_net = DynamicNet(3)
sample_input = torch.randn(4)
output = dynamic_net(sample_input, 'relu')
---
for parameter in dynamic_net.named_parameters():
  print(parameter)
: ('linears.0.weight', Parameter containing:
tensor([[-1.2051,  0.7601,  1.1065,  0.1963],
        [ 3.0592,  0.4354,  1.6598,  0.9828],
        [-0.4446,  0.4628,  0.8774,  1.6848],
        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))
('linears.0.bias', Parameter containing:
tensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))
('linears.1.weight', Parameter containing:
tensor([[ 2.1113, -0.0623, -1.0806,  0.3508],
        [-0.0550,  1.5317,  1.1064, -0.5562],
        [-0.4028, -0.6942,  1.5793, -1.0140],
        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))
('linears.1.bias', Parameter containing:
tensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))
('linears.2.weight', Parameter containing:
tensor([[-2.6340, -0.3887, -0.9979,  0.0767],
        [-0.3526,  0.8756, -1.5847, -0.6016],
        [-0.3269, -0.1608,  0.2897, -2.0829],
        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))
('linears.2.bias', Parameter containing:
tensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))
('final.weight', Parameter containing:
tensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))
('final.bias', Parameter containing:
tensor([0.3381], requires_grad=True))
---
# Move all parameters to a CUDA device
dynamic_net.to(device='cuda')

# Change precision of all parameters
dynamic_net.to(dtype=torch.float64)

dynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))
: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)
---
# Define a function to initialize Linear weights.
# Note that no_grad() is used here to avoid tracking this computation in the autograd graph.
@torch.no_grad()
def init_weights(m):
  if isinstance(m, nn.Linear):
    nn.init.xavier_normal_(m.weight)
    m.bias.fill_(0.0)

# Apply the function recursively on the module and its submodules.
dynamic_net.apply(init_weights)
---
# Create the network (from previous section) and optimizer
net = Net()
optimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)

# Run a sample training loop that ""teaches"" the network
# to output the constant zero function
for _ in range(10000):
  input = torch.randn(4)
  output = net(input)
  loss = torch.abs(output)
  net.zero_grad()
  loss.backward()
  optimizer.step()

# After training, switch the module to eval mode to do inference, compute performance metrics, etc.
# (see discussion below for a description of training and evaluation modes)
...
net.eval()
...
---
print(net.l1.weight)
: Parameter containing:
tensor([[-0.0013],
        [ 0.0030],
        [-0.0008]], requires_grad=True)
---
class ModalModule(nn.Module):
  def __init__(self):
    super().__init__()

  def forward(self, x):
    if self.training:
      # Add a constant only in training mode.
      return x + 1.
    else:
      return x


m = ModalModule()
x = torch.randn(4)

print('training mode output: {}'.format(m(x)))
: tensor([1.6614, 1.2669, 1.0617, 1.6213, 0.5481])

m.eval()
print('evaluation mode output: {}'.format(m(x)))
: tensor([ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519])
---
# Save the module
torch.save(net.state_dict(), 'net.pt')

...

# Load the module later on
new_net = Net()
new_net.load_state_dict(torch.load('net.pt'))
: <All keys matched successfully>
---
class RunningMean(nn.Module):
  def __init__(self, num_features, momentum=0.9):
    super().__init__()
    self.momentum = momentum
    self.register_buffer('mean', torch.zeros(num_features))
  def forward(self, x):
    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x
    return self.mean
---
m = RunningMean(4)
for _ in range(10):
  input = torch.randn(4)
  m(input)

print(m.state_dict())
: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))

# Serialized form will contain the 'mean' tensor
torch.save(m.state_dict(), 'mean.pt')

m_loaded = RunningMean(4)
m_loaded.load_state_dict(torch.load('mean.pt'))
assert(torch.all(m.mean == m_loaded.mean))
---
self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)
---
# Moves all module parameters and buffers to the specified device / dtype
m.to(device='cuda', dtype=torch.float64)
---
for buffer in m.named_buffers():
  print(buffer)
---
class StatefulModule(nn.Module):
  def __init__(self):
    super().__init__()
    # Setting a nn.Parameter as an attribute of the module automatically registers the tensor
    # as a parameter of the module.
    self.param1 = nn.Parameter(torch.randn(2))

    # Alternative string-based way to register a parameter.
    self.register_parameter('param2', nn.Parameter(torch.randn(3)))

    # Reserves the ""param3"" attribute as a parameter, preventing it from being set to anything
    # except a parameter. ""None"" entries like this will not be present in the module's state_dict.
    self.register_parameter('param3', None)

    # Registers a list of parameters.
    self.param_list = nn.ParameterList([nn.Parameter(torch.randn(2)) for i in range(3)])

    # Registers a dictionary of parameters.
    self.param_dict = nn.ParameterDict({
      'foo': nn.Parameter(torch.randn(3)),
      'bar': nn.Parameter(torch.randn(4))
    })

    # Registers a persistent buffer (one that appears in the module's state_dict).
    self.register_buffer('buffer1', torch.randn(4), persistent=True)

    # Registers a non-persistent buffer (one that does not appear in the module's state_dict).
    self.register_buffer('buffer2', torch.randn(5), persistent=False)

    # Reserves the ""buffer3"" attribute as a buffer, preventing it from being set to anything
    # except a buffer. ""None"" entries like this will not be present in the module's state_dict.
    self.register_buffer('buffer3', None)

    # Adding a submodule registers its parameters as parameters of the module.
    self.linear = nn.Linear(2, 3)

m = StatefulModule()

# Save and load state_dict.
torch.save(m.state_dict(), 'state.pt')
m_loaded = StatefulModule()
m_loaded.load_state_dict(torch.load('state.pt'))

# Note that non-persistent buffer ""buffer2"" and reserved attributes ""param3"" and ""buffer3"" do
# not appear in the state_dict.
print(m_loaded.state_dict())
: OrderedDict([('param1', tensor([-0.0322,  0.9066])),
               ('param2', tensor([-0.4472,  0.1409,  0.4852])),
               ('buffer1', tensor([ 0.6949, -0.1944,  1.2911, -2.1044])),
               ('param_list.0', tensor([ 0.4202, -0.1953])),
               ('param_list.1', tensor([ 1.5299, -0.8747])),
               ('param_list.2', tensor([-1.6289,  1.4898])),
               ('param_dict.bar', tensor([-0.6434,  1.5187,  0.0346, -0.4077])),
               ('param_dict.foo', tensor([-0.0845, -1.4324,  0.7022])),
               ('linear.weight', tensor([[-0.3915, -0.6176],
                                         [ 0.6062, -0.5992],
                                         [ 0.4452, -0.2843]])),
               ('linear.bias', tensor([-0.3710, -0.0795, -0.3947]))])
---
# Initialize module directly onto GPU.
m = nn.Linear(5, 3, device='cuda')

# Initialize module with 16-bit floating point parameters.
m = nn.Linear(5, 3, dtype=torch.half)

# Skip default parameter initialization and perform custom (e.g. orthogonal) initialization.
m = torch.nn.utils.skip_init(nn.Linear, 5, 3)
nn.init.orthogonal_(m.weight)
---
m = nn.BatchNorm2d(3, dtype=torch.half)
print(m.running_mean)
: tensor([0., 0., 0.], dtype=torch.float16)
---
torch.manual_seed(1)

def forward_pre_hook(m, inputs):
  # Allows for examination and modification of the input before the forward pass.
  # Note that inputs are always wrapped in a tuple.
  input = inputs[0]
  return input + 1.

def forward_hook(m, inputs, output):
  # Allows for examination of inputs / outputs and modification of the outputs
  # after the forward pass. Note that inputs are always wrapped in a tuple while outputs
  # are passed as-is.

  # Residual computation a la ResNet.
  return output + inputs[0]

def backward_hook(m, grad_inputs, grad_outputs):
  # Allows for examination of grad_inputs / grad_outputs and modification of
  # grad_inputs used in the rest of the backwards pass. Note that grad_inputs and
  # grad_outputs are always wrapped in tuples.
  new_grad_inputs = [torch.ones_like(gi) * 42. for gi in grad_inputs]
  return new_grad_inputs

# Create sample module & input.
m = nn.Linear(3, 3)
x = torch.randn(2, 3, requires_grad=True)

# ==== Demonstrate forward hooks. ====
# Run input through module before and after adding hooks.
print('output with no forward hooks: {}'.format(m(x)))
: output with no forward hooks: tensor([[-0.5059, -0.8158,  0.2390],
                                        [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)

# Note that the modified input results in a different output.
forward_pre_hook_handle = m.register_forward_pre_hook(forward_pre_hook)
print('output with forward pre hook: {}'.format(m(x)))
: output with forward pre hook: tensor([[-0.5752, -0.7421,  0.4942],
                                        [-0.0736,  0.5461,  0.0838]], grad_fn=<AddmmBackward>)

# Note the modified output.
forward_hook_handle = m.register_forward_hook(forward_hook)
print('output with both forward hooks: {}'.format(m(x)))
: output with both forward hooks: tensor([[-1.0980,  0.6396,  0.4666],
                                          [ 0.3634,  0.6538,  1.0256]], grad_fn=<AddBackward0>)

# Remove hooks; note that the output here matches the output before adding hooks.
forward_pre_hook_handle.remove()
forward_hook_handle.remove()
print('output after removing forward hooks: {}'.format(m(x)))
: output after removing forward hooks: tensor([[-0.5059, -0.8158,  0.2390],
                                               [-0.0043,  0.4724, -0.1714]], grad_fn=<AddmmBackward>)

# ==== Demonstrate backward hooks. ====
m(x).sum().backward()
print('x.grad with no backwards hook: {}'.format(x.grad))
: x.grad with no backwards hook: tensor([[ 0.4497, -0.5046,  0.3146],
                                         [ 0.4497, -0.5046,  0.3146]])

# Clear gradients before running backward pass again.
m.zero_grad()
x.grad.zero_()

m.register_full_backward_hook(backward_hook)
m(x).sum().backward()
print('x.grad with backwards hook: {}'.format(x.grad))
: x.grad with backwards hook: tensor([[42., 42., 42.],
                                      [42., 42., 42.]])",1752175588.7333221
https://pytorch.org/docs/stable/xpu.html,torch.xpu — PyTorch 2.7 documentation,"torch.xpu ¶ This package introduces support for the XPU backend, specifically tailored for
Intel GPU optimization. This package is lazily initialized, so you can always import it, and use is_available() to determine if your system supports XPU. StreamContext Context-manager that selects a given stream. current_device Return the index of a currently selected device. current_stream Return the currently selected Stream for a given device. device Context-manager that changes the selected device. device_count Return the number of XPU device available. device_of Context-manager that changes the current device to that of given object. get_arch_list Return list XPU architectures this library was compiled for. get_device_capability Get the xpu capability of a device. get_device_name Get the name of a device. get_device_properties Get the properties of a device. get_gencode_flags Return XPU AOT(ahead-of-time) build flags this library was compiled with. get_stream_from_external Return a Stream from an external SYCL queue. init Initialize PyTorch's XPU state. is_available Return a bool indicating if XPU is currently available. is_initialized Return whether PyTorch's XPU state has been initialized. set_device Set the current device. set_stream Set the current stream.This is a wrapper API to set the stream. stream Wrap around the Context-manager StreamContext that selects a given stream. synchronize Wait for all kernels in all streams on a XPU device to complete. Random Number Generator ¶ get_rng_state Return the random number generator state of the specified GPU as a ByteTensor. get_rng_state_all Return a list of ByteTensor representing the random number states of all devices. initial_seed Return the current random seed of the current GPU. manual_seed Set the seed for generating random numbers for the current GPU. manual_seed_all Set the seed for generating random numbers on all GPUs. seed Set the seed for generating random numbers to a random number for the current GPU. seed_all Set the seed for generating random numbers to a random number on all GPUs. set_rng_state Set the random number generator state of the specified GPU. set_rng_state_all Set the random number generator state of all devices. Streams and events ¶ Event Wrapper around a XPU event. Stream Wrapper around a XPU stream. Memory management ¶ empty_cache Release all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application. max_memory_allocated Return the maximum GPU memory occupied by tensors in bytes for a given device. max_memory_reserved Return the maximum GPU memory managed by the caching allocator in bytes for a given device. mem_get_info Return the global free and total GPU memory for a given device. memory_allocated Return the current GPU memory occupied by tensors in bytes for a given device. memory_reserved Return the current GPU memory managed by the caching allocator in bytes for a given device. memory_stats Return a dictionary of XPU memory allocator statistics for a given device. memory_stats_as_nested_dict Return the result of memory_stats() as a nested dictionary. reset_accumulated_memory_stats Reset the ""accumulated"" (historical) stats tracked by the XPU memory allocator. reset_peak_memory_stats Reset the ""peak"" stats tracked by the XPU memory allocator.",3338,0,7,,1752175589.804595
https://pytorch.org/docs/stable/notes/broadcasting.html,Broadcasting semantics — PyTorch 2.7 documentation,"Broadcasting semantics ¶ Many PyTorch operations support NumPy’s broadcasting semantics.
See https://numpy.org/doc/stable/user/basics.broadcasting.html for details. In short, if a PyTorch operation supports broadcast, then its Tensor arguments can be
automatically expanded to be of equal sizes (without making copies of the data). General semantics ¶ Two tensors are “broadcastable” if the following rules hold: Each tensor has at least one dimension. When iterating over the dimension sizes, starting at the trailing dimension,
the dimension sizes must either be equal, one of them is 1, or one of them
does not exist. For Example: >>> x = torch . empty ( 5 , 7 , 3 ) >>> y = torch . empty ( 5 , 7 , 3 ) # same shapes are always broadcastable (i.e. the above rules always hold) >>> x = torch . empty (( 0 ,)) >>> y = torch . empty ( 2 , 2 ) # x and y are not broadcastable, because x does not have at least 1 dimension # can line up trailing dimensions >>> x = torch . empty ( 5 , 3 , 4 , 1 ) >>> y = torch . empty ( 3 , 1 , 1 ) # x and y are broadcastable. # 1st trailing dimension: both have size 1 # 2nd trailing dimension: y has size 1 # 3rd trailing dimension: x size == y size # 4th trailing dimension: y dimension doesn't exist # but: >>> x = torch . empty ( 5 , 2 , 4 , 1 ) >>> y = torch . empty ( 3 , 1 , 1 ) # x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3 If two tensors x , y are “broadcastable”, the resulting tensor size
is calculated as follows: If the number of dimensions of x and y are not equal, prepend 1
to the dimensions of the tensor with fewer dimensions to make them equal length. Then, for each dimension size, the resulting dimension size is the max of the sizes of x and y along that dimension. For Example: # can line up trailing dimensions to make reading easier >>> x = torch . empty ( 5 , 1 , 4 , 1 ) >>> y = torch . empty ( 3 , 1 , 1 ) >>> ( x + y ) . size () torch . Size ([ 5 , 3 , 4 , 1 ]) # but not necessary: >>> x = torch . empty ( 1 ) >>> y = torch . empty ( 3 , 1 , 7 ) >>> ( x + y ) . size () torch . Size ([ 3 , 1 , 7 ]) >>> x = torch . empty ( 5 , 2 , 4 , 1 ) >>> y = torch . empty ( 3 , 1 , 1 ) >>> ( x + y ) . size () RuntimeError : The size of tensor a ( 2 ) must match the size of tensor b ( 3 ) at non - singleton dimension 1 In-place semantics ¶ One complication is that in-place operations do not allow the in-place tensor to change shape
as a result of the broadcast. For Example: >>> x = torch . empty ( 5 , 3 , 4 , 1 ) >>> y = torch . empty ( 3 , 1 , 1 ) >>> ( x . add_ ( y )) . size () torch.Size([5, 3, 4, 1]) # but: >>> x = torch . empty ( 1 , 3 , 1 ) >>> y = torch . empty ( 3 , 1 , 7 ) >>> ( x . add_ ( y )) . size () RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2. Backwards compatibility ¶ Prior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes,
as long as the number of elements in each tensor was equal.  The pointwise operation would then be carried
out by viewing each tensor as 1-dimensional.  PyTorch now supports broadcasting and the “1-dimensional”
pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are
not broadcastable, but have the same number of elements. Note that the introduction of broadcasting can cause backwards incompatible changes in the case where
two tensors do not have the same shape, but are broadcastable and have the same number of elements.
For Example: >>> torch . add ( torch . ones ( 4 , 1 ), torch . randn ( 4 )) would previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]).
In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist,
you may set torch.utils.backcompat.broadcast_warning.enabled to True , which will generate a python warning
in such cases. For Example: >>> torch . utils . backcompat . broadcast_warning . enabled = True >>> torch . add ( torch . ones ( 4 , 1 ), torch . ones ( 4 )) __main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements. Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.",4344,5,7,">>> x=torch.empty(5,7,3)
>>> y=torch.empty(5,7,3)
# same shapes are always broadcastable (i.e. the above rules always hold)

>>> x=torch.empty((0,))
>>> y=torch.empty(2,2)
# x and y are not broadcastable, because x does not have at least 1 dimension

# can line up trailing dimensions
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are broadcastable.
# 1st trailing dimension: both have size 1
# 2nd trailing dimension: y has size 1
# 3rd trailing dimension: x size == y size
# 4th trailing dimension: y dimension doesn't exist

# but:
>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(  3,1,1)
# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3
---
# can line up trailing dimensions to make reading easier
>>> x=torch.empty(5,1,4,1)
>>> y=torch.empty(  3,1,1)
>>> (x+y).size()
torch.Size([5, 3, 4, 1])

# but not necessary:
>>> x=torch.empty(1)
>>> y=torch.empty(3,1,7)
>>> (x+y).size()
torch.Size([3, 1, 7])

>>> x=torch.empty(5,2,4,1)
>>> y=torch.empty(3,1,1)
>>> (x+y).size()
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1
---
>>> x=torch.empty(5,3,4,1)
>>> y=torch.empty(3,1,1)
>>> (x.add_(y)).size()
torch.Size([5, 3, 4, 1])

# but:
>>> x=torch.empty(1,3,1)
>>> y=torch.empty(3,1,7)
>>> (x.add_(y)).size()
RuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.
---
>>> torch.add(torch.ones(4,1), torch.randn(4))
---
>>> torch.utils.backcompat.broadcast_warning.enabled=True
>>> torch.add(torch.ones(4,1), torch.ones(4))
__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.
Changing behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.",1752175590.8623693
https://pytorch.org/docs/stable/community/governance.html,PyTorch Governance | Mechanics — PyTorch 2.7 documentation,"PyTorch Governance | Mechanics ¶ Summary ¶ PyTorch adopts a technical governance structure that is hierarchical. A community of contributors who file issues, make pull requests,
and contribute to the project. A small set of module maintainers drive each module of the PyTorch
project. They are overseen by core maintainers , who drive the
overall project direction. The core maintainers have a lead core maintainer who is the catch-all decision maker. All maintainers are expected to have a strong bias towards
PyTorch’s design philosophy. Beyond the maintainers, the community is encouraged to contribute,
file issues, make proposals, review pull requests and be present
in the community. Given contributions and willingness to invest,
anyone can be accepted as a maintainer and provided write access
or ownership of parts of the codebase. Technical governance is strictly separated from business governance.
Separating technical from business governance ensures that there is
no way for any person or company to “buy their way into” the
technical guidance of the project. Additionally, membership in
the technical governance process is for individuals , not companies.
That is, there are no seats reserved for specific companies, and
membership is associated with the person rather than the company
employing that person. Module Maintainers ¶ Modules are defined as GitHub repositories within the PyTorch org,
or as directories within the core repository pytorch/pytorch .
Each module will have its own maintainer group. Maintainer
groups are responsible for reviewing and approving commits,
improving design, and changing the scope of the module.
Each maintainer group may adopt its own rules and procedures
for making decisions (majority vote being default). Module
maintainers have the right to dispute decisions made by other
module maintainers – especially if it affects them. When
disputes are made, the module maintainer group should
provide a reasonable and public explanation of the dispute,
the relevant arguments, and the resolution. In the exceptional
cases where module maintainers cannot come to a conclusion
themselves, they will escalate to core maintainers for review.
The escalations are resolved by the core maintainers in
accordance with their rules and procedures. Each maintainer group should publish publicly available
communication for their module (a vision, rough roadmap,
design docs, any disputes and dispute resolutions) so that
contributors and other interested parties understand the
future direction of the project and can participate in discussion. Responsibilities of the maintainer includes: Triaging high priority issues of the module Triaging and reviewing and landing high priority pull requests of the module Supporting public documentation related to the module Running public developer meetings Core Maintainers ¶ The core maintainers are expected to have a deep understanding
of the PyTorch code base and design philosophies. Their responsibilities
include: Articulating a cohesive long-term vision for the project Negotiating and resolving contentious issues in ways
acceptable to all parties involved Receiving broad requests for changes from stakeholders of
PyTorch and evaluating / accepting them (small module-level
requests are handled by module maintainers) The core maintainers as a group have the power to veto any
decision made at a Module maintainer level. The core
maintainers have power to resolve disputes as they see fit.
The core maintainers should publicly articulate their
decision-making, and give a clear reasoning for their
decisions, vetoes and dispute resolution. The core maintainers are admins of the PyTorch GitHub Org
and are listed in Maintainers . Lead Core Maintainer (BDFL) ¶ There may be decisions in which the core maintainers cannot
come to a consensus. To make such difficult decisions, the
core maintainers have an assigned and publicly declared Lead
Core Maintainer amongst them, also commonly known in open-source
governance models as a BDFL. The Lead Core Maintainer should publicly articulate their
decision-making, and give a clear reasoning for their
decisions. The Lead Core Maintainer is also responsible for
confirming or removing core maintainers. Nominating, Confirming and Removing Maintainers ¶ The Principles ¶ Membership in module maintainer groups is given to individuals on merit basis after they demonstrated strong expertise of the
component through contributions, reviews and discussions and are
aligned with how the component fits in overall PyTorch direction. For membership in the maintainer group the individual has to
demonstrate strong and continued alignment with the overall
PyTorch principles. No term limits for module maintainers or core maintainers Light criteria of moving module maintenance to ‘emeritus’
status if they don’t actively participate over long periods
of time. Each module maintainer group may define the inactive
period that’s appropriate for that module. The membership is for an individual, not a company. The Process for Nomination ¶ Each module has its own process. Please contact module maintainers for more information.
However, if there is no process identified, you can file a request to the core
maintainers by submitting this form .
Core maintainers are meeting every three months. If you are submitting a request to the core maintainers, the information in your request
must include the following items: The nominees depth and breadth of code, review and design
contributions on the module Testimonials (positive and negative) of the nominee’s interactions
with the maintainers, users, and the community General testimonials of support from the maintainers The core maintainers then evaluate all information and make
a final decision to Confirm or Decline the nomination. The
decision of the core maintainers has to be articulated well
and would be public. The Process for Removal ¶ Similar to the process for nomination, anyone in the community
can nominate a person to be removed from a Module maintainer
position or a Core maintainer position. A person can also self-nominate to be removed The core maintainers (excluding persons with conflict of
interest) will request or put together more information around
the following: Their activity (or lack of) on the project Their changing thinking of the space, which results in
conflict with the overall direction of the project Other information that makes them unfit to be a maintainer,
such as Code of Conduct issues, their activity outside the
scope of the project that conflicts with the project’s values Conflicts of interest : filial or romantic relationships The core maintainers then evaluate all information and make
a final decision to Confirm or Decline the removal. The decision
of the core maintainers has to be articulated well and would be
public. Nominating Core Maintainers ¶ Any core or module maintainer can nominate someone to become a
core maintainer The lead maintainer (BDFL) is responsible for evaluating the
nomination. The lead maintainer requests or puts together more information
around the strength of the candidate to be a core maintainer: Letters of support from other core and module maintainers General letters of support from stakeholders within the PyTorch
community Any new relevant information that is befitting for the candidacy The lead maintainer evaluates all information and makes a final
decision to Confirm or Decline the nomination, with a clear public
articulation of their reasoning behind the decision. Removing the Lead Core Maintainer and Nominating a New Lead Core Maintainer ¶ A super-majority of core maintainers (75%) can choose to
remove the Lead Core Maintainer After a removal of the Lead Core Maintainer or in unforeseen
circumstances (such as permanent unavailability of the Lead Core
Maintainer), the core maintainers follow a Ranked-Choice voting
method to elect a new Lead Core Maintainer. Add, Remove, and Re-Scope Modules and Projects ¶ The core maintainers together are responsible for taking
decisions on adding, removing and re-scoping new modules
in the PyTorch org, either as new repositories in the
PyTorch GitHub org, or as folders in the pytorch/pytorch repository. They invite proposals from members in the community
(including themselves) for such changes.
The proposals are open-ended, but should have some basic
ground-work to make a convincing case to make change. The
following is an example approach to this process: Interview researchers / stakeholders, talk to community, gather issues; Read papers, attend conferences, build example pipelines based on experience; Create a state of the world - make sure this change is necessary,
for example adding a new project or module is worth the maintenance
cost; or removing a project or module will not remove too much value
from PyTorch; Create a proposal; the proposal covers the maintainership, development
and community plan once the proposal is approved. The core maintainers take final decisions on the proposal, articulating
the reasoning behind the decision publicly. Decision Making ¶ Uncontroversial Changes ¶ Primary work happens through issues and pull requests on
GitHub. Maintainers should avoid pushing their changes directly to
the PyTorch repository, instead relying on pull requests. Approving a
pull request by a core or module maintainer allows it to be merged
without further process. Core and module maintainers, as listed on
the Maintainers page and within CODEOWNERS ultimately approve these changes. Notifying relevant experts about an issue or a pull request
is important. Reviews from experts in the given interest area are
strongly preferred, especially on pull request approvals. Failure to do
so might end up with the change being reverted by the relevant expert. Controversial Decision Process ¶ Substantial changes in a given interest area require a GitHub issue to
be opened for discussion. This includes: Any semantic or syntactic change to the PyTorch framework or library. Backwards-incompatible changes to the Python or C++ API. Additions to the core framework or library, including substantial new
functionality within an existing library. Removal of core features or platform support Core and module maintainers ultimately approve these changes. General Project Policies ¶ PyTorch has been established as PyTorch a Series of LF Projects, LLC.
Policies applicable to PyTorch and participants in PyTorch, including
guidelines on the usage of trademarks, are located at https://www.lfprojects.org/policies/ . PyTorch participants acknowledge that the copyright in all new contributions
will be retained by the copyright holder as independent works of authorship
and that no contributor or copyright holder will be required to assign copyrights
to the project. Except as described below, all code contributions to the project
must be made using the 3-Clause-BSD License available here: https://opensource.org/licenses/BSD-3-Clause (the “Project License”).
All outbound code will be made available under the Project License.
The Maintainers may approve the use of an alternative open license or
licenses for inbound or outbound contributions on an exception basis. FAQ ¶ Q: What if I would like to own (or partly own) a part of the project
such as a feature area or domain library, for example Linear Algebra or Torch Vision ? This is absolutely possible.
The first step is to start contributing to the existing project area and
supporting its health and success. In addition to this, you can
make a proposal through a GitHub issue for new functionality or changes
to improve the project area. Q: What if I am a company looking to use PyTorch internally for
development, can I be granted or purchase a board seat to drive the
project direction? No, the PyTorch project is strictly driven by the
a maintainer project philosophy and clearly separates technical
governance from business governance. However, if you want to be
involved in sponsorship and support, you can become involved in the
PyTorch Foundation (PTF) and sponsorship through this. You can also
have individual engineers look to become maintainers, but this is
not guaranteed and is merit-based. Q: Does the PyTorch project support grants or ways to support
independent developers using or contributing to the project? No, not
at this point. We are however looking at ways to better support the
community of independent developers around PyTorch. If you have
suggestions or inputs, please reach out on the PyTorch forums to
discuss. Q: How do I contribute code to the project? If the change is
relatively minor, a pull request on GitHub can be opened up immediately
for review and merge by the project committers. For larger changes,
please open an issue to make a proposal to discuss prior. Please also
see the PyTorch Contributor
Wiki for contribution
for a walkthrough. Q: Can I become a committer on the project? Unfortunately, the
current commit process to PyTorch involves an interaction with Facebook
infrastructure that can only be triggered by Facebook employees. We are
however looking at ways to expand the committer base to individuals
outside of Facebook and will provide an update when the tooling exists
to allow this. Q: What if I would like to deliver a PyTorch tutorial at a conference
or otherwise? Do I need to be ‘officially’ a committer to do this? No,
we encourage community members to showcase their work wherever and
whenever they can. Please reach out to marketing @ pytorch . org for marketing support.",13539,0,20,,1752175591.9089801
https://pytorch.org/docs/versions.html,No Title,PyTorch Documentation,21,0,1,,1752175592.951971
https://pytorch.org/docs/stable/community/contribution_guide.html,PyTorch Contribution Guide — PyTorch 2.7 documentation,"Note This page has been deprecated. Please refer to the Contribution Guide on the PyTorch Wiki. PyTorch Contribution Guide ¶ PyTorch is a GPU-accelerated Python tensor computation package for
building deep neural networks using a tape-based autograd systems. Contribution Process ¶ The PyTorch organization is governed by PyTorch
Governance and the technical guide to contributing
can be found in CONTRIBUTING.md . The PyTorch development process involves a healthy amount of open
discussions between the core development team and the community. PyTorch operates similarly to most open source projects on GitHub.
However, if you’ve never contributed to an open source project before,
here is the basic process. Figure out what you’re going to work on. The majority of open
source contributions come from people scratching their own itches.
However, if you don’t know what you want to work on, or are just
looking to get more acquainted with the project, here are some tips
for how to find appropriate tasks: Look through the issue
tracker and see if
there are any issues you know how to fix. Issues that are
confirmed by other contributors tend to be better to investigate.
We also maintain some labels for issues that are likely to be
good for new people, e.g., bootcamp and 1hr , although
these labels are less well maintained. Join us on dev discuss and let us know you’re interested in getting to
know PyTorch. We’re very happy to help out researchers and
partners get up to speed with the codebase. Figure out the scope of your change and reach out for design
comments on a GitHub issue if it’s large. The majority of pull
requests are small; in that case, no need to let us know about what
you want to do, just get cracking. But if the change is going to be
large, it’s usually a good idea to get some design comments about it
first by submitting an RFC . If you don’t know how big a change is going to be, we can help you
figure it out! Just post about it on issues or dev discuss . Some feature additions are very standardized; for example, lots of
people add new operators or optimizers to PyTorch. Design
discussion in these cases boils down mostly to, “Do we want this
operator/optimizer?” Giving evidence for its utility, e.g., usage
in peer reviewed papers, or existence in other frameworks, helps a
bit when making this case. Adding operators / algorithms from recently-released research is generally not accepted unless there is overwhelming evidence that
this newly published work has ground-breaking results and will eventually
become a standard in the field. If you are not sure where your method falls,
open an issue first before implementing a PR. Core changes and refactors can be quite difficult to coordinate
since the pace of development on the PyTorch main branch is quite fast.
Definitely reach out about fundamental or cross-cutting changes;
we can often give guidance about how to stage such changes into
more easily reviewable pieces. Code it out! See the CONTRIBUTING.md file for advice for working with PyTorch in a
technical form. Open a pull request. If you are not ready for the pull request to be reviewed, create a draft
pull request first - you can later convert it to a full PR by pressing
“Ready for review” button. You can also prepend the title of the PR with
“[WIP]” (“work in progress”) while it’s still in draft. We will ignore
draft PRs when doing review passes. If you are working on a complex change,
it’s good to start things off as a draft, because you will need to spend
time looking at CI results to see if things worked out or not. Find an appropriate reviewer for your change. We have some folks
who regularly go through the PR queue and try to review
everything, but if you happen to know who the maintainer for a
given subsystem affected by your patch is, feel free to include
them directly on the pull request. You can learn more about Persons of Interest that could review your code. Iterate on the pull request until it’s accepted! We’ll try our best to minimize the number of review round trips and
block PRs only when there are major issues. For the most common
issues in pull requests, take a look at Common Mistakes . Once a pull request is accepted and CI is passing, there is
nothing else you need to do; we will merge the PR for you. Getting Started ¶ Proposing New Features ¶ New feature ideas are best discussed on a specific issue. Please include
as much information as you can, any accompanying data, and your proposed
solution. The PyTorch team and community frequently review new issues
and comments where they think they can help. If you feel confident in
your solution, go ahead and implement it. Reporting Issues ¶ If you’ve identified an issue, first search through the list of
existing issues on the
repo. If you are unable to find a similar issue, then create a new one.
Supply as much information you can to reproduce the problematic
behavior. Also, include any additional insights like the behavior you
expect. Implementing Features or Fixing Bugs ¶ If you want to fix a specific issue, it’s best to comment on the
individual issue with your intent. However, we do not lock or assign
issues except in cases where we have worked with the developer before.
It’s best to strike up a conversation on the issue and discuss your
proposed solution. The PyTorch team can provide guidance that saves you
time. Issues that are labeled first-new-issue, low, or medium priority provide
the best entrance points and are great places to start. Adding Tutorials ¶ A great deal of the tutorials on pytorch.org come from the community itself and we welcome additional contributions.
To learn more about how to contribute a new tutorial you can learn more
here: PyTorch.org Tutorial Contribution Guide on
GitHub Improving Documentation & Tutorials ¶ We aim to produce high quality documentation and tutorials. On rare
occasions that content includes typos or bugs. If you find something you
can fix, send us a pull request for consideration. Take a look at the Documentation section to learn how our system
works. Participating in Online Discussions ¶ You can find active discussions happening on the PyTorch Discussion
Forums for users as well as the PyTorch Dev Discussion Forums for developers and maintainers. Submitting Pull Requests to Fix Open Issues ¶ You can view a list of all open issues here . Commenting on an
issue is a great way to get the attention of the team. From here you can
share your ideas and how you plan to resolve the issue. For more challenging issues, the team will provide feedback and
direction for how to best solve the issue. If you’re not able to fix the issue yourself, commenting and sharing
whether you can reproduce the issue can help the team
identify problem areas. Reviewing Open Pull Requests ¶ We appreciate your help reviewing and commenting on pull requests. Our
team strives to keep the number of open pull requests at a manageable
size, we respond quickly for more information if we need it, and we
merge PRs that we think are useful. However, due to the high level of
interest, additional eyes on the pull requests are always appreciated. Improving Code Readability ¶ Improving code readability helps everyone. It is often better to submit a
small number of pull requests that touch a few files versus a large pull
request that touches many files. Starting a discussion in the PyTorch
forum here or on an issue related to
your improvement is the best way to get started. Adding Test Cases to Make the Codebase More Robust ¶ Additional test coverage is appreciated. Promoting PyTorch ¶ Your use of PyTorch in your projects, research papers, write ups, blogs,
or general discussions around the internet helps to raise awareness for
PyTorch and our growing community. Please reach out to marketing @ pytorch . org for marketing support. Triaging Issues ¶ If you feel that an issue could benefit from a particular tag or level
of complexity, comment on the issue and share your opinion. If you
feel an issue isn’t categorized properly, comment and let the team know. About Open Source Development ¶ If this is your first time contributing to an open source project, some
aspects of the development process may seem unusual to you. There is no way to “claim” issues. People often want to “claim”
an issue when they decide to work on it, to ensure that there isn’t
wasted work when someone else ends up working on it. This doesn’t
really work too well in open source, since someone may decide to work
on something, and end up not having time to do it. Feel free to give
information in an advisory fashion, but at the end of the day, we
will take running code and rough consensus to move forward quickly. There is a high bar for new functionality. Unlike
in a corporate environment, where the person who wrote code
implicitly “owns” it and can be expected to take care of it for the
code’s lifetime, once a pull request is merged into an open
source project, it immediately becomes the collective responsibility
of all maintainers on the project. When we merge code, we are saying
that we, the maintainers, can review subsequent changes and
make a bugfix to the code. This naturally leads to a higher standard
of contribution. Common Mistakes To Avoid ¶ Did you add tests? (Or if the change is hard to test, did you
describe how you tested your change?) We have a few motivations for why we ask for tests: to help us tell if we break it later to help us tell if the patch is correct in the first place
(yes, we did review it, but as Knuth says, “beware of the
following code, for I have not run it, merely proven it
correct”) When is it OK not to add a test? Sometimes a change can’t be
conveniently tested, or the change is so obviously correct (and
unlikely to be broken) that it’s OK not to test it. On the
contrary, if a change seems likely (or is known to be likely)
to be accidentally broken, it’s important to put in the time to
work out a testing strategy. Is your PR too long? It’s easier for us to review and merge small PRs. The difficulty of
reviewing a PR scales nonlinearly with its size. When is it OK to submit a large PR? It helps a lot if there was a
corresponding design discussion in an issue, with sign off from
the people who are going to review your diff. We can also help
give advice about how to split up a large change into individually
shippable parts. Similarly, it helps if there is a complete
description of the contents of the PR: it’s easier to review code
if we know what’s inside! Comments for subtle things? In cases where the behavior of your code
is nuanced, please include extra comments and documentation to allow
us to better understand the intention of your code. Did you add a hack? Sometimes, the right answer is a hack. But
usually, we will have to discuss it. Do you want to touch a very core component? To prevent
major regressions, pull requests that touch core components receive
extra scrutiny. Make sure you’ve discussed your changes with the team
before undertaking major changes. Want to add a new feature? If you want to add new features,
comment your intention on the related issue. Our team tries to
comment on and provide feedback to the community. It’s better to have
an open discussion with the team and the rest of the community before
building new features. This helps us stay aware of what you’re
working on and increases the chance that it’ll be merged. Did you touch code unrelated to the PR? To aid in code review,
please only include files in your pull request that are directly
related to your changes. Frequently Asked Questions ¶ How can I contribute as a reviewer? There is lots of value if
community developers reproduce issues, try out new functionality, or
otherwise help us identify or troubleshoot issues. Commenting on
tasks or pull requests with your environment details is helpful and
appreciated. CI tests failed, what does it mean? Maybe your PR is based
off a broken main branch? You can try to rebase your change on top
of the latest main branch. You can also see the current status of
main branch’s CI at https://hud.pytorch.org/ . What are the most high risk changes? Anything that touches build
configuration is a risky area. Please avoid changing these unless
you’ve had a discussion with the team beforehand. Hey, a commit showed up on my branch, what’s up with that? Sometimes another community member will provide a patch or fix to
your pull request or branch. This is often needed for getting CI tests
to pass. On Documentation ¶ Python Docs ¶ PyTorch documentation is generated from python source using Sphinx . Generated HTML is
copied to the docs folder in the main branch of pytorch.github.io ,
and is served via GitHub pages. Site: https://pytorch.org/docs GitHub: https://github.com/pytorch/pytorch/tree/main/docs Served from: https://github.com/pytorch/pytorch.github.io/tree/master/docs C++ Docs ¶ For C++ code we use Doxygen to generate the content files. The C++ docs
are built on a special server and the resulting files are copied to the https://github.com/pytorch/cppdocs repo, and are served from GitHub
pages. Site: https://pytorch.org/cppdocs GitHub: https://github.com/pytorch/pytorch/tree/main/docs/cpp Served from: https://github.com/pytorch/cppdocs Tutorials ¶ PyTorch tutorials are documents used to help understand using PyTorch to
accomplish specific tasks or to understand more holistic concepts.
Tutorials are built using Sphinx-Gallery from executable python source files, or from restructured-text (rst)
files. Site: https://pytorch.org/tutorials GitHub: https://github.com/pytorch/tutorials Tutorials Build Overview ¶ For tutorials, pull
requests trigger a
rebuild of the entire site using CircleCI to test the effects of the
change. This build is sharded into 9 worker builds and takes around 40
minutes total. At the same time, we do a Netlify build using make
html-noplot , which builds the site without rendering the notebook
output into pages for quick review. After a PR is accepted, the site is rebuilt and deployed using GitHub
Actions. Contributing a New Tutorial ¶ See PyTorch.org Tutorial Contribution
Guide .",14139,0,27,,1752175594.0067062
https://pytorch.org/docs/stable/notes/amp_examples.html,Automatic Mixed Precision examples — PyTorch 2.7 documentation,"Automatic Mixed Precision examples ¶ Ordinarily, “automatic mixed precision training” means training with torch.autocast and torch.amp.GradScaler together. Instances of torch.autocast enable autocasting for chosen regions.
Autocasting automatically chooses the precision for operations to improve performance
while maintaining accuracy. Instances of torch.amp.GradScaler help perform the steps of
gradient scaling conveniently.  Gradient scaling improves convergence for networks with float16 (by default on CUDA and XPU)
gradients by minimizing gradient underflow, as explained here . torch.autocast and torch.amp.GradScaler are modular.
In the samples below, each is used as its individual documentation suggests. (Samples here are illustrative.  See the Automatic Mixed Precision recipe for a runnable walkthrough.) Typical Mixed Precision Training Working with Unscaled Gradients Gradient clipping Working with Scaled Gradients Gradient accumulation Gradient penalty Working with Multiple Models, Losses, and Optimizers Working with Multiple GPUs DataParallel in a single process DistributedDataParallel, one GPU per process DistributedDataParallel, multiple GPUs per process Autocast and Custom Autograd Functions Functions with multiple inputs or autocastable ops Functions that need a particular dtype Typical Mixed Precision Training ¶ # Creates model and optimizer in default precision model = Net () . cuda () optimizer = optim . SGD ( model . parameters (), ... ) # Creates a GradScaler once at the beginning of training. scaler = GradScaler () for epoch in epochs : for input , target in data : optimizer . zero_grad () # Runs the forward pass with autocasting. with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output = model ( input ) loss = loss_fn ( output , target ) # Scales loss.  Calls backward() on scaled loss to create scaled gradients. # Backward passes under autocast are not recommended. # Backward ops run in the same dtype autocast chose for corresponding forward ops. scaler . scale ( loss ) . backward () # scaler.step() first unscales the gradients of the optimizer's assigned params. # If these gradients do not contain infs or NaNs, optimizer.step() is then called, # otherwise, optimizer.step() is skipped. scaler . step ( optimizer ) # Updates the scale for next iteration. scaler . update () Working with Unscaled Gradients ¶ All gradients produced by scaler.scale(loss).backward() are scaled.  If you wish to modify or inspect
the parameters’ .grad attributes between backward() and scaler.step(optimizer) ,  you should
unscale them first.  For example, gradient clipping manipulates a set of gradients such that their global norm
(see torch.nn.utils.clip_grad_norm_() ) or maximum magnitude (see torch.nn.utils.clip_grad_value_() )
is < = <= <= some user-imposed threshold.  If you attempted to clip without unscaling, the gradients’ norm/maximum
magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for unscaled gradients) would be invalid. scaler.unscale_(optimizer) unscales gradients held by optimizer ’s assigned parameters.
If your model or models contain other parameters that were assigned to another optimizer
(say optimizer2 ), you may call scaler.unscale_(optimizer2) separately to unscale those
parameters’ gradients as well. Gradient clipping ¶ Calling scaler.unscale_(optimizer) before clipping enables you to clip unscaled gradients as usual: scaler = GradScaler () for epoch in epochs : for input , target in data : optimizer . zero_grad () with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output = model ( input ) loss = loss_fn ( output , target ) scaler . scale ( loss ) . backward () # Unscales the gradients of optimizer's assigned params in-place scaler . unscale_ ( optimizer ) # Since the gradients of optimizer's assigned params are unscaled, clips as usual: torch . nn . utils . clip_grad_norm_ ( model . parameters (), max_norm ) # optimizer's gradients are already unscaled, so scaler.step does not unscale them, # although it still skips optimizer.step() if the gradients contain infs or NaNs. scaler . step ( optimizer ) # Updates the scale for next iteration. scaler . update () scaler records that scaler.unscale_(optimizer) was already called for this optimizer
this iteration, so scaler.step(optimizer) knows not to redundantly unscale gradients before
(internally) calling optimizer.step() . Warning unscale_ should only be called once per optimizer per step call,
and only after all gradients for that optimizer’s assigned parameters have been accumulated.
Calling unscale_ twice for a given optimizer between each step triggers a RuntimeError. Working with Scaled Gradients ¶ Gradient accumulation ¶ Gradient accumulation adds gradients over an effective batch of size batch_per_iter * iters_to_accumulate ( * num_procs if distributed).  The scale should be calibrated for the effective batch, which means inf/NaN checking,
step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity.
Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective
batch are accumulated.  If grads are unscaled (or the scale factor changes) before accumulation is complete,
the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor)
after which it’s impossible to recover the accumulated unscaled grads step must apply. Therefore, if you want to unscale_ grads (e.g., to allow clipping unscaled grads),
call unscale_ just before step , after all (scaled) grads for the upcoming step have been accumulated.  Also, only call update at the end of iterations
where you called step for a full effective batch: scaler = GradScaler () for epoch in epochs : for i , ( input , target ) in enumerate ( data ): with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output = model ( input ) loss = loss_fn ( output , target ) loss = loss / iters_to_accumulate # Accumulates scaled gradients. scaler . scale ( loss ) . backward () if ( i + 1 ) % iters_to_accumulate == 0 : # may unscale_ here if desired (e.g., to allow clipping unscaled gradients) scaler . step ( optimizer ) scaler . update () optimizer . zero_grad () Gradient penalty ¶ A gradient penalty implementation commonly creates gradients using torch.autograd.grad() , combines them to create the penalty value,
and adds the penalty value to the loss. Here’s an ordinary example of an L2 penalty without gradient scaling or autocasting: for epoch in epochs : for input , target in data : optimizer . zero_grad () output = model ( input ) loss = loss_fn ( output , target ) # Creates gradients grad_params = torch . autograd . grad ( outputs = loss , inputs = model . parameters (), create_graph = True ) # Computes the penalty term and adds it to the loss grad_norm = 0 for grad in grad_params : grad_norm += grad . pow ( 2 ) . sum () grad_norm = grad_norm . sqrt () loss = loss + grad_norm loss . backward () # clip gradients here, if desired optimizer . step () To implement a gradient penalty with gradient scaling, the outputs Tensor(s)
passed to torch.autograd.grad() should be scaled.  The resulting gradients
will therefore be scaled, and should be unscaled before being combined to create the
penalty value. Also, the penalty term computation is part of the forward pass, and therefore should be
inside an autocast context. Here’s how that looks for the same L2 penalty: scaler = GradScaler () for epoch in epochs : for input , target in data : optimizer . zero_grad () with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output = model ( input ) loss = loss_fn ( output , target ) # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params scaled_grad_params = torch . autograd . grad ( outputs = scaler . scale ( loss ), inputs = model . parameters (), create_graph = True ) # Creates unscaled grad_params before computing the penalty. scaled_grad_params are # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_: inv_scale = 1. / scaler . get_scale () grad_params = [ p * inv_scale for p in scaled_grad_params ] # Computes the penalty term and adds it to the loss with autocast ( device_type = 'cuda' , dtype = torch . float16 ): grad_norm = 0 for grad in grad_params : grad_norm += grad . pow ( 2 ) . sum () grad_norm = grad_norm . sqrt () loss = loss + grad_norm # Applies scaling to the backward call as usual. # Accumulates leaf gradients that are correctly scaled. scaler . scale ( loss ) . backward () # may unscale_ here if desired (e.g., to allow clipping unscaled gradients) # step() and update() proceed as usual. scaler . step ( optimizer ) scaler . update () Working with Multiple Models, Losses, and Optimizers ¶ If your network has multiple losses, you must call scaler.scale on each of them individually.
If your network has multiple optimizers, you may call scaler.unscale_ on any of them individually,
and you must call scaler.step on each of them individually. However, scaler.update should only be called once,
after all optimizers used this iteration have been stepped: scaler = torch . amp . GradScaler () for epoch in epochs : for input , target in data : optimizer0 . zero_grad () optimizer1 . zero_grad () with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output0 = model0 ( input ) output1 = model1 ( input ) loss0 = loss_fn ( 2 * output0 + 3 * output1 , target ) loss1 = loss_fn ( 3 * output0 - 5 * output1 , target ) # (retain_graph here is unrelated to amp, it's present because in this # example, both backward() calls share some sections of graph.) scaler . scale ( loss0 ) . backward ( retain_graph = True ) scaler . scale ( loss1 ) . backward () # You can choose which optimizers receive explicit unscaling, if you # want to inspect or modify the gradients of the params they own. scaler . unscale_ ( optimizer0 ) scaler . step ( optimizer0 ) scaler . step ( optimizer1 ) scaler . update () Each optimizer checks its gradients for infs/NaNs and makes an independent decision
whether or not to skip the step.  This may result in one optimizer skipping the step
while the other one does not.  Since step skipping occurs rarely (every several hundred iterations)
this should not impede convergence.  If you observe poor convergence after adding gradient scaling
to a multiple-optimizer model, please report a bug. Working with Multiple GPUs ¶ The issues described here only affect autocast . GradScaler ‘s usage is unchanged. DataParallel in a single process ¶ Even if torch.nn.DataParallel spawns threads to run the forward pass on each device.
The autocast state is propagated in each one and the following will work: model = MyModel () dp_model = nn . DataParallel ( model ) # Sets autocast in the main thread with autocast ( device_type = 'cuda' , dtype = torch . float16 ): # dp_model's internal threads will autocast. output = dp_model ( input ) # loss_fn also autocast loss = loss_fn ( output ) DistributedDataParallel, one GPU per process ¶ torch.nn.parallel.DistributedDataParallel ’s documentation recommends one GPU per process for best
performance.  In this case, DistributedDataParallel does not spawn threads internally,
so usages of autocast and GradScaler are not affected. DistributedDataParallel, multiple GPUs per process ¶ Here torch.nn.parallel.DistributedDataParallel may spawn a side thread to run the forward pass on each
device, like torch.nn.DataParallel . The fix is the same :
apply autocast as part of your model’s forward method to ensure it’s enabled in side threads. Autocast and Custom Autograd Functions ¶ If your network uses custom autograd functions (subclasses of torch.autograd.Function ), changes are required for
autocast compatibility if any function takes multiple floating-point Tensor inputs, wraps any autocastable op (see the Autocast Op Reference ), or requires a particular dtype (for example, if it wraps CUDA extensions that were only compiled for dtype ). In all cases, if you’re importing the function and can’t alter its definition, a safe fallback
is to disable autocast and force execution in float32 ( or dtype ) at any points of use where errors occur: with autocast ( device_type = 'cuda' , dtype = torch . float16 ): ... with autocast ( device_type = 'cuda' , dtype = torch . float16 , enabled = False ): output = imported_function ( input1 . float (), input2 . float ()) If you’re the function’s author (or can alter its definition) a better solution is to use the torch.amp.custom_fwd() and torch.amp.custom_bwd() decorators as shown in
the relevant case below. Functions with multiple inputs or autocastable ops ¶ Apply custom_fwd and custom_bwd (with no arguments) to forward and backward respectively.  These ensure forward executes with the current autocast state and backward executes with the same autocast state as forward (which can prevent type mismatch errors): class MyMM ( torch . autograd . Function ): @staticmethod @custom_fwd def forward ( ctx , a , b ): ctx . save_for_backward ( a , b ) return a . mm ( b ) @staticmethod @custom_bwd def backward ( ctx , grad ): a , b = ctx . saved_tensors return grad . mm ( b . t ()), a . t () . mm ( grad ) Now MyMM can be invoked anywhere, without disabling autocast or manually casting inputs: mymm = MyMM . apply with autocast ( device_type = 'cuda' , dtype = torch . float16 ): output = mymm ( input1 , input2 ) Functions that need a particular dtype ¶ Consider a custom function that requires torch.float32 inputs.
Apply custom_fwd(device_type='cuda', cast_inputs=torch.float32) to forward and custom_bwd(device_type='cuda') to backward .
If forward runs in an autocast-enabled region, the decorators cast floating-point Tensor
inputs to float32 on designated device assigned by the argument device_type , CUDA in this example, and locally disable autocast during forward and backward : class MyFloat32Func ( torch . autograd . Function ): @staticmethod @custom_fwd ( device_type = 'cuda' , cast_inputs = torch . float32 ) def forward ( ctx , input ): ctx . save_for_backward ( input ) ... return fwd_output @staticmethod @custom_bwd ( device_type = 'cuda' ) def backward ( ctx , grad ): ... Now MyFloat32Func can be invoked anywhere, without manually disabling autocast or casting inputs: func = MyFloat32Func . apply with autocast ( device_type = 'cuda' , dtype = torch . float16 ): # func will run in float32, regardless of the surrounding autocast state output = func ( input )",14561,12,18,"# Creates model and optimizer in default precision
model = Net().cuda()
optimizer = optim.SGD(model.parameters(), ...)

# Creates a GradScaler once at the beginning of training.
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()

        # Runs the forward pass with autocasting.
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
        # Backward passes under autocast are not recommended.
        # Backward ops run in the same dtype autocast chose for corresponding forward ops.
        scaler.scale(loss).backward()

        # scaler.step() first unscales the gradients of the optimizer's assigned params.
        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
        # otherwise, optimizer.step() is skipped.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()
---
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
        scaler.scale(loss).backward()

        # Unscales the gradients of optimizer's assigned params in-place
        scaler.unscale_(optimizer)

        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,
        # although it still skips optimizer.step() if the gradients contain infs or NaNs.
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()
---
scaler = GradScaler()

for epoch in epochs:
    for i, (input, target) in enumerate(data):
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)
            loss = loss / iters_to_accumulate

        # Accumulates scaled gradients.
        scaler.scale(loss).backward()

        if (i + 1) % iters_to_accumulate == 0:
            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)

            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
---
for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Creates gradients
        grad_params = torch.autograd.grad(outputs=loss,
                                          inputs=model.parameters(),
                                          create_graph=True)

        # Computes the penalty term and adds it to the loss
        grad_norm = 0
        for grad in grad_params:
            grad_norm += grad.pow(2).sum()
        grad_norm = grad_norm.sqrt()
        loss = loss + grad_norm

        loss.backward()

        # clip gradients here, if desired

        optimizer.step()
---
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output = model(input)
            loss = loss_fn(output, target)

        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params
        scaled_grad_params = torch.autograd.grad(outputs=scaler.scale(loss),
                                                 inputs=model.parameters(),
                                                 create_graph=True)

        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are
        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:
        inv_scale = 1./scaler.get_scale()
        grad_params = [p * inv_scale for p in scaled_grad_params]

        # Computes the penalty term and adds it to the loss
        with autocast(device_type='cuda', dtype=torch.float16):
            grad_norm = 0
            for grad in grad_params:
                grad_norm += grad.pow(2).sum()
            grad_norm = grad_norm.sqrt()
            loss = loss + grad_norm

        # Applies scaling to the backward call as usual.
        # Accumulates leaf gradients that are correctly scaled.
        scaler.scale(loss).backward()

        # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)

        # step() and update() proceed as usual.
        scaler.step(optimizer)
        scaler.update()
---
scaler = torch.amp.GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer0.zero_grad()
        optimizer1.zero_grad()
        with autocast(device_type='cuda', dtype=torch.float16):
            output0 = model0(input)
            output1 = model1(input)
            loss0 = loss_fn(2 * output0 + 3 * output1, target)
            loss1 = loss_fn(3 * output0 - 5 * output1, target)

        # (retain_graph here is unrelated to amp, it's present because in this
        # example, both backward() calls share some sections of graph.)
        scaler.scale(loss0).backward(retain_graph=True)
        scaler.scale(loss1).backward()

        # You can choose which optimizers receive explicit unscaling, if you
        # want to inspect or modify the gradients of the params they own.
        scaler.unscale_(optimizer0)

        scaler.step(optimizer0)
        scaler.step(optimizer1)

        scaler.update()
---
model = MyModel()
dp_model = nn.DataParallel(model)

# Sets autocast in the main thread
with autocast(device_type='cuda', dtype=torch.float16):
    # dp_model's internal threads will autocast.
    output = dp_model(input)
    # loss_fn also autocast
    loss = loss_fn(output)
---
with autocast(device_type='cuda', dtype=torch.float16):
    ...
    with autocast(device_type='cuda', dtype=torch.float16, enabled=False):
        output = imported_function(input1.float(), input2.float())
---
class MyMM(torch.autograd.Function):
    @staticmethod
    @custom_fwd
    def forward(ctx, a, b):
        ctx.save_for_backward(a, b)
        return a.mm(b)
    @staticmethod
    @custom_bwd
    def backward(ctx, grad):
        a, b = ctx.saved_tensors
        return grad.mm(b.t()), a.t().mm(grad)
---
mymm = MyMM.apply

with autocast(device_type='cuda', dtype=torch.float16):
    output = mymm(input1, input2)
---
class MyFloat32Func(torch.autograd.Function):
    @staticmethod
    @custom_fwd(device_type='cuda', cast_inputs=torch.float32)
    def forward(ctx, input):
        ctx.save_for_backward(input)
        ...
        return fwd_output
    @staticmethod
    @custom_bwd(device_type='cuda')
    def backward(ctx, grad):
        ...
---
func = MyFloat32Func.apply

with autocast(device_type='cuda', dtype=torch.float16):
    # func will run in float32, regardless of the surrounding autocast state
    output = func(input)",1752175595.0911472
https://pytorch.org/docs/stable/notes/hip.html,HIP (ROCm) semantics — PyTorch 2.7 documentation,"HIP (ROCm) semantics ¶ ROCm™ is AMD’s open source software platform for GPU-accelerated high
performance computing and machine learning. HIP is ROCm’s C++ dialect designed
to ease conversion of CUDA applications to portable C++ code. HIP is used when
converting existing CUDA applications like PyTorch to portable C++ and for new
projects that require portability between AMD and NVIDIA. HIP Interfaces Reuse the CUDA Interfaces ¶ PyTorch for HIP intentionally reuses the existing torch.cuda interfaces.
This helps to accelerate the porting of existing PyTorch code and models because
very few code changes are necessary, if any. The example from CUDA semantics will work exactly the same for HIP: cuda = torch . device ( 'cuda' ) # Default HIP device cuda0 = torch . device ( 'cuda:0' ) # 'rocm' or 'hip' are not valid, use 'cuda' cuda2 = torch . device ( 'cuda:2' ) # GPU 2 (these are 0-indexed) x = torch . tensor ([ 1. , 2. ], device = cuda0 ) # x.device is device(type='cuda', index=0) y = torch . tensor ([ 1. , 2. ]) . cuda () # y.device is device(type='cuda', index=0) with torch . cuda . device ( 1 ): # allocates a tensor on GPU 1 a = torch . tensor ([ 1. , 2. ], device = cuda ) # transfers a tensor from CPU to GPU 1 b = torch . tensor ([ 1. , 2. ]) . cuda () # a.device and b.device are device(type='cuda', index=1) # You can also use ``Tensor.to`` to transfer a tensor: b2 = torch . tensor ([ 1. , 2. ]) . to ( device = cuda ) # b.device and b2.device are device(type='cuda', index=1) c = a + b # c.device is device(type='cuda', index=1) z = x + y # z.device is device(type='cuda', index=0) # even within a context, you can specify the device # (or give a GPU index to the .cuda call) d = torch . randn ( 2 , device = cuda2 ) e = torch . randn ( 2 ) . to ( cuda2 ) f = torch . randn ( 2 ) . cuda ( cuda2 ) # d.device, e.device, and f.device are all device(type='cuda', index=2) Checking for HIP ¶ Whether you are using PyTorch for CUDA or HIP, the result of calling is_available() will be the same. If you are using a PyTorch
that has been built with GPU support, it will return True . If you must check
which version of PyTorch you are using, refer to this example below: if torch . cuda . is_available () and torch . version . hip : # do something specific for HIP elif torch . cuda . is_available () and torch . version . cuda : # do something specific for CUDA TensorFloat-32(TF32) on ROCm ¶ TF32 is not supported on ROCm. Memory management ¶ PyTorch uses a caching memory allocator to speed up memory allocations. This
allows fast memory deallocation without device synchronizations. However, the
unused memory managed by the allocator will still show as if used in rocm-smi . You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by
tensors, and use memory_reserved() and max_memory_reserved() to monitor the total amount of memory
managed by the caching allocator. Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used
by other GPU applications. However, the occupied GPU memory by tensors will not
be freed so it can not increase the amount of GPU memory available for PyTorch. For more advanced users, we offer more comprehensive memory benchmarking via memory_stats() . We also offer the capability to capture a
complete snapshot of the memory allocator state via memory_snapshot() , which can help you understand the
underlying allocation patterns produced by your code. To debug memory errors, set PYTORCH_NO_HIP_MEMORY_CACHING=1 in your environment to disable caching. PYTORCH_NO_CUDA_MEMORY_CACHING=1 is also accepted for ease of porting. hipBLAS workspaces ¶ For each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that
handle and stream combination executes a hipBLAS kernel that requires a workspace.  In order to
avoid repeatedly allocating workspaces, these workspaces are not deallocated unless torch._C._cuda_clearCublasWorkspaces() is called; note that it’s the same function for CUDA or
HIP. The workspace size per allocation can be specified via the environment variable HIPBLAS_WORKSPACE_CONFIG with the format :[SIZE]:[COUNT] .  As an example, the environment
variable HIPBLAS_WORKSPACE_CONFIG=:4096:2:16:8 specifies a total size of 2 * 4096 + 8 * 16 KiB or 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force
hipBLAS to avoid using workspaces, set HIPBLAS_WORKSPACE_CONFIG=:0:0 . For convenience, CUBLAS_WORKSPACE_CONFIG is also accepted. hipFFT/rocFFT plan cache ¶ Setting the size of the cache for hipFFT/rocFFT plans is not supported. torch.distributed backends ¶ Currently, only the “nccl” and “gloo” backends for torch.distributed are supported on ROCm. CUDA API to HIP API mappings in C++ ¶ Please refer: https://rocmdocs.amd.com/en/latest/Programming_Guides/HIP_API_Guide.html NOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not
semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and
hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks. For example: Instead of using #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 to implicitly exclude ROCm/HIP, use the following to not take the code path for ROCm/HIP: #if defined(CUDA_VERSION) && CUDA_VERSION >= 11000 && !defined(USE_ROCM) Alternatively, if it is desired to take the code path for ROCm/HIP: #if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || defined(USE_ROCM) Or if it is desired to take the code path for ROCm/HIP only for specific HIP versions: #if (defined(CUDA_VERSION) && CUDA_VERSION >= 11000) || (defined(USE_ROCM) && ROCM_VERSION >= 40300) Refer to CUDA Semantics doc ¶ For any sections not listed here, please refer to the CUDA semantics doc: CUDA semantics Enabling kernel asserts ¶ Kernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled
by recompiling the PyTorch from source. Please add below line as an argument to cmake command parameters: - DROCM_FORCE_ENABLE_GPU_ASSERTS : BOOL = ON",6116,3,14,"cuda = torch.device('cuda')     # Default HIP device
cuda0 = torch.device('cuda:0')  # 'rocm' or 'hip' are not valid, use 'cuda'
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)
---
if torch.cuda.is_available() and torch.version.hip:
    # do something specific for HIP
elif torch.cuda.is_available() and torch.version.cuda:
    # do something specific for CUDA
---
-DROCM_FORCE_ENABLE_GPU_ASSERTS:BOOL=ON",1752175596.184336
https://pytorch.org/docs/stable/notes/autograd.html,Autograd mechanics — PyTorch 2.7 documentation,"Autograd mechanics ¶ This note will present an overview of how autograd works and records the
operations. It’s not strictly necessary to understand all this, but we recommend
getting familiar with it, as it will help you write more efficient, cleaner
programs, and can aid you in debugging. How autograd encodes the history ¶ Autograd is a reverse automatic differentiation system.  Conceptually,
autograd records a graph recording all of the operations that created
the data as you execute operations, giving you a directed acyclic graph
whose leaves are the input tensors and roots are the output tensors.
By tracing this graph from roots to leaves, you can automatically
compute the gradients using the chain rule. Internally, autograd represents this graph as a graph of Function objects (really expressions), which can be apply() ed to compute the result of
evaluating the graph.  When computing the forward pass, autograd
simultaneously performs the requested computations and builds up a graph
representing the function that computes the gradient (the .grad_fn attribute of each torch.Tensor is an entry point into this graph).
When the forward pass is completed, we evaluate this graph in the
backwards pass to compute the gradients. An important thing to note is that the graph is recreated from scratch at every
iteration, and this is exactly what allows for using arbitrary Python control
flow statements, that can change the overall shape and size of the graph at
every iteration. You don’t have to encode all possible paths before you
launch the training - what you run is what you differentiate. Saved tensors ¶ Some operations need intermediary results to be saved during the forward pass
in order to execute the backward pass. For example, the function x ↦ x 2 x\mapsto x^2 x ↦ x 2 saves the input x x x to compute the gradient. When defining a custom Python Function , you can use save_for_backward() to save
tensors during the forward pass and saved_tensors to retrieve them
during the backward pass. See Extending PyTorch for more information. For operations that PyTorch defines (e.g. torch.pow() ), tensors are
automatically saved as needed. You can explore (for educational or debugging
purposes) which tensors are saved by a certain grad_fn by looking for its
attributes starting with the prefix _saved . x = torch . randn ( 5 , requires_grad = True ) y = x . pow ( 2 ) print ( x . equal ( y . grad_fn . _saved_self )) # True print ( x is y . grad_fn . _saved_self ) # True In the previous code, y.grad_fn._saved_self refers to the same Tensor object as x .
But that may not always be the case. For instance: x = torch . randn ( 5 , requires_grad = True ) y = x . exp () print ( y . equal ( y . grad_fn . _saved_result )) # True print ( y is y . grad_fn . _saved_result ) # False Under the hood, to prevent reference cycles, PyTorch has packed the tensor
upon saving and unpacked it into a different tensor for reading. Here, the
tensor you get from accessing y.grad_fn._saved_result is a different tensor
object than y (but they still share the same storage). Whether a tensor will be packed into a different tensor object depends on
whether it is an output of its own grad_fn , which is an implementation detail
subject to change and that users should not rely on. You can control how PyTorch does packing / unpacking with Hooks for saved tensors . Gradients for non-differentiable functions ¶ The gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable.
Unfortunately many of the functions we use in practice do not have this property ( relu or sqrt at 0 , for example).
To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order: If the function is differentiable and thus a gradient exists at the current point, use it. If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction). If the function is concave (at least locally), use the super-gradient of minimum norm (consider -f(x) and apply the previous point). If the function is defined, define the gradient at the current point by continuity (note that inf is possible here, for example for sqrt(0) ). If multiple values are possible, pick one arbitrarily. If the function is not defined ( sqrt(-1) , log(-1) or most functions when the input is NaN , for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use NaN as the gradient, but for performance reasons, some functions will use other values ( log(-1) , for example). If the function is not a deterministic mapping (i.e. it is not a mathematical function ), it will be marked as non-differentiable. This will make it error out in the backward if used on tensors that require grad outside of a no_grad environment. Locally disabling gradient computation ¶ There are several mechanisms available from Python to locally disable gradient
computation: To disable gradients across entire blocks of code, there are context managers
like no-grad mode and inference mode.
For more fine-grained exclusion of subgraphs from gradient computation,
there is setting the requires_grad field of a tensor. Below, in addition to discussing the mechanisms above, we also describe
evaluation mode ( nn.Module.eval() ), a method that is not used
to disable gradient computation but, because of its name, is often mixed up with the three. Setting requires_grad ¶ requires_grad is a flag, defaulting to false unless wrapped
in a nn.Parameter , that allows for fine-grained exclusion of
subgraphs from gradient computation. It takes effect in both the
forward and backward passes: During the forward pass, an operation is only recorded in the backward graph if
at least one of its input tensors require grad.
During the backward pass ( .backward() ), only leaf tensors with requires_grad=True will have gradients accumulated into their .grad fields. It is important to note that even though every tensor has this flag, setting it only makes sense for leaf tensors (tensors that do not have a grad_fn , e.g., a nn.Module ’s parameters).
Non-leaf tensors (tensors that do have grad_fn ) are tensors that have a
backward graph associated with them. Thus their gradients will be needed
as an intermediary result to compute the gradient for a leaf tensor that
requires grad. From this definition, it is clear that all non-leaf tensors
will automatically have require_grad=True . Setting requires_grad should be the main way you control which parts
of the model are part of the gradient computation, for example, if you need to
freeze parts of your pretrained model during model fine-tuning. To freeze parts of your model, simply apply .requires_grad_(False) to
the parameters that you don’t want updated. And as described above,
since computations that use these parameters as inputs would not be recorded in
the forward pass, they won’t have their .grad fields updated in the backward
pass because they won’t be part of the backward graph in the first place, as
desired. Because this is such a common pattern, requires_grad can also be set at
the module level with nn.Module.requires_grad_() .
When applied to a module, .requires_grad_() takes effect on all
of the module’s parameters (which have requires_grad=True by default). Grad Modes ¶ Apart from setting requires_grad there are also three grad modes that can
be selected from Python that can affect how computations in PyTorch are
processed by autograd internally: default mode (grad mode), no-grad mode,
and inference mode, all of which can be togglable via context managers and
decorators. Mode Excludes operations from being recorded in backward graph Skips additional autograd tracking overhead Tensors created while the mode is enabled can be used in grad-mode later Examples default ✓ Forward pass no-grad ✓ ✓ Optimizer updates inference ✓ ✓ Data processing, model evaluation Default Mode (Grad Mode) ¶ The “default mode” is the mode we are implicitly in when no other modes like
no-grad and inference mode are enabled. To be contrasted with
“no-grad mode” the default mode is also sometimes called “grad mode”. The most important thing to know about the default mode is that it is the only
mode in which requires_grad takes effect. requires_grad is always overridden
to be False in both the two other modes. No-grad Mode ¶ Computations in no-grad mode behave as if none of the inputs require grad.
In other words, computations in no-grad mode are never recorded in the backward graph
even if there are inputs that have require_grad=True . Enable no-grad mode when you need to perform operations that should not be
recorded by autograd, but you’d still like to use the outputs of these
computations in grad mode later. This context manager makes it convenient to
disable gradients for a block of code or function without
having to temporarily set tensors to have requires_grad=False , and then
back to True . For example, no-grad mode might be useful when writing an optimizer: when
performing the training update you’d like to update parameters
in-place without the update being recorded by autograd.
You also intend to use the updated parameters for computations in
grad mode in the next forward pass. The implementations in torch.nn.init also
rely on no-grad mode when initializing the parameters as to avoid
autograd tracking when updating the initialized parameters in-place. Inference Mode ¶ Inference mode is the extreme version of no-grad mode. Just like in no-grad
mode, computations in inference mode are not recorded in the backward graph, but
enabling inference mode will allow PyTorch to speed up your model even more.
This better runtime comes with a drawback: tensors created in inference mode
will not be able to be used in computations to be recorded by autograd after
exiting inference mode. Enable inference mode when you are performing computations that do not have
interactions with autograd, AND you don’t plan on using the tensors created
in inference mode in any computation that is to be recorded by autograd later. It is recommended that you try out inference mode in the parts of your code
that do not require autograd tracking (e.g., data processing and model evaluation).
If it works out of the box
for your use case it’s a free performance win. If you run into errors after
enabling inference mode, check that you are not using tensors created in
inference mode in computations that are recorded by autograd after exiting inference
mode. If you cannot avoid such use in your case, you can always switch back
to no-grad mode. For details on inference mode please see Inference Mode . For implementation details of inference mode see RFC-0011-InferenceMode . Evaluation Mode ( nn.Module.eval() ) ¶ Evaluation mode is not a mechanism to locally disable gradient computation.
It is included here anyway because it is sometimes confused to be such a mechanism. Functionally, module.eval() (or equivalently module.train(False) ) are completely
orthogonal to no-grad mode and inference mode. How model.eval() affects
your model depends entirely on the specific modules used in your model and
whether they define any training-mode specific behavior. You are responsible for calling model.eval() and model.train() if your
model relies on modules such as torch.nn.Dropout and torch.nn.BatchNorm2d that may behave
differently depending on training mode, for example, to avoid updating your
BatchNorm running statistics on validation data. It is recommended that you always use model.train() when
training and model.eval() when evaluating your model (validation/testing) even
if you aren’t sure your model has training-mode specific behavior, because a
module you are using might be updated to behave differently in training and
eval modes. In-place operations with autograd ¶ Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
lower memory usage by any significant amount. Unless you’re operating
under heavy memory pressure, you might never need to use them. There are two main reasons that limit the applicability of in-place operations: In-place operations can potentially overwrite values required to compute
gradients. Every in-place operation requires the implementation to rewrite the
computational graph. Out-of-place versions simply allocate new objects and
keep references to the old graph, while in-place operations, require
changing the creator of all inputs to the Function representing
this operation. This can be tricky, especially if there are many Tensors
that reference the same storage (e.g. created by indexing or transposing),
and in-place functions will raise an error if the storage of
modified inputs is referenced by any other Tensor . In-place correctness checks ¶ Every tensor keeps a version counter, that is incremented every time it is
marked dirty in any operation. When a Function saves any tensors for backward,
a version counter of their containing Tensor is saved as well. Once you access self.saved_tensors it is checked, and if it is greater than the saved value
an error is raised. This ensures that if you’re using in-place
functions and not seeing any errors, you can be sure that the computed
gradients are correct. Multithreaded Autograd ¶ The autograd engine is responsible for running all the backward operations
necessary to compute the backward pass. This section will describe all the details
that can help you make the best use of it in a multithreaded environment. (This is
relevant only for PyTorch 1.6+ as the behavior in previous version was different.) User could train their model with multithreading code (e.g. Hogwild training), and
does not block on the concurrent backward computations, example code could be: # Define a train function to be used in different threads def train_fn (): x = torch . ones ( 5 , 5 , requires_grad = True ) # forward y = ( x + 3 ) * ( x + 4 ) * 0.5 # backward y . sum () . backward () # potential optimizer update # User write their own threading code to drive the train_fn threads = [] for _ in range ( 10 ): p = threading . Thread ( target = train_fn , args = ()) p . start () threads . append ( p ) for p in threads : p . join () Note that some behaviors that user should be aware of: Concurrency on CPU ¶ When you run backward() or grad() via python or C++ API in multiple
threads on CPU, you are expecting to see extra concurrency instead of
serializing all the backward calls in a specific order during execution
(behavior before PyTorch 1.6). Non-determinism ¶ If you are calling backward() from multiple threads concurrently and have
shared inputs (i.e. Hogwild CPU training), then non-determinism should be expected.
This can occur because parameters are automatically shared across threads,
as such, multiple threads may access and try to accumulate the same .grad attribute during gradient accumulation. This is technically not safe, and
it might result in race condition and the result might be invalid to use. Users developing multithreaded models featuring shared parameters should have the
threading model in mind and should understand the issues described above. The functional API torch.autograd.grad() may be used to calculate the
gradients instead of backward() to avoid non-determinism. Graph retaining ¶ If part of the autograd graph is shared between threads, i.e. run first
part of forward single thread, then run second part in multiple threads,
then the first part of graph is shared. In this case different threads
execute grad() or backward() on the same graph might have issue of
destroying the graph on the fly of one thread, and the other thread will
crash in this case. Autograd will error out to the user similar to what call backward() twice with out retain_graph=True , and let the user know
they should use retain_graph=True . Thread Safety on Autograd Node ¶ Since Autograd allows the caller thread to drive its backward execution for
potential parallelism, it’s important that we ensure thread safety on CPU with
parallel backward() calls that share part/whole of the GraphTask. Custom Python autograd.Function s are automatically thread safe because of GIL.
For built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom autograd::Function s, the Autograd Engine uses thread mutex locking to ensure
thread safety on autograd Nodes that might have state write/read. No thread safety on C++ hooks ¶ Autograd relies on the user to write thread safe C++ hooks. If you want the hook
to be correctly applied in multithreading environment, you will need to write
proper thread locking code to ensure the hooks are thread safe. Autograd for Complex Numbers ¶ The short version: When you use PyTorch to differentiate any function f ( z ) f(z) f ( z ) with complex domain and/or codomain,
the gradients are computed under the assumption that the function is a part of a larger real-valued
loss function g ( i n p u t ) = L g(input)=L g ( in p u t ) = L . The gradient computed is ∂ L ∂ z ∗ \frac{\partial L}{\partial z^*} ∂ z ∗ ∂ L ​ (note the conjugation of z), the negative of which is precisely the direction of steepest descent
used in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers
work out of the box with complex parameters. This convention matches TensorFlow’s convention for complex
differentiation, but is different from JAX (which computes ∂ L ∂ z \frac{\partial L}{\partial z} ∂ z ∂ L ​ ). If you have a real-to-real function which internally uses complex
operations, the convention here doesn’t matter: you will always get
the same result that you would have gotten if it had been implemented
with only real operations. If you are curious about the mathematical details, or want to know how
to define complex derivatives in PyTorch, read on. What are complex derivatives? ¶ The mathematical definition of complex-differentiability takes the
limit definition of a derivative and generalizes it to operate on
complex numbers. Consider a function f : C → C f: ℂ → ℂ f : C → C , f ( z = x + y j ) = u ( x , y ) + v ( x , y ) j f(z=x+yj) = u(x, y) + v(x, y)j f ( z = x + y j ) = u ( x , y ) + v ( x , y ) j where u u u and v v v are two variable real valued functions
and j j j is the imaginary unit. Using the derivative definition, we can write: f ′ ( z ) = lim ⁡ h → 0 , h ∈ C f ( z + h ) − f ( z ) h f'(z) = \lim_{h \to 0, h \in C} \frac{f(z+h) - f(z)}{h} f ′ ( z ) = h → 0 , h ∈ C lim ​ h f ( z + h ) − f ( z ) ​ In order for this limit to exist, not only must u u u and v v v must be
real differentiable, but f f f must also satisfy the Cauchy-Riemann equations .  In
other words: the limit computed for real and imaginary steps ( h h h )
must be equal. This is a more restrictive condition. The complex differentiable functions are commonly known as holomorphic
functions. They are well behaved, have all the nice properties that
you’ve seen from real differentiable functions, but are practically of no
use in the optimization world. For optimization problems, only real valued objective
functions are used in the research community since complex numbers are not part of any
ordered field and so having complex valued loss does not make much sense. It also turns out that no interesting real-valued objective fulfill the
Cauchy-Riemann equations. So the theory with holomorphic function cannot be
used for optimization and most people therefore use the Wirtinger calculus. Wirtinger Calculus comes into the picture … ¶ So, we have this great theory of complex differentiability and
holomorphic functions, and we can’t use any of it at all, because many
of the commonly used functions are not holomorphic. What’s a poor
mathematician to do? Well, Wirtinger observed that even if f ( z ) f(z) f ( z ) isn’t holomorphic, one could rewrite it as a two variable function f ( z , z ∗ ) f(z, z*) f ( z , z ∗ ) which is always holomorphic. This is because real and
imaginary of the components of z z z can be expressed in terms of z z z and z ∗ z^* z ∗ as: R e ( z ) = z + z ∗ 2 I m ( z ) = z − z ∗ 2 j \begin{aligned}
    \mathrm{Re}(z) &= \frac {z + z^*}{2} \\
    \mathrm{Im}(z) &= \frac {z - z^*}{2j}
\end{aligned} Re ( z ) Im ( z ) ​ = 2 z + z ∗ ​ = 2 j z − z ∗ ​ ​ Wirtinger calculus suggests to study f ( z , z ∗ ) f(z, z^*) f ( z , z ∗ ) instead, which is
guaranteed to be holomorphic if f f f was real differentiable (another
way to think of it is as a change of coordinate system, from f ( x , y ) f(x, y) f ( x , y ) to f ( z , z ∗ ) f(z, z^*) f ( z , z ∗ ) .)  This function has partial derivatives ∂ ∂ z \frac{\partial }{\partial z} ∂ z ∂ ​ and ∂ ∂ z ∗ \frac{\partial}{\partial z^{*}} ∂ z ∗ ∂ ​ .
We can use the chain rule to establish a
relationship between these partial derivatives and the partial
derivatives w.r.t., the real and imaginary components of z z z . ∂ ∂ x = ∂ z ∂ x ∗ ∂ ∂ z + ∂ z ∗ ∂ x ∗ ∂ ∂ z ∗ = ∂ ∂ z + ∂ ∂ z ∗ ∂ ∂ y = ∂ z ∂ y ∗ ∂ ∂ z + ∂ z ∗ ∂ y ∗ ∂ ∂ z ∗ = 1 j ∗ ( ∂ ∂ z − ∂ ∂ z ∗ ) \begin{aligned}
    \frac{\partial }{\partial x} &= \frac{\partial z}{\partial x} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial x} * \frac{\partial }{\partial z^*} \\
                                 &= \frac{\partial }{\partial z} + \frac{\partial }{\partial z^*}   \\
    \\
    \frac{\partial }{\partial y} &= \frac{\partial z}{\partial y} * \frac{\partial }{\partial z} + \frac{\partial z^*}{\partial y} * \frac{\partial }{\partial z^*} \\
                                 &= 1j * \left(\frac{\partial }{\partial z} - \frac{\partial }{\partial z^*}\right)
\end{aligned} ∂ x ∂ ​ ∂ y ∂ ​ ​ = ∂ x ∂ z ​ ∗ ∂ z ∂ ​ + ∂ x ∂ z ∗ ​ ∗ ∂ z ∗ ∂ ​ = ∂ z ∂ ​ + ∂ z ∗ ∂ ​ = ∂ y ∂ z ​ ∗ ∂ z ∂ ​ + ∂ y ∂ z ∗ ​ ∗ ∂ z ∗ ∂ ​ = 1 j ∗ ( ∂ z ∂ ​ − ∂ z ∗ ∂ ​ ) ​ From the above equations, we get: ∂ ∂ z = 1 / 2 ∗ ( ∂ ∂ x − 1 j ∗ ∂ ∂ y ) ∂ ∂ z ∗ = 1 / 2 ∗ ( ∂ ∂ x + 1 j ∗ ∂ ∂ y ) \begin{aligned}
    \frac{\partial }{\partial z} &= 1/2 * \left(\frac{\partial }{\partial x} - 1j * \frac{\partial }{\partial y}\right)   \\
    \frac{\partial }{\partial z^*} &= 1/2 * \left(\frac{\partial }{\partial x} + 1j * \frac{\partial }{\partial y}\right)
\end{aligned} ∂ z ∂ ​ ∂ z ∗ ∂ ​ ​ = 1/2 ∗ ( ∂ x ∂ ​ − 1 j ∗ ∂ y ∂ ​ ) = 1/2 ∗ ( ∂ x ∂ ​ + 1 j ∗ ∂ y ∂ ​ ) ​ which is the classic definition of Wirtinger calculus that you would find on Wikipedia . There are a lot of beautiful consequences of this change. For one, the Cauchy-Riemann equations translate into simply saying that ∂ f ∂ z ∗ = 0 \frac{\partial f}{\partial z^*} = 0 ∂ z ∗ ∂ f ​ = 0 (that is to say, the function f f f can be written
entirely in terms of z z z , without making reference to z ∗ z^* z ∗ ). Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should
take while making variable update is given by ∂ L o s s ∂ z ∗ \frac{\partial Loss}{\partial z^*} ∂ z ∗ ∂ L oss ​ (not ∂ L o s s ∂ z \frac{\partial Loss}{\partial z} ∂ z ∂ L oss ​ ). For more reading, check out: https://arxiv.org/pdf/0906.4835.pdf How is Wirtinger Calculus useful in optimization? ¶ Researchers in audio and other fields, more commonly, use gradient
descent to optimize real valued loss functions with complex variables.
Typically, these people treat the real and imaginary values as separate
channels that can be updated. For a step size α / 2 \alpha/2 α /2 and loss L L L , we can write the following equations in R 2 ℝ^2 R 2 : x n + 1 = x n − ( α / 2 ) ∗ ∂ L ∂ x y n + 1 = y n − ( α / 2 ) ∗ ∂ L ∂ y \begin{aligned}
    x_{n+1} &= x_n - (\alpha/2) * \frac{\partial L}{\partial x}  \\
    y_{n+1} &= y_n - (\alpha/2) * \frac{\partial L}{\partial y}
\end{aligned} x n + 1 ​ y n + 1 ​ ​ = x n ​ − ( α /2 ) ∗ ∂ x ∂ L ​ = y n ​ − ( α /2 ) ∗ ∂ y ∂ L ​ ​ How do these equations translate into complex space C ℂ C ? z n + 1 = x n − ( α / 2 ) ∗ ∂ L ∂ x + 1 j ∗ ( y n − ( α / 2 ) ∗ ∂ L ∂ y ) = z n − α ∗ 1 / 2 ∗ ( ∂ L ∂ x + j ∂ L ∂ y ) = z n − α ∗ ∂ L ∂ z ∗ \begin{aligned}
    z_{n+1} &= x_n - (\alpha/2) * \frac{\partial L}{\partial x} + 1j * (y_n - (\alpha/2) * \frac{\partial L}{\partial y}) \\
            &= z_n - \alpha * 1/2 * \left(\frac{\partial L}{\partial x} + j \frac{\partial L}{\partial y}\right) \\
            &= z_n - \alpha * \frac{\partial L}{\partial z^*}
\end{aligned} z n + 1 ​ ​ = x n ​ − ( α /2 ) ∗ ∂ x ∂ L ​ + 1 j ∗ ( y n ​ − ( α /2 ) ∗ ∂ y ∂ L ​ ) = z n ​ − α ∗ 1/2 ∗ ( ∂ x ∂ L ​ + j ∂ y ∂ L ​ ) = z n ​ − α ∗ ∂ z ∗ ∂ L ​ ​ Something very interesting has happened: Wirtinger calculus tells us
that we can simplify the complex variable update formula above to only
refer to the conjugate Wirtinger derivative ∂ L ∂ z ∗ \frac{\partial L}{\partial z^*} ∂ z ∗ ∂ L ​ , giving us exactly the step we take in optimization. Because the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative
when you differentiate a function with a real valued loss. How does PyTorch compute the conjugate Wirtinger derivative? ¶ Typically, our derivative formulas take in grad_output as an input,
representing the incoming Vector-Jacobian product that we’ve already
computed, aka, ∂ L ∂ s ∗ \frac{\partial L}{\partial s^*} ∂ s ∗ ∂ L ​ , where L L L is the loss of the entire computation (producing a real loss) and s s s is the output of our function. The goal here is to compute ∂ L ∂ z ∗ \frac{\partial L}{\partial z^*} ∂ z ∗ ∂ L ​ , where z z z is the input of
the function.  It turns out that in the case of real loss, we can
get away with only calculating ∂ L ∂ s ∗ \frac{\partial L}{\partial s^*} ∂ s ∗ ∂ L ​ ,
even though the chain rule implies that we also need to
have access to ∂ L ∂ s \frac{\partial L}{\partial s} ∂ s ∂ L ​ .  If you want
to skip this derivation, look at the last equation in this section
and then skip to the next section. Let’s continue working with f : C → C f: ℂ → ℂ f : C → C defined as f ( z ) = f ( x + y j ) = u ( x , y ) + v ( x , y ) j f(z) = f(x+yj) = u(x, y) + v(x, y)j f ( z ) = f ( x + y j ) = u ( x , y ) + v ( x , y ) j . As discussed above,
autograd’s gradient convention is centered around optimization for real
valued loss functions, so let’s assume f f f is a part of larger
real valued loss function g g g . Using chain rule, we can write: (1) ¶ ∂ L ∂ z ∗ = ∂ L ∂ u ∗ ∂ u ∂ z ∗ + ∂ L ∂ v ∗ ∂ v ∂ z ∗ \frac{\partial L}{\partial z^*} = \frac{\partial L}{\partial u} * \frac{\partial u}{\partial z^*} + \frac{\partial L}{\partial v} * \frac{\partial v}{\partial z^*} ∂ z ∗ ∂ L ​ = ∂ u ∂ L ​ ∗ ∂ z ∗ ∂ u ​ + ∂ v ∂ L ​ ∗ ∂ z ∗ ∂ v ​ Now using Wirtinger derivative definition, we can write: ∂ L ∂ s = 1 / 2 ∗ ( ∂ L ∂ u − ∂ L ∂ v j ) ∂ L ∂ s ∗ = 1 / 2 ∗ ( ∂ L ∂ u + ∂ L ∂ v j ) \begin{aligned}
    \frac{\partial L}{\partial s} = 1/2 * \left(\frac{\partial L}{\partial u} - \frac{\partial L}{\partial v} j\right) \\
    \frac{\partial L}{\partial s^*} = 1/2 * \left(\frac{\partial L}{\partial u} + \frac{\partial L}{\partial v} j\right)
\end{aligned} ∂ s ∂ L ​ = 1/2 ∗ ( ∂ u ∂ L ​ − ∂ v ∂ L ​ j ) ∂ s ∗ ∂ L ​ = 1/2 ∗ ( ∂ u ∂ L ​ + ∂ v ∂ L ​ j ) ​ It should be noted here that since u u u and v v v are real
functions, and L L L is real by our assumption that f f f is a
part of a real valued function, we have: (2) ¶ ( ∂ L ∂ s ) ∗ = ∂ L ∂ s ∗ \left( \frac{\partial L}{\partial s} \right)^* = \frac{\partial L}{\partial s^*} ( ∂ s ∂ L ​ ) ∗ = ∂ s ∗ ∂ L ​ i.e., ∂ L ∂ s \frac{\partial L}{\partial s} ∂ s ∂ L ​ equals to g r a d _ o u t p u t ∗ grad\_output^* g r a d _ o u tp u t ∗ . Solving the above equations for ∂ L ∂ u \frac{\partial L}{\partial u} ∂ u ∂ L ​ and ∂ L ∂ v \frac{\partial L}{\partial v} ∂ v ∂ L ​ , we get: (3) ¶ ∂ L ∂ u = ∂ L ∂ s + ∂ L ∂ s ∗ ∂ L ∂ v = 1 j ∗ ( ∂ L ∂ s − ∂ L ∂ s ∗ ) \begin{aligned}
    \frac{\partial L}{\partial u} = \frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*} \\
    \frac{\partial L}{\partial v} = 1j * \left(\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*}\right)
\end{aligned} ∂ u ∂ L ​ = ∂ s ∂ L ​ + ∂ s ∗ ∂ L ​ ∂ v ∂ L ​ = 1 j ∗ ( ∂ s ∂ L ​ − ∂ s ∗ ∂ L ​ ) ​ Substituting (3) in (1) , we get: ∂ L ∂ z ∗ = ( ∂ L ∂ s + ∂ L ∂ s ∗ ) ∗ ∂ u ∂ z ∗ + 1 j ∗ ( ∂ L ∂ s − ∂ L ∂ s ∗ ) ∗ ∂ v ∂ z ∗ = ∂ L ∂ s ∗ ( ∂ u ∂ z ∗ + ∂ v ∂ z ∗ j ) + ∂ L ∂ s ∗ ∗ ( ∂ u ∂ z ∗ − ∂ v ∂ z ∗ j ) = ∂ L ∂ s ∗ ∂ ( u + v j ) ∂ z ∗ + ∂ L ∂ s ∗ ∗ ∂ ( u + v j ) ∗ ∂ z ∗ = ∂ L ∂ s ∗ ∂ s ∂ z ∗ + ∂ L ∂ s ∗ ∗ ∂ s ∗ ∂ z ∗ \begin{aligned}
    \frac{\partial L}{\partial z^*} &= \left(\frac{\partial L}{\partial s} + \frac{\partial L}{\partial s^*}\right) * \frac{\partial u}{\partial z^*} + 1j * \left(\frac{\partial L}{\partial s} - \frac{\partial L}{\partial s^*}\right) * \frac{\partial v}{\partial z^*}  \\
                                    &= \frac{\partial L}{\partial s} * \left(\frac{\partial u}{\partial z^*} + \frac{\partial v}{\partial z^*} j\right) + \frac{\partial L}{\partial s^*} * \left(\frac{\partial u}{\partial z^*} - \frac{\partial v}{\partial z^*} j\right)  \\
                                    &= \frac{\partial L}{\partial s} * \frac{\partial (u + vj)}{\partial z^*} + \frac{\partial L}{\partial s^*} * \frac{\partial (u + vj)^*}{\partial z^*}  \\
                                    &= \frac{\partial L}{\partial s} * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * \frac{\partial s^*}{\partial z^*}    \\
\end{aligned} ∂ z ∗ ∂ L ​ ​ = ( ∂ s ∂ L ​ + ∂ s ∗ ∂ L ​ ) ∗ ∂ z ∗ ∂ u ​ + 1 j ∗ ( ∂ s ∂ L ​ − ∂ s ∗ ∂ L ​ ) ∗ ∂ z ∗ ∂ v ​ = ∂ s ∂ L ​ ∗ ( ∂ z ∗ ∂ u ​ + ∂ z ∗ ∂ v ​ j ) + ∂ s ∗ ∂ L ​ ∗ ( ∂ z ∗ ∂ u ​ − ∂ z ∗ ∂ v ​ j ) = ∂ s ∂ L ​ ∗ ∂ z ∗ ∂ ( u + v j ) ​ + ∂ s ∗ ∂ L ​ ∗ ∂ z ∗ ∂ ( u + v j ) ∗ ​ = ∂ s ∂ L ​ ∗ ∂ z ∗ ∂ s ​ + ∂ s ∗ ∂ L ​ ∗ ∂ z ∗ ∂ s ∗ ​ ​ Using (2) , we get: (4) ¶ ∂ L ∂ z ∗ = ( ∂ L ∂ s ∗ ) ∗ ∗ ∂ s ∂ z ∗ + ∂ L ∂ s ∗ ∗ ( ∂ s ∂ z ) ∗ = ( g r a d _ o u t p u t ) ∗ ∗ ∂ s ∂ z ∗ + g r a d _ o u t p u t ∗ ( ∂ s ∂ z ) ∗ \begin{aligned}
    \frac{\partial L}{\partial z^*} &= \left(\frac{\partial L}{\partial s^*}\right)^* * \frac{\partial s}{\partial z^*} + \frac{\partial L}{\partial s^*} * \left(\frac{\partial s}{\partial z}\right)^*  \\
                                    &= \boxed{ (grad\_output)^* * \frac{\partial s}{\partial z^*} + grad\_output * \left(\frac{\partial s}{\partial z}\right)^* }       \\
\end{aligned} ∂ z ∗ ∂ L ​ ​ = ( ∂ s ∗ ∂ L ​ ) ∗ ∗ ∂ z ∗ ∂ s ​ + ∂ s ∗ ∂ L ​ ∗ ( ∂ z ∂ s ​ ) ∗ = ( g r a d _ o u tp u t ) ∗ ∗ ∂ z ∗ ∂ s ​ + g r a d _ o u tp u t ∗ ( ∂ z ∂ s ​ ) ∗ ​ ​ This last equation is the important one for writing your own gradients,
as it decomposes our derivative formula into a simpler one that is easy
to compute by hand. How can I write my own derivative formula for a complex function? ¶ The above boxed equation gives us the general formula for all
derivatives on complex functions.  However, we still need to
compute ∂ s ∂ z \frac{\partial s}{\partial z} ∂ z ∂ s ​ and ∂ s ∂ z ∗ \frac{\partial s}{\partial z^*} ∂ z ∗ ∂ s ​ .
There are two ways you could do this: The first way is to just use the definition of Wirtinger derivatives directly and calculate ∂ s ∂ z \frac{\partial s}{\partial z} ∂ z ∂ s ​ and ∂ s ∂ z ∗ \frac{\partial s}{\partial z^*} ∂ z ∗ ∂ s ​ by
using ∂ s ∂ x \frac{\partial s}{\partial x} ∂ x ∂ s ​ and ∂ s ∂ y \frac{\partial s}{\partial y} ∂ y ∂ s ​ (which you can compute in the normal way). The second way is to use the change of variables trick and rewrite f ( z ) f(z) f ( z ) as a two variable function f ( z , z ∗ ) f(z, z^*) f ( z , z ∗ ) , and compute
the conjugate Wirtinger derivatives by treating z z z and z ∗ z^* z ∗ as independent variables. This is often easier; for example, if the function in question is holomorphic, only z z z will be used (and ∂ s ∂ z ∗ \frac{\partial s}{\partial z^*} ∂ z ∗ ∂ s ​ will be zero). Let’s consider the function f ( z = x + y j ) = c ∗ z = c ∗ ( x + y j ) f(z = x + yj) = c * z = c * (x+yj) f ( z = x + y j ) = c ∗ z = c ∗ ( x + y j ) as an example, where c ∈ R c \in ℝ c ∈ R . Using the first way to compute the Wirtinger derivatives, we have. ∂ s ∂ z = 1 / 2 ∗ ( ∂ s ∂ x − ∂ s ∂ y j ) = 1 / 2 ∗ ( c − ( c ∗ 1 j ) ∗ 1 j ) = c ∂ s ∂ z ∗ = 1 / 2 ∗ ( ∂ s ∂ x + ∂ s ∂ y j ) = 1 / 2 ∗ ( c + ( c ∗ 1 j ) ∗ 1 j ) = 0 \begin{aligned}
    \frac{\partial s}{\partial z} &= 1/2 * \left(\frac{\partial s}{\partial x} - \frac{\partial s}{\partial y} j\right) \\
                                  &= 1/2 * (c - (c * 1j) * 1j)  \\
                                  &= c                          \\
    \\
    \\
    \frac{\partial s}{\partial z^*} &= 1/2 * \left(\frac{\partial s}{\partial x} + \frac{\partial s}{\partial y} j\right) \\
                                    &= 1/2 * (c + (c * 1j) * 1j)  \\
                                    &= 0                          \\
\end{aligned} ∂ z ∂ s ​ ∂ z ∗ ∂ s ​ ​ = 1/2 ∗ ( ∂ x ∂ s ​ − ∂ y ∂ s ​ j ) = 1/2 ∗ ( c − ( c ∗ 1 j ) ∗ 1 j ) = c = 1/2 ∗ ( ∂ x ∂ s ​ + ∂ y ∂ s ​ j ) = 1/2 ∗ ( c + ( c ∗ 1 j ) ∗ 1 j ) = 0 ​ Using (4) , and grad_output = 1.0 (which is the default grad output value used when backward() is called on a scalar output in PyTorch), we get: ∂ L ∂ z ∗ = 1 ∗ 0 + 1 ∗ c = c \frac{\partial L}{\partial z^*} = 1 * 0 + 1 * c = c ∂ z ∗ ∂ L ​ = 1 ∗ 0 + 1 ∗ c = c Using the second way to compute Wirtinger derivatives, we directly get: ∂ s ∂ z = ∂ ( c ∗ z ) ∂ z = c ∂ s ∂ z ∗ = ∂ ( c ∗ z ) ∂ z ∗ = 0 \begin{aligned}
   \frac{\partial s}{\partial z} &= \frac{\partial (c*z)}{\partial z}       \\
                                 &= c                                       \\
    \frac{\partial s}{\partial z^*} &= \frac{\partial (c*z)}{\partial z^*}       \\
                                 &= 0
\end{aligned} ∂ z ∂ s ​ ∂ z ∗ ∂ s ​ ​ = ∂ z ∂ ( c ∗ z ) ​ = c = ∂ z ∗ ∂ ( c ∗ z ) ​ = 0 ​ And using (4) again, we get ∂ L ∂ z ∗ = c \frac{\partial L}{\partial z^*} = c ∂ z ∗ ∂ L ​ = c . As you can see, the second way involves lesser calculations, and comes
in more handy for faster calculations. What about cross-domain functions? ¶ Some functions map from complex inputs to real outputs, or vice versa.
These functions form a special case of (4) , which we can derive using the
chain rule: For f : C → R f: ℂ → ℝ f : C → R , we get: ∂ L ∂ z ∗ = 2 ∗ g r a d _ o u t p u t ∗ ∂ s ∂ z ∗ \frac{\partial L}{\partial z^*} = 2 * grad\_output * \frac{\partial s}{\partial z^{*}} ∂ z ∗ ∂ L ​ = 2 ∗ g r a d _ o u tp u t ∗ ∂ z ∗ ∂ s ​ For f : R → C f: ℝ → ℂ f : R → C , we get: ∂ L ∂ z ∗ = 2 ∗ R e ( g r a d _ o u t p u t ∗ ∗ ∂ s ∂ z ∗ ) \frac{\partial L}{\partial z^*} = 2 * \mathrm{Re}(grad\_output^* * \frac{\partial s}{\partial z^{*}}) ∂ z ∗ ∂ L ​ = 2 ∗ Re ( g r a d _ o u tp u t ∗ ∗ ∂ z ∗ ∂ s ​ ) Hooks for saved tensors ¶ You can control how saved tensors are packed / unpacked by defining a pair of pack_hook / unpack_hook hooks.  The pack_hook function should take a tensor as its single argument
but can return any python object (e.g. another tensor, a tuple, or even a
string containing a filename). The unpack_hook function takes as its single
argument the output of pack_hook and should return a tensor to be used in
the backward pass. The tensor returned by unpack_hook only needs to have
the same content as the tensor passed as input to pack_hook . In particular,
any autograd-related metadata can be ignored as they will be overwritten during
unpacking. An example of such pair is: class SelfDeletingTempFile (): def __init__ ( self ): self . name = os . path . join ( tmp_dir , str ( uuid . uuid4 ())) def __del__ ( self ): os . remove ( self . name ) def pack_hook ( tensor ): temp_file = SelfDeletingTempFile () torch . save ( tensor , temp_file . name ) return temp_file def unpack_hook ( temp_file ): return torch . load ( temp_file . name ) Notice that the unpack_hook should not delete the temporary file because it
might be called multiple times: the temporary file should be alive for as long
as the returned SelfDeletingTempFile object is alive.  In the above example,
we prevent leaking the temporary file by closing it when it is no longer needed
(on deletion of the SelfDeletingTempFile object). Note We guarantee that pack_hook will only be called once but unpack_hook can
be called as many times as the backward pass requires it and we expect it to
return the same data each time. Warning Performing inplace operations on the input of any of the functions is forbidden
as they may lead to unexpected side-effects. PyTorch will throw an error if the
input to a pack hook is modified inplace but does not catch the case where the
input to an unpack hook is modified inplace. Registering hooks for a saved tensor ¶ You can register a pair of hooks on a saved tensor by calling the register_hooks() method on a SavedTensor object. Those objects are exposed as attributes of a grad_fn and start with the _raw_saved_ prefix. x = torch . randn ( 5 , requires_grad = True ) y = x . pow ( 2 ) y . grad_fn . _raw_saved_self . register_hooks ( pack_hook , unpack_hook ) The pack_hook method is called as soon as the pair is registered.
The unpack_hook method is called each time the saved tensor needs to be
accessed, either by means of y.grad_fn._saved_self or during the backward
pass. Warning If you maintain a reference to a SavedTensor after the saved
tensors have been released (i.e. after backward has been called), calling
its register_hooks() is forbidden.
PyTorch will throw an error most of the time but it may fail
to do so in some cases and undefined behavior may arise. Registering default hooks for saved tensors ¶ Alternatively, you can use the context-manager saved_tensors_hooks to register a pair of
hooks which will be applied to all saved tensors that are created in
that context. Example: # Only save on disk tensors that have size >= 1000 SAVE_ON_DISK_THRESHOLD = 1000 def pack_hook ( x ): if x . numel () < SAVE_ON_DISK_THRESHOLD : return x temp_file = SelfDeletingTempFile () torch . save ( tensor , temp_file . name ) return temp_file def unpack_hook ( tensor_or_sctf ): if isinstance ( tensor_or_sctf , torch . Tensor ): return tensor_or_sctf return torch . load ( tensor_or_sctf . name ) class Model ( nn . Module ): def forward ( self , x ): with torch . autograd . graph . saved_tensors_hooks ( pack_hook , unpack_hook ): # ... compute output output = x return output model = Model () net = nn . DataParallel ( model ) The hooks defined with this context manager are thread-local.
Hence, the following code will not produce the desired effects because the hooks do not go
through DataParallel . # Example what NOT to do net = nn . DataParallel ( model ) with torch . autograd . graph . saved_tensors_hooks ( pack_hook , unpack_hook ): output = net ( input ) Note that using those hooks disables all the optimization in place to reduce
Tensor object creation. For example: with torch . autograd . graph . saved_tensors_hooks ( lambda x : x , lambda x : x ): x = torch . randn ( 5 , requires_grad = True ) y = x * x Without the hooks, x , y.grad_fn._saved_self and y.grad_fn._saved_other all refer to the same tensor object.
With the hooks, PyTorch will pack and unpack x into two new tensor objects
that share the same storage with the original x (no copy performed). Backward Hooks execution ¶ This section will discuss when different hooks fire or don’t fire.
Then it will discuss the order in which they are fired.
The hooks that will be covered are: backward hooks registered to Tensor via torch.Tensor.register_hook() , post-accumulate-grad hooks registered to
Tensor via torch.Tensor.register_post_accumulate_grad_hook() , post-hooks
registered to Node via torch.autograd.graph.Node.register_hook() , and
pre-hooks registered to Node via torch.autograd.graph.Node.register_prehook() . Whether a particular hook will be fired ¶ Hooks registered to a Tensor via torch.Tensor.register_hook() are executed when gradients are being computed for that Tensor. (Note that this does not require
the Tensor’s grad_fn to be executed. For example, if the Tensor is passed
as part of the inputs argument to torch.autograd.grad() ,
the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.) Hooks registered to a Tensor via torch.Tensor.register_post_accumulate_grad_hook() are executed after the gradients have been accumulated for that Tensor, meaning the
Tensor’s grad field has been set. Whereas hooks registered via torch.Tensor.register_hook() are run as gradients are being computed, hooks registered via torch.Tensor.register_post_accumulate_grad_hook() are only triggered once the Tensor’s grad field is updated by autograd at the end of
the backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf
Tensors. Registering a hook via torch.Tensor.register_post_accumulate_grad_hook() on a non-leaf Tensor will error, even if you call backward(retain_graph=True) . Hooks registered to torch.autograd.graph.Node using torch.autograd.graph.Node.register_hook() or torch.autograd.graph.Node.register_prehook() are only fired if
the Node it was registered to is executed. Whether a particular Node is executed may depend on whether the backward pass was called with torch.autograd.grad() or torch.autograd.backward() .
Specifically, you should be aware of these differences when you register a hook on a
Node corresponding to a Tensor that you are passing to torch.autograd.grad() or torch.autograd.backward() as part of the inputs argument. If you are using torch.autograd.backward() , all of the above mentioned hooks will be executed,
whether or not you specified the inputs argument. This is because .backward() executes all
Nodes, even if they correspond to a Tensor specified as an input.
(Note that the execution of this additional Node corresponding to Tensors passed as inputs is usually unnecessary, but done anyway. This behavior is subject to change;
you should not depend on it.) On the other hand, if you are using torch.autograd.grad() , the backward hooks registered
to Nodes that correspond to the Tensors passed to input may not be executed, because
those Nodes will not be executed unless there is another input that depends on the gradient
result of this Node. The order in which the different hooks are fired ¶ The order in which things happen are: hooks registered to Tensor are executed pre-hooks registered to Node are executed (if Node is executed). the .grad field is updated for Tensors that retain_grad Node is executed (subject to rules above) for leaf Tensors that have .grad accumulated, post-accumulate-grad hooks are executed post-hooks registered to Node are executed (if Node is executed) If multiple hooks of the same type are registered on the same Tensor or Node
they are executed in the order in which they are registered.
Hooks that are executed later can observe the modifications to the gradient made by
earlier hooks. Special hooks ¶ torch.autograd.graph.register_multi_grad_hook() is implemented using hooks registered
to Tensors. Each individual Tensor hook is fired following the Tensor hook ordering
defined above and the registered multi-grad hook is called when the last Tensor gradient
is computed. torch.nn.modules.module.register_module_full_backward_hook() is implemented using hooks
registered to Node. As the forward is computed, hooks are registered to grad_fn corresponding
to the inputs and outputs of the module. Because a module may take multiple inputs and return
multiple outputs, a dummy custom autograd Function is first applied to the inputs of the module
before forward and the outputs of the module before the output of forward is returned to ensure
that those Tensors share a single grad_fn, which we can then attach our hooks to. Behavior of Tensor hooks when Tensor is modified in-place ¶ Usually hooks registered to a Tensor receive the gradient of the outputs with respect to that
Tensor, where the value of the Tensor is taken to be its value at the time backward is computed. However, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks
registered before in-place modification similarly receive gradients of the outputs with
respect to the Tensor, but the value of the Tensor is taken to be its value before
in-place modification. If you prefer the behavior in the former case,
you should register them to the Tensor after all in-place modifications to it have been made.
For example: t = torch . tensor ( 1. , requires_grad = True ) . sin () t . cos_ () t . register_hook ( fn ) t . backward () Furthermore, it can be helpful to know that under the hood,
when hooks are registered to a Tensor, they actually become permanently bound to the grad_fn
of that Tensor, so if that Tensor is then modified in-place,
even though the Tensor now has a new grad_fn, hooks registered before it was
modified in-place will continue to be associated with the old grad_fn, e.g. they will
fire when that Tensor’s old grad_fn is reached in the graph by the autograd engine.",45135,9,37,"x = torch.randn(5, requires_grad=True)
y = x.pow(2)
print(x.equal(y.grad_fn._saved_self))  # True
print(x is y.grad_fn._saved_self)  # True
---
x = torch.randn(5, requires_grad=True)
y = x.exp()
print(y.equal(y.grad_fn._saved_result))  # True
print(y is y.grad_fn._saved_result)  # False
---
# Define a train function to be used in different threads
def train_fn():
    x = torch.ones(5, 5, requires_grad=True)
    # forward
    y = (x + 3) * (x + 4) * 0.5
    # backward
    y.sum().backward()
    # potential optimizer update


# User write their own threading code to drive the train_fn
threads = []
for _ in range(10):
    p = threading.Thread(target=train_fn, args=())
    p.start()
    threads.append(p)

for p in threads:
    p.join()
---
class SelfDeletingTempFile():
    def __init__(self):
        self.name = os.path.join(tmp_dir, str(uuid.uuid4()))

    def __del__(self):
        os.remove(self.name)

def pack_hook(tensor):
    temp_file = SelfDeletingTempFile()
    torch.save(tensor, temp_file.name)
    return temp_file

def unpack_hook(temp_file):
    return torch.load(temp_file.name)
---
x = torch.randn(5, requires_grad=True)
y = x.pow(2)
y.grad_fn._raw_saved_self.register_hooks(pack_hook, unpack_hook)
---
# Only save on disk tensors that have size >= 1000
SAVE_ON_DISK_THRESHOLD = 1000

def pack_hook(x):
    if x.numel() < SAVE_ON_DISK_THRESHOLD:
        return x
    temp_file = SelfDeletingTempFile()
    torch.save(tensor, temp_file.name)
    return temp_file

def unpack_hook(tensor_or_sctf):
    if isinstance(tensor_or_sctf, torch.Tensor):
        return tensor_or_sctf
    return torch.load(tensor_or_sctf.name)

class Model(nn.Module):
    def forward(self, x):
        with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
          # ... compute output
          output = x
        return output

model = Model()
net = nn.DataParallel(model)
---
# Example what NOT to do

net = nn.DataParallel(model)
with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
    output = net(input)
---
with torch.autograd.graph.saved_tensors_hooks(lambda x: x, lambda x: x):
    x = torch.randn(5, requires_grad=True)
    y = x * x
---
t = torch.tensor(1., requires_grad=True).sin()
t.cos_()
t.register_hook(fn)
t.backward()",1752175597.462221
https://pytorch.org/docs/stable/,PyTorch documentation — PyTorch 2.7 documentation,"PyTorch documentation ¶ PyTorch is an optimized tensor library for deep learning using GPUs and CPUs. Features described in this documentation are classified by release status: Stable: These features will be maintained long-term and there should generally
be no major performance limitations or gaps in documentation.
We also expect to maintain backwards compatibility (although
breaking changes can happen and notice will be given one release ahead
of time). Beta: These features are tagged as Beta because the API may change based on
user feedback, because the performance needs to improve, or because
coverage across operators is not yet complete. For Beta features, we are
committing to seeing the feature through to the Stable classification.
We are not, however, committing to backwards compatibility. Prototype: These features are typically not available as part of
binary distributions like PyPI or Conda, except sometimes behind run-time
flags, and are at an early stage for feedback and testing. Indices and tables ¶ Index Module Index",1045,0,5,,1752175598.5188153
https://pytorch.org/docs/stable/notes/randomness.html,Reproducibility — PyTorch 2.7 documentation,"Reproducibility ¶ Completely reproducible results are not guaranteed across PyTorch releases,
individual commits, or different platforms. Furthermore, results may not be
reproducible between CPU and GPU executions, even when using identical seeds. However, there are some steps you can take to limit the number of sources of
nondeterministic behavior for a specific platform, device, and PyTorch release.
First, you can control sources of randomness that can cause multiple executions
of your application to behave differently. Second, you can configure PyTorch
to avoid using nondeterministic algorithms for some operations, so that multiple
calls to those operations, given the same inputs, will produce the same result. Warning Deterministic operations are often slower than nondeterministic operations, so
single-run performance may decrease for your model. However, determinism may
save time in development by facilitating experimentation, debugging, and
regression testing. Controlling sources of randomness ¶ PyTorch random number generator ¶ You can use torch.manual_seed() to seed the RNG for all devices (both
CPU and CUDA): import torch torch . manual_seed ( 0 ) Some PyTorch operations may use random numbers internally. torch.svd_lowrank() does this, for instance. Consequently, calling it
multiple times back-to-back with the same input arguments may give different
results. However, as long as torch.manual_seed() is set to a constant
at the beginning of an application and all other sources of nondeterminism have
been eliminated, the same series of random numbers will be generated each time
the application is run in the same environment. It is also possible to obtain identical results from an operation that uses
random numbers by setting torch.manual_seed() to the same value between
subsequent calls. Python ¶ For custom operators, you might need to set python seed as well: import random random . seed ( 0 ) Random number generators in other libraries ¶ If you or any of the libraries you are using rely on NumPy, you can seed the global
NumPy RNG with: import numpy as np np . random . seed ( 0 ) However, some applications and libraries may use NumPy Random Generator objects,
not the global RNG
( https://numpy.org/doc/stable/reference/random/generator.html ), and those will
need to be seeded consistently as well. If you are using any other libraries that use random number generators, refer to
the documentation for those libraries to see how to set consistent seeds for them. CUDA convolution benchmarking ¶ The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism
across multiple executions of an application. When a cuDNN convolution is called with a
new set of size parameters, an optional feature can run multiple convolution algorithms,
benchmarking them to find the fastest one. Then, the fastest algorithm will be used
consistently during the rest of the process for the corresponding set of size parameters.
Due to benchmarking noise and different hardware, the benchmark may select different
algorithms on subsequent runs, even on the same machine. Disabling the benchmarking feature with torch.backends.cudnn.benchmark = False causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced
performance. However, if you do not need reproducibility across multiple executions of your application,
then performance might improve if the benchmarking feature is enabled with torch.backends.cudnn.benchmark = True . Note that this setting is different from the torch.backends.cudnn.deterministic setting discussed below. Avoiding nondeterministic algorithms ¶ torch.use_deterministic_algorithms() lets you configure PyTorch to use
deterministic algorithms instead of nondeterministic ones where available, and
to throw an error if an operation is known to be nondeterministic (and without
a deterministic alternative). Please check the documentation for torch.use_deterministic_algorithms() for a full list of affected operations. If an operation does not act correctly
according to the documentation, or if you need a deterministic implementation
of an operation that does not have one, please submit an issue: https://github.com/pytorch/pytorch/issues?q=label:%22module:%20determinism%22 For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_() will throw an error: >>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ... When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a
nondeterministic algorithm, but when the deterministic flag is turned on, its alternate
deterministic implementation will be used: >>> import torch >>> torch . use_deterministic_algorithms ( True ) >>> torch . bmm ( torch . randn ( 2 , 2 , 2 ) . to_sparse () . cuda (), torch . randn ( 2 , 2 , 2 ) . cuda ()) tensor([[[ 1.1900, -2.3409], [ 0.4796,  0.8003]], [[ 0.1509,  1.8027], [ 0.0333, -1.1444]]], device='cuda:0') Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you
should set the environment variable CUBLAS_WORKSPACE_CONFIG according to CUDA documentation: https://docs.nvidia.com/cuda/cublas/index.html#results-reproducibility CUDA convolution determinism ¶ While disabling CUDA convolution benchmarking (discussed above) ensures that
CUDA selects the same algorithm each time an application is run, that algorithm
itself may be nondeterministic, unless either torch.use_deterministic_algorithms(True) or torch.backends.cudnn.deterministic = True is set. The latter setting
controls only this behavior, unlike torch.use_deterministic_algorithms() which will make other PyTorch operations behave deterministically, too. CUDA RNN and LSTM ¶ In some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior.
See torch.nn.RNN() and torch.nn.LSTM() for details and workarounds. Filling uninitialized memory ¶ Operations like torch.empty() and torch.Tensor.resize_() can return
tensors with uninitialized memory that contain undefined values. Using such a
tensor as an input to another operation is invalid if determinism is required,
because the output will be nondeterministic. But there is nothing to actually
prevent such invalid code from being run. So for safety, torch.utils.deterministic.fill_uninitialized_memory is set to True by default, which will fill the uninitialized memory with a known value if torch.use_deterministic_algorithms(True) is set. This will prevent the
possibility of this kind of nondeterministic behavior. However, filling uninitialized memory is detrimental to performance. So if your
program is valid and does not use uninitialized memory as the input to an
operation, then this setting can be turned off for better performance. DataLoader ¶ DataLoader will reseed workers following Randomness in multi-process data loading algorithm.
Use worker_init_fn() and generator to preserve reproducibility: def seed_worker ( worker_id ): worker_seed = torch . initial_seed () % 2 ** 32 numpy . random . seed ( worker_seed ) random . seed ( worker_seed ) g = torch . Generator () g . manual_seed ( 0 ) DataLoader ( train_dataset , batch_size = batch_size , num_workers = num_workers , worker_init_fn = seed_worker , generator = g , )",7542,6,14,"import torch
torch.manual_seed(0)
---
import random
random.seed(0)
---
import numpy as np
np.random.seed(0)
---
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
RuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set
'torch.use_deterministic_algorithms(True)'. ...
---
>>> import torch
>>> torch.use_deterministic_algorithms(True)
>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())
tensor([[[ 1.1900, -2.3409],
         [ 0.4796,  0.8003]],
        [[ 0.1509,  1.8027],
         [ 0.0333, -1.1444]]], device='cuda:0')
---
def seed_worker(worker_id):
    worker_seed = torch.initial_seed() % 2**32
    numpy.random.seed(worker_seed)
    random.seed(worker_seed)

g = torch.Generator()
g.manual_seed(0)

DataLoader(
    train_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    worker_init_fn=seed_worker,
    generator=g,
)",1752175599.566987
https://pytorch.org/docs/stable/notes/ddp.html,Distributed Data Parallel — PyTorch 2.7 documentation,"Distributed Data Parallel ¶ Warning The implementation of torch.nn.parallel.DistributedDataParallel evolves over time. This design note is written based on the state as of v1.4. torch.nn.parallel.DistributedDataParallel (DDP) transparently performs
distributed data parallel training. This page describes how it works and reveals
implementation details. Example ¶ Let us start with a simple torch.nn.parallel.DistributedDataParallel example. This example uses a torch.nn.Linear as the local model, wraps
it with DDP, and then runs one forward pass, one backward pass, and an optimizer
step on the DDP model. After that, parameters on the local model will be
updated, and all models on different processes should be exactly the same. import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim import os from torch.nn.parallel import DistributedDataParallel as DDP def example ( rank , world_size ): # create default process group dist . init_process_group ( ""gloo"" , rank = rank , world_size = world_size ) # create local model model = nn . Linear ( 10 , 10 ) . to ( rank ) # construct DDP model ddp_model = DDP ( model , device_ids = [ rank ]) # define loss function and optimizer loss_fn = nn . MSELoss () optimizer = optim . SGD ( ddp_model . parameters (), lr = 0.001 ) # forward pass outputs = ddp_model ( torch . randn ( 20 , 10 ) . to ( rank )) labels = torch . randn ( 20 , 10 ) . to ( rank ) # backward pass loss_fn ( outputs , labels ) . backward () # update parameters optimizer . step () def main (): world_size = 2 mp . spawn ( example , args = ( world_size ,), nprocs = world_size , join = True ) if __name__ == ""__main__"" : # Environment variables which need to be # set when using c10d's default ""env"" # initialization mode. os . environ [ ""MASTER_ADDR"" ] = ""localhost"" os . environ [ ""MASTER_PORT"" ] = ""29500"" main () DDP works with TorchDynamo.  When used with TorchDynamo, apply the DDP model wrapper
before compiling the model, such that torchdynamo can apply DDPOptimizer (graph-break optimizations) based on DDP bucket sizes.  (See TorchDynamo DDPOptimizer for more information.) ddp_model = DDP ( model , device_ids = [ rank ]) ddp_model = torch . compile ( ddp_model ) Internal Design ¶ This section reveals how it works under the hood of torch.nn.parallel.DistributedDataParallel by diving into details of
every step in one iteration. Prerequisite : DDP relies on c10d ProcessGroup for communications.
Hence, applications must create ProcessGroup instances before constructing
DDP. Construction : The DDP constructor takes a reference to the local module,
and broadcasts state_dict() from the process with rank 0 to all other
processes in the group to make sure that all model replicas start from the
exact same state. Then, each DDP process creates a local Reducer , which
later will take care of the gradients synchronization during the backward
pass. To improve communication efficiency, the Reducer organizes parameter
gradients into buckets, and reduces one bucket at a time. Bucket size can be
configured by setting the bucket_cap_mb argument in DDP constructor. The
mapping from parameter gradients to buckets is determined at the construction
time, based on the bucket size limit and parameter sizes. Model parameters are
allocated into buckets in (roughly) the reverse order of Model.parameters() from the given model. The reason for using the reverse
order is because DDP expects gradients to become ready during the backward
pass in approximately that order. The figure below shows an example. Note
that, the grad0 and grad1 are in bucket1 , and the other two
gradients are in bucket0 . Of course, this assumption might not always
be true, and when that happens it could hurt DDP backward speed as the Reducer cannot kick off the communication at the earliest possible time.
Besides bucketing, the Reducer also registers autograd hooks during
construction, one hook per parameter. These hooks will be triggered during
the backward pass when the gradient becomes ready. Forward Pass : The DDP takes the input and passes it to the local model,
and then analyzes the output from the local model if find_unused_parameters is set to True . This mode allows running
backward on a subgraph of the model, and DDP finds out which parameters are
involved in the backward pass by traversing the autograd graph from the model
output and marking all unused parameters as ready for reduction. During the
backward pass, the Reducer would only wait for unready parameters, but it
would still reduce all buckets. Marking a parameter gradient as ready does not
help DDP skip buckets as for now, but it will prevent DDP from waiting for
absent gradients forever during the backward pass. Note that traversing the
autograd graph introduces extra overheads, so applications should only set find_unused_parameters to True when necessary. Backward Pass : The backward() function is directly invoked on the loss Tensor , which is out of DDP’s control, and DDP uses autograd hooks
registered at construction time to trigger gradients synchronizations. When
one gradient becomes ready, its corresponding DDP hook on that grad
accumulator will fire, and DDP will then mark that parameter gradient as
ready for reduction. When gradients in one bucket are all ready, the Reducer kicks off an asynchronous allreduce on that bucket to
calculate mean of gradients across all processes. When all buckets are ready,
the Reducer will block waiting for all allreduce operations to finish.
When this is done, averaged gradients are written to the param.grad field
of all parameters. So after the backward pass, the grad field on the same
corresponding parameter across different DDP processes should be the same. Optimizer Step : From the optimizer’s perspective, it is optimizing a local
model. Model replicas on all DDP processes can keep in sync because they all
start from the same state and they have the same averaged gradients in
every iteration. Note DDP requires Reducer instances on all processes to invoke allreduce in exactly the same order, which is done by always running allreduce in the bucket index order instead of actual bucket ready order. Mismatched allreduce order across processes can lead to wrong results or DDP backward
hang. Implementation ¶ Below are pointers to the DDP implementation components. The stacked graph shows
the structure of the code. ProcessGroup ¶ ProcessGroup.hpp :
contains the abstract API of all process group implementations. The c10d library provides 3 implementations out of the box, namely, ProcessGroupGloo , ProcessGroupNCCL , and ProcessGroupMPI . DistributedDataParallel uses ProcessGroup::broadcast() to send
model states from the process with rank 0 to others during initialization
and ProcessGroup::allreduce() to sum gradients. Store.hpp :
assists the rendezvous service for process group instances to find each other. DistributedDataParallel ¶ distributed.py :
is the Python entry point for DDP. It implements the initialization steps and
the forward function for the nn.parallel.DistributedDataParallel module which call into C++ libraries. Its _sync_param function performs
intra-process parameter synchronization when one DDP process works on multiple
devices, and it also broadcasts model buffers from the process with rank 0 to
all other processes. The inter-process parameter synchronization happens in Reducer.cpp . comm.h :
implements the coalesced broadcast helper function which is invoked to
broadcast model states during initialization and synchronize model buffers
before the forward pass. reducer.h :
provides the core implementation for gradient synchronization in the backward
pass. It has three entry point functions: Reducer : The constructor is called in distributed.py which registers Reducer::autograd_hook() to gradient accumulators. autograd_hook() function will be invoked by the autograd engine when
a gradient becomes ready. prepare_for_backward() is called at the end of DDP forward pass in distributed.py . It traverses the autograd graph to find unused
parameters when find_unused_parameters is set to True in DDP
constructor. TorchDynamo DDPOptimizer ¶ DDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards.
AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph,
because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes. TorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets
during backwards.  Note: the goal is to break the graph during backwards, and the simplest implementation is to
break the forward graphs and then call AotAutograd and compilation on each section.  This allows DDP’s allreduce hooks
to fire in-between sections of backwards, and schedule communications to overlap with compute. See this blog post for
a more in-depth explanation and experimental results, or read the docs and code at torch/_dynamo/optimizations/distributed.py To Debug DDPOptimizer, set TORCH_LOGS=’ddp_graphs’ for full graph dumps. For logs without graphs, add any of ‘dynamo’, ‘distributed’, or ‘dist_ddp’ to TORCH_LOGS (for basic info about bucket boundaries).  To disable DDPOptimizer, set torch._dynamo.config.optimize_ddp=False .
DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.",9523,2,10,"import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
import os
from torch.nn.parallel import DistributedDataParallel as DDP


def example(rank, world_size):
    # create default process group
    dist.init_process_group(""gloo"", rank=rank, world_size=world_size)
    # create local model
    model = nn.Linear(10, 10).to(rank)
    # construct DDP model
    ddp_model = DDP(model, device_ids=[rank])
    # define loss function and optimizer
    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    # forward pass
    outputs = ddp_model(torch.randn(20, 10).to(rank))
    labels = torch.randn(20, 10).to(rank)
    # backward pass
    loss_fn(outputs, labels).backward()
    # update parameters
    optimizer.step()

def main():
    world_size = 2
    mp.spawn(example,
        args=(world_size,),
        nprocs=world_size,
        join=True)

if __name__==""__main__"":
    # Environment variables which need to be
    # set when using c10d's default ""env""
    # initialization mode.
    os.environ[""MASTER_ADDR""] = ""localhost""
    os.environ[""MASTER_PORT""] = ""29500""
    main()
---
ddp_model = DDP(model, device_ids=[rank])
ddp_model = torch.compile(ddp_model)",1752175600.6315124
https://pytorch.org/docs/stable/notes/fsdp.html,FSDP Notes — PyTorch 2.7 documentation,"FSDP Notes ¶ FSDP Prefetch Nuances ¶ For overlapping forward all-gathers with forward compute, there are two possible mechanisms: Implicit forward prefetching (always enabled) Explicit forward prefetching ( forward_prefetch=True ) Implicit forward prefetching refers to relying on issuing the all-gathers from a separate CUDA
stream to allow for overlapping an all-gather with forward compute issued before it (from the CPU
perspective). For example, if we have layer 0 all-gather -> layer 0 forward compute -> layer 1
all-gather -> …, then layer 1 all-gather can overlap with layer 0 forward compute even though the
CPU thread issued it afterwards. (The 1st all-gather will not be able to overlap with anything.) Explicit forward prefetching refers to changing the CPU thread’s issue order: e.g. layer 0
all-gather -> layer 1 all-gather -> layer 0 forward compute -> …. In eager mode, there is no way to
know in general which layer is the next layer (e.g. layer 1 in the example) when still executing on
layer 0. Therefore, explicit forward prefetching should only be used for models whose execution
order is fixed from iteration to iteration (which we sometimes call “static graph”). An example of a
model that does not satisfy this constraint is FLAVA ). Explicit forward prefetching only saves the time taken to issue a layer’s forward compute kernels at
the cost that the next all-gather’s output tensor must be allocated while the current one is still
in use. By issuing the next all- gather before the current forward compute kernels, the next
all-gather can start sooner on GPU. For most LLM workloads, this is not the case, so there is no
motivation for enabling forward_prefetch=True . In contrast, for backward , we must use explicit backward prefetching or else there will be 0 overlap
of communication and computation. The reason is because we use a single NCCL process group for both
all-gather and reduce-scatter (partially because in earlier NCCL versions, it was not safe to use
multiple concurrently on the same device over the same ranks). A single NCCL process group means a
single internal NCCL stream on which reduce-scatters and all-gathers run serially. As such, unless
we explicitly reorder the CPU issue order to be next all-gather -> current reduce-scatter, then the
current reduce-scatter would block the next all-gather and hence the next backward computation,
preventing the current reduce-scatter from overlapping. Communication payload size ¶ In FSDP the communications are: all-gather on parameters in forward all-gather on parameters in backward reduce-scatter on gradients in backward If activation checkpointing ( checkpoint() ) is used there is no
additional communication since the parameters are prefetched anyway during backward . In the FSDP design, the communication payload per rank is determined as follows: Each call to FullyShardedDataParallel creates one communication group consisting of the parameters in module.parameters() except any already assigned to a nested FullyShardedDataParallel instance. For example, for Llama, if you apply FullyShardedDataParallel to every
transformer block and also to the root module, then there is one communication group for each
transformer block and finally one communication group with the initial embedding and final linear.
Each communication group corresponds to a single all-gather call and single reduce-scatter call. In
that way, how you apply FullyShardedDataParallel determines the communication size. In
general, applying FSDP to each transformer block is a good heuristic for LLMs, and it is hard to do
better than that given the current design. Let’s consider an example where we have a Transformer-based model sharded over 8 GPUs, where the
sharding happens at the transformer block-level only, and each transformer block contains 1.6B
parameters and the parameters are in fp32 (4 bytes each). Which means that once sharded, each
transformer block will contain 0.2B parameters on each rank. The forward pass will communicate in chunks of 0.2*4 = 0.8GB in all-gather The backward pass will communicate 2 times 0.8GB each (1x all-gather and 1x reduce-scatter) In other words there will be 3 communications with a payload of 0.8GB each. If the model was
comprised of 10 transformer blocks there would be a total of 30 communications for a total of 30*0.8=24GB . To formalize the payload size per communication per rank is total_transformer_block_params_in_B*dtype_bytes/num_gpus (GBs). Please note that in this example we didn’t include the additional communications required for the
embedding, which should be accounted for as well. And the math would depend on whether the input and
output embeddings are tied or not. If they aren’t tied there will be 2x more communications. FSDP buffers sizes ¶ First, let’s cover the buffers allocated for communications: forward currently requires 2x all-gather buffer size. Here is why: As explained in FSDP Prefetch Nuances in the case of explicit forward prefetching
( forward_prefetch=True`) case of layer 0 all-gather -> layer 0 forward compute -> layer 1 all-gather there is a need for 2 all-gather-sized buffers, because one buffer is used in the current ``forward while the other is used to do the prefetching. While the implicit forward prefetching ( forward_prefetch=False , default) case of the same sequence in theory should need only 1 buffer, in reality it’s still 2x all-gather-sized buffers. The reason is that in the flat-parameter FSDP design, we do not copy-out of the all-gather buffer. The parameters used for compute are directly viewed into the all-gather buffer (in fact, the main benefit of the “flat parameter” is exactly this reason). In that case, while ‘layer 1 all-gather’ is overlapping with ‘layer 0 forward compute’, the ‘layer 0 forward compute’ is using the parameters viewed into the ‘layer 0 all-gather’ buffer. A natural question then is, when would you want forward_prefetch=False ? For static-graph models (like most LLMs), there is a major technical reason. It is more that, practically, we added this option quickly for some CPU-bound internal models and have not tested every code path with it in unit testing, so we are less confident in it. forward_prefetching=False can be slightly easier to reason about since we do not have to check the recorded forward order as a possible ‘failure mode’; a module’s all-gather can always be found under its own record_function label in its profiler trace. backward currently requires at least 2x all-gather buffer size and potentially a bit more. Here is why: The current FSDP design uses recordStream to manage allocations produced in one stream consumed in another, which can lead to more memory usage than expected. How much more can be “non-deterministic” in that it depends on GPU kernel timing relative to the CPU. The limit_all_gathers=True argument is a mitigation to that - for more details refer to this discussion is FSDP & CUDACachingAllocator . The way existing FSDP works with autograd: Existing FSDP all-gathers the flat_param , which is the autograd leaf. It calls torch.split to get 1D views into the flat_param corresponding to its constituent original parameters. It calls torch.view on each 1D split to view back to ND. This means that in backward , we end up with ViewBackward (ND -> 1D) and SplitWithSizesBackward (which is a concat). In particular, each individual gradient is computed as a separate allocation, and an explicit concat happens to construct the reduce-scatter input buffer. This implies actually a 2x buffer size for reduce-scatter at that peak memory point. In summary, for backward , it is about 2x buffer size for reduce-scatter plus any recordStream effects. Second, let’s discuss the additional buffers: Once the sharded parameters are gathered from all ranks, they require an additional buffer of total_transformer_block_params_in_B*dtype_bytes for the full parameters - so continuing the example from earlier if each transformer block is 1.6B parameters and the parameters are in fp32, then it’d be 1.6*4=6.4GB buffer. And there is a need for 2 of those buffers, since there is one currently being used and another being prefetched. To summarize, we have: 2 times communication buffers of total_transformer_block_params_in_B*dtype_bytes/num_gpus 2 times unsharded transformer block parameters buffer ``total_transformer_block_params_in_B*dtype_bytes or if you have been following the example: 2*1.6*4/8=1.6GB 2**1.6*4=12.8GB and the total of 14.4GB . Now let’s briefly discuss what happens to the embeddings as we have left those out from the calculations: Given the rule we discussed that you included in the note starting with “the communication buffer
size is determined as follows”, we can analyze as follows: Suppose we apply FSDP to the root module (e.g. the Transformer class). Suppose we further apply FSDP to each transformer block (e.g. the TransformerBlock class). Most commonly, the embedding and final linear projection are direct children of the root Transformer class. Following our rule, that means that the embedding and final linear projection are assigned to the root Transformer ’s flat parameter. We have _another_ special rule, which is that the root does not free its parameters after forward because they will be anyways immediately all-gathered in backward. Putting this together, this means that the root’s flat parameter including the embedding and final projection are all-gathered to begin forward and kept in GPU memory until the end of backward. If the embedding and final linear are not weight-tied, then we _could_ further apply FSDP to the embedding and to the final linear. For weight-tied parameters, we require them to be part of the same flat parameter (or else it would get double-counted). That would allow the embedding to be freed after its usage in forward and only all-gathered toward the end of backward. Hopefully, this gives a better sense – each FSDP module gets assigned parameters in its module.parameters except those already assigned to another nested FSDP module, and the FSDP module’s forward defines the ‘live’ interval for its parameters. Hence, the nested nn.Module structure can affect the all-gather/free schedule and hence the memory/throughput performance.",10280,0,7,,1752175601.6799061
https://pytorch.org/docs/stable/mtia.memory.html,torch.mtia.memory — PyTorch 2.7 documentation,"torch.mtia.memory ¶ The MTIA backend is implemented out of the tree, only interfaces are be defined here. This package adds support for device memory management implemented in MTIA. memory_stats Return a dictionary of MTIA memory allocator statistics for a given device.",270,0,4,,1752175602.7637434
https://pytorch.org/docs/stable/community/design.html,PyTorch Design Philosophy — PyTorch 2.7 documentation,"PyTorch Design Philosophy ¶ This document is designed to help contributors and module maintainers
understand the high-level design principles that have developed over
time in PyTorch. These are not meant to be hard-and-fast rules, but to
serve as a guide to help trade off different concerns and to resolve
disagreements that may come up while developing PyTorch. For more
information on contributing, module maintainership, and how to escalate a
disagreement to the Core Maintainers, please see PyTorch
Governance . Design Principles ¶ Principle 1: Usability over Performance ¶ This principle may be surprising! As one Hacker News poster wrote: PyTorch is amazing! […] Although I’m confused. How can a ML framework be
not obsessed with speed/performance? See Hacker News discussion on
PyTorch . Soumith’s blog post on Growing the PyTorch
Community goes into this in some depth, but at a high-level: PyTorch’s primary goal is usability A secondary goal is to have reasonable performance We believe the ability to maintain our flexibility to support
researchers who are building on top of our abstractions remains
critical. We can’t see what the future of what workloads will be, but we
know we want them to be built first on PyTorch and that requires
flexibility. In more concrete terms, we operate in a usability-first manner and try
to avoid jumping to restriction-first regimes (for example, static shapes,
graph-mode only) without a clear-eyed view of the tradeoffs. Often there
is a temptation to impose strict user restrictions upfront because it
can simplify implementation, but this comes with risks: The performance may not be worth the user friction, either because
the performance benefit is not compelling enough or it only applies to
a relatively narrow set of subproblems. Even if the performance benefit is compelling, the restrictions can
fragment the ecosystem into different sets of limitations that can
quickly become incomprehensible to users. We want users to be able to seamlessly move their PyTorch code to
different hardware and software platforms, to interoperate with
different libraries and frameworks, and to experience the full richness
of the PyTorch user experience, not a least common denominator subset. Principle 2: Simple Over Easy ¶ Here, we borrow from The Zen of
Python : Explicit is better than implicit Simple is better than complex A more concise way of describing these two goals is Simple Over
Easy . Let’s start with an example because simple and easy are
often used interchangeably in everyday English. Consider how one may
model devices in PyTorch: Simple / Explicit (to understand, debug): every tensor is associated
with a device. The user explicitly specifies tensor device movement.
Operations that require cross-device movement result in an error. Easy / Implicit (to use): the user does not have to worry about
devices; the system figures out the globally optimal device
placement. In this specific case, and as a general design philosophy, PyTorch
favors exposing simple and explicit building blocks rather than APIs
that are easy-to-use by practitioners. The simple version is immediately
understandable and debuggable by a new PyTorch user: you get a clear
error if you call an operator requiring cross-device movement at the
point in the program where the operator is actually invoked. The easy
solution may let a new user move faster initially, but debugging such a
system can be complex: How did the system make its determination? What
is the API for plugging into such a system and how are objects
represented in its IR? Some classic arguments in favor of this sort of design come from A
Note on Distributed
Computation (TLDR: Do not
model resources with very different performance characteristics
uniformly, the details will leak) and the End-to-End
Principle (TLDR: building smarts into the lower-layers of the stack can prevent
building performant features at higher layers in the stack, and often
doesn’t work anyway). For example, we could build operator-level or
global device movement rules, but the precise choices aren’t obvious and
building an extensible mechanism has unavoidable complexity and latency
costs. A caveat here is that this does not mean that higher-level “easy” APIs
are not valuable; certainly there is a value in, for example,
higher-levels in the stack to support efficient tensor computations
across heterogeneous compute in a large cluster. Instead, what we mean
is that focusing on simple lower-level building blocks helps inform the
easy API while still maintaining a good experience when users need to
leave the beaten path. It also allows space for innovation and the
growth of more opinionated tools at a rate we cannot support in the
PyTorch core library, but ultimately benefit from, as evidenced by
our rich ecosystem . In other
words, not automating at the start allows us to potentially reach levels
of good automation faster. Principle 3: Python First with Best In Class Language Interoperability ¶ This principle began as Python First : PyTorch is not a Python binding into a monolithic C++ framework.
It is built to be deeply integrated into Python. You can use it
naturally like you would use NumPy , SciPy , scikit-learn ,
or other Python libraries. You can write your new neural network
layers in Python itself, using your favorite libraries and use
packages such as Cython and Numba . Our goal is to not reinvent
the wheel where appropriate. One thing PyTorch has needed to deal with over the years is Python
overhead: we first rewrote the autograd engine in C++, then the majority
of operator definitions, then developed TorchScript and the C++
frontend. Still, working in Python provides easily the best experience for our
users: it is flexible, familiar, and perhaps most importantly, has a
huge ecosystem of scientific computing libraries and extensions
available for use. This fact motivates a few of our most recent
contributions, which attempt to hit a Pareto optimal point close to the
Python usability end of the curve: TorchDynamo ,
a Python frame evaluation tool capable of speeding up existing
eager-mode PyTorch programs with minimal user intervention. torch_function and torch_dispatch extension points, which have enabled Python-first functionality to be
built on-top of C++ internals, such as the torch.fx
tracer and functorch respectively. These design principles are not hard-and-fast rules, but hard won
choices and anchor how we built PyTorch to be the debuggable, hackable
and flexible framework it is today. As we have more contributors and
maintainers, we look forward to applying these core principles with you
across our libraries and ecosystem. We are also open to evolving them as
we learn new things and the AI space evolves, as we know it will.",6779,0,8,,1752175603.8064044
https://pytorch.org/docs/stable/notes/gradcheck.html,Gradcheck mechanics — PyTorch 2.7 documentation,"Gradcheck mechanics ¶ This note presents an overview of how the gradcheck() and gradgradcheck() functions work. It will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives.
This note also covers both the default behavior of gradcheck as well as the case where fast_mode=True argument is passed (referred to as fast gradcheck below). Notations and background information Default backward mode gradcheck behavior Real-to-real functions Complex-to-real functions Functions with complex outputs Fast backward mode gradcheck Fast gradcheck for real-to-real functions Fast gradcheck for complex-to-real functions Fast gradcheck for functions with complex outputs Gradgradcheck implementation Notations and background information ¶ Throughout this note, we will use the following convention: x x x , y y y , a a a , b b b , v v v , u u u , u r ur u r and u i ui u i are real-valued vectors and z z z is a complex-valued vector that can be rewritten in terms of two real-valued vectors as z = a + i b z = a + i b z = a + ib . N N N and M M M are two integers that we will use for the dimension of the input and output space respectively. f : R N → R M f: \mathcal{R}^N \to \mathcal{R}^M f : R N → R M is our basic real-to-real function such that y = f ( x ) y = f(x) y = f ( x ) . g : C N → R M g: \mathcal{C}^N \to \mathcal{R}^M g : C N → R M is our basic complex-to-real function such that y = g ( z ) y = g(z) y = g ( z ) . For the simple real-to-real case, we write as J f J_f J f ​ the Jacobian matrix associated with f f f of size M × N M \times N M × N .
This matrix contains all the partial derivatives such that the entry at position ( i , j ) (i, j) ( i , j ) contains ∂ y i ∂ x j \frac{\partial y_i}{\partial x_j} ∂ x j ​ ∂ y i ​ ​ .
Backward mode AD is then computing, for a given vector v v v of size M M M , the quantity v T J f v^T J_f v T J f ​ .
Forward mode AD on the other hand is computing, for a given vector u u u of size N N N , the quantity J f u J_f u J f ​ u . For functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at Autograd for Complex Numbers . The constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus.
In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called W W W below) and the Conjugate Wirtinger derivative (called C W CW C W below).
Both W W W and C W CW C W need to be propagated because in general, despite their name, one is not the complex conjugate of the other. To avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions.
In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers). Under this assumption, using W W W and C W CW C W definitions, we can show that W = C W ∗ W = CW^* W = C W ∗ (we use ∗ * ∗ to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered.
To simplify internal computations, PyTorch uses 2 ∗ C W 2 * CW 2 ∗ C W as the value it backwards and returns when the user asks for gradients.
Similarly to the real case, when the output is actually in R M \mathcal{R}^M R M , backward mode AD does not compute 2 ∗ C W 2 * CW 2 ∗ C W but only v T ( 2 ∗ C W ) v^T (2 * CW) v T ( 2 ∗ C W ) for a given vector v ∈ R M v \in \mathcal{R}^M v ∈ R M . For forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in R \mathcal{R} R . Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in R \mathcal{R} R and in this case, using W W W and C W CW C W definitions, we can show that W = C W W = CW W = C W for the intermediary functions.
To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes 2 ∗ C W 2 * CW 2 ∗ C W .
Similarly to the real case, when the input is actually in R N \mathcal{R}^N R N , forward mode AD does not compute 2 ∗ C W 2 * CW 2 ∗ C W but only ( 2 ∗ C W ) u (2 * CW) u ( 2 ∗ C W ) u for a given vector u ∈ R N u \in \mathcal{R}^N u ∈ R N . Default backward mode gradcheck behavior ¶ Real-to-real functions ¶ To test a function f : R N → R M , x → y f: \mathcal{R}^N \to \mathcal{R}^M, x \to y f : R N → R M , x → y , we reconstruct the full Jacobian matrix J f J_f J f ​ of size M × N M \times N M × N in two ways: analytically and numerically.
The analytical version uses our backward mode AD while the numerical version uses finite difference.
The two reconstructed Jacobian matrices are then compared elementwise for equality. Default real input numerical evaluation ¶ If we consider the elementary case of a one-dimensional function ( N = M = 1 N = M = 1 N = M = 1 ), then we can use the basic finite difference formula from the wikipedia article . We use the “central difference” for better numerical properties: ∂ y ∂ x ≈ f ( x + e p s ) − f ( x − e p s ) 2 ∗ e p s \frac{\partial y}{\partial x} \approx \frac{f(x + eps) - f(x - eps)}{2 * eps} ∂ x ∂ y ​ ≈ 2 ∗ e p s f ( x + e p s ) − f ( x − e p s ) ​ This formula easily generalizes for multiple outputs ( M > 1 M \gt 1 M > 1 ) by having ∂ y ∂ x \frac{\partial y}{\partial x} ∂ x ∂ y ​ be a column vector of size M × 1 M \times 1 M × 1 like f ( x + e p s ) f(x + eps) f ( x + e p s ) .
In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely f ( x + e p s ) f(x + eps) f ( x + e p s ) and f ( x − e p s ) f(x - eps) f ( x − e p s ) ). It is more computationally expensive to handle the case with multiple inputs ( N > 1 N \gt 1 N > 1 ). In this scenario, we loop over all the inputs one after the other and apply the e p s eps e p s perturbation for each element of x x x one after the other. This allows us to reconstruct the J f J_f J f ​ matrix column by column. Default real input analytical evaluation ¶ For the analytical evaluation, we use the fact, as described above, that backward mode AD computes v T J f v^T J_f v T J f ​ .
For functions with a single output, we simply use v = 1 v = 1 v = 1 to recover the full Jacobian matrix with a single backward pass. For functions with more than one output, we resort to a for-loop which iterates over the outputs where each v v v is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the J f J_f J f ​ matrix row by row. Complex-to-real functions ¶ To test a function g : C N → R M , z → y g: \mathcal{C}^N \to \mathcal{R}^M, z \to y g : C N → R M , z → y with z = a + i b z = a + i b z = a + ib , we reconstruct the (complex-valued) matrix that contains 2 ∗ C W 2 * CW 2 ∗ C W . Default complex input numerical evaluation ¶ Consider the elementary case where N = M = 1 N = M = 1 N = M = 1 first. We know from (chapter 3 of) this research paper that: C W : = ∂ y ∂ z ∗ = 1 2 ∗ ( ∂ y ∂ a + i ∂ y ∂ b ) CW := \frac{\partial y}{\partial z^*} = \frac{1}{2} * (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b}) C W := ∂ z ∗ ∂ y ​ = 2 1 ​ ∗ ( ∂ a ∂ y ​ + i ∂ b ∂ y ​ ) Note that ∂ y ∂ a \frac{\partial y}{\partial a} ∂ a ∂ y ​ and ∂ y ∂ b \frac{\partial y}{\partial b} ∂ b ∂ y ​ , in the above equation, are R → R \mathcal{R} \to \mathcal{R} R → R derivatives.
To evaluate these numerically, we use the method described above for the real-to-real case.
This allows us to compute the C W CW C W matrix and then multiply it by 2 2 2 . Note that the code, as of time of writing, computes this value in a slightly convoluted way: # Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105 # Notation changes in this code block: # s here is y above # x, y here are a, b above ds_dx = compute_gradient ( eps ) ds_dy = compute_gradient ( eps * 1 j ) # conjugate wirtinger derivative conj_w_d = 0.5 * ( ds_dx + ds_dy * 1 j ) # wirtinger derivative w_d = 0.5 * ( ds_dx - ds_dy * 1 j ) d [ d_idx ] = grad_out . conjugate () * conj_w_d + grad_out * w_d . conj () # Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`. Default complex input analytical evaluation ¶ Since backward mode AD computes exactly twice the C W CW C W derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs. Functions with complex outputs ¶ In this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued.
This means that using autograd directly on this function is not well defined.
To solve this, we will replace the test of the function h : P N → C M h: \mathcal{P}^N \to \mathcal{C}^M h : P N → C M (where P \mathcal{P} P can be either R \mathcal{R} R or C \mathcal{C} C ), with two functions: h r hr h r and h i hi hi such that: h r ( q ) : = r e a l ( f ( q ) ) h i ( q ) : = i m a g ( f ( q ) ) \begin{aligned}
    hr(q) &:= real(f(q)) \\
    hi(q) &:= imag(f(q))
\end{aligned} h r ( q ) hi ( q ) ​ := re a l ( f ( q )) := ima g ( f ( q )) ​ where q ∈ P q \in \mathcal{P} q ∈ P .
We then do a basic gradcheck for both h r hr h r and h i hi hi using either the real-to-real or complex-to-real case described above, depending on P \mathcal{P} P . Note that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the r e a l real re a l or i m a g imag ima g functions manually by passing the grad_out \text{grad\_out} grad_out arguments to the different functions.
When grad_out = 1 \text{grad\_out} = 1 grad_out = 1 , then we are considering h r hr h r .
When grad_out = 1 j \text{grad\_out} = 1j grad_out = 1 j , then we are considering h i hi hi . Fast backward mode gradcheck ¶ While the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices.
This section presents a way to perform gradcheck in a faster way without affecting its correctness.
The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user. The high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians. Fast gradcheck for real-to-real functions ¶ The scalar quantity that we want to compute here is v T J f u v^T J_f u v T J f ​ u for a given random vector v ∈ R M v \in \mathcal{R}^M v ∈ R M and a random unit norm vector u ∈ R N u \in \mathcal{R}^N u ∈ R N . For the numerical evaluation, we can efficiently compute J f u ≈ f ( x + u ∗ e p s ) − f ( x − u ∗ e p s ) 2 ∗ e p s . J_f u \approx \frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}. J f ​ u ≈ 2 ∗ e p s f ( x + u ∗ e p s ) − f ( x − u ∗ e p s ) ​ . We then perform the dot product between this vector and v v v to get the scalar value of interest. For the analytical version, we can use backward mode AD to compute v T J f v^T J_f v T J f ​ directly. We then perform the dot product with u u u to get the expected value. Fast gradcheck for complex-to-real functions ¶ Similar to the real-to-real case, we want to perform a reduction of the full matrix. But the 2 ∗ C W 2 * CW 2 ∗ C W matrix is complex-valued and so in this case, we will compare to complex scalars. Due to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value: s : = 2 ∗ v T ( r e a l ( C W ) u r + i ∗ i m a g ( C W ) u i ) s := 2 * v^T (real(CW) ur + i * imag(CW) ui) s := 2 ∗ v T ( re a l ( C W ) u r + i ∗ ima g ( C W ) u i ) where v ∈ R M v \in \mathcal{R}^M v ∈ R M , u r ∈ R N ur \in \mathcal{R}^N u r ∈ R N and u i ∈ R N ui \in \mathcal{R}^N u i ∈ R N . Fast complex input numerical evaluation ¶ We first consider how to compute s s s with a numerical method. To do so, keeping in mind that we’re considering g : C N → R M , z → y g: \mathcal{C}^N \to \mathcal{R}^M, z \to y g : C N → R M , z → y with z = a + i b z = a + i b z = a + ib , and that C W = 1 2 ∗ ( ∂ y ∂ a + i ∂ y ∂ b ) CW = \frac{1}{2} * (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b}) C W = 2 1 ​ ∗ ( ∂ a ∂ y ​ + i ∂ b ∂ y ​ ) ,  we rewrite it as follows: s = 2 ∗ v T ( r e a l ( C W ) u r + i ∗ i m a g ( C W ) u i ) = 2 ∗ v T ( 1 2 ∗ ∂ y ∂ a u r + i ∗ 1 2 ∗ ∂ y ∂ b u i ) = v T ( ∂ y ∂ a u r + i ∗ ∂ y ∂ b u i ) = v T ( ( ∂ y ∂ a u r ) + i ∗ ( ∂ y ∂ b u i ) ) \begin{aligned}
    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\
      &= 2 * v^T (\frac{1}{2} * \frac{\partial y}{\partial a} ur + i * \frac{1}{2} * \frac{\partial y}{\partial b} ui) \\
      &= v^T (\frac{\partial y}{\partial a} ur + i * \frac{\partial y}{\partial b} ui) \\
      &= v^T ((\frac{\partial y}{\partial a} ur) + i * (\frac{\partial y}{\partial b} ui))
\end{aligned} s ​ = 2 ∗ v T ( re a l ( C W ) u r + i ∗ ima g ( C W ) u i ) = 2 ∗ v T ( 2 1 ​ ∗ ∂ a ∂ y ​ u r + i ∗ 2 1 ​ ∗ ∂ b ∂ y ​ u i ) = v T ( ∂ a ∂ y ​ u r + i ∗ ∂ b ∂ y ​ u i ) = v T (( ∂ a ∂ y ​ u r ) + i ∗ ( ∂ b ∂ y ​ u i )) ​ In this formula, we can see that ∂ y ∂ a u r \frac{\partial y}{\partial a} ur ∂ a ∂ y ​ u r and ∂ y ∂ b u i \frac{\partial y}{\partial b} ui ∂ b ∂ y ​ u i can be evaluated the same way as the fast version for the real-to-real case.
Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued v v v vector. Fast complex input analytical evaluation ¶ For the analytical case, things are simpler and we rewrite the formula as: s = 2 ∗ v T ( r e a l ( C W ) u r + i ∗ i m a g ( C W ) u i ) = v T r e a l ( 2 ∗ C W ) u r + i ∗ v T i m a g ( 2 ∗ C W ) u i ) = r e a l ( v T ( 2 ∗ C W ) ) u r + i ∗ i m a g ( v T ( 2 ∗ C W ) ) u i \begin{aligned}
    s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\
      &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\
      &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui
\end{aligned} s ​ = 2 ∗ v T ( re a l ( C W ) u r + i ∗ ima g ( C W ) u i ) = v T re a l ( 2 ∗ C W ) u r + i ∗ v T ima g ( 2 ∗ C W ) u i ) = re a l ( v T ( 2 ∗ C W )) u r + i ∗ ima g ( v T ( 2 ∗ C W )) u i ​ We can thus use the fact that the backward mode AD provides us with an efficient way to compute v T ( 2 ∗ C W ) v^T (2 * CW) v T ( 2 ∗ C W ) and then perform a dot product of the real part with u r ur u r and the imaginary part with u i ui u i before reconstructing the final complex scalar s s s . Why not use a complex u u u ¶ At this point, you might be wondering why we did not select a complex u u u and just performed the reduction 2 ∗ v T C W u ′ 2 * v^T CW u' 2 ∗ v T C W u ′ .
To dive into this, in this paragraph, we will use the complex version of u u u noted u ′ = u r ′ + i u i ′ u' = ur' + i ui' u ′ = u r ′ + i u i ′ .
Using such complex u ′ u' u ′ , the problem is that when doing the numerical evaluation, we would need to compute: 2 ∗ C W u ′ = ( ∂ y ∂ a + i ∂ y ∂ b ) ( u r ′ + i u i ′ ) = ∂ y ∂ a u r ′ + i ∂ y ∂ a u i ′ + i ∂ y ∂ b u r ′ − ∂ y ∂ b u i ′ \begin{aligned}
    2*CW u' &= (\frac{\partial y}{\partial a} + i \frac{\partial y}{\partial b})(ur' + i ui') \\
            &= \frac{\partial y}{\partial a} ur' + i \frac{\partial y}{\partial a} ui' + i \frac{\partial y}{\partial b} ur' - \frac{\partial y}{\partial b} ui'
\end{aligned} 2 ∗ C W u ′ ​ = ( ∂ a ∂ y ​ + i ∂ b ∂ y ​ ) ( u r ′ + i u i ′ ) = ∂ a ∂ y ​ u r ′ + i ∂ a ∂ y ​ u i ′ + i ∂ b ∂ y ​ u r ′ − ∂ b ∂ y ​ u i ′ ​ Which would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above).
Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above. Fast gradcheck for functions with complex outputs ¶ Just like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function. Gradgradcheck implementation ¶ PyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing. This feature is implemented by considering the function F : x , v → v T J f F: x, v \to v^T J_f F : x , v → v T J f ​ and use the gradcheck defined above on this function.
Note that v v v in this case is just a random vector with the same type as f ( x ) f(x) f ( x ) . The fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function F F F .",17703,1,21,"# Code from https://github.com/pytorch/pytorch/blob/58eb23378f2a376565a66ac32c93a316c45b6131/torch/autograd/gradcheck.py#L99-L105
# Notation changes in this code block:
# s here is y above
# x, y here are a, b above

ds_dx = compute_gradient(eps)
ds_dy = compute_gradient(eps * 1j)
# conjugate wirtinger derivative
conj_w_d = 0.5 * (ds_dx + ds_dy * 1j)
# wirtinger derivative
w_d = 0.5 * (ds_dx - ds_dy * 1j)
d[d_idx] = grad_out.conjugate() * conj_w_d + grad_out * w_d.conj()

# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.",1752175605.0264182
https://pytorch.org/docs/stable/notes/large_scale_deployments.html,Features for large-scale deployments — PyTorch 2.7 documentation,"Features for large-scale deployments ¶ Fleet-wide operator profiling API usage logging Attaching metadata to saved TorchScript models Build environment considerations Common extension points This note talks about several extension points and tricks that might be useful
when running PyTorch within a larger system or operating multiple systems using
PyTorch in a larger organization. It doesn’t cover topics of deploying models to production. Check torch.jit or one of the corresponding tutorials. The note assumes that you either build PyTorch from source in your
organization or have an ability to statically link additional code to be loaded
when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that
can be triggered once in a centralized place, e.g. in static initialization
code. Fleet-wide operator profiling ¶ PyTorch comes with torch.autograd.profiler capable of measuring time
taken by individual operators on demand. One can use the same mechanism to do
“always ON” measurements for any process running PyTorch. It might be useful for
gathering information about PyTorch workloads running in a given process or
across the entire set of machines. New callbacks for any operator invocation can be added with torch::addGlobalCallback . Hooks will be called with torch::RecordFunction struct that describes invocation
context (e.g. name ). If enabled, RecordFunction::inputs() contains arguments
of the function represented as torch::IValue variant type. Note, that inputs
logging is relatively expensive and thus has to be enabled explicitly. The operator callbacks also have access to c10::ThreadLocalDebugInfo::get() interface that returns a pointer to the struct holding the debug information.
This debug information can be set earlier by using at::DebugInfoGuard object.
Debug information is propagated through the forward (including async fork tasks) and backward passes and can be useful for passing some extra information
about execution environment (e.g. model id) from the higher layers of the
application down to the operator callbacks. Invoking callbacks adds some overhead, so usually it’s useful to just randomly
sample operator invocations. This can be enabled on per-callback basis with an
optional sampling rate passed into torch::addGlobalCallback . Note, that addGlobalCallback is not thread-safe and can be called only when no
PyTorch operator is running. Usually, it’s a good idea to call them once during
initialization. Here’s an example: // Called somewhere in the program beginning void init () { // Sample one in a hundred operator runs randomly addGlobalCallback ( RecordFunctionCallback ( & onFunctionEnter , & onFunctionExit ) . needsInputs ( true ) . samplingProb ( 0.01 ) ); // Note, to enable observers in the model calling thread, // call enableRecordFunction() in the thread before running a model } void onFunctionEnter ( const RecordFunction & fn ) { std :: cerr << ""Before function "" << fn . name () << "" with "" << fn . inputs (). size () << "" inputs"" << std :: endl ; } void onFunctionExit ( const RecordFunction & fn ) { std :: cerr << ""After function "" << fn . name (); } API usage logging ¶ When running in a broader ecosystem, for example in managed job scheduler, it’s
often useful to track which binaries invoke particular PyTorch APIs. There
exists simple instrumentation injected at several important API points that
triggers a given callback. Because usually PyTorch is invoked in one-off python
scripts, the callback fires only once for a given process for each of the APIs. c10::SetAPIUsageHandler can be used to register API usage instrumentation
handler. Passed argument is going to be an “api key” identifying used point, for
example python.import for PyTorch extension import or torch.script.compile if TorchScript compilation was triggered. SetAPIUsageLogger ([]( const std :: string & event_name ) { std :: cerr << ""API was used: "" << event_name << std :: endl ; }); Note for developers: new API trigger points can be added in code with C10_LOG_API_USAGE_ONCE(""my_api"") in C++ or torch._C._log_api_usage_once(""my.api"") in Python. Attaching metadata to saved TorchScript models ¶ TorchScript modules can be saved as an archive file that bundles serialized
parameters and module code as TorchScript (see torch.jit.save() ). It’s
often convenient to bundle additional information together with the model, for
example, description of model producer or auxiliary artifacts. It can be achieved by passing the _extra_files argument to torch.jit.save() and torch::jit::load to store and retrieve
arbitrary binary blobs during saving process. Since TorchScript files are
regular ZIP archives, extra information gets stored as regular files inside
archive’s extra/ directory. There’s also a global hook allowing to attach extra files to any TorchScript
archive produced in the current process. It might be useful to tag models with
producer metadata, akin to JPEG metadata produced by digital cameras. Example
usage might look like: SetExportModuleExtraFilesHook ([]( const Module & ) { ExtraFilesMap files ; files [ ""producer_info.json"" ] = ""{ \"" user \"" : \"" "" + getenv ( ""USER"" ) + "" \"" }"" ; return files ; }); Build environment considerations ¶ TorchScript’s compilation needs to have access to the original python files as
it uses python’s inspect.getsource call. In certain production environments
it might require explicitly deploying .py files along with precompiled .pyc . Common extension points ¶ PyTorch APIs are generally loosely coupled and it’s easy to replace a component
with specialized version. Common extension points include: Custom operators implemented in C++ - see tutorial for more details . Custom data reading can be often integrated directly by invoking corresponding python library. Existing functionality of torch.utils.data can be utilized by extending Dataset or IterableDataset .",5885,3,9,"// Called somewhere in the program beginning
void init() {
    // Sample one in a hundred operator runs randomly
    addGlobalCallback(
      RecordFunctionCallback(
        &onFunctionEnter,
        &onFunctionExit)
      .needsInputs(true)
      .samplingProb(0.01)
    );
    // Note, to enable observers in the model calling thread,
    // call enableRecordFunction() in the thread before running a model
}

void onFunctionEnter(const RecordFunction& fn) {
    std::cerr << ""Before function "" << fn.name()
              << "" with "" << fn.inputs().size() << "" inputs"" << std::endl;
}

void onFunctionExit(const RecordFunction& fn) {
    std::cerr << ""After function "" << fn.name();
}
---
SetAPIUsageLogger([](const std::string& event_name) {
    std::cerr << ""API was used: "" << event_name << std::endl;
});
---
SetExportModuleExtraFilesHook([](const Module&) {
    ExtraFilesMap files;
    files[""producer_info.json""] = ""{\""user\"": \"""" + getenv(""USER"") + ""\""}"";
    return files;
});",1752175606.1045597
https://pytorch.org/docs/stable/notes/multiprocessing.html,Multiprocessing best practices — PyTorch 2.7 documentation,"Multiprocessing best practices ¶ torch.multiprocessing is a drop in replacement for Python’s multiprocessing module. It supports the exact same operations,
but extends it, so that all tensors sent through a multiprocessing.Queue , will have their data moved into shared
memory and will only send a handle to another process. Note When a Tensor is sent to another process, the Tensor data is shared. If torch.Tensor.grad is
not None , it is also shared. After a Tensor without
a torch.Tensor.grad field is sent to the other process, it
creates a standard process-specific .grad Tensor that
is not automatically shared across all processes, unlike how the Tensor ’s data has been shared. This allows to implement various training methods, like Hogwild, A3C, or any
others that require asynchronous operation. CUDA in multiprocessing ¶ The CUDA runtime does not support the fork start method; either the spawn or forkserver start method are
required to use CUDA in subprocesses. Note The start method can be set via either creating a context with multiprocessing.get_context(...) or directly using multiprocessing.set_start_method(...) . Unlike CPU tensors, the sending process is required to keep the original tensor
as long as the receiving process retains a copy of the tensor. It is implemented
under the hood but requires users to follow the best practices for the program
to run correctly. For example, the sending process must stay alive as long as
the consumer process has references to the tensor, and the refcounting can not
save you if the consumer process exits abnormally via a fatal signal. See this section . See also: Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel Best practices and tips ¶ Avoiding and fighting deadlocks ¶ There are a lot of things that can go wrong when a new process is spawned, with
the most common cause of deadlocks being background threads. If there’s any
thread that holds a lock or imports a module, and fork is called, it’s very
likely that the subprocess will be in a corrupted state and will deadlock or
fail in a different way. Note that even if you don’t, Python built in
libraries do - no need to look further than multiprocessing . multiprocessing.Queue is actually a very complex class, that
spawns multiple threads used to serialize, send and receive objects, and they
can cause aforementioned problems too. If you find yourself in such situation
try using a SimpleQueue , that doesn’t
use any additional threads. We’re trying our best to make it easy for you and ensure these deadlocks don’t
happen but some things are out of our control. If you have any issues you can’t
cope with for a while, try reaching out on forums, and we’ll see if it’s an
issue we can fix. Reuse buffers passed through a Queue ¶ Remember that each time you put a Tensor into a multiprocessing.Queue , it has to be moved into shared memory.
If it’s already shared, it is a no-op, otherwise it will incur an additional
memory copy that can slow down the whole process. Even if you have a pool of
processes sending data to a single one, make it send the buffers back - this
is nearly free and will let you avoid a copy when sending next batch. Asynchronous multiprocess training (e.g. Hogwild) ¶ Using torch.multiprocessing , it is possible to train a model
asynchronously, with parameters either shared all the time, or being
periodically synchronized. In the first case, we recommend sending over the whole
model object, while in the latter, we advise to only send the state_dict() . We recommend using multiprocessing.Queue for passing all kinds
of PyTorch objects between processes. It is possible to e.g. inherit the tensors
and storages already in shared memory, when using the fork start method,
however it is very bug prone and should be used with care, and only by advanced
users. Queues, even though they’re sometimes a less elegant solution, will work
properly in all cases. Warning You should be careful about having global statements, that are not guarded
with an if __name__ == '__main__' . If a different start method than fork is used, they will be executed in all subprocesses. Hogwild ¶ A concrete Hogwild implementation can be found in the examples repository ,
but to showcase the overall structure of the code, there’s also a minimal
example below as well: import torch.multiprocessing as mp from model import MyModel def train ( model ): # Construct data_loader, optimizer, etc. for data , labels in data_loader : optimizer . zero_grad () loss_fn ( model ( data ), labels ) . backward () optimizer . step () # This will update the shared parameters if __name__ == '__main__' : num_processes = 4 model = MyModel () # NOTE: this is required for the ``fork`` method to work model . share_memory () processes = [] for rank in range ( num_processes ): p = mp . Process ( target = train , args = ( model ,)) p . start () processes . append ( p ) for p in processes : p . join () CPU in multiprocessing ¶ Inappropriate multiprocessing can lead to CPU oversubscription, causing
different processes to compete for CPU resources, resulting in low
efficiency. This tutorial will explain what CPU oversubscription is and how to
avoid it. CPU oversubscription ¶ CPU oversubscription is a technical term that refers to a situation
where the total number of vCPUs allocated to a system exceeds the total
number of vCPUs available on the hardware. This leads to severe contention for CPU resources. In such cases, there
is frequent switching between processes, which increases processes
switching overhead and decreases overall system efficiency. See CPU oversubscription with the code examples in the Hogwild
implementation found in the example
repository . When running the training example with the following command on CPU
using 4 processes: python main.py --num-processes 4 Assuming there are N vCPUs available on the machine, executing the above
command will generate 4 subprocesses. Each subprocess will allocate N
vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the
machine only has N vCPUs available. Consequently, the different
processes will compete for resources, leading to frequent process
switching. The following observations indicate the presence of CPU over
subscription: High CPU Utilization: By using the htop command, you can observe
that the CPU utilization is consistently high, often reaching or
exceeding its maximum capacity. This indicates that the demand for
CPU resources exceeds the available physical cores, causing
contention and competition among processes for CPU time. Frequent Context Switching with Low System Efficiency: In an
oversubscribed CPU scenario, processes compete for CPU time, and the
operating system needs to rapidly switch between different processes
to allocate resources fairly. This frequent context switching adds
overhead and reduces the overall system efficiency. Avoid CPU oversubscription ¶ A good way to avoid CPU oversubscription is proper resource allocation.
Ensure that the number of processes or threads running concurrently does
not exceed the available CPU resources. In this case, a solution would be to specify the appropriate number of
threads in the subprocesses. This can be achieved by setting the number
of threads for each process using the torch.set_num_threads(int) function in subprocess. Assuming there are N vCPUs on the machine and M processes will be
generated, the maximum num_threads value used by each process would
be floor(N/M) . To avoid CPU oversubscription in the mnist_hogwild
example, the following changes are needed for the file train.py in example
repository . def train ( rank , args , model , device , dataset , dataloader_kwargs ): torch . manual_seed ( args . seed + rank ) #### define the num threads used in current sub-processes torch . set_num_threads ( floor ( N / M )) train_loader = torch . utils . data . DataLoader ( dataset , ** dataloader_kwargs ) optimizer = optim . SGD ( model . parameters (), lr = args . lr , momentum = args . momentum ) for epoch in range ( 1 , args . epochs + 1 ): train_epoch ( epoch , args , model , device , train_loader , optimizer ) Set num_thread for each process using torch.set_num_threads(floor(N/M)) . where you replace N with the
number of vCPUs available and M with the chosen number of processes. The
appropriate num_thread value will vary depending on the specific
task at hand. However, as a general guideline, the maximum value for the num_thread should be floor(N/M) to avoid CPU oversubscription.
In the mnist_hogwild training example, after avoiding CPU over
subscription, you can achieve a 30x performance boost.",8655,3,13,"import torch.multiprocessing as mp
from model import MyModel

def train(model):
    # Construct data_loader, optimizer, etc.
    for data, labels in data_loader:
        optimizer.zero_grad()
        loss_fn(model(data), labels).backward()
        optimizer.step()  # This will update the shared parameters

if __name__ == '__main__':
    num_processes = 4
    model = MyModel()
    # NOTE: this is required for the ``fork`` method to work
    model.share_memory()
    processes = []
    for rank in range(num_processes):
        p = mp.Process(target=train, args=(model,))
        p.start()
        processes.append(p)
    for p in processes:
        p.join()
---
python main.py --num-processes 4
---
def train(rank, args, model, device, dataset, dataloader_kwargs):
    torch.manual_seed(args.seed + rank)

    #### define the num threads used in current sub-processes
    torch.set_num_threads(floor(N/M))

    train_loader = torch.utils.data.DataLoader(dataset, **dataloader_kwargs)

    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)
    for epoch in range(1, args.epochs + 1):
        train_epoch(epoch, args, model, device, train_loader, optimizer)",1752175607.195065
https://pytorch.org/docs/stable/notes/numerical_accuracy.html,Numerical accuracy — PyTorch 2.7 documentation,"Numerical accuracy ¶ In modern computers, floating point numbers are represented using IEEE 754 standard.
For more details on floating point arithmetic and IEEE 754 standard, please see Floating point arithmetic In particular, note that floating point provides limited accuracy (about 7 decimal digits
for single precision floating point numbers, about 16 decimal digits for double precision
floating point numbers) and that floating point addition and multiplication are not
associative, so the order of the operations affects the results.
Because of this, PyTorch is not guaranteed
to produce bitwise identical results for floating point computations that are
mathematically identical. Similarly, bitwise identical results are not guaranteed across
PyTorch releases, individual commits, or different platforms. In particular, CPU and GPU
results can be different even for bitwise-identical inputs and even after controlling for
the sources of randomness. Batched computations or slice computations ¶ Many operations in PyTorch support batched computation, where the same operation is performed
for the elements of the batches of inputs. An example of this is torch.mm() and torch.bmm() . It is possible to implement batched computation as a loop over batch elements,
and apply the necessary math operations to the individual batch elements, for efficiency reasons
we are not doing that, and typically perform computation for the whole batch. The mathematical
libraries that we are calling, and PyTorch internal implementations of operations can produces
slightly different results in this case, compared to non-batched computations. In particular,
let A and B be 3D tensors with the dimensions suitable for batched matrix multiplication.
Then (A@B)[0] (the first element of the batched result) is not guaranteed to be bitwise
identical to A[0]@B[0] (the matrix product of the first elements of the input batches)
even though mathematically it’s an identical computation. Similarly, an operation applied to a tensor slice is not guaranteed to produce results that are
identical to the slice of the result of the same operation applied to the full tensor. E.g. let A be a 2-dimensional tensor. A.sum(-1)[0] is not guaranteed to be bitwise equal to A[:,0].sum() . Extremal values ¶ When inputs contain large values such that intermediate results may overflow the range of the
used datatype, the end result may overflow too, even though it is representable in the original
datatype. E.g.: import torch a = torch . tensor ([ 1e20 , 1e20 ]) # fp32 type by default a . norm () # produces tensor(inf) a . double () . norm () # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32 Linear algebra ( torch.linalg ) ¶ Non-finite values ¶ The external libraries (backends) that torch.linalg uses provide no guarantees on their behaviour
when the inputs have non-finite values like inf or NaN . As such, neither does PyTorch.
The operations may return a tensor with non-finite values, or raise an exception, or even segfault. Consider using torch.isfinite() before calling these functions to detect this situation. Extremal values in linalg ¶ Functions within torch.linalg have more Extremal Values than other PyTorch functions. Solvers and Inverses assume that the input matrix A is invertible. If it is close to
being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return
incorrect results. These matrices are said to be ill-conditioned .
If provided with ill-conditioned inputs, the result of these functions they may vary when using the same inputs on different
devices or when using different backends via the keyword driver . Spectral operations like svd , eig , and eigh may also return incorrect results (and their gradients may be infinite)
when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions
struggle to converge for these inputs. Running the computation in float64 (as NumPy does by default) often helps, but it does not solve these issues in all cases.
Analyzing the spectrum of the inputs via torch.linalg.svdvals() or their condition number via torch.linalg.cond() may help to detect these issues. TensorFloat-32(TF32) on Nvidia Ampere (and later) devices ¶ On Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions.
When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read.
This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input).
By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32.
We recommend enabling TF32 tensor cores for matrix multiplications with torch.backends.cuda.matmul.allow_tf32 = True if your network does not need full float32 precision.
If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with torch.backends.cudnn.allow_tf32 = False . For more information see TensorFloat32 . Reduced Precision Reduction for FP16 and BF16 GEMMs ¶ Half-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., inf values when the final result should be be representable in half-precision).
If reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False A similar flag exists for BF16 GEMM operations and is turned on by default. If BF16
reduced-precision reductions are problematic, they can be turned off with torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False For more information see allow_fp16_reduced_precision_reduction and allow_bf16_reduced_precision_reduction Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA) ¶ A naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul. For scenarios where reduced-precision reductions are preferred for speed, they can be enabled with the following setting: torch.backends.cuda.allow_fp16_bf16_reduction_math_sdp(True) Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices ¶ On AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior. rocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged. When training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows: forward backward Env unset original alternate Env set to 1 alternate alternate Env set to 0 original original The following is the list of operations where rocBLAS may be used: torch.addbmm torch.addmm torch.baddbmm torch.bmm torch.mm torch.nn.GRUCell torch.nn.LSTMCell torch.nn.Linear torch.sparse.addmm the following torch._C._ConvBackend implementations: slowNd slowNd_transposed slowNd_dilated slowNd_dilated_transposed The following is the list of operations where MIOpen may be used: torch.nn.Conv[Transpose]Nd the following torch._C._ConvBackend implementations: ConvBackend::Miopen ConvBackend::MiopenDepthwise ConvBackend::MiopenTranspose",9560,1,13,"import torch
a=torch.tensor([1e20, 1e20]) # fp32 type by default
a.norm() # produces tensor(inf)
a.double().norm() # produces tensor(1.4142e+20, dtype=torch.float64), representable in fp32",1752175608.24835
https://pytorch.org/docs/stable/notes/extending.html,Extending PyTorch — PyTorch 2.7 documentation,"Extending PyTorch ¶ In this note we’ll cover ways of extending torch.nn , torch.autograd , torch , and writing custom C++ extensions. Adding new operators ¶ PyTorch offers a large library of operators that work on Tensors (e.g. torch.add() , torch.sum() , etc). However, you may wish to bring a new custom operation to PyTorch
and have it behave like PyTorch’s built-in operators. In order to do so, you must
register the custom operation with PyTorch via the Python torch.library or C++ TORCH_LIBRARY
APIs. Please see PyTorch Custom Operators Landing Page for more details. Extending torch.autograd ¶ Adding operations to autograd requires implementing a new Function subclass for each operation. Recall that Functions
are what autograd uses to encode the operation history and compute
gradients. The first part of this doc is focused on backward mode AD as it is the most widely used
feature. A section at the end discusses the extensions for forward mode AD. When to use ¶ In general, implement a custom function if you want to perform computations in your model
that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but
still wish for your operation to chain with other ops and work with the autograd engine. In some situations, custom functions can also be used to improve performance and
memory usage: If you implemented your forward and backward passes using a C++ extension ,
you can wrap them in Function to interface with the autograd
engine. If you’d like to reduce the number of buffers saved for the backward pass,
custom functions can be used to combine ops together. When not to use ¶ If you can already write your function in terms of PyTorch’s built-in ops, its
backward graph is (most likely) already able to be recorded by autograd. In this case, you do
not need to implement the backward function yourself. Consider using a plain
old Python function. If you need to maintain state, i.e., trainable parameters, you should (also) use a
custom module. See the section below for more information on extending torch.nn . If you’d like to alter the gradients during the backward pass or perform a side
effect, consider registering a tensor or Module hook. How to use ¶ Take the following steps:
1. Subclass Function and implement the forward() ,
(optional) setup_context() and backward() methods.
2. Call the proper methods on the ctx argument.
3. Declare whether your function supports double backward .
4. Validate whether your gradients are correct using gradcheck. Step 1: After subclassing Function , you’ll need to define 3 methods: forward() is the code that performs the operation. It can take
as many arguments as you want, with some of them being optional, if you
specify the default values. All kinds of Python objects are accepted here. Tensor arguments that track history (i.e., with requires_grad=True ) will be converted to ones that don’t track history
before the call, and their use will be registered in the graph. Note that this
logic won’t traverse lists/dicts/any other data structures and will only
consider tensors that are direct arguments to the call. You can
return either a single Tensor output, or a tuple of
tensors if there are multiple outputs. Also, please refer to the
docs of Function to find descriptions of useful methods that can be
called only from forward() . setup_context() (optional). One can either write a “combined” forward() that
accepts a ctx object or (as of PyTorch 2.0) a separate forward() that does
not accept ctx and a setup_context() method where the ctx modification happens.
The forward() should have the compute and setup_context() should
only be responsible for the ctx modification (and not have any compute).
In general the separate forward() and setup_context() is closer to how
PyTorch native operations work and therefore more composable with various PyTorch subsystems.
See Combined or separate forward() and setup_context() for more details. backward() (or vjp() ) defines the gradient formula.
It will be given as many Tensor arguments as there were outputs, with each
of them representing gradient w.r.t. that output. It is important NEVER to modify
these in-place. It should return as many tensors as there
were inputs, with each of them containing the gradient w.r.t. its
corresponding input. If your inputs didn’t require gradient
( needs_input_grad is a tuple of booleans indicating
whether each input needs gradient computation), or were non- Tensor objects, you can return python:None . Also, if you have optional
arguments to forward() you can return more gradients than there
were inputs, as long as they’re all None . Step 2: It is your responsibility to use the functions in ctx properly in order to ensure that the new Function works properly with
the autograd engine. save_for_backward() must be
used to save any tensors to be used in the backward pass. Non-tensors should
be stored directly on ctx . If tensors that are neither input nor output
are saved for backward your Function may not support double backward
(see step 3). mark_dirty() must be used to
mark any input that is modified inplace by the forward function. mark_non_differentiable() must
be used to tell the engine if an output is not differentiable. By
default all output tensors that are of differentiable type will be set
to require gradient. Tensors of non-differentiable type (i.e., integral types)
are never marked as requiring gradients. set_materialize_grads() can be
used to tell the autograd engine to optimize gradient computations in the cases where
the output does not depend on the input by not materializing grad tensors given to backward
function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for
which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior
to calling backward, and so your code will need to handle such objects as if they were
tensors filled with zeros. The default value of this setting is True. Step 3: If your Function does not support double backward
you should explicitly declare this by decorating backward with the once_differentiable() . With this decorator, attempts to
perform double backward through your function will produce an error.
See our double backward tutorial for more information on double backward. Step 4: It is recommended that you use torch.autograd.gradcheck() to check whether your backward function correctly computes gradients of the
forward by computing the Jacobian matrix using your backward function and
comparing the value element-wise with the Jacobian computed numerically using
finite-differencing. Example ¶ Below you can find code for a Linear function, with
additional comments: # Inherit from Function class LinearFunction ( Function ): # Note that forward, setup_context, and backward are @staticmethods @staticmethod def forward ( input , weight , bias ): output = input . mm ( weight . t ()) if bias is not None : output += bias . unsqueeze ( 0 ) . expand_as ( output ) return output @staticmethod # inputs is a Tuple of all of the inputs passed to forward. # output is the output of the forward(). def setup_context ( ctx , inputs , output ): input , weight , bias = inputs ctx . save_for_backward ( input , weight , bias ) # This function has only a single output, so it gets only one gradient @staticmethod def backward ( ctx , grad_output ): # This is a pattern that is very convenient - at the top of backward # unpack saved_tensors and initialize all gradients w.r.t. inputs to # None. Thanks to the fact that additional trailing Nones are # ignored, the return statement is simple even when the function has # optional inputs. input , weight , bias = ctx . saved_tensors grad_input = grad_weight = grad_bias = None # These needs_input_grad checks are optional and there only to # improve efficiency. If you want to make your code simpler, you can # skip them. Returning gradients for inputs that don't require it is # not an error. if ctx . needs_input_grad [ 0 ]: grad_input = grad_output . mm ( weight ) if ctx . needs_input_grad [ 1 ]: grad_weight = grad_output . t () . mm ( input ) if bias is not None and ctx . needs_input_grad [ 2 ]: grad_bias = grad_output . sum ( 0 ) return grad_input , grad_weight , grad_bias Now, to make it easier to use these custom ops, we recommend either aliasing
them or wrapping them in a function. Wrapping in a function lets us support
default arguments and keyword arguments: # Option 1: alias linear = LinearFunction . apply # Option 2: wrap in a function, to support default args and keyword args. def linear ( input , weight , bias = None ): return LinearFunction . apply ( input , weight , bias ) Here, we give an additional example of a function that is parametrized by
non-Tensor arguments: class MulConstant ( Function ): @staticmethod def forward ( tensor , constant ): return tensor * constant @staticmethod def setup_context ( ctx , inputs , output ): # ctx is a context object that can be used to stash information # for backward computation tensor , constant = inputs ctx . constant = constant @staticmethod def backward ( ctx , grad_output ): # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx . constant , None And here, we optimize the above example by calling set_materialize_grads(False): class MulConstant ( Function ): @staticmethod def forward ( tensor , constant ): return tensor * constant @staticmethod def setup_context ( ctx , inputs , output ): tensor , constant = inputs ctx . set_materialize_grads ( False ) ctx . constant = constant @staticmethod def backward ( ctx , grad_output ): # Here we must handle None grad_output tensor. In this case we # can skip unnecessary computations and just return None. if grad_output is None : return None , None # We return as many input gradients as there were arguments. # Gradients of non-Tensor arguments to forward must be None. return grad_output * ctx . constant , None If you need any “intermediate” Tensors computed in forward() to be saved,
either they must be returned as outputs, or combine forward and setup_context() (see Combined or separate forward() and setup_context() ).
Note that this means if you want gradients to flow through those intermediate values, you
need to define the gradient formula for them (see also the double backward tutorial ): class MyCube ( torch . autograd . Function ): @staticmethod def forward ( x ): # We wish to save dx for backward. In order to do so, it must # be returned as an output. dx = 3 * x ** 2 result = x ** 3 return result , dx @staticmethod def setup_context ( ctx , inputs , output ): x , = inputs result , dx = output ctx . save_for_backward ( x , dx ) @staticmethod def backward ( ctx , grad_output , grad_dx ): x , dx = ctx . saved_tensors # In order for the autograd.Function to work with higher-order # gradients, we must add the gradient contribution of `dx`, # which is grad_dx * 6 * x. result = grad_output * dx + grad_dx * 6 * x return result # Wrap MyCube in a function so that it is clearer what the output is def my_cube ( x ): result , dx = MyCube . apply ( x ) return result Note Inputs to backward , i.e., grad_output , can also be tensors that
track history. So if backward is implemented with differentiable
operations, (e.g., invocation of another custom Function ), higher order derivatives will work.
In this case, the tensors saved with save_for_backward can also be used
in the backward and have gradients flowing back but tensors saved in the ctx won’t have gradients flowing back for them.
If you need gradients to flow back for a Tensor saved in the ctx , you should
make it an output of the custom Function and save it with save_for_backward . You probably want to check if the backward method you implemented actually
computes the derivatives of your function. It is possible by comparing with
numerical approximations using small finite differences: from torch.autograd import gradcheck # gradcheck takes a tuple of tensors as input, check if your gradient # evaluated with these tensors are close enough to numerical # approximations and returns True if they all verify this condition. input = ( torch . randn ( 20 , 20 , dtype = torch . double , requires_grad = True ), torch . randn ( 30 , 20 , dtype = torch . double , requires_grad = True )) test = gradcheck ( linear , input , eps = 1e-6 , atol = 1e-4 ) print ( test ) See Numerical gradient checking for more details on finite-difference gradient comparisons.
If your function is used in higher order derivatives (differentiating the backward pass) you
can use the gradgradcheck function from the same package to check higher order derivatives. Combined or separate forward() and setup_context() ¶ There are two main ways to define Function . Either: define a forward() that combines the forward compute logic with setup_context() (as of PyTorch 2.0) define a separate forward() and setup_context() We recommend the second option (separate forward() and setup_context() )
because that is closer to how PyTorch native operations are implemented and it composes
with torch.func transforms. However, we plan to support both approaches going forward;
combining forward() with setup_context() : leads to more flexibility since
you are able to save intermediates without returning them as output. Please see the previous section for how to define Function with separate forward() and setup_context() . Here is an example of how to define a Function with combined forward() and setup_context() : class LinearFunction ( Function ): @staticmethod # ctx is the first argument to forward def forward ( ctx , input , weight , bias = None ): # The forward pass can use ctx. ctx . save_for_backward ( input , weight , bias ) output = input . mm ( weight . t ()) if bias is not None : output += bias . unsqueeze ( 0 ) . expand_as ( output ) return output @staticmethod def backward ( ctx , grad_output ): input , weight , bias = ctx . saved_tensors grad_input = grad_weight = grad_bias = None if ctx . needs_input_grad [ 0 ]: grad_input = grad_output . mm ( weight ) if ctx . needs_input_grad [ 1 ]: grad_weight = grad_output . t () . mm ( input ) if bias is not None and ctx . needs_input_grad [ 2 ]: grad_bias = grad_output . sum ( 0 ) return grad_input , grad_weight , grad_bias Forward mode AD ¶ Overriding the forward mode AD formula has a very similar API with some different subtleties.
You can implement the jvp() function. It will be given as many Tensor arguments as there were inputs, with each
of them representing gradient w.r.t. that input. It should return as many tensors as there
were outputs, with each of them containing the gradient w.r.t. its corresponding output.
The jvp() will be called just after the forward() method, before the apply() returns. jvp() has a few subtle differences with the backward() function: You can use the ctx to pass any data from the forward() to the jvp() function.
If that state will not be needed for the backward() ,
you can explicitly free it by doing del ctx.foo at the end of the jvp() function. The implementation of jvp() must be backward differentiable or explicitly check that
none of the given forward mode gradient has requires_grad set. The jvp() function must match the view/inplace behavior of forward() .
For example, if the i th input is modified inplace, then the i th gradient must be updated inplace.
Similarly, if the j th output is a view of the k th input. Then the returned j th output gradient must be
a view of the given k th input gradient. Because the user cannot specify which gradient needs to be computed, the jvp() function should
always compute gradients for all the outputs. The forward mode gradients do respect the flag set by set_materialize_grads() and you can get None input gradients when this is disabled. torch.func transforms and/or torch.vmap() ¶ Please see Extending torch.func with autograd.Function for details. Extending torch.nn ¶ nn exports two kinds of interfaces - modules and their functional
versions. You can extend it in both ways, but we recommend using modules for
all kinds of layers, that hold any parameters or buffers, and recommend using
a functional form parameter-less operations like activation functions, pooling,
etc. Adding a functional version of an operation is already fully covered in the
section above. Adding a Module ¶ Since nn heavily utilizes autograd , adding a new Module requires implementing a Function that performs the operation and can compute the gradient. From now on let’s
assume that we want to implement a Linear module and we have the function
implemented as in the listing above. There’s very little code required to
add this. Now, there are two functions that need to be implemented: __init__ ( optional ) - takes in arguments such as kernel sizes, numbers
of features, etc. and initializes parameters and buffers. forward() - instantiates a Function and
uses it to perform the operation. It’s very similar to a functional wrapper
shown above. This is how a Linear module can be implemented: class Linear ( nn . Module ): def __init__ ( self , input_features , output_features , bias = True ): super () . __init__ () self . input_features = input_features self . output_features = output_features # nn.Parameter is a special kind of Tensor, that will get # automatically registered as Module's parameter once it's assigned # as an attribute. Parameters and buffers need to be registered, or # they won't appear in .parameters() (doesn't apply to buffers), and # won't be converted when e.g. .cuda() is called. You can use # .register_buffer() to register buffers. # nn.Parameters require gradients by default. self . weight = nn . Parameter ( torch . empty ( output_features , input_features )) if bias : self . bias = nn . Parameter ( torch . empty ( output_features )) else : # You should always register all possible parameters, but the # optional ones can be None if you want. self . register_parameter ( 'bias' , None ) # Not a very smart way to initialize weights nn . init . uniform_ ( self . weight , - 0.1 , 0.1 ) if self . bias is not None : nn . init . uniform_ ( self . bias , - 0.1 , 0.1 ) def forward ( self , input ): # See the autograd section for explanation of what happens here. return LinearFunction . apply ( input , self . weight , self . bias ) def extra_repr ( self ): # (Optional)Set the extra information about this module. You can test # it by printing an object of this class. return 'input_features= {} , output_features= {} , bias= {} ' . format ( self . input_features , self . output_features , self . bias is not None ) Extending torch Python API ¶ You can create custom types that emulate Tensor by defining a custom
class with methods that match Tensor . But what if you want to be able
to pass these types to functions like torch.add() in the top-level torch namespace that accept Tensor operands? If your custom Python type defines a method named __torch_function__ , PyTorch
will invoke your __torch_function__ implementation when an instance of your
custom class is passed to a function in the torch namespace. This makes
it possible to define custom implementations for any of the functions in the torch namespace which your __torch_function__ implementation can call,
allowing your users to make use of your custom type with existing PyTorch
workflows that they have already written for Tensor . This works with
“duck” types that are unrelated to Tensor as well as user-defined
subclasses of Tensor . Extending torch with a Tensor -like type ¶ Note This functionality is inspired by the NumPy __array_function__ protocol. See the NumPy documentation and NEP-0018 for
more details. To make this concrete, let’s begin with a simple example that illustrates the
API dispatch mechanism. We’ll create a custom type that represents a 2D scalar
tensor, parametrized by the order N and value along the diagonal entries, value : class ScalarTensor ( object ): def __init__ ( self , N , value ): self . _N = N self . _value = value def __repr__ ( self ): return ""ScalarTensor(N= {} , value= {} )"" . format ( self . _N , self . _value ) def tensor ( self ): return self . _value * torch . eye ( self . _N ) This first iteration of the design isn’t very useful. The main functionality of ScalarTensor is to provide a more compact string representation of a scalar
tensor than in the base tensor class: >>> d = ScalarTensor ( 5 , 2 ) >>> d ScalarTensor(N=5, value=2) >>> d . tensor () tensor([[2., 0., 0., 0., 0.], [0., 2., 0., 0., 0.], [0., 0., 2., 0., 0.], [0., 0., 0., 2., 0.], [0., 0., 0., 0., 2.]]) If we try to use this object with the torch API, we will run
into issues: >>> import torch >>> torch . mean ( d ) TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor Adding a __torch_function__ implementation to ScalarTensor makes it
possible for the above operation to succeed. Let’s re-do our implementation,
this time adding a __torch_function__ implementation: HANDLED_FUNCTIONS = {} class ScalarTensor ( object ): def __init__ ( self , N , value ): self . _N = N self . _value = value def __repr__ ( self ): return ""ScalarTensor(N= {} , value= {} )"" . format ( self . _N , self . _value ) def tensor ( self ): return self . _value * torch . eye ( self . _N ) @classmethod def __torch_function__ ( cls , func , types , args = (), kwargs = None ): if kwargs is None : kwargs = {} if func not in HANDLED_FUNCTIONS or not all ( issubclass ( t , ( torch . Tensor , ScalarTensor )) for t in types ): return NotImplemented return HANDLED_FUNCTIONS [ func ]( * args , ** kwargs ) The __torch_function__ method takes four arguments: func , a reference
to the torch API function that is being overridden, types , the list of
types of Tensor-likes that implement __torch_function__ , args , the
tuple of arguments passed to the function, and kwargs , the dict of keyword
arguments passed to the function. It uses a global dispatch table named HANDLED_FUNCTIONS to store custom implementations. The keys of this
dictionary are functions in the torch namespace and the values are
implementations for ScalarTensor . Note Using a global dispatch table is not a mandated part of the __torch_function__ API, it is just a useful design pattern for
structuring your override implementations. This class definition isn’t quite enough to make torch.mean do the right
thing when we pass it a ScalarTensor – we also need to define an
implementation for torch.mean for ScalarTensor operands and add the
implementation to the HANDLED_FUNCTIONS dispatch table dictionary. One way
of doing this is to define a decorator: import functools def implements ( torch_function ): """"""Register a torch function override for ScalarTensor"""""" def decorator ( func ): functools . update_wrapper ( func , torch_function ) HANDLED_FUNCTIONS [ torch_function ] = func return func return decorator which can be applied to the implementation of our override: @implements ( torch . mean ) def mean ( input ): return float ( input . _value ) / input . _N With this change we can now use torch.mean with ScalarTensor : >>> d = ScalarTensor ( 5 , 2 ) >>> torch . mean ( d ) 0.4 Of course torch.mean is an example of the simplest kind of function to
override since it only takes one operand. We can use the same machinery to
override a function that takes more than one operand, any one of which might be
a tensor or tensor-like that defines __torch_function__ , for example for torch.add() : def ensure_tensor ( data ): if isinstance ( data , ScalarTensor ): return data . tensor () return torch . as_tensor ( data ) @implements ( torch . add ) def add ( input , other ): try : if input . _N == other . _N : return ScalarTensor ( input . _N , input . _value + other . _value ) else : raise ValueError ( ""Shape mismatch!"" ) except AttributeError : return torch . add ( ensure_tensor ( input ), ensure_tensor ( other )) This version has a fast path for when both operands are ScalarTensor instances and also a slower path which degrades to converting the data to
tensors when either operand is not a ScalarTensor . That makes the override
function correctly when either operand is a ScalarTensor or a regular Tensor : >>> s = ScalarTensor ( 2 , 2 ) >>> torch . add ( s , s ) ScalarTensor(N=2, value=4) >>> t = torch . tensor ([[ 1 , 1 ,], [ 1 , 1 ]]) >>> torch . add ( s , t ) tensor([[3., 1.], [1., 3.]]) Note that our implementation of add does not take alpha or out as
keyword arguments like torch.add() does: >>> torch . add ( s , s , alpha = 2 ) TypeError: add() got an unexpected keyword argument 'alpha' For speed and flexibility the __torch_function__ dispatch mechanism does not
check that the signature of an override function matches the signature of the
function being overridden in the torch API. For some applications ignoring
optional arguments would be fine but to ensure full compatibility with Tensor , user implementations of torch API functions should take care to
exactly emulate the API of the function that is being overridden. Functions in the torch API that do not have explicit overrides will
return NotImplemented from __torch_function__ . If all operands with __torch_function__ defined on them return NotImplemented , PyTorch will
raise a TypeError . This means that most of the time operations that do not
have explicit overrides for a type will raise a TypeError when an instance
of such a type is passed: >>> torch . mul ( s , 3 ) TypeError: no implementation found for 'torch.mul' on types that implement __torch_function__: [ScalarTensor] In practice this means that if you would like to implement your overrides using
a __torch_function__ implementation along these lines, you will need to
explicitly implement the full torch API or the entire subset of the API
that you care about for your use case. This may be a tall order as the full torch API is quite extensive. Another option is to not return NotImplemented for operations that are not
handled but to instead pass a Tensor to the original torch function when no override is available. For example, if we change our
implementation of __torch_function__ for ScalarTensor to the one below: @classmethod def __torch_function__ ( cls , func , types , args = (), kwargs = None ): if kwargs is None : kwargs = {} if func not in HANDLED_FUNCTIONS or not all ( issubclass ( t , ( torch . Tensor , ScalarTensor )) for t in types ): args = [ a . tensor () if hasattr ( a , 'tensor' ) else a for a in args ] return func ( * args , ** kwargs ) return HANDLED_FUNCTIONS [ func ]( * args , ** kwargs ) Then torch.mul() will work correctly, although the return type will always
be a Tensor rather than a ScalarTensor , even if both operands
are ScalarTensor instances: >>> s = ScalarTensor ( 2 , 2 ) >>> torch . mul ( s , s ) tensor([[4., 0.], [0., 4.]]) Also see the MetadataTensor example below for another variation on this
pattern but instead always returns a MetadataTensor to propagate metadata
through operations in the torch API. The __torch_function__ protocol is designed for full coverage of the API,
partial coverage may lead to undesirable results, in particular, certain
functions raising a TypeError . This is especially true for subclasses,
where all three of torch.add , torch.Tensor.__add__ and torch.Tensor.add must be covered, even if they return exactly the same result. Failing to do
this may also lead to infinite recursion. If one requires the implementation
of a function from torch.Tensor subclasses, they must use super().__torch_function__ inside their implementation. Subclassing torch.Tensor ¶ As of version 1.7.0, methods on torch.Tensor and functions in public torch.* namespaces applied on torch.Tensor subclasses
will return subclass instances instead of torch.Tensor instances: >>> class SubTensor ( torch . Tensor ): ... pass >>> type ( torch . add ( SubTensor ([ 0 ]), SubTensor ([ 1 ]))) . __name__ 'SubTensor' >>> type ( torch . add ( SubTensor ([ 0 ]), torch . tensor ([ 1 ]))) . __name__ 'SubTensor' If multiple subclasses exist, the lowest one in the hierarchy will be chosen by
default. If there is no unique way to determine such a case, then a TypeError is raised: >>> type ( torch . add ( SubTensor2 ([ 0 ]), SubTensor ([ 1 ]))) . __name__ 'SubTensor2' >>> type ( torch . add ( SubTensor2 ([ 0 ]), torch . tensor ([ 1 ]))) . __name__ 'SubTensor2' >>> torch . add ( SubTensor ([ 0 ]), OtherSubTensor ([ 1 ])) Traceback (most recent call last): File ""<stdin>"" , line 1 , in <module> TypeError : no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor] If one wishes to have a global override for all tensor methods, one can use __torch_function__ . Here is an example that logs all function/method
calls: class LoggingTensor ( torch . Tensor ): @classmethod def __torch_function__ ( cls , func , types , args = (), kwargs = None ): # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion if func is not torch . Tensor . __repr__ : logging . info ( f ""func: { func . __name__ } , args: { args !r} , kwargs: { kwargs !r} "" ) if kwargs is None : kwargs = {} return super () . __torch_function__ ( func , types , args , kwargs ) However, if one instead wishes to override a method on the Tensor subclass,
there one can do so either by directly overriding the method (by defining
it for a subclass), or by using __torch_function__ and matching with func . One should be careful within __torch_function__ for subclasses to always
call super().__torch_function__(func, ...) instead of func directly,
as was the case before version 1.7.0. Failing to do this may cause func to recurse back into __torch_function__ and therefore cause infinite
recursion. Extending torch with a Tensor wrapper type ¶ Another useful case is a type that wraps a Tensor , either as an
attribute or via subclassing. Below we implement a special case of this sort of
type, a MetadataTensor that attaches a dictionary of metadata to a Tensor that is propagated through torch operations. Since this
is a generic sort of wrapping for the full torch API, we do not need to
individually implement each override so we can make the __torch_function__ implementation more permissive about what operations are allowed: class MetadataTensor ( object ): def __init__ ( self , data , metadata = None , ** kwargs ): self . _t = torch . as_tensor ( data , ** kwargs ) self . _metadata = metadata def __repr__ ( self ): return ""Metadata: \n {} \n\n data: \n {} "" . format ( self . _metadata , self . _t ) @classmethod def __torch_function__ ( cls , func , types , args = (), kwargs = None ): if kwargs is None : kwargs = {} metadatas = tuple ( a . _metadata for a in args if hasattr ( a , '_metadata' )) args = [ getattr ( a , '_t' , a ) for a in args ] assert len ( metadatas ) > 0 ret = func ( * args , ** kwargs ) return MetadataTensor ( ret , metadata = metadatas [ 0 ]) This simple implementation won’t necessarily work with every function in the torch API but it is good enough to capture most common operations: >>> metadata = { 'owner' : 'Ministry of Silly Walks' } >>> m = MetadataTensor ([[ 1 , 2 ], [ 3 , 4 ]], metadata = metadata ) >>> t = torch . tensor ([[ 1 , 2 ], [ 1 , 2 ]]) >>> torch . add ( t , m ) Metadata: {'owner': 'Ministry of Silly Walks'} data: tensor([[2, 4], [4, 6]]) >>> torch . mul ( t , m ) Metadata: {'owner': 'Ministry of Silly Walks'} data: tensor([[1, 4], [3, 8]]) Operations on multiple types that define __torch_function__ ¶ It is possible to use the torch API with multiple distinct types that each have
a __torch_function__ implementation, but special care must be taken. In such
a case the rules are: The dispatch operation gathers all distinct implementations of __torch_function__ for each operand and calls them in order: subclasses
before superclasses, and otherwise left to right in the operator expression. If any value other than NotImplemented is returned, that value is
returned as the result. Implementations can register that they do not
implement an operation by returning NotImplemented . If all of the __torch_function__ implementations return NotImplemented , PyTorch raises a TypeError . Testing Coverage of Overrides for the PyTorch API ¶ One troublesome aspect of implementing __torch_function__ is that if some
operations do and others do not have overrides, users will at best see an
inconsistent experience, or at worst will see errors raised at runtime when they
use a function that does not have an override. To ease this process, PyTorch
provides a developer-facing API for ensuring full support for __torch_function__ overrides. This API is private and may be subject to
changes without warning in the future. First, to get a listing of all overridable functions, use torch.overrides._get_overridable_functions . This returns a dictionary whose
keys are namespaces in the PyTorch Python API and whose values are a list of
functions in that namespace that can be overridden. For example, let’s print the
names of the first 5 functions in torch.nn.functional that can be
overridden: >>> from torch.overrides import get_overridable_functions >>> func_dict = get_overridable_functions () >>> nn_funcs = func_dict [ torch . nn . functional ] >>> print ([ f . __name__ for f in nn_funcs [: 5 ]) ['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d', 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices'] This listing of functions makes it possible to iterate over all overridable
functions, however in practice this is not enough to write tests for all of
these functions without laboriously and manually copying the signature of each
function for each test. To ease this process, the torch.overrides._get_testing_overrides function returns a dictionary mapping
overridable functions in the PyTorch API to dummy lambda functions that have
the same signature as the original function but unconditionally return -1. These
functions are most useful to use with inspect to analyze the function
signature of the original PyTorch function: >>> import inspect >>> from torch.overrides import get_testing_overrides >>> override_dict = get_testing_overrides () >>> dummy_add = override_dict [ torch . add ] >>> inspect . signature ( dummy_add ) <Signature (input, other, out=None)> Finally, torch.overrides.get_ignored_functions returns a tuple of functions
that explicitly cannot be overridden by __torch_function__ . This list can be
useful to confirm that a function that isn’t present in the dictionary returned
by get_overridable_functions cannot be overridden. Extending torch native API ¶ While __torch_function__ allows one to effectively extend PyTorch’s pure Python
components’ behavior, it does not allow one to extend the parts of
PyTorch implemented in C++. To that end, a Tensor subclass can also
define __torch_dispatch__ which will be able to override the behavior at the
C++ level. To effectively use this feature, it is important to know how the native part of
PyTorch is implemented. The most important component there is what we call the
“dispatcher” (the best description can be found in this blog post even though it is slightly outdated). As
hinted by its name, it is responsible for calling the right backend
function for a specific call of a function. For example, when calling torch.add(a, b) , the dispatcher will inspect both arguments, figure out which
“feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU,
CUDA, MPS, etc) should be used for this specific call and finally call all the
right kernels.
A very common thing done by a kernel is to “redispatch”. For example, when running your
neural network on GPU with autocast, the first call will be the autocast kernel that
will handle any potential autocast logic and redispatch down. The next feature in line
will be autograd that will properly create the autograd graph and then redispatch down.
Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel
and return the final result. On the way out, autograd will attach the graph to the
output and, finally, autocast will have a chance to do any update it needs on exit. One configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in DispatchKey.h inside the DispatchKey enum. For the purpose of extending torch, the important subset of the ordering for this discussion is: vmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends The most important key for the purpose of this discussion is Python as every Tensor subclass with the __torch_dispatch__ method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided func again will perform a “redispatch”. Some important implications of this implementation are: This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc). If any high level feature implements a given function without redispatching, it will never reach the Python key and so the __torch_dispatch__ callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead. When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None). Our native functions are lazily populated as torch.ops.{namespace}.{func_name}.{overload_name} as callable Python objects to enable easily interacting with them from Python. The func object given to __torch_dispatch__ is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code. In a similar way where __torch_function__ is able to interpose on all of torch’s Python API and Tensor methods, __torch_dispatch__ is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: torch.add(a, 2) and a + 2 will lead to exactly the same aten call.
Most of these functions are defined in native_functions.yaml which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen.
Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions. It is also possible to add new native functions using torch.library . This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions. You can find many examples of __torch_dispatch__ -based subclasses in the subclass zoo repo. __torch_dispatch__ calling convention ¶ @classmethod def __torch_dispatch__ ( cls , func , types , args = (), kwargs = None ): pass When a user calls an operator with inputs that have __torch_dispatch__ , that call
may be forwarded to the __torch_dispatch__ . args and kwargs get normalized before
the call to __torch_dispatch__ , that is: the kwargs consist of keyword-only arguments in the operator’s schema.
If a kwarg is equal to its default value (in the schema), it will not be passed. the args consists of all other arguments, no matter how they were passed
to the operator (positional vs keyword).
If an arg is equal to its default value, and
it is the right-most positional arg or all the args to the right of it
are not passed, it will not be passed. Extending all torch API with Modes ¶ Unfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch’s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive. To address this use case, we introduced the concept of “Mode”. These exist for __torch_function__ and __torch_dispatch__ overrides, are created by subclassing respectively torch.overrides.TorchFunctionMode and torch.utils._python_dispatch.TorchDispatchMode , and are used as a context manager. To simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass.
This means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first. It is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doing with self: . Here is an example that shows logging modes of each type: import torch from torch.overrides import TorchFunctionMode , resolve_name from torch.utils._python_dispatch import TorchDispatchMode class FunctionLog ( TorchFunctionMode ): def __torch_function__ ( self , func , types , args , kwargs = None ): print ( f ""Function Log: { resolve_name ( func ) } (* { args } , ** { kwargs } )"" ) return func ( * args , ** ( kwargs or {})) class DispatchLog ( TorchDispatchMode ): def __torch_dispatch__ ( self , func , types , args , kwargs = None ): print ( f ""Dispatch Log: { func } (* { args } , ** { kwargs } )"" ) return func ( * args , ** ( kwargs or {})) def f (): a = torch . rand ( 10 , requires_grad = True ) b = a * 2 b . sum () . backward () print ( ""TorchFunctionMode logging:"" ) with FunctionLog (): f () print ( ""TorchDispatchMode logging:"" ) with DispatchLog (): f () Which prints the following, with extra comments: TorchFunctionMode logging : Function Log : torch . rand ( * ( 10 ,), ** { 'requires_grad' : True }) Function Log : torch . Tensor . mul ( * ( tensor ([ 0.7164 , 0.9897 , 0.1745 , 0.9336 , 0.4287 , 0.7989 , 0.2169 , 0.7474 , 0.5624 , 0.5970 ], requires_grad = True ), 2 ), ** None ) Function Log : torch . Tensor . sum ( * ( tensor ([ 1.4328 , 1.9794 , 0.3490 , 1.8671 , 0.8573 , 1.5977 , 0.4338 , 1.4948 , 1.1249 , 1.1939 ], grad_fn =< MulBackward0 > ),), ** None ) # Note that at the python level, we only see the call to backward but not what happens in the autograd engine. Function Log : torch . Tensor . backward ( * ( tensor ( 12.3307 , grad_fn =< SumBackward0 > ),), ** { 'gradient' : None , 'retain_graph' : None , 'create_graph' : False , 'inputs' : None }) TorchDispatchMode logging : # Here the requires_grad flag from autograd is removed while default arguments were populated. Dispatch Log : aten . rand . default ( * ([ 10 ],), ** { 'device' : device ( type = 'cpu' ), 'pin_memory' : False }) Dispatch Log : aten . mul . Tensor ( * ( tensor ([ 0.2151 , 0.6018 , 0.8415 , 0.9060 , 0.2974 , 0.7708 , 0.6668 , 0.0352 , 0.7948 , 0.6023 ], requires_grad = True ), 2 ), ** {}) Dispatch Log : aten . sum . default ( * ( tensor ([ 0.4303 , 1.2036 , 1.6831 , 1.8120 , 0.5949 , 1.5416 , 1.3335 , 0.0705 , 1.5897 , 1.2046 ], grad_fn =< MulBackward0 > ),), ** {}) # Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient. Dispatch Log : aten . ones_like . default ( * ( tensor ( 11.4637 , grad_fn =< SumBackward0 > ),), ** { 'pin_memory' : False , 'memory_format' : torch . preserve_format }) # This is the backward of the sum Dispatch Log : aten . expand . default ( * ( tensor ( 1. ), [ 10 ]), ** {}) Dispatch Log : aten . mul . Tensor ( * ( tensor ([ 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ]), 2 ), ** {}) Dispatch Log : aten . detach . default ( * ( tensor ([ 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ]),), ** {}) Dispatch Log : aten . detach . default ( * ( tensor ([ 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. , 2. ]),), ** {})",44581,31,24,"# Inherit from Function
class LinearFunction(Function):

    # Note that forward, setup_context, and backward are @staticmethods
    @staticmethod
    def forward(input, weight, bias):
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    # inputs is a Tuple of all of the inputs passed to forward.
    # output is the output of the forward().
    def setup_context(ctx, inputs, output):
        input, weight, bias = inputs
        ctx.save_for_backward(input, weight, bias)

    # This function has only a single output, so it gets only one gradient
    @staticmethod
    def backward(ctx, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
---
# Option 1: alias
linear = LinearFunction.apply

# Option 2: wrap in a function, to support default args and keyword args.
def linear(input, weight, bias=None):
    return LinearFunction.apply(input, weight, bias)
---
class MulConstant(Function):
    @staticmethod
    def forward(tensor, constant):
        return tensor * constant

    @staticmethod
    def setup_context(ctx, inputs, output):
        # ctx is a context object that can be used to stash information
        # for backward computation
        tensor, constant = inputs
        ctx.constant = constant

    @staticmethod
    def backward(ctx, grad_output):
        # We return as many input gradients as there were arguments.
        # Gradients of non-Tensor arguments to forward must be None.
        return grad_output * ctx.constant, None
---
class MulConstant(Function):
    @staticmethod
    def forward(tensor, constant):
        return tensor * constant

    @staticmethod
    def setup_context(ctx, inputs, output):
        tensor, constant = inputs
        ctx.set_materialize_grads(False)
        ctx.constant = constant

    @staticmethod
    def backward(ctx, grad_output):
        # Here we must handle None grad_output tensor. In this case we
        # can skip unnecessary computations and just return None.
        if grad_output is None:
            return None, None

        # We return as many input gradients as there were arguments.
        # Gradients of non-Tensor arguments to forward must be None.
        return grad_output * ctx.constant, None
---
class MyCube(torch.autograd.Function):
    @staticmethod
    def forward(x):
        # We wish to save dx for backward. In order to do so, it must
        # be returned as an output.
        dx = 3 * x ** 2
        result = x ** 3
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        # In order for the autograd.Function to work with higher-order
        # gradients, we must add the gradient contribution of `dx`,
        # which is grad_dx * 6 * x.
        result = grad_output * dx + grad_dx * 6 * x
        return result

# Wrap MyCube in a function so that it is clearer what the output is
def my_cube(x):
    result, dx = MyCube.apply(x)
    return result
---
from torch.autograd import gradcheck

# gradcheck takes a tuple of tensors as input, check if your gradient
# evaluated with these tensors are close enough to numerical
# approximations and returns True if they all verify this condition.
input = (torch.randn(20,20,dtype=torch.double,requires_grad=True), torch.randn(30,20,dtype=torch.double,requires_grad=True))
test = gradcheck(linear, input, eps=1e-6, atol=1e-4)
print(test)
---
class LinearFunction(Function):
    @staticmethod
    # ctx is the first argument to forward
    def forward(ctx, input, weight, bias=None):
        # The forward pass can use ctx.
        ctx.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    @staticmethod
    def backward(ctx, grad_output):
        input, weight, bias = ctx.saved_tensors
        grad_input = grad_weight = grad_bias = None

        if ctx.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if ctx.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and ctx.needs_input_grad[2]:
            grad_bias = grad_output.sum(0)

        return grad_input, grad_weight, grad_bias
---
class Linear(nn.Module):
    def __init__(self, input_features, output_features, bias=True):
        super().__init__()
        self.input_features = input_features
        self.output_features = output_features

        # nn.Parameter is a special kind of Tensor, that will get
        # automatically registered as Module's parameter once it's assigned
        # as an attribute. Parameters and buffers need to be registered, or
        # they won't appear in .parameters() (doesn't apply to buffers), and
        # won't be converted when e.g. .cuda() is called. You can use
        # .register_buffer() to register buffers.
        # nn.Parameters require gradients by default.
        self.weight = nn.Parameter(torch.empty(output_features, input_features))
        if bias:
            self.bias = nn.Parameter(torch.empty(output_features))
        else:
            # You should always register all possible parameters, but the
            # optional ones can be None if you want.
            self.register_parameter('bias', None)

        # Not a very smart way to initialize weights
        nn.init.uniform_(self.weight, -0.1, 0.1)
        if self.bias is not None:
            nn.init.uniform_(self.bias, -0.1, 0.1)

    def forward(self, input):
        # See the autograd section for explanation of what happens here.
        return LinearFunction.apply(input, self.weight, self.bias)

    def extra_repr(self):
        # (Optional)Set the extra information about this module. You can test
        # it by printing an object of this class.
        return 'input_features={}, output_features={}, bias={}'.format(
            self.input_features, self.output_features, self.bias is not None
        )
---
class ScalarTensor(object):
   def __init__(self, N, value):
       self._N = N
       self._value = value

   def __repr__(self):
       return ""ScalarTensor(N={}, value={})"".format(self._N, self._value)

   def tensor(self):
       return self._value * torch.eye(self._N)
---
>>> d = ScalarTensor(5, 2)
>>> d
ScalarTensor(N=5, value=2)
>>> d.tensor()
tensor([[2., 0., 0., 0., 0.],
        [0., 2., 0., 0., 0.],
        [0., 0., 2., 0., 0.],
        [0., 0., 0., 2., 0.],
        [0., 0., 0., 0., 2.]])
---
>>> import torch
>>> torch.mean(d)
TypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor
---
HANDLED_FUNCTIONS = {}
class ScalarTensor(object):
    def __init__(self, N, value):
        self._N = N
        self._value = value

    def __repr__(self):
        return ""ScalarTensor(N={}, value={})"".format(self._N, self._value)

    def tensor(self):
        return self._value * torch.eye(self._N)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        if func not in HANDLED_FUNCTIONS or not all(
            issubclass(t, (torch.Tensor, ScalarTensor))
            for t in types
        ):
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)
---
import functools
def implements(torch_function):
    """"""Register a torch function override for ScalarTensor""""""
    def decorator(func):
        functools.update_wrapper(func, torch_function)
        HANDLED_FUNCTIONS[torch_function] = func
        return func
    return decorator
---
@implements(torch.mean)
def mean(input):
    return float(input._value) / input._N
---
>>> d = ScalarTensor(5, 2)
>>> torch.mean(d)
0.4
---
def ensure_tensor(data):
    if isinstance(data, ScalarTensor):
        return data.tensor()
    return torch.as_tensor(data)

@implements(torch.add)
def add(input, other):
   try:
       if input._N == other._N:
           return ScalarTensor(input._N, input._value + other._value)
       else:
           raise ValueError(""Shape mismatch!"")
   except AttributeError:
       return torch.add(ensure_tensor(input), ensure_tensor(other))
---
>>> s = ScalarTensor(2, 2)
>>> torch.add(s, s)
ScalarTensor(N=2, value=4)
>>> t = torch.tensor([[1, 1,], [1, 1]])
>>> torch.add(s, t)
tensor([[3., 1.],
        [1., 3.]])
---
>>> torch.add(s, s, alpha=2)
TypeError: add() got an unexpected keyword argument 'alpha'
---
>>> torch.mul(s, 3)
TypeError: no implementation found for 'torch.mul' on types that
implement __torch_function__: [ScalarTensor]
---
@classmethod
def __torch_function__(cls, func, types, args=(), kwargs=None):
    if kwargs is None:
        kwargs = {}
    if func not in HANDLED_FUNCTIONS or not all(
            issubclass(t, (torch.Tensor, ScalarTensor))
            for t in types
        ):
        args = [a.tensor() if hasattr(a, 'tensor') else a for a in args]
        return func(*args, **kwargs)
    return HANDLED_FUNCTIONS[func](*args, **kwargs)
---
>>> s = ScalarTensor(2, 2)
>>> torch.mul(s, s)
tensor([[4., 0.],
        [0., 4.]])
---
>>> class SubTensor(torch.Tensor):
...     pass
>>> type(torch.add(SubTensor([0]), SubTensor([1]))).__name__
'SubTensor'
>>> type(torch.add(SubTensor([0]), torch.tensor([1]))).__name__
'SubTensor'
---
>>> type(torch.add(SubTensor2([0]), SubTensor([1]))).__name__
'SubTensor2'
>>> type(torch.add(SubTensor2([0]), torch.tensor([1]))).__name__
'SubTensor2'
>>> torch.add(SubTensor([0]), OtherSubTensor([1]))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]
---
class LoggingTensor(torch.Tensor):
    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion
        if func is not torch.Tensor.__repr__:
            logging.info(f""func: {func.__name__}, args: {args!r}, kwargs: {kwargs!r}"")
        if kwargs is None:
            kwargs = {}
        return super().__torch_function__(func, types, args, kwargs)
---
class MetadataTensor(object):
    def __init__(self, data, metadata=None, **kwargs):
        self._t = torch.as_tensor(data, **kwargs)
        self._metadata = metadata

    def __repr__(self):
        return ""Metadata:\n{}\n\ndata:\n{}"".format(self._metadata, self._t)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))
        args = [getattr(a, '_t', a) for a in args]
        assert len(metadatas) > 0
        ret = func(*args, **kwargs)
        return MetadataTensor(ret, metadata=metadatas[0])
---
>>> metadata = {'owner': 'Ministry of Silly Walks'}
>>> m = MetadataTensor([[1, 2], [3, 4]], metadata=metadata)
>>> t = torch.tensor([[1, 2], [1, 2]])
>>> torch.add(t, m)
Metadata:
{'owner': 'Ministry of Silly Walks'}

data:
tensor([[2, 4],
        [4, 6]])
>>> torch.mul(t, m)
Metadata:
{'owner': 'Ministry of Silly Walks'}

data:
tensor([[1, 4],
        [3, 8]])
---
>>> from torch.overrides import get_overridable_functions
>>> func_dict = get_overridable_functions()
>>> nn_funcs = func_dict[torch.nn.functional]
>>> print([f.__name__ for f in nn_funcs[:5])
['adaptive_avg_pool1d', 'adaptive_avg_pool2d', 'adaptive_avg_pool3d',
 'adaptive_max_pool1d', 'adaptive_max_pool1d_with_indices']
---
>>> import inspect
>>> from torch.overrides import get_testing_overrides
>>> override_dict = get_testing_overrides()
>>> dummy_add = override_dict[torch.add]
>>> inspect.signature(dummy_add)
<Signature (input, other, out=None)>
---
@classmethod
def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
    pass
---
import torch
from torch.overrides import TorchFunctionMode, resolve_name
from torch.utils._python_dispatch import TorchDispatchMode

class FunctionLog(TorchFunctionMode):
    def __torch_function__(self, func, types, args, kwargs=None):
        print(f""Function Log: {resolve_name(func)}(*{args}, **{kwargs})"")
        return func(*args, **(kwargs or {}))

class DispatchLog(TorchDispatchMode):
    def __torch_dispatch__(self, func, types, args, kwargs=None):
        print(f""Dispatch Log: {func}(*{args}, **{kwargs})"")
        return func(*args, **(kwargs or {}))

def f():
    a = torch.rand(10, requires_grad=True)
    b = a * 2
    b.sum().backward()

print(""TorchFunctionMode logging:"")
with FunctionLog():
    f()

print(""TorchDispatchMode logging:"")
with DispatchLog():
    f()
---
TorchFunctionMode logging:
Function Log: torch.rand(*(10,), **{'requires_grad': True})
Function Log: torch.Tensor.mul(*(tensor([0.7164, 0.9897, 0.1745, 0.9336, 0.4287, 0.7989, 0.2169, 0.7474, 0.5624,
        0.5970], requires_grad=True), 2), **None)
Function Log: torch.Tensor.sum(*(tensor([1.4328, 1.9794, 0.3490, 1.8671, 0.8573, 1.5977, 0.4338, 1.4948, 1.1249,
        1.1939], grad_fn=<MulBackward0>),), **None)
# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.
Function Log: torch.Tensor.backward(*(tensor(12.3307, grad_fn=<SumBackward0>),), **{'gradient': None, 'retain_graph': None, 'create_graph': False, 'inputs': None})

TorchDispatchMode logging:
# Here the requires_grad flag from autograd is removed while default arguments were populated.
Dispatch Log: aten.rand.default(*([10],), **{'device': device(type='cpu'), 'pin_memory': False})
Dispatch Log: aten.mul.Tensor(*(tensor([0.2151, 0.6018, 0.8415, 0.9060, 0.2974, 0.7708, 0.6668, 0.0352, 0.7948,
        0.6023], requires_grad=True), 2), **{})
Dispatch Log: aten.sum.default(*(tensor([0.4303, 1.2036, 1.6831, 1.8120, 0.5949, 1.5416, 1.3335, 0.0705, 1.5897,
        1.2046], grad_fn=<MulBackward0>),), **{})
# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.
Dispatch Log: aten.ones_like.default(*(tensor(11.4637, grad_fn=<SumBackward0>),), **{'pin_memory': False, 'memory_format': torch.preserve_format})
# This is the backward of the sum
Dispatch Log: aten.expand.default(*(tensor(1.), [10]), **{})
Dispatch Log: aten.mul.Tensor(*(tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 2), **{})
Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})
Dispatch Log: aten.detach.default(*(tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]),), **{})",1752175609.4155157
https://pytorch.org/docs/stable/notes/faq.html,Frequently Asked Questions — PyTorch 2.7 documentation,"Frequently Asked Questions ¶ My model reports “cuda runtime error(2): out of memory” ¶ As the error message suggests, you have run out of memory on your
GPU.  Since we often deal with large amounts of data in PyTorch,
small mistakes can rapidly cause your program to use up all of your
GPU; fortunately, the fixes in these cases are often simple.
Here are a few common things to check: Don’t accumulate history across your training loop. By default, computations involving variables that require gradients
will keep history.  This means that you should avoid using such
variables in computations which will live beyond your training loops,
e.g., when tracking statistics. Instead, you should detach the variable
or access its underlying data. Sometimes, it can be non-obvious when differentiable variables can
occur.  Consider the following training loop (abridged from source ): total_loss = 0 for i in range ( 10000 ): optimizer . zero_grad () output = model ( input ) loss = criterion ( output ) loss . backward () optimizer . step () total_loss += loss Here, total_loss is accumulating history across your training loop, since loss is a differentiable variable with autograd history. You can fix this by
writing total_loss += float(loss) instead. Other instances of this problem: 1 . Don’t hold onto tensors and variables you don’t need. If you assign a Tensor or Variable to a local, Python will not
deallocate until the local goes out of scope.  You can free
this reference by using del x .  Similarly, if you assign
a Tensor or Variable to a member variable of an object, it will
not deallocate until the object goes out of scope.  You will
get the best memory usage if you don’t hold onto temporaries
you don’t need. The scopes of locals can be larger than you expect.  For example: for i in range ( 5 ): intermediate = f ( input [ i ]) result += g ( intermediate ) output = h ( result ) return output Here, intermediate remains live even while h is executing,
because its scope extrudes past the end of the loop.  To free it
earlier, you should del intermediate when you are done with it. Avoid running RNNs on sequences that are too large. The amount of memory required to backpropagate through an RNN scales
linearly with the length of the RNN input; thus, you will run out of memory
if you try to feed an RNN a sequence that is too long. The technical term for this phenomenon is backpropagation through time ,
and there are plenty of references for how to implement truncated
BPTT, including in the word language model example; truncation is handled by the repackage function as described in this forum post . Don’t use linear layers that are too large. A linear layer nn.Linear(m, n) uses O ( n m ) O(nm) O ( nm ) memory: that is to say,
the memory requirements of the weights
scales quadratically with the number of features.  It is very easy
to blow through your memory this way (and remember that you will need at least twice the size of the
weights, since you also need to store the gradients.) Consider checkpointing. You can trade-off memory for compute by using checkpoint . My GPU memory isn’t freed properly ¶ PyTorch uses a caching memory allocator to speed up memory allocations. As a
result, the values shown in nvidia-smi usually don’t reflect the true
memory usage. See Memory management for more details about GPU
memory management. If your GPU memory isn’t freed even after Python quits, it is very likely that
some Python subprocesses are still alive. You may find them via ps -elf | grep python and manually kill them with kill -9 [pid] . My out of memory exception handler can’t allocate memory ¶ You may have some code that tries to recover from out of memory errors. try : run_model ( batch_size ) except RuntimeError : # Out of memory for _ in range ( batch_size ): run_model ( 1 ) But find that when you do run out of memory, your recovery code can’t allocate
either. That’s because the python exception object holds a reference to the
stack frame where the error was raised. Which prevents the original tensor
objects from being freed. The solution is to move you OOM recovery code outside
of the except clause. oom = False try : run_model ( batch_size ) except RuntimeError : # Out of memory oom = True if oom : for _ in range ( batch_size ): run_model ( 1 ) My data loader workers return identical random numbers ¶ You are likely using other libraries to generate random numbers in the dataset
and worker subprocesses are started via fork . See torch.utils.data.DataLoader ’s documentation for how to
properly set up random seeds in workers with its worker_init_fn option. My recurrent network doesn’t work with data parallelism ¶ There is a subtlety in using the pack sequence -> recurrent network -> unpack sequence pattern in a Module with DataParallel or data_parallel() . Input to each the forward() on
each device will only be part of the entire input. Because the unpack operation torch.nn.utils.rnn.pad_packed_sequence() by default only pads up to the
longest input it sees, i.e., the longest on that particular device, size
mismatches will happen when results are gathered together. Therefore, you can
instead take advantage of the total_length argument of pad_packed_sequence() to make sure that the forward() calls return sequences of same length. For example, you can
write: from torch.nn.utils.rnn import pack_padded_sequence , pad_packed_sequence class MyModule ( nn . Module ): # ... __init__, other methods, etc. # padded_input is of shape [B x T x *] (batch_first mode) and contains # the sequences sorted by lengths #   B is the batch size #   T is max sequence length def forward ( self , padded_input , input_lengths ): total_length = padded_input . size ( 1 ) # get the max sequence length packed_input = pack_padded_sequence ( padded_input , input_lengths , batch_first = True ) packed_output , _ = self . my_lstm ( packed_input ) output , _ = pad_packed_sequence ( packed_output , batch_first = True , total_length = total_length ) return output m = MyModule () . cuda () dp_m = nn . DataParallel ( m ) Additionally, extra care needs to be taken when batch dimension is dim 1 (i.e., batch_first=False ) with data parallelism. In this case, the first
argument of pack_padded_sequence padding_input will be of shape [T x B x *] and should be scattered along dim 1 , but the second argument input_lengths will be of shape [B] and should be scattered along dim 0 . Extra code to manipulate the tensor shapes will be needed.",6482,5,9,"total_loss = 0
for i in range(10000):
    optimizer.zero_grad()
    output = model(input)
    loss = criterion(output)
    loss.backward()
    optimizer.step()
    total_loss += loss
---
for i in range(5):
    intermediate = f(input[i])
    result += g(intermediate)
output = h(result)
return output
---
try:
    run_model(batch_size)
except RuntimeError: # Out of memory
    for _ in range(batch_size):
        run_model(1)
---
oom = False
try:
    run_model(batch_size)
except RuntimeError: # Out of memory
    oom = True

if oom:
    for _ in range(batch_size):
        run_model(1)
---
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

class MyModule(nn.Module):
    # ... __init__, other methods, etc.

    # padded_input is of shape [B x T x *] (batch_first mode) and contains
    # the sequences sorted by lengths
    #   B is the batch size
    #   T is max sequence length
    def forward(self, padded_input, input_lengths):
        total_length = padded_input.size(1)  # get the max sequence length
        packed_input = pack_padded_sequence(padded_input, input_lengths,
                                            batch_first=True)
        packed_output, _ = self.my_lstm(packed_input)
        output, _ = pad_packed_sequence(packed_output, batch_first=True,
                                        total_length=total_length)
        return output


m = MyModule().cuda()
dp_m = nn.DataParallel(m)",1752175610.471393
https://pytorch.org/docs/stable/community/persons_of_interest.html,PyTorch Governance | Maintainers — PyTorch 2.7 documentation,"PyTorch Governance | Maintainers ¶ Responsibilities ¶ Triage and fix high priority issues assigned to the module or library Triage, review, and land high priority pull requests assigned to the module or library Answer module or library questions on discuss.pytorch.org and dev-discuss.pytorch.org Maintain public user and development documentation Run meetings and share minutes plus roadmap on a half or quarterly basis Lead Core Maintainer (BDFL) ¶ Soumith Chintala ( soumith ) Core Maintainers ¶ Soumith Chintala ( soumith ) Edward Yang ( ezyang ) Greg Chanan ( gchanan ) Dmytro Dzhulgakov ( dzhulgakov ) Nikita Shulga ( malfet ) Alban Desmaison ( albanD ) Piotr Bialecki ( ptrblck ) Module-level maintainers ¶ NN APIs (torch.nn) ¶ Mikayla Gawarecki ( mikaylagawarecki ) Alban Desmaison ( albanD ) Joel Schlosser ( jbschlosser ) (emeritus) Greg Chanan ( gchanan ) (emeritus) Soumith Chintala ( soumith ) (emeritus) Sam Gross ( colesbury ) (emeritus) Adam Paszke ( apaszke ) Optimizers (torch.optim) ¶ Jane Xu ( janeyx99 ) Alban Desmaison ( albanD ) Joel Schlosser ( jbschlosser ) (emeritus) Soumith Chintala ( soumith ) (emeritus) Ilqar Ramazanli ( iramazanli ) (emeritus) Vincent Quenneville-Belair ( vincentqb ) Autograd (torch.autograd) ¶ Jeffrey Wan ( soulitzer ) Alban Desmaison ( alband ) Edward Yang ( ezyang ) (emeritus) Adam Paszke ( apaszke ) TorchDynamo ¶ Animesh Jain ( anijain2305 ) Jason Ansel ( jansel ) Edward Yang ( ezyang ) TorchInductor ¶ Elias Ellison ( eellison ) Horace He ( Chillee ) Shunting Zhang ( shunting314 ) Jason Ansel ( jansel ) Jiong Gong ( jgong5 ) Cudagraph Tree ¶ Elias Ellison ( eellison ) PT2 Dispatcher ¶ Brian Hirsh ( bdhirsh ) Richard Zou ( zou3519 ) Horace He ( Chillee ) Edward Yang ( ezyang ) PT2 Export (torch.export) ¶ Avik Chaudhuri ( avikchaudhuri ) Yanan Cao ( gmagogsfm ) AOT Inductor (AOTI) & AOTI Runtime ¶ Bin Bao ( desertfire ) Angela Yi ( angelayi ) Yang Chen ( chenyang78 ) Compilers (JIT / TorchScript / Package / Deploy) ¶ (emeritus) Elias Ellison ( eellison ) (emeritus) Michael Suo ( suo ) (emeritus) Yanan Cao ( gmagogsfm ) (emeritus) James Reed ( jamesr66a ) (emeritus) Jason Ansel ( jansel ) (emeritus) Jiong Gong ( jgong5 ) (emeritus) Zach Devito ( zdevito ) Distributions & RNG ¶ Fritz Obermeyer ( fritzo ) Neeraj Pradhan ( neerajprad ) Alican Bozkurt ( alicanb ) (emeritus) Vishwak Srinivasan ( vishwakftw ) Distributed ¶ Will Constable ( wconstab ) Howard Huang ( H-Huang ) Wanchao Liang ( wanchaol ) Ke Wen ( kwen2501 ) Chien-Chin Huang ( fegin ) Tristan Rice ( d4l3k ) (emeritus) Shen Li ( mrshenli ) (emeritus) Pritam Damania ( pritamdamania87 ) (emeritus) Yanli Zhao ( zhaojuanmao ) (emeritus) Rohan Varma ( rohan-varma ) (emeritus) Junjie Wang ( fduwjj ) (emeritus) Alisson Azzolini ( aazzolini ) (emeritus) James Reed ( jamesr66a ) (emeritus) Kiuk Chung ( kiukchung ) (emeritus) Pieter Noordhuis ( pietern ) (emeritus) Mingzhe Li ( mingzhe09088 ) (emeritus) Omkar Salpekar ( osalpekar ) Multiprocessing ¶ (emeritus) Simon Wang ( SsnL ) (emeritus) Vitaly Fedyunin ( VitalyFedyunin ) (emeritus) Adam Paszke ( apaszke ) Linear Algebra (torch.linalg) ¶ Mario Lezcano ( lezcano ) (emeritus) Mike Ruberry ( mruberry ) (emeritus) Ivan Yashchuk ( IvanYashchuk ) (emeritus) Vishwak Srinivasan ( vishwakftw ) (emeritus) Nikita Vedeneev ( nikitaved ) Sparse (torch.sparse) ¶ (emeritus) Pearu Peterson ( pearu ) (emeritus) Nikita Vedeneev ( nikitaved ) (emeritus) Ivan Yashchuk ( IvanYashchuk ) (emeritus) Christian Puhrsch ( cpuhrsch ) (emeritus) Andrew James ( amjames ) NestedTensor (torch.nested) ¶ Joel Schlosser ( jbschlosser ) Christian Puhrsch ( cpuhrsch ) Driss Guessous ( drisspg ) Mikayla Gawarecki ( mikaylagawarecki ) Alban Desmaison ( albanD ) (emeritus) Natalia Gimelshein ( ngimel ) MaskedTensor (torch.masked) ¶ Christian Puhrsch ( cpuhrsch ) (emeritus) George Qi ( george-qi ) Fast Fourier Transform (torch.fft) ¶ (emeritus) Mike Ruberry ( mruberry ) (emeritus) Peter Bell ( peterbell10 ) MKLDNN ¶ Xiaobing Zhang ( XiaobingSuper ) Mingfei Ma ( mingfeima ) Jiong Gong ( jgong5 ) (emeritus) Xiaoqiang Zheng ( zheng-xq ) (emeritus) Sam Gross ( colesbury ) (emeritus) Christian Puhrsch ( cpuhrsch ) (emeritus) Ilia Cherniavskii ( ilia-cher ) (emeritus) Junjie Bai ( bddppq ) (emeritus) Yinghai Lu ( yinghai ) (emeritus) Vitaly Fedyunin ( VitalyFedyunin ) (emeritus) Jianhui Li ( Jianhui-Li ) CUDA ¶ Natalia Gimelshein ( ngimel ) Edward Yang ( ezyang ) Piotr Bialecki ( ptrblck ) Christian Sarofeen ( csarofeen ) (emeritus) Andrew Tulloch ( ajtulloch ) (emeritus) Xiaoqiang Zheng ( zheng-xq ) AMD/ROCm/HIP ¶ Jeff Daily ( jeffdaily ) Jithun Nair ( jithunnair-amd ) (emeritus) Junjie Bai ( bddppq ) Build + CI ¶ Nikita Shulga ( malfet ) Eli Uriegas ( seemethere ) Alban Desmaison ( alband ) Andrey Talman ( atalman ) Zain Rizvi ( ZainRizvi ) (emeritus) Mikey Dagitses ( dagitses ) (emeritus) Omkar Salpekar ( osalpekar ) (emeritus) Nirav Mehta ( mehtanirav ) (emeritus) Zhuojie Zhou ( zhouzhuojie ) (emeritus) Edward Yang ( ezyang ) (emeritus) Karl Ostmo ( kostmo ) Performance Tools ¶ Taylor Robie ( robieta ) Xu Zhao ( xuzhao9 ) (emeritus) Victor Bittorf ( bitfort ) (emeritus) Gisle Dankel ( gdankel ) (emeritus) Natalia Gimelshein ( ngimel ) (emeritus) Mingzhe Li ( mingzhe09088 ) C++ API ¶ (emeritus) Joel Schlosser ( jbschlosser ) (emeritus) Will Feng ( yf225 ) C10 utils and operator dispatch ¶ Brian Hirsh ( bdhirsh ) Edward Yang ( ezyang ) (emeritus) Dmytro Dzhulgakov ( dzhulgakov ) (emeritus) Sebastian Messmer ( smessmer ) ONNX exporter ¶ Shubham Bhokare ( shubhambhokare1 ) Justin Chu ( justinchuby ) Xavier Dupré ( xadupre ) Titai Wang ( titaiwangms ) (emeritus) Bowen Bao ( BowenBao ) (emeritus) Thiago Crepaldi ( thiagocrepaldi ) (emeritus) Aaron Bockover ( abock ) (emeritus) Gary Miguel ( garymm ) (emeritus) Lara Haidar ( lara-hdr ) (emeritus) Lu Fang ( houseroad ) (emeritus) Negin Raoof ( neginraoof ) (emeritus) Spandan Tiwari ( spandantiwari ) LiteInterpreter ¶ (emeritus) David Reiss ( dreiss ) (emeritus) Raziel Guevara ( raziel ) (emeritus) Linbin Yu ( linbinyu ) (emeritus) Ivan Kobzarev ( IvanKobzarev ) (emeritus) Tao Xu ( xta0 ) Quantization (torch/ao) ¶ Mark Saroufim ( msaroufim ) Vasiliy Kuznetsov ( vkuzo ) Jerry Zhang ( jerryzh168 ) (emeritus) Zafar Takhirov ( z-a-f ) (emeritus) Raghuraman Krishnamoorthi ( raghuramank100 ) Windows ¶ (emeritus) Guoliang Hua ( nbcsm ) (emeritus) Teng Gao ( gaoteng-git ) (emeritus) Peter Johnson ( peterjc123 ) Apple M1/MPS/Metal ¶ Kulin Seth ( kulinseth ) Alban Desmaison ( alband ) Nikita Shulga ( malfet ) (emeritus) Ramin Azarmehr ( razarmehr ) PowerPC ¶ (emeritus) Alfredo Mendoza ( avmgithub ) x86 CPU ¶ Mingfei Ma ( mingfeima ) Jiong Gong ( jgong5 ) AArch64 CPU ¶ Sunita Nadampalli ( snadampal ) Docs / Tutorials ¶ Svetlana Karslioglu ( svekars ) Library-level maintainers ¶ XLA ¶ Jack Cao ( JackCaoG ) Daniel Sohn ( jysohn23 ) Zach Cain ( zcain117 ) Brian Hirsh ( bdhirsh ) Gregory Chanan ( gchanan ) (emeritus) Ailing Zhang ( ailzhang ) (emeritus) Davide Libenzi ( dlibenzi ) (emeritus) Alex Suhan ( asuhan ) TorchServe ¶ Li Ning ( lxning ) Ankith Gunapal ( agunapal ) Hamid Shojanazeri ( HamidShojanazeri ) (emeritus) Mark Saroufim ( msaroufIm ) (emeritus) Manoj Rao ( mycpuorg ) (emeritus) Vamshi Dantu ( vdantu ) (emeritus) Dhanasekar Karuppasamy ( dhanainme ) TorchVision ¶ Nicolas Hug ( NicolasHug ) Philip Meier ( pmeier ) Victor Fomin ( vfdev-5 ) (emeritus) Francisco Massa ( fmassa ) (emeritus) Vasilis Vryniotis ( datumbox ) (emeritus) Yosua Michael Maranatha ( YosuaMichael ) (emeritus) Joao Gomes ( jdsgomes ) TorchText ¶ (emeritus) Nayef Ahmed ( Nayef211 ) (emeritus) Parmeet Singh Bhatia ( parmeet ) (emeritus) Guanheng George Zhang ( zhangguanheng66 ) (emeritus) Christian Puhrsch ( cpuhrsch ) TorchAudio ¶ Moto Hira ( mthrok ) (emeritus) Jeff Hwang ( hwangjeff ) (emeritus) Caroline Chen ( carolineechen ) (emeritus) Xiaohui Zhang ( xiaohui-zhang ) (emeritus) Zhaoheng Ni ( nateanl ) (emeritus) Christian Puhrsch ( cpuhrsch ) (emeritus) Vincent QB ( vincentqb ) TorchRec ¶ Colin Taylor ( colin2328 ) Paul Zhang ( PaulZhang12 ) (emeritus) Dmytro Ivchenko ( divchenko ) TorchX ¶ (emeritus) Tristan Rice ( d4l3k ) (emeritus) Kiuk Chung ( kiukchung ) TorchData ¶ Andrew Ho ( andrewkho ) Divyansh Khanna ( divyanshk ) TorchArrow ¶ (emeritus) Wenlei Xie ( wenleix ) (emeritus) Vitaly Fedyunin ( VitalyFedyunin ) ExecuTorch (Edge, Mobile) ¶ Mergen Nachin ( mergennachin ) Kimish Patel ( kimishpatel ) Dave Bort ( dbort ) Martin Yuan ( iseeyuan ) TorchTune ¶ Kartikay Khandelwal ( kartikayk ) Evan Smothers ( ebsmothers ) Joe Cummings ( joecummings ) TorchChat ¶ Jack Khuu ( Jack-Khuu ) Jesse White ( byjlw ) (emeritus) Michael Gschwind ( mikekgfb ) TorchCodec ¶ Nicolas Hug ( nicolashug ) Ahmad Sharif ( ahmadsharif1 ) Scott Schneider ( scotts )",8806,0,56,,1752175611.5379488
https://pytorch.org/docs/stable/autograd.html,Automatic differentiation package - torch.autograd — PyTorch 2.7 documentation,"Automatic differentiation package - torch.autograd ¶ torch.autograd provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. It requires minimal changes to the existing code - you only need to declare Tensor s
for which gradients should be computed with the requires_grad=True keyword.
As of now, we only support autograd for floating point Tensor types (
half, float, double and bfloat16) and complex Tensor types (cfloat, cdouble). backward Compute the sum of gradients of given tensors with respect to graph leaves. grad Compute and return the sum of gradients of outputs with respect to the inputs. Forward-mode Automatic Differentiation ¶ Warning This API is in beta. Even though the function signatures are very unlikely to change, improved
operator coverage is planned before we consider this stable. Please see the forward-mode AD tutorial for detailed steps on how to use this API. forward_ad.dual_level Context-manager for forward AD, where all forward AD computation must occur within the dual_level context. forward_ad.make_dual Associate a tensor value with its tangent to create a ""dual tensor"" for forward AD gradient computation. forward_ad.unpack_dual Unpack a ""dual tensor"" to get both its Tensor value and its forward AD gradient. forward_ad.enter_dual_level Enter a new forward grad level. forward_ad.exit_dual_level Exit a forward grad level. forward_ad.UnpackedDualTensor Namedtuple returned by unpack_dual() containing the primal and tangent components of the dual tensor. Functional higher level API ¶ Warning This API is in beta. Even though the function signatures are very unlikely to change, major
improvements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above
and allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return
only Tensors.
If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set,
you can use a lambda to capture them.
For example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another
tensor that should be considered constant and a boolean flag as f(input, constant, flag=flag) you can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input) . functional.jacobian Compute the Jacobian of a given function. functional.hessian Compute the Hessian of a given scalar function. functional.vjp Compute the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Compute the dot product between the Jacobian of the given function at the point given by the inputs and a vector v . functional.vhp Compute the dot product between vector v and Hessian of a  given scalar function at a specified point. functional.hvp Compute the dot product between the scalar function's Hessian and a vector v at a specified point. Locally disabling gradient computation ¶ See Locally disabling gradient computation for more information on the differences
between no-grad and inference mode as well as other related mechanisms that
may be confused with the two. Also see Locally disabling gradient computation for a list of functions that can be used to locally disable gradients. Default gradient layouts ¶ When a non-sparse param receives a non-sparse gradient during torch.autograd.backward() or torch.Tensor.backward() param.grad is accumulated as follows. If param.grad is initially None : If param ’s memory is non-overlapping and dense, .grad is
created with strides matching param (thus matching param ’s
layout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False , backward() accumulates into .grad in-place, which preserves its strides. If create_graph=True , backward() replaces .grad with a
new tensor .grad + new grad , which attempts (but does not guarantee)
matching the preexisting .grad ’s strides. The default behavior (letting .grad s be None before the first backward() , such that their layout is created according to 1 or 2,
and retained over time according to 3 or 4) is recommended for best performance.
Calls to model.zero_grad() or optimizer.zero_grad() will not affect .grad layouts. In fact, resetting all .grad s to None before each
accumulation phase, e.g.: for iterations ... ... for param in model . parameters (): param . grad = None loss . backward () such that they’re recreated according to 1 or 2 every time,
is a valid alternative to model.zero_grad() or optimizer.zero_grad() that may improve performance for some networks. Manual gradient layouts ¶ If you need manual control over .grad ’s strides,
assign param.grad = a zeroed tensor with desired strides
before the first backward() , and never reset it to None .
3 guarantees your layout is preserved as long as create_graph=False .
4 indicates your layout is likely preserved even if create_graph=True . In-place operations on Tensors ¶ Supporting in-place operations in autograd is a hard matter, and we discourage
their use in most cases. Autograd’s aggressive buffer freeing and reuse makes
it very efficient and there are very few occasions when in-place operations
actually lower memory usage by any significant amount. Unless you’re operating
under heavy memory pressure, you might never need to use them. In-place correctness checks ¶ All Tensor s keep track of in-place operations applied to them, and
if the implementation detects that a tensor was saved for backward in one of
the functions, but it was modified in-place afterwards, an error will be raised
once backward pass is started. This ensures that if you’re using in-place
functions and not seeing any errors, you can be sure that the computed
gradients are correct. Variable (deprecated) ¶ Warning The Variable API has been deprecated: Variables are no longer necessary to
use autograd with tensors. Autograd automatically supports Tensors with requires_grad set to True . Below please find a quick guide on what
has changed: Variable(tensor) and Variable(tensor, requires_grad) still work as expected,
but they return Tensors instead of Variables. var.data is the same thing as tensor.data . Methods such as var.backward(), var.detach(), var.register_hook() now work on tensors
with the same method names. In addition, one can now create tensors with requires_grad=True using factory
methods such as torch.randn() , torch.zeros() , torch.ones() , and others
like the following: autograd_tensor = torch.randn((2, 3, 4), requires_grad=True) Tensor autograd functions ¶ torch.Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self . torch.Tensor.requires_grad Is True if gradients need to be computed for this Tensor, False otherwise. torch.Tensor.is_leaf All Tensors that have requires_grad which is False will be leaf Tensors by convention. torch.Tensor.backward ([gradient, ...]) Computes the gradient of current tensor wrt graph leaves. torch.Tensor.detach Returns a new Tensor, detached from the current graph. torch.Tensor.detach_ Detaches the Tensor from the graph that created it, making it a leaf. torch.Tensor.register_hook (hook) Registers a backward hook. torch.Tensor.register_post_accumulate_grad_hook (hook) Registers a backward hook that runs after grad accumulation. torch.Tensor.retain_grad () Enables this Tensor to have their grad populated during backward() . Function ¶ class torch.autograd. Function ( * args , ** kwargs ) [source] [source] ¶ Base class to create custom autograd.Function . To create a custom autograd.Function , subclass this class and implement
the forward() and backward() static methods. Then, to use your custom
op in the forward pass, call the class method apply . Do not call forward() directly. To ensure correctness and best performance, make sure you are calling the
correct methods on ctx and validating your backward function using torch.autograd.gradcheck() . See Extending torch.autograd for more details on how to use this class. Examples: >>> class Exp ( Function ): >>> @staticmethod >>> def forward ( ctx , i ): >>> result = i . exp () >>> ctx . save_for_backward ( result ) >>> return result >>> >>> @staticmethod >>> def backward ( ctx , grad_output ): >>> result , = ctx . saved_tensors >>> return grad_output * result >>> >>> # Use it by calling the apply method: >>> output = Exp . apply ( input ) Function.forward Define the forward of the custom autograd Function. Function.backward Define a formula for differentiating the operation with backward mode automatic differentiation. Function.jvp Define a formula for differentiating the operation with forward mode automatic differentiation. Function.vmap Define the behavior for this autograd.Function underneath torch.vmap() . Context method mixins ¶ When creating a new Function , the following methods are available to ctx . function.FunctionCtx.mark_dirty Mark given tensors as modified in an in-place operation. function.FunctionCtx.mark_non_differentiable Mark outputs as non-differentiable. function.FunctionCtx.save_for_backward Save given tensors for a future call to backward() . function.FunctionCtx.set_materialize_grads Set whether to materialize grad tensors. Custom Function utilities ¶ Decorator for backward method. function.once_differentiable Base custom Function used to build PyTorch utilities function.BackwardCFunction This class is used for internal autograd work. function.InplaceFunction This class is here only for backward compatibility reasons. function.NestedIOFunction This class is here only for backward compatibility reasons. Numerical gradient checking ¶ gradcheck Check gradients computed via small finite differences against analytical gradients wrt tensors in inputs that are of floating point or complex type and with requires_grad=True . gradgradcheck Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in inputs and grad_outputs that are of floating point or complex type and with requires_grad=True . GradcheckError Error raised by gradcheck() and gradgradcheck() . Profiler ¶ Autograd includes a profiler that lets you inspect the cost of different
operators inside your model - both on the CPU and GPU. There are three modes
implemented at the moment - CPU-only using profile .
nvprof based (registers both CPU and GPU activity) using emit_nvtx .
and vtune profiler based using emit_itt . class torch.autograd.profiler. profile ( enabled = True , * , use_cuda = False , use_device = None , record_shapes = False , with_flops = False , profile_memory = False , with_stack = False , with_modules = False , use_kineto = False , use_cpu = True , experimental_config = None , acc_events = False , custom_trace_id_callback = None ) [source] [source] ¶ Context manager that manages autograd profiler state and holds a summary of results. Under the hood it just records events of functions being executed in C++ and
exposes those events to Python. You can wrap any code into it and it will
only report runtime of PyTorch functions.
Note: profiler is thread local and is automatically propagated into the async tasks Parameters enabled ( bool , optional ) – Setting this to False makes this context manager a no-op. use_cuda ( bool , optional ) – Enables timing of CUDA events as well
using the cudaEvent API. (will be deprecated) use_device ( str , optional ) – Enables timing of device events.
Adds approximately 4us of overhead to each tensor operation when use cuda.
The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’. record_shapes ( bool , optional ) – If shapes recording is set, information
about input dimensions will be collected. This allows one to see which
dimensions have been used under the hood and further group by them
using prof.key_averages(group_by_input_shape=True). Please note that
shape recording might skew your profiling data. It is recommended to
use separate runs with and without shape recording to validate the timing.
Most likely the skew will be negligible for bottom most events (in a case
of nested function calls). But for higher level functions the total
self cpu time might be artificially increased because of the shape
collection. with_flops ( bool , optional ) – If with_flops is set, the profiler will estimate
the FLOPs (floating point operations) value using the operator’s input shape.
This allows one to estimate the hardware performance. Currently,
this option only works for the matrix multiplication and 2D convolution operators. profile_memory ( bool , optional ) – track tensor memory allocation/deallocation. with_stack ( bool , optional ) – record source information (file and line number) for the ops. with_modules ( bool ) – record module hierarchy (including function names)
corresponding to the callstack of the op. e.g. If module A’s forward call’s
module B’s forward which contains an aten::add op,
then aten::add’s module hierarchy is A.B
Note that this support exist, at the moment, only for TorchScript models
and not eager mode models. use_kineto ( bool , optional ) – experimental, enable profiling with Kineto profiler. use_cpu ( bool , optional ) – profile CPU events; setting to False requires use_kineto=True and can be used to lower the overhead for GPU-only profiling. experimental_config ( _ExperimentalConfig ) – A set of experimental options
used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed. acc_events ( bool ) – Enable the accumulation of FunctionEvents across multiple profiling cycles Example >>> x = torch . randn (( 1 , 1 ), requires_grad = True ) >>> with torch . autograd . profiler . profile () as prof : >>> for _ in range ( 100 ): # any normal python code, really! >>> y = x ** 2 >>> y . backward () >>> # NOTE: some columns were removed for brevity >>> print ( prof . key_averages () . table ( sort_by = ""self_cpu_time_total"" )) -----------------------------------  ---------------  ---------------  --------------- Name                                 Self CPU total   CPU time avg     Number of Calls -----------------------------------  ---------------  ---------------  --------------- mul                                  32.048ms         32.048ms         200 pow                                  27.041ms         27.041ms         200 PowBackward0                         9.727ms          55.483ms         100 torch::autograd::AccumulateGrad      9.148ms          9.148ms          100 torch::autograd::GraphRoot           691.816us        691.816us        100 -----------------------------------  ---------------  ---------------  --------------- profiler.profile.export_chrome_trace Export an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU. profiler.profile.total_average Averages all events. profiler.parse_nvprof_trace profiler.EnforceUnique Raises an error if a key is seen more than once. profiler.KinetoStepTracker Provides an abstraction for incrementing the step count globally. profiler.record_function Context manager/function decorator that adds a label to a code block/function when running autograd profiler. profiler_util.Interval profiler_util.Kernel profiler_util.MemRecordsAcc Acceleration structure for accessing mem_records in interval. profiler_util.StringTable class torch.autograd.profiler. emit_nvtx ( enabled = True , record_shapes = False ) [source] [source] ¶ Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: nvprof -- profile - from - start off - o trace_name . prof -- < regular command here > Unfortunately, there’s no way to force nvprof to flush the data it collected
to disk, so for CUDA profiling one has to use this context manager to annotate
nvprof traces and wait for the process to exit before inspecting them.
Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or torch.autograd.profiler.load_nvprof() can load the results for inspection
e.g. in Python REPL. Parameters enabled ( bool , optional ) – Setting enabled=False makes this context manager a no-op.
Default: True . record_shapes ( bool , optional ) – If record_shapes=True , the nvtx range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by [] .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.
Default: False Example >>> with torch . cuda . profiler . profile (): ... model ( x ) # Warmup CUDA memory allocator and profiler ... with torch . autograd . profiler . emit_nvtx (): ... model ( x ) Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,
correlating each backward-pass op with the corresponding forward-pass op can be difficult.
To ease this task, emit_nvtx appends sequence number information to the ranges it
generates. During the forward pass, each function range is decorated with seq=<N> . seq is a running
counter, incremented each time a new backward Function object is created and stashed for backward.
Thus, the seq=<N> annotation associated with each forward function range tells you that
if a backward Function object is created by this forward function,
the backward object will receive sequence number N.
During the backward pass, the top-level range wrapping each C++ backward Function’s apply() call is decorated with stashed seq=<M> . M is the sequence number that
the backward object was created with.  By comparing stashed seq numbers in backward with seq numbers in forward, you can track down which forward op created each backward Function. Any functions executed during the backward pass are also decorated with seq=<N> .  During
default backward (with create_graph=False ) this information is irrelevant, and in fact, N may simply be 0 for all such functions.  Only the top-level ranges associated with
backward Function objects’ apply() methods are useful, as a way to correlate these Function
objects with the earlier forward pass. Double-backward If, on the other hand, a backward pass with create_graph=True is underway (in other words,
if you are setting up for a double-backward), each function’s execution during backward
is given a nonzero, useful seq=<N> .  Those functions may themselves create Function objects
to be executed later during double-backward, just as the original functions in the forward pass did.
The relationship between backward and double-backward is conceptually the same as the relationship
between forward and backward: The functions still emit current-sequence-number-tagged ranges,
the Function objects they create still stash those sequence numbers, and during the eventual
double-backward, the Function objects’ apply() ranges are still tagged with stashed seq numbers, which can be compared to seq numbers from the backward pass. class torch.autograd.profiler. emit_itt ( enabled = True , record_shapes = False ) [source] [source] ¶ Context manager that makes every autograd operation emit an ITT range. It is useful when running the program under Intel(R) VTune Profiler: vtune <-- vtune - flags > < regular command here > The Instrumentation and Tracing Technology (ITT) API enables your application to generate and
control the collection of trace data during its execution across different Intel tools.
This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager,
you will be able to see labled ranges in Intel(R) VTune Profiler GUI. Parameters enabled ( bool , optional ) – Setting enabled=False makes this context manager a no-op.
Default: True . record_shapes ( bool , optional ) – If record_shapes=True , the itt range wrapping
each autograd op will append information about the sizes of Tensor arguments received
by that op, in the following format: [[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...] Non-tensor arguments will be represented by [] .
Arguments will be listed in the order they are received by the backend op.
Please note that this order may not match the order in which those arguments were passed
on the Python side.  Also note that shape recording may increase the overhead of itt range creation.
Default: False Example >>> with torch . autograd . profiler . emit_itt (): ... model ( x ) profiler.load_nvprof Open an nvprof trace file and parses autograd annotations. Debugging and anomaly detection ¶ class torch.autograd. detect_anomaly ( check_nan = True ) [source] [source] ¶ Context-manager that enable anomaly detection for the autograd engine. This does two things: Running the forward pass with detection enabled will allow the backward
pass to print the traceback of the forward operation that created the failing
backward function. If check_nan is True , any backward computation that generate “nan”
value will raise an error. Default True . Warning This mode should be enabled only for debugging as the different tests
will slow down your program execution. Example >>> import torch >>> from torch import autograd >>> class MyFunc ( autograd . Function ): ... @staticmethod ... def forward ( ctx , inp ): ... return inp . clone () ... @staticmethod ... def backward ( ctx , gO ): ... # Error during the backward pass ... raise RuntimeError ( ""Some error in backward"" ) ... return gO . clone () >>> def run_fn ( a ): ... out = MyFunc . apply ( a ) ... return out . sum () >>> inp = torch . rand ( 10 , 10 , requires_grad = True ) >>> out = run_fn ( inp ) >>> out . backward () Traceback (most recent call last): File ""<stdin>"", line 1, in <module> File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward allow_unreachable=True)  # allow_unreachable flag File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply return self._forward_cls.backward(self, *args) File ""<stdin>"", line 8, in backward RuntimeError: Some error in backward >>> with autograd . detect_anomaly (): ... inp = torch . rand ( 10 , 10 , requires_grad = True ) ... out = run_fn ( inp ) ... out . backward () Traceback of forward call that caused the error: File ""tmp.py"", line 53, in <module> out = run_fn(inp) File ""tmp.py"", line 44, in run_fn out = MyFunc.apply(a) Traceback (most recent call last): File ""<stdin>"", line 4, in <module> File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward torch.autograd.backward(self, gradient, retain_graph, create_graph) File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward allow_unreachable=True)  # allow_unreachable flag File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply return self._forward_cls.backward(self, *args) File ""<stdin>"", line 8, in backward RuntimeError: Some error in backward class torch.autograd. set_detect_anomaly ( mode , check_nan = True ) [source] [source] ¶ Context-manager that sets the anomaly detection for the autograd engine on or off. set_detect_anomaly will enable or disable the autograd anomaly detection
based on its argument mode .
It can be used as a context-manager or as a function. See detect_anomaly above for details of the anomaly detection behaviour. Parameters mode ( bool ) – Flag whether to enable anomaly detection ( True ),
or disable ( False ). check_nan ( bool ) – Flag whether to raise an error when the backward
generate “nan” grad_mode.set_multithreading_enabled Context-manager that sets multithreaded backwards on or off. Autograd graph ¶ Autograd exposes methods that allow one to inspect the graph and interpose behavior during
the backward pass. The grad_fn attribute of a torch.Tensor holds a torch.autograd.graph.Node if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is
enabled and at least one of the inputs required gradients), or None otherwise. graph.Node.name Return the name. graph.Node.metadata Return the metadata. graph.Node.next_functions graph.Node.register_hook Register a backward hook. graph.Node.register_prehook Register a backward pre-hook. graph.increment_version Update autograd metadata tracking whether the given Tensor was modified in place. Some operations need intermediary results to be saved during the forward pass
in order to execute the backward pass.
These intermediary results are saved as attributes on the grad_fn and can be accessed.
For example: >>> a = torch . tensor ([ 0. , 0. , 0. ], requires_grad = True ) >>> b = a . exp () >>> print ( isinstance ( b . grad_fn , torch . autograd . graph . Node )) True >>> print ( dir ( b . grad_fn )) ['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad'] >>> print ( torch . allclose ( b . grad_fn . _saved_result , b )) True You can also define how these saved tensors should be packed / unpacked using hooks.
A common application is to trade compute for memory by saving those intermediary results
to disk or to CPU instead of leaving them on the GPU. This is especially useful if you
notice your model fits on GPU during evaluation, but not training.
Also see Hooks for saved tensors . class torch.autograd.graph. saved_tensors_hooks ( pack_hook , unpack_hook ) [source] [source] ¶ Context-manager that sets a pair of pack / unpack hooks for saved tensors. Use this context-manager to define how intermediary results of an operation
should be packed before saving, and unpacked on retrieval. In that context, the pack_hook function will be called everytime an
operation saves a tensor for backward (this includes intermediary results
saved using save_for_backward() but
also those recorded by a PyTorch-defined operation). The output of pack_hook is then stored in the computation graph instead of the
original tensor. The unpack_hook is called when the saved tensor needs to be accessed,
namely when executing torch.Tensor.backward() or torch.autograd.grad() . It takes as argument the packed object
returned by pack_hook and should return a tensor which has the same
content as the original tensor (passed as input to the corresponding pack_hook ). The hooks should have the following signatures: pack_hook(tensor: Tensor) -> Any unpack_hook(Any) -> Tensor where the return value of pack_hook is a valid input to unpack_hook . In general, you want unpack_hook(pack_hook(t)) to be equal to t in terms
of value, size, dtype and device. Example: >>> def pack_hook ( x ): ... print ( ""Packing"" , x ) ... return x >>> >>> def unpack_hook ( x ): ... print ( ""Unpacking"" , x ) ... return x >>> >>> a = torch . ones ( 5 , requires_grad = True ) >>> b = torch . ones ( 5 , requires_grad = True ) * 2 >>> with torch . autograd . graph . saved_tensors_hooks ( pack_hook , unpack_hook ): ... y = a * b Packing tensor([1., 1., 1., 1., 1.], requires_grad=True) Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>) >>> y . sum () . backward () Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True) Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>) Warning Performing an inplace operation on the input to either hooks may lead
to undefined behavior. Warning Only one pair of hooks is allowed at a time. When recursively nesting this
context-manager, only the inner-most pair of hooks will be applied. class torch.autograd.graph. save_on_cpu ( pin_memory = False , device_type = 'cuda' ) [source] [source] ¶ Context manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward. When performing operations within this context manager, intermediary
results saved in the graph during the forward pass will be moved to CPU,
then copied back to the original device when needed for the backward pass.
If the graph was already on CPU, no tensor copy is performed. Use this context-manager to trade compute for GPU memory usage (e.g.
when your model doesn’t fit in GPU memory during training). Parameters pin_memory ( bool ) – If True tensors will be saved to CPU pinned memory
during packing and copied to GPU asynchronously during unpacking.
Defaults to False .
Also see Use pinned memory buffers . Example: >>> a = torch . randn ( 5 , requires_grad = True , device = ""cuda"" ) >>> b = torch . randn ( 5 , requires_grad = True , device = ""cuda"" ) >>> c = torch . randn ( 5 , requires_grad = True , device = ""cuda"" ) >>> >>> def f ( a , b , c ): ... prod_1 = a * b # a and b are saved on GPU ... with torch . autograd . graph . save_on_cpu (): ... prod_2 = prod_1 * c # prod_1 and c are saved on CPU ... y = prod_2 * a # prod_2 and a are saved on GPU ... return y >>> >>> y = f ( a , b , c ) >>> del a , b , c # for illustration only >>> # the content of a, b, and prod_2 are still alive on GPU >>> # the content of prod_1 and c only live on CPU >>> y . sum () . backward () # all CPU tensors are moved back to GPU, for backward >>> # all intermediary tensors are released (deleted) after the call to backward class torch.autograd.graph. disable_saved_tensors_hooks ( error_message ) [source] [source] ¶ Context-manager that disables the saved tensors default hooks feature. Useful for if you are creating a feature that does not work with saved
tensors default hooks. Parameters error_message ( str ) – When saved tensors default hooks are used when they
have been are disabled, a RuntimeError with this
error message gets raised. Return type Generator [None, None, None] Example: >>> message = ""saved tensors default hooks are disabled"" >>> with torch . autograd . graph . disable_saved_tensors_hooks ( message ): ... # Raises RuntimeError: saved tensors default hooks are disabled ... with torch . autograd . graph . save_on_cpu (): ... pass class torch.autograd.graph. register_multi_grad_hook ( tensors , fn , * , mode = 'all' ) [source] [source] ¶ Register a multi-grad backward hook. There are two supported modes: ""all"" and ""any"" . Under the ""all"" mode, the hook will be called after gradients with respect to every tensor in tensors have been computed. If a tensor is in tensors but
is not part of the graph, or if a tensor is not needed to compute the gradients
for any inputs specified for the current .backward() or .grad() call,
this tensor will be ignored and the hook will not wait for its gradient to be
computed. After every non-ignored tensor’s gradient has been computed, fn will be
called with those gradients. None will be passed for tensors that did not
have their gradients computed. Under the ""any"" mode, the hook will be called after the first gradient
with respect to a tensor in tensors has been computed. The hook
will be called with that gradient as its argument. The hook should not modify its arguments. This function returns a handle with a method handle.remove() that removes the hook. Note See Backward Hooks execution for more information on how when this hook
is executed, and how its execution is ordered relative to other hooks. Example: >>> import torch >>> >>> a = torch . rand ( 2 , 3 , requires_grad = True ) >>> b = torch . rand ( 2 , 3 , requires_grad = True ) >>> c = a * b >>> d = a * b >>> >>> def fn ( grads ): ... print ([ g is not None for g in grads ]) ... >>> torch . autograd . graph . register_multi_grad_hook (( a , b , c , d ), fn ) >>> >>> c . sum () . backward ( retain_graph = True ) [True, True, True, False] >>> c . sum () . backward ( inputs = ( a ,), retain_graph = True ) [True, False, True, False] >>> Return type RemovableHandle class torch.autograd.graph. allow_mutation_on_saved_tensors [source] [source] ¶ Context manager under which mutating tensors saved for backward is allowed. Under this context manager, tensors saved for backward are cloned on mutation,
so the original version can still be used during backward. Normally, mutating a tensor
saved for backward will result in an error raised when it’s used during backward. To ensure the correct behavior, both the forward and backward should be run under
the same context manager. Returns An _AllowMutationOnSavedContext object storing the state managed by this
context manager. This object can be useful for debugging purposes. The state
managed by the context manager is automatically cleared upon exiting. Return type Generator [ _AllowMutationOnSavedContext , None, None] Example: >>> import torch >>> with torch . autograd . graph . allow_mutation_on_saved_tensors (): ... # forward ... a = torch . ones ( 2 , 3 , requires_grad = True ) ... b = a . clone () ... out = ( b ** 2 ) . sum () ... b . sin_ () ... # backward ... out . sum () . backward () ... tensor([[0.8415, 0.8415, 0.8415], [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>) class torch.autograd.graph. GradientEdge ( node , output_nr ) [source] [source] ¶ Object representing a given gradient edge within the autograd graph. To get the gradient edge where a given Tensor gradient will be computed,
you can do edge = autograd.graph.get_gradient_edge(tensor) . torch.autograd.graph. get_gradient_edge ( tensor ) [source] [source] ¶ Get the gradient edge for computing the gradient of the given Tensor. In particular, it is equivalent to call g = autograd.grad(loss, input) and g = autograd.grad(loss, get_gradient_edge(input)) . Return type GradientEdge",34215,14,20,"for iterations...
    ...
    for param in model.parameters():
        param.grad = None
    loss.backward()
---
>>> class Exp(Function):
>>>     @staticmethod
>>>     def forward(ctx, i):
>>>         result = i.exp()
>>>         ctx.save_for_backward(result)
>>>         return result
>>>
>>>     @staticmethod
>>>     def backward(ctx, grad_output):
>>>         result, = ctx.saved_tensors
>>>         return grad_output * result
>>>
>>> # Use it by calling the apply method:
>>> output = Exp.apply(input)
---
>>> x = torch.randn((1, 1), requires_grad=True)
>>> with torch.autograd.profiler.profile() as prof:
>>>     for _ in range(100):  # any normal python code, really!
>>>         y = x ** 2
>>>         y.backward()
>>> # NOTE: some columns were removed for brevity
>>> print(prof.key_averages().table(sort_by=""self_cpu_time_total""))
-----------------------------------  ---------------  ---------------  ---------------
Name                                 Self CPU total   CPU time avg     Number of Calls
-----------------------------------  ---------------  ---------------  ---------------
mul                                  32.048ms         32.048ms         200
pow                                  27.041ms         27.041ms         200
PowBackward0                         9.727ms          55.483ms         100
torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
torch::autograd::GraphRoot           691.816us        691.816us        100
-----------------------------------  ---------------  ---------------  ---------------
---
nvprof --profile-from-start off -o trace_name.prof -- <regular command here>
---
>>> with torch.cuda.profiler.profile():
...     model(x)  # Warmup CUDA memory allocator and profiler
...     with torch.autograd.profiler.emit_nvtx():
...         model(x)
---
vtune <--vtune-flags> <regular command here>
---
>>> with torch.autograd.profiler.emit_itt():
...     model(x)
---
>>> import torch
>>> from torch import autograd
>>> class MyFunc(autograd.Function):
...     @staticmethod
...     def forward(ctx, inp):
...         return inp.clone()
...     @staticmethod
...     def backward(ctx, gO):
...         # Error during the backward pass
...         raise RuntimeError(""Some error in backward"")
...         return gO.clone()
>>> def run_fn(a):
...     out = MyFunc.apply(a)
...     return out.sum()
>>> inp = torch.rand(10, 10, requires_grad=True)
>>> out = run_fn(inp)
>>> out.backward()
    Traceback (most recent call last):
      File ""<stdin>"", line 1, in <module>
      File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File ""<stdin>"", line 8, in backward
    RuntimeError: Some error in backward
>>> with autograd.detect_anomaly():
...     inp = torch.rand(10, 10, requires_grad=True)
...     out = run_fn(inp)
...     out.backward()
    Traceback of forward call that caused the error:
      File ""tmp.py"", line 53, in <module>
        out = run_fn(inp)
      File ""tmp.py"", line 44, in run_fn
        out = MyFunc.apply(a)
    Traceback (most recent call last):
      File ""<stdin>"", line 4, in <module>
      File ""/your/pytorch/install/torch/_tensor.py"", line 93, in backward
        torch.autograd.backward(self, gradient, retain_graph, create_graph)
      File ""/your/pytorch/install/torch/autograd/__init__.py"", line 90, in backward
        allow_unreachable=True)  # allow_unreachable flag
      File ""/your/pytorch/install/torch/autograd/function.py"", line 76, in apply
        return self._forward_cls.backward(self, *args)
      File ""<stdin>"", line 8, in backward
    RuntimeError: Some error in backward
---
>>> a = torch.tensor([0., 0., 0.], requires_grad=True)
>>> b = a.exp()
>>> print(isinstance(b.grad_fn, torch.autograd.graph.Node))
True
>>> print(dir(b.grad_fn))
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']
>>> print(torch.allclose(b.grad_fn._saved_result, b))
True
---
>>> def pack_hook(x):
...     print(""Packing"", x)
...     return x
>>>
>>> def unpack_hook(x):
...     print(""Unpacking"", x)
...     return x
>>>
>>> a = torch.ones(5, requires_grad=True)
>>> b = torch.ones(5, requires_grad=True) * 2
>>> with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
...     y = a * b
Packing tensor([1., 1., 1., 1., 1.], requires_grad=True)
Packing tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)
>>> y.sum().backward()
Unpacking tensor([1., 1., 1., 1., 1.], requires_grad=True)
Unpacking tensor([2., 2., 2., 2., 2.], grad_fn=<MulBackward0>)
---
>>> a = torch.randn(5, requires_grad=True, device=""cuda"")
>>> b = torch.randn(5, requires_grad=True, device=""cuda"")
>>> c = torch.randn(5, requires_grad=True, device=""cuda"")
>>>
>>> def f(a, b, c):
...     prod_1 = a * b           # a and b are saved on GPU
...     with torch.autograd.graph.save_on_cpu():
...         prod_2 = prod_1 * c  # prod_1 and c are saved on CPU
...     y = prod_2 * a           # prod_2 and a are saved on GPU
...     return y
>>>
>>> y = f(a, b, c)
>>> del a, b, c  # for illustration only
>>> # the content of a, b, and prod_2 are still alive on GPU
>>> # the content of prod_1 and c only live on CPU
>>> y.sum().backward()  # all CPU tensors are moved back to GPU, for backward
>>> # all intermediary tensors are released (deleted) after the call to backward
---
>>> message = ""saved tensors default hooks are disabled""
>>> with torch.autograd.graph.disable_saved_tensors_hooks(message):
...     # Raises RuntimeError: saved tensors default hooks are disabled
...     with torch.autograd.graph.save_on_cpu():
...         pass
---
>>> import torch
>>>
>>> a = torch.rand(2, 3, requires_grad=True)
>>> b = torch.rand(2, 3, requires_grad=True)
>>> c = a * b
>>> d = a * b
>>>
>>> def fn(grads):
...     print([g is not None for g in grads])
...
>>> torch.autograd.graph.register_multi_grad_hook((a, b, c, d), fn)
>>>
>>> c.sum().backward(retain_graph=True)
[True, True, True, False]
>>> c.sum().backward(inputs=(a,), retain_graph=True)
[True, False, True, False]
>>>
---
>>> import torch
>>> with torch.autograd.graph.allow_mutation_on_saved_tensors():
...     # forward
...     a = torch.ones(2, 3, requires_grad=True)
...     b = a.clone()
...     out = (b**2).sum()
...     b.sin_()
...     # backward
...     out.sum().backward()
...
tensor([[0.8415, 0.8415, 0.8415],
        [0.8415, 0.8415, 0.8415]], grad_fn=<SinBackward0>)",1752175612.6688097
https://pytorch.org/docs/stable/notes/custom_operators.html,PyTorch Custom Operators Landing Page — PyTorch 2.7 documentation,PyTorch Custom Operators Landing Page ¶ This page has moved. Redirecting to the new page…,89,0,4,,1752175613.716098
https://pytorch.org/docs/stable/tensor_view.html,Tensor Views — PyTorch 2.7 documentation,"Tensor Views ¶ PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data
with its base tensor. Supporting View avoids explicit data copy, thus allows us to do fast and memory efficient
reshaping, slicing and element-wise operations. For example, to get a view of an existing tensor t , you can call t.view(...) . >>> t = torch . rand ( 4 , 4 ) >>> b = t . view ( 2 , 8 ) >>> t . storage () . data_ptr () == b . storage () . data_ptr () # `t` and `b` share the same underlying data. True # Modifying view tensor changes base tensor as well. >>> b [ 0 ][ 0 ] = 3.14 >>> t [ 0 ][ 0 ] tensor(3.14) Since views share underlying data with its base tensor, if you edit the data
in the view, it will be reflected in the base tensor as well. Typically a PyTorch op returns a new tensor as output, e.g. add() .
But in case of view ops, outputs are views of input tensors to avoid unnecessary data copy.
No data movement occurs when creating a view, view tensor just changes the way
it interprets the same data. Taking a view of contiguous tensor could potentially produce a non-contiguous tensor.
Users should pay additional attention as contiguity might have implicit performance impact. transpose() is a common example. >>> base = torch . tensor ([[ 0 , 1 ],[ 2 , 3 ]]) >>> base . is_contiguous () True >>> t = base . transpose ( 0 , 1 ) # `t` is a view of `base`. No data movement happened here. # View tensors might be non-contiguous. >>> t . is_contiguous () False # To get a contiguous tensor, call `.contiguous()` to enforce # copying data when `t` is not contiguous. >>> c = t . contiguous () For reference, here’s a full list of view ops in PyTorch: Basic slicing and indexing op, e.g. tensor[0, 2:, 1:7:2] returns a view of base tensor , see note below. adjoint() as_strided() detach() diagonal() expand() expand_as() movedim() narrow() permute() select() squeeze() transpose() t() T H mT mH real imag view_as_real() unflatten() unfold() unsqueeze() view() view_as() unbind() split() hsplit() vsplit() tensor_split() split_with_sizes() swapaxes() swapdims() chunk() indices() (sparse tensor only) values() (sparse tensor only) Note When accessing the contents of a tensor via indexing, PyTorch follows Numpy behaviors
that basic indexing returns views, while advanced indexing returns a copy.
Assignment via either basic or advanced indexing is in-place. See more examples in Numpy indexing documentation . It’s also worth mentioning a few ops with special behaviors: reshape() , reshape_as() and flatten() can return either a view or new tensor, user code shouldn’t rely on whether it’s view or not. contiguous() returns itself if input tensor is already contiguous, otherwise it returns a new contiguous tensor by copying data. For a more detailed walk-through of PyTorch internal implementation,
please refer to ezyang’s blogpost about PyTorch Internals .",2904,2,4,">>> t = torch.rand(4, 4)
>>> b = t.view(2, 8)
>>> t.storage().data_ptr() == b.storage().data_ptr()  # `t` and `b` share the same underlying data.
True
# Modifying view tensor changes base tensor as well.
>>> b[0][0] = 3.14
>>> t[0][0]
tensor(3.14)
---
>>> base = torch.tensor([[0, 1],[2, 3]])
>>> base.is_contiguous()
True
>>> t = base.transpose(0, 1)  # `t` is a view of `base`. No data movement happened here.
# View tensors might be non-contiguous.
>>> t.is_contiguous()
False
# To get a contiguous tensor, call `.contiguous()` to enforce
# copying data when `t` is not contiguous.
>>> c = t.contiguous()",1752175614.768467
https://pytorch.org/docs/stable/notes/extending.func.html,Extending torch.func with autograd.Function — PyTorch 2.7 documentation,"Extending torch.func with autograd.Function ¶ So you’d like to use torch.autograd.Function with the torch.func transforms like torch.vmap() , torch.func.grad() , etc. There are two main use cases: you wish to call code that does not contain PyTorch operations and
have it work with function transforms. That is, the torch.autograd.Function ’s
forward/backward/etc calls into functions from other systems like C++, CUDA, numpy. you wish to specify custom gradient rules, like
JAX’s custom_vjp/custom_jvp PyTorch combines both of these concepts into torch.autograd.Function . Basic Usage ¶ This guide assumes you are familiar with Extending torch.autograd ,
which explains how to use torch.autograd.Function . torch.autograd.Function can either have a forward() that accepts a ctx object,
or it can have separate forward() (that does not accept ctx ) and a setup_context() staticmethod that modifies the ctx object. Only the latter is supported with function transforms: forward() is the code that performs the operation and it should not accept
a ctx object. setup_context(ctx, inputs, output) is the code where you can
call methods on ctx . Here is where you should save Tensors for backward
(by calling ctx.save_for_backward(*tensors) ), or save non-Tensors
(by assigning them to the ctx object). Because setup_context() accepts only inputs and output ,
the only quantities that can be saved are either objects (such as Tensors) in
the inputs or outputs or quantities (like Tensor.shape ) derived from them.
If you wish to save a non-input intermediate activation from Function.forward() for backward, then you’ll need to return it as an
output from forward() so that it gets passed to setup_context() . Depending on the transform, to support reverse-mode AD ( torch.func.grad() , torch.func.vjp() ),
the torch.autograd.Function needs a backward() staticmethod. to support torch.vmap() , the torch.autograd.Function needs a vmap() staticmethod. to support torch.func.jvp() , the torch.autograd.Function needs a jvp() staticmethod. to support compositions of transforms (like torch.func.jacrev() , torch.func.jacfwd() , torch.func.hessian() ) – you may need multiple
of the above. In order for the torch.autograd.Function to be arbitrarily composable with function
transforms, we recommend that all other staticmethods other than forward() and setup_context() must be transformable: that is, they must consist of only PyTorch
operators or call other torch.autograd.Function (that may call into C++/CUDA/etc). Let’s go over some examples of common use cases. Example 1: autograd.Function calls into another system ¶ A common case is a torch.autograd.Function with both forward() and backward() calling
into another system (like C++, CUDA, numpy, triton). import torch import numpy as np def to_numpy ( tensor ): return tensor . cpu () . numpy () class NumpySort ( torch . autograd . Function ): # Note that forward does not take ctx @staticmethod def forward ( x , dim ): device = x . device x = to_numpy ( x ) ind = np . argsort ( x , axis = dim ) ind_inv = np . argsort ( ind , axis = dim ) result = np . take_along_axis ( x , ind , axis = dim ) # Any intermediates to be saved in backward must be returned as # outputs. return ( # The desired output torch . tensor ( result , device = device ), # intermediate to save for backward torch . tensor ( ind , device = device ), # intermediate to save for backward torch . tensor ( ind_inv , device = device ), ) # setup_context is responsible for calling methods and/or assigning to # the ctx object. Please do not do additional compute (e.g. add # Tensors together) in setup_context. @staticmethod def setup_context ( ctx , inputs , output ): x , dim = inputs # Note that output is whatever you returned from forward. # If you returned multiple values, then output is a Tuple of multiple values. # If you returned a single Tensor, then output is a Tensor. # If you returned a Tuple with a single Tensor, then output is a # Tuple with a single Tensor. _ , ind , ind_inv = output ctx . mark_non_differentiable ( ind , ind_inv ) # Tensors must be saved via ctx.save_for_backward. Please do not # assign them directly onto the ctx object. ctx . save_for_backward ( ind , ind_inv ) # Non-tensors may be saved by assigning them as attributes on the ctx object. ctx . dim = dim @staticmethod def backward ( ctx , grad_output , _0 , _1 ): # For the autograd.Function to be arbitrarily composable with function # transforms, all staticmethod other than forward and setup_context # must be implemented in a ""transformable"" way; that is, they must # only consist of PyTorch operations or autograd.Function. # # For example, this allows us to do double backwards and/or compute # second order gradients. # # We've written the backward pass of NumpySort in terms of another # autograd.Function, NumpyTake. ind , ind_inv = ctx . saved_tensors return NumpyTake . apply ( grad_output , ind_inv , ind , ctx . dim ), None class NumpyTake ( torch . autograd . Function ): @staticmethod def forward ( x , ind , ind_inv , dim ): device = x . device x = to_numpy ( x ) ind = to_numpy ( ind ) return torch . tensor ( np . take_along_axis ( x , ind , dim ), device = device ) @staticmethod def setup_context ( ctx , inputs , output ): x , ind , ind_inv , dim = inputs ctx . save_for_backward ( ind , ind_inv ) ctx . dim = dim @staticmethod def backward ( ctx , grad_output ): ind , ind_inv = ctx . saved_tensors result = NumpyTake . apply ( grad_output , ind_inv , ind , ctx . dim ) return result , None , None , None Now, to make it easier to use NumpySort (to hide away the intermediates we
returned as outputs, as well as allow default args and kwargs), we create a new
function that invokes it: def numpy_sort ( x , dim =- 1 ): result , _ , _ = NumpySort . apply ( x , dim ) return result And here’s a sanity check: x = torch . randn ( 2 , 3 ) grad_x = torch . func . grad ( lambda x : numpy_sort ( x ) . sum ())( x ) assert torch . allclose ( grad_x , torch . ones_like ( x )) Example 2: autograd.Function specifies custom gradient rules ¶ Another common case is an torch.autograd.Function that is implemented with PyTorch
operations. PyTorch is able to compute gradients for PyTorch operations automatically,
but perhaps we wish to customize how the gradients are computed. Some reasons why
we may want a custom backward different from the one PyTorch gives us are: improving numeric stability changing the performance characteristics of the backward changing how edge cases are handled (e.g. nans, inf) modifying the gradient (e.g. gradient clipping) Here’s an example of an torch.autograd.Function for the function y = x ** 3 where we
change the performance characteristics (some computation that would normally happen
during the backward pass, computing dx, happens in the forward pass). class MyCube ( torch . autograd . Function ): @staticmethod def forward ( x ): result = x ** 3 # In regular PyTorch, if we had just run y = x ** 3, then the backward # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done # that computation here in the forward pass instead. dx = 3 * x ** 2 return result , dx @staticmethod def setup_context ( ctx , inputs , output ): x , = inputs result , dx = output ctx . save_for_backward ( x , dx ) @staticmethod def backward ( ctx , grad_output , grad_dx ): x , dx = ctx . saved_tensors # In order for the autograd.Function to work with higher-order # gradients, we must add the gradient contribution of `dx`. result = grad_output * dx + grad_dx * 6 * x return result Now, to make it easier to use NumpySort (and hide away the intermediates we
returned as outputs) we create a new function that invokes it: def my_cube ( x ): result , _ = MyCube . apply ( x ) return result Here’s a sanity check computing the second-order gradients: x = torch . randn ([]) ggx = torch . func . grad ( torch . func . grad ( my_cube ))( x ) assert torch . allclose ( ggx , 6 * x ) Limitations and gotchas ¶ Warning Please read these limitations of torch.autograd.Function with torch.func transforms
carefully. We are not able to catch many of these situations and error out
gracefully so they will lead to undefined behavior. Please do not capture Tensors that are being transformed over, have
requires_grad=True, or are dual tensors, into the methods of the torch.autograd.Function . The way to be completely safe is to ensure that the only
Tensors being used inside any method of the torch.autograd.Function must be directly
passed as inputs (or via the ctx object) rather than come from outside
the torch.autograd.Function . torch.autograd.Function does not handle Tensors in pytrees (arbitrary nested
Python data structures that may or may not contain Tensors). For
those Tensors to be tracked by autograd, they must be passed directly as
an argument to torch.autograd.Function . This is in contrast to
jax.{custom_vjp, custom_jvp}, which do accept pytrees. Please only use save_for_backward() or save_for_forward() to save Tensors.
Please do not assign Tensors or collections of Tensors directly onto the ctx object -
these Tensors will not get tracked torch.vmap() Support ¶ To use an torch.autograd.Function with torch.vmap() , you must either: provide a vmap() staticmethod that tells us the behavior of the torch.autograd.Function under torch.vmap() ask us to autogenerate it by setting generate_vmap_rule=True . Automatically generate a vmap rule ¶ If your torch.autograd.Function fulfills the following additional constraints, then we
are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you
want custom behavior under vmap, please manually define a vmap staticmethod (see next section). Warning We are not easily able to check for the following constraints and error
out gracefully. Violation of the constraints may lead to undefined
behavior. The torch.autograd.Function ’s forward() , backward() (if it exists) and jvp() (if it exists) staticmethods must be transformable via torch.vmap() . That
is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom
CUDA kernels). Example: class MyCube ( torch . autograd . Function ): # Set generate_vmap_rule to True to ask PyTorch to automatically generate # a vmap rule. generate_vmap_rule = True @staticmethod def forward ( x ): result = x ** 3 dx = 3 * x ** 2 return result , dx @staticmethod def setup_context ( ctx , inputs , output ): x , = inputs result , dx = output ctx . save_for_backward ( x , dx ) @staticmethod def backward ( ctx , grad_output , grad_dx ): x , dx = ctx . saved_tensors result = grad_output * dx + grad_dx * 6 * x return result def my_cube ( x ): result , dx = MyCube . apply ( x ) return result x = torch . randn ( 3 ) result = torch . vmap ( my_cube )( x ) assert torch . allclose ( result , x ** 3 ) Defining the vmap staticmethod ¶ If your torch.autograd.Function calls into another system (like NumPy, C++, CUDA, triton),
then to get it to work with torch.vmap() or transforms that use it, you’ll
need to manually define a vmap() staticmethod. Depending on what transforms you want to use and your use case, you may not need
to add a vmap() staticmethod to all of your torch.autograd.Function : For example, torch.func.jacrev() performs vmap() over the backward pass.
So if you’re only interested in using torch.func.jacrev() , only
the backward() staticmethod needs to be vmappable. We do recommend ensuring all of your torch.autograd.Function have support for torch.vmap() though, especially if you are writing a third-party library and you want your torch.autograd.Function to work with all combinations of torch.func() transforms. Conceptually, the vmap staticmethod is responsible for defining how the forward() should behave under torch.vmap() . That is, it defines how to transform
the forward() to run over inputs with an additional dimension (the dimension
being vmapped over). This is similar to how torch.vmap() is implemented over
PyTorch operations: for each operation, we define a vmap rule (sometimes also
referred to as a “batching rule”). Here’s how to define the vmap() staticmethod: the signature is vmap(info, in_dims: Tuple[Optional[int]], *args) , where *args is the same as the args to forward() . The vmap staticmethod is responsible for defining how the forward() should behave
under torch.vmap() . That is, given inputs with an additional dimension
(specified by in_dims ), how do we compute the batched version of forward() ? For each arg in args , in_dims has a corresponding Optional[int] .
It is None if the arg is not a Tensor or if the arg is not being vmapped over,
otherwise, it is an integer specifying what dimension of the Tensor is being vmapped
over. info is a collection of additional metadata that may be helpful: info.batch_size specifies the size of the dimension being vmapped over, while info.randomness is the randomness option that was passed to torch.vmap() . The return of the vmap staticmethod is a tuple of (output, out_dims) . Similar
to in_dims , out_dims should be of the same structure as output and contain
one out_dim per output that specifies if the output has the vmapped
dimension and what index it is in. Example: def to_numpy ( tensor ): return tensor . cpu () . numpy () class NumpySort ( torch . autograd . Function ): @staticmethod def forward ( x , dim ): device = x . device x = to_numpy ( x ) ind = np . argsort ( x , axis = dim ) ind_inv = np . argsort ( ind , axis = dim ) result = np . take_along_axis ( x , ind , axis = dim ) return ( torch . tensor ( result , device = device ), torch . tensor ( ind , device = device ), torch . tensor ( ind_inv , device = device ), ) @staticmethod def setup_context ( ctx , inputs , output ): x , dim = inputs _ , ind , ind_inv = output ctx . mark_non_differentiable ( ind , ind_inv ) ctx . save_for_backward ( ind , ind_inv ) ctx . dim = dim @staticmethod def backward ( ctx , grad_output , _0 , _1 ): ind , ind_inv = ctx . saved_tensors return NumpyTake . apply ( grad_output , ind_inv , ind , ctx . dim ), None # The signature of the vmap staticmethod is: # vmap(info, in_dims: Tuple[Optional[int]], *args) # where *args is the same as the arguments to `forward`. @staticmethod def vmap ( info , in_dims , x , dim ): # For every input (x and dim), in_dims stores an Optional[int] # that is: # - None if the input is not being vmapped over or if the input #   is not a Tensor # - an integer if the input is being vmapped over that represents #   the index of the dimension being vmapped over. x_bdim , _ = in_dims # A ""vmap rule"" is the logic of how to perform the operation given # inputs with one additional dimension. In NumpySort, x has an # additional dimension (x_bdim). The vmap rule is simply # to call NumpySort again but pass it a different `dim`. x = x . movedim ( x_bdim , 0 ) # Handle negative dims correctly dim = dim if dim >= 0 else dim + x . dim () - 1 result = NumpySort . apply ( x , dim + 1 ) # The vmap rule must return a tuple of two things # 1. the output. Should be the same amount of things #    as returned by the forward(). # 2. one Optional[int] for each output specifying if each output # is being vmapped over, and if so, the index of the # dimension being vmapped over. # # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the # dimension being vmapped over to the front of `x`, that appears at # dimension 0 of all outputs. # The return is (output, out_dims) -- output is a tuple of 3 Tensors # and out_dims is a Tuple of 3 Optional[int] return NumpySort . apply ( x , dim + 1 ), ( 0 , 0 , 0 ) class NumpyTake ( torch . autograd . Function ): @staticmethod def forward ( x , ind , ind_inv , dim ): device = x . device x = to_numpy ( x ) ind = to_numpy ( ind ) return torch . tensor ( np . take_along_axis ( x , ind , dim ), device = device ) @staticmethod def setup_context ( ctx , inputs , output ): x , ind , ind_inv , dim = inputs ctx . save_for_backward ( ind , ind_inv ) ctx . dim = dim @staticmethod def backward ( ctx , grad_output ): ind , ind_inv = ctx . saved_tensors result = NumpyTake . apply ( grad_output , ind_inv , ind , ctx . dim ) return result , None , None , None @staticmethod def vmap ( info , in_dims , x , ind , ind_inv , dim ): x_bdim , ind_bdim , ind_inv_bdim , _ = in_dims # The strategy is: expand {x, ind, ind_inv} to all have the dimension # being vmapped over. # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim). # Handle negative dims by wrapping them to be positive logical_dim = x . dim () if x_bdim is None else x_bdim - 1 dim = dim if dim >= 0 else dim + logical_dim def maybe_expand_bdim_at_front ( x , x_bdim ): if x_bdim is None : return x . expand ( info . batch_size , * x . shape ) return x . movedim ( x_bdim , 0 ) # If the Tensor doesn't have the dimension being vmapped over, # expand it out. Otherwise, move it to the front of the Tensor x = maybe_expand_bdim_at_front ( x , x_bdim ) ind = maybe_expand_bdim_at_front ( ind , ind_bdim ) ind_inv = maybe_expand_bdim_at_front ( ind_inv , ind_inv_bdim ) # The return is a tuple (output, out_dims). Since output is a Tensor, # then out_dims is an Optional[int] (instead of being a Tuple). return NumpyTake . apply ( x , ind , ind_inv , dim + 1 ), 0 def numpy_sort ( x , dim =- 1 ): result , _ , _ = NumpySort . apply ( x , dim ) return result x = torch . randn ( 2 , 3 ) result = torch . vmap ( numpy_sort )( x ) assert torch . allclose ( result , numpy_sort ( result , 1 )) Note The vmap staticmethod should aim to preserve the semantics of the
entire Function . That is, (pseudocode) grad(vmap(MyFunc)) should be replaceable with a grad(map(MyFunc)) . If your autograd.Function has any custom behavior in the backward pass, please
keep this in mind. Note It is a legitimate use case to write a custom vmap staticmethod for a Function that PyTorch is able to generate a vmap
rule for via generate_vmap_rule=True . You may wish to do this if the
generated vmap rule doesn’t have the semantics you’re looking for. torch.func.jvp() Support ¶ To support forward-mode AD, a torch.autograd.Function must have a jvp() staticmethod.
Please see Forward mode AD for details.",18153,8,12,"import torch
import numpy as np

def to_numpy(tensor):
    return tensor.cpu().numpy()

class NumpySort(torch.autograd.Function):
    # Note that forward does not take ctx
    @staticmethod
    def forward(x, dim):
        device = x.device
        x = to_numpy(x)
        ind = np.argsort(x, axis=dim)
        ind_inv = np.argsort(ind, axis=dim)
        result = np.take_along_axis(x, ind, axis=dim)
        # Any intermediates to be saved in backward must be returned as
        # outputs.
        return (
            # The desired output
            torch.tensor(result, device=device),
            # intermediate to save for backward
            torch.tensor(ind, device=device),
            # intermediate to save for backward
            torch.tensor(ind_inv, device=device),
        )

    # setup_context is responsible for calling methods and/or assigning to
    # the ctx object. Please do not do additional compute (e.g. add
    # Tensors together) in setup_context.
    @staticmethod
    def setup_context(ctx, inputs, output):
        x, dim = inputs
        # Note that output is whatever you returned from forward.
        # If you returned multiple values, then output is a Tuple of multiple values.
        # If you returned a single Tensor, then output is a Tensor.
        # If you returned a Tuple with a single Tensor, then output is a
        # Tuple with a single Tensor.
        _, ind, ind_inv = output
        ctx.mark_non_differentiable(ind, ind_inv)
        # Tensors must be saved via ctx.save_for_backward. Please do not
        # assign them directly onto the ctx object.
        ctx.save_for_backward(ind, ind_inv)
        # Non-tensors may be saved by assigning them as attributes on the ctx object.
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output, _0, _1):
        # For the autograd.Function to be arbitrarily composable with function
        # transforms, all staticmethod other than forward and setup_context
        # must be implemented in a ""transformable"" way; that is, they must
        # only consist of PyTorch operations or autograd.Function.
        #
        # For example, this allows us to do double backwards and/or compute
        # second order gradients.
        #
        # We've written the backward pass of NumpySort in terms of another
        # autograd.Function, NumpyTake.
        ind, ind_inv = ctx.saved_tensors
        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None

class NumpyTake(torch.autograd.Function):
    @staticmethod
    def forward(x, ind, ind_inv, dim):
        device = x.device
        x = to_numpy(x)
        ind = to_numpy(ind)
        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, ind, ind_inv, dim = inputs
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output):
        ind, ind_inv = ctx.saved_tensors
        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
        return result, None, None, None
---
def numpy_sort(x, dim=-1):
    result, _, _ = NumpySort.apply(x, dim)
    return result
---
x = torch.randn(2, 3)
grad_x = torch.func.grad(lambda x: numpy_sort(x).sum())(x)
assert torch.allclose(grad_x, torch.ones_like(x))
---
class MyCube(torch.autograd.Function):
    @staticmethod
    def forward(x):
        result = x ** 3
        # In regular PyTorch, if we had just run y = x ** 3, then the backward
        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done
        # that computation here in the forward pass instead.
        dx = 3 * x ** 2
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        # In order for the autograd.Function to work with higher-order
        # gradients, we must add the gradient contribution of `dx`.
        result = grad_output * dx + grad_dx * 6 * x
        return result
---
def my_cube(x):
    result, _ = MyCube.apply(x)
    return result
---
x = torch.randn([])
ggx = torch.func.grad(torch.func.grad(my_cube))(x)
assert torch.allclose(ggx, 6 * x)
---
class MyCube(torch.autograd.Function):
    # Set generate_vmap_rule to True to ask PyTorch to automatically generate
    # a vmap rule.
    generate_vmap_rule = True

    @staticmethod
    def forward(x):
        result = x ** 3
        dx = 3 * x ** 2
        return result, dx

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, = inputs
        result, dx = output
        ctx.save_for_backward(x, dx)

    @staticmethod
    def backward(ctx, grad_output, grad_dx):
        x, dx = ctx.saved_tensors
        result = grad_output * dx + grad_dx * 6 * x
        return result

def my_cube(x):
    result, dx = MyCube.apply(x)
    return result

x = torch.randn(3)
result = torch.vmap(my_cube)(x)
assert torch.allclose(result, x ** 3)
---
def to_numpy(tensor):
    return tensor.cpu().numpy()

class NumpySort(torch.autograd.Function):
    @staticmethod
    def forward(x, dim):
        device = x.device
        x = to_numpy(x)
        ind = np.argsort(x, axis=dim)
        ind_inv = np.argsort(ind, axis=dim)
        result = np.take_along_axis(x, ind, axis=dim)
        return (
            torch.tensor(result, device=device),
            torch.tensor(ind, device=device),
            torch.tensor(ind_inv, device=device),
        )

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, dim = inputs
        _, ind, ind_inv = output
        ctx.mark_non_differentiable(ind, ind_inv)
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output, _0, _1):
        ind, ind_inv = ctx.saved_tensors
        return NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim), None

    # The signature of the vmap staticmethod is:
    # vmap(info, in_dims: Tuple[Optional[int]], *args)
    # where *args is the same as the arguments to `forward`.
    @staticmethod
    def vmap(info, in_dims, x, dim):
        # For every input (x and dim), in_dims stores an Optional[int]
        # that is:
        # - None if the input is not being vmapped over or if the input
        #   is not a Tensor
        # - an integer if the input is being vmapped over that represents
        #   the index of the dimension being vmapped over.
        x_bdim, _ = in_dims

        # A ""vmap rule"" is the logic of how to perform the operation given
        # inputs with one additional dimension. In NumpySort, x has an
        # additional dimension (x_bdim). The vmap rule is simply
        # to call NumpySort again but pass it a different `dim`.
        x = x.movedim(x_bdim, 0)
        # Handle negative dims correctly
        dim = dim if dim >= 0 else dim + x.dim() - 1
        result = NumpySort.apply(x, dim + 1)

        # The vmap rule must return a tuple of two things
        # 1. the output. Should be the same amount of things
        #    as returned by the forward().
        # 2. one Optional[int] for each output specifying if each output
        # is being vmapped over, and if so, the index of the
        # dimension being vmapped over.
        #
        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the
        # dimension being vmapped over to the front of `x`, that appears at
        # dimension 0 of all outputs.
        # The return is (output, out_dims) -- output is a tuple of 3 Tensors
        # and out_dims is a Tuple of 3 Optional[int]
        return NumpySort.apply(x, dim + 1), (0, 0, 0)

class NumpyTake(torch.autograd.Function):
    @staticmethod
    def forward(x, ind, ind_inv, dim):
        device = x.device
        x = to_numpy(x)
        ind = to_numpy(ind)
        return torch.tensor(np.take_along_axis(x, ind, dim), device=device)

    @staticmethod
    def setup_context(ctx, inputs, output):
        x, ind, ind_inv, dim = inputs
        ctx.save_for_backward(ind, ind_inv)
        ctx.dim = dim

    @staticmethod
    def backward(ctx, grad_output):
        ind, ind_inv = ctx.saved_tensors
        result = NumpyTake.apply(grad_output, ind_inv, ind, ctx.dim)
        return result, None, None, None

    @staticmethod
    def vmap(info, in_dims, x, ind, ind_inv, dim):
        x_bdim, ind_bdim, ind_inv_bdim, _ = in_dims

        # The strategy is: expand {x, ind, ind_inv} to all have the dimension
        # being vmapped over.
        # Then, call back into NumpyTake(expanded_x, expanded_ind, expanded_ind_inv, new_dim).

        # Handle negative dims by wrapping them to be positive
        logical_dim = x.dim() if x_bdim is None else x_bdim - 1
        dim = dim if dim >= 0 else dim + logical_dim

        def maybe_expand_bdim_at_front(x, x_bdim):
            if x_bdim is None:
                return x.expand(info.batch_size, *x.shape)
            return x.movedim(x_bdim, 0)

        # If the Tensor doesn't have the dimension being vmapped over,
        # expand it out. Otherwise, move it to the front of the Tensor
        x = maybe_expand_bdim_at_front(x, x_bdim)
        ind = maybe_expand_bdim_at_front(ind, ind_bdim)
        ind_inv = maybe_expand_bdim_at_front(ind_inv, ind_inv_bdim)

        # The return is a tuple (output, out_dims). Since output is a Tensor,
        # then out_dims is an Optional[int] (instead of being a Tuple).
        return NumpyTake.apply(x, ind, ind_inv, dim + 1), 0

def numpy_sort(x, dim=-1):
    result, _, _ = NumpySort.apply(x, dim)
    return result

x = torch.randn(2, 3)
result = torch.vmap(numpy_sort)(x)
assert torch.allclose(result, numpy_sort(result, 1))",1752175615.8745651
https://pytorch.org/docs/stable/mtia.html,torch.mtia — PyTorch 2.7 documentation,"torch.mtia ¶ The MTIA backend is implemented out of the tree, only interfaces are be defined here. This package enables an interface for accessing MTIA backend in python StreamContext Context-manager that selects a given stream. current_device Return the index of a currently selected device. current_stream Return the currently selected Stream for a given device. default_stream Return the default Stream for a given device. device_count Return the number of MTIA devices available. init is_available Return true if MTIA device is available is_initialized Return whether PyTorch's MTIA state has been initialized. memory_stats Return a dictionary of MTIA memory allocator statistics for a given device. get_device_capability Return capability of a given device as a tuple of (major version, minor version). empty_cache Empty the MTIA device cache. record_memory_history Enable/Disable the memory profiler on MTIA allocator snapshot Return a dictionary of MTIA memory allocator history set_device Set the current device. set_stream Set the current stream.This is a wrapper API to set the stream. stream Wrap around the Context-manager StreamContext that selects a given stream. synchronize Waits for all jobs in all streams on a MTIA device to complete. device Context-manager that changes the selected device. set_rng_state Sets the random number generator state. get_rng_state Returns the random number generator state as a ByteTensor. DeferredMtiaCallError Streams and events ¶ Event Query and record Stream status to identify or control dependencies across Stream and measure timing. Stream An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.",1697,0,5,,1752175616.9722264
https://pytorch.org/docs/stable/optim.html,torch.optim — PyTorch 2.7 documentation,"torch.optim ¶ torch.optim is a package implementing various optimization algorithms. Most commonly used methods are already supported, and the interface is general
enough, so that more sophisticated ones can also be easily integrated in the
future. How to use an optimizer ¶ To use torch.optim you have to construct an optimizer object that will hold
the current state and will update the parameters based on the computed gradients. Constructing it ¶ To construct an Optimizer you have to give it an iterable containing the
parameters (all should be Parameter s) or named parameters
(tuples of (str, Parameter )) to optimize. Then,
you can specify optimizer-specific options such as the learning rate, weight decay, etc. Example: optimizer = optim . SGD ( model . parameters (), lr = 0.01 , momentum = 0.9 ) optimizer = optim . Adam ([ var1 , var2 ], lr = 0.0001 ) Named parameters example: optimizer = optim . SGD ( model . named_parameters (), lr = 0.01 , momentum = 0.9 ) optimizer = optim . Adam ([( 'layer0' , var1 ), ( 'layer1' , var2 )], lr = 0.0001 ) Per-parameter options ¶ Optimizer s also support specifying per-parameter options. To do this, instead
of passing an iterable of Variable s, pass in an iterable of dict s. Each of them will define a separate parameter group, and should contain
a params key, containing a list of parameters belonging to it. Other keys
should match the keyword arguments accepted by the optimizers, and will be used
as optimization options for this group. For example, this is very useful when one wants to specify per-layer learning rates: optim . SGD ([ { 'params' : model . base . parameters (), 'lr' : 1e-2 }, { 'params' : model . classifier . parameters ()} ], lr = 1e-3 , momentum = 0.9 ) optim . SGD ([ { 'params' : model . base . named_parameters (), 'lr' : 1e-2 }, { 'params' : model . classifier . named_parameters ()} ], lr = 1e-3 , momentum = 0.9 ) This means that model.base ’s parameters will use a learning rate of 1e-2 , whereas model.classifier ’s parameters will stick to the default learning rate of 1e-3 .
Finally a momentum of 0.9 will be used for all parameters. Note You can still pass options as keyword arguments. They will be used as
defaults, in the groups that didn’t override them. This is useful when you
only want to vary a single option, while keeping all others consistent
between parameter groups. Also consider the following example related to the distinct penalization of parameters.
Remember that parameters() returns an iterable that
contains all learnable parameters, including biases and other
parameters that may prefer distinct penalization. To address this, one can specify
individual penalization weights for each parameter group: bias_params = [ p for name , p in self . named_parameters () if 'bias' in name ] others = [ p for name , p in self . named_parameters () if 'bias' not in name ] optim . SGD ([ { 'params' : others }, { 'params' : bias_params , 'weight_decay' : 0 } ], weight_decay = 1e-2 , lr = 1e-2 ) In this manner, bias terms are isolated from non-bias terms, and a weight_decay of 0 is set specifically for the bias terms, as to avoid any penalization for
this group. Taking an optimization step ¶ All optimizers implement a step() method, that updates the
parameters. It can be used in two ways: optimizer.step() ¶ This is a simplified version supported by most optimizers. The function can be
called once the gradients are computed using e.g. backward() . Example: for input , target in dataset : optimizer . zero_grad () output = model ( input ) loss = loss_fn ( output , target ) loss . backward () optimizer . step () optimizer.step(closure) ¶ Some optimization algorithms such as Conjugate Gradient and LBFGS need to
reevaluate the function multiple times, so you have to pass in a closure that
allows them to recompute your model. The closure should clear the gradients,
compute the loss, and return it. Example: for input , target in dataset : def closure (): optimizer . zero_grad () output = model ( input ) loss = loss_fn ( output , target ) loss . backward () return loss optimizer . step ( closure ) Base class ¶ class torch.optim. Optimizer ( params , defaults ) [source] [source] ¶ Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic
ordering that is consistent between runs. Examples of objects that don’t
satisfy those properties are sets and iterators over values of dictionaries. Parameters params ( iterable ) – an iterable of torch.Tensor s or dict s. Specifies what Tensors should be optimized. defaults ( dict [ str , Any ] ) – (dict): a dict containing default values of optimization
options (used when a parameter group doesn’t specify them). Optimizer.add_param_group Add a param group to the Optimizer s param_groups . Optimizer.load_state_dict Load the optimizer state. Optimizer.register_load_state_dict_pre_hook Register a load_state_dict pre-hook which will be called before load_state_dict() is called. It should have the following signature::. Optimizer.register_load_state_dict_post_hook Register a load_state_dict post-hook which will be called after load_state_dict() is called. It should have the following signature::. Optimizer.state_dict Return the state of the optimizer as a dict . Optimizer.register_state_dict_pre_hook Register a state dict pre-hook which will be called before state_dict() is called. Optimizer.register_state_dict_post_hook Register a state dict post-hook which will be called after state_dict() is called. Optimizer.step Perform a single optimization step to update parameter. Optimizer.register_step_pre_hook Register an optimizer step pre hook which will be called before optimizer step. Optimizer.register_step_post_hook Register an optimizer step post hook which will be called after optimizer step. Optimizer.zero_grad Reset the gradients of all optimized torch.Tensor s. Algorithms ¶ Adadelta Implements Adadelta algorithm. Adafactor Implements Adafactor algorithm. Adagrad Implements Adagrad algorithm. Adam Implements Adam algorithm. AdamW Implements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance. SparseAdam SparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients. Adamax Implements Adamax algorithm (a variant of Adam based on infinity norm). ASGD Implements Averaged Stochastic Gradient Descent. LBFGS Implements L-BFGS algorithm. NAdam Implements NAdam algorithm. RAdam Implements RAdam algorithm. RMSprop Implements RMSprop algorithm. Rprop Implements the resilient backpropagation algorithm. SGD Implements stochastic gradient descent (optionally with momentum). Many of our algorithms have various implementations optimized for performance,
readability and/or generality, so we attempt to default to the generally fastest
implementation for the current device if no particular implementation has been
specified by the user. We have 3 major categories of implementations: for-loop, foreach (multi-tensor), and
fused. The most straightforward implementations are for-loops over the parameters with
big chunks of computation. For-looping is usually slower than our foreach
implementations, which combine parameters into a multi-tensor and run the big chunks
of computation all at once, thereby saving many sequential kernel calls. A few of our
optimizers have even faster fused implementations, which fuse the big chunks of
computation into one kernel. We can think of foreach implementations as fusing
horizontally and fused implementations as fusing vertically on top of that. In general, the performance ordering of the 3 implementations is fused > foreach > for-loop.
So when applicable, we default to foreach over for-loop. Applicable means the foreach
implementation is available, the user has not specified any implementation-specific kwargs
(e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused
should be even faster than foreach, the implementations are newer and we would like to give
them more bake-in time before flipping the switch everywhere. We summarize the stability status
for each implementation on the second table below, you are welcome to try them out though! Below is a table showing the available and default implementations of each algorithm: Algorithm Default Has foreach? Has fused? Adadelta foreach yes no Adafactor for-loop no no Adagrad foreach yes yes (cpu only) Adam foreach yes yes AdamW foreach yes yes SparseAdam for-loop no no Adamax foreach yes no ASGD foreach yes no LBFGS for-loop no no NAdam foreach yes no RAdam foreach yes no RMSprop foreach yes no Rprop foreach yes no SGD foreach yes yes Below table is showing the stability status for fused implementations: Algorithm CPU CUDA MPS Adadelta unsupported unsupported unsupported Adafactor unsupported unsupported unsupported Adagrad beta unsupported unsupported Adam beta stable beta AdamW beta stable beta SparseAdam unsupported unsupported unsupported Adamax unsupported unsupported unsupported ASGD unsupported unsupported unsupported LBFGS unsupported unsupported unsupported NAdam unsupported unsupported unsupported RAdam unsupported unsupported unsupported RMSprop unsupported unsupported unsupported Rprop unsupported unsupported unsupported SGD beta beta beta How to adjust learning rate ¶ torch.optim.lr_scheduler.LRScheduler provides several methods to adjust the learning
rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer’s update; e.g., you
should write your code this way: Example: optimizer = optim . SGD ( model . parameters (), lr = 0.01 , momentum = 0.9 ) scheduler = ExponentialLR ( optimizer , gamma = 0.9 ) for epoch in range ( 20 ): for input , target in dataset : optimizer . zero_grad () output = model ( input ) loss = loss_fn ( output , target ) loss . backward () optimizer . step () scheduler . step () Most learning rate schedulers can be called back-to-back (also referred to as
chaining schedulers). The result is that each scheduler is applied one after the
other on the learning rate obtained by the one preceding it. Example: optimizer = optim . SGD ( model . parameters (), lr = 0.01 , momentum = 0.9 ) scheduler1 = ExponentialLR ( optimizer , gamma = 0.9 ) scheduler2 = MultiStepLR ( optimizer , milestones = [ 30 , 80 ], gamma = 0.1 ) for epoch in range ( 20 ): for input , target in dataset : optimizer . zero_grad () output = model ( input ) loss = loss_fn ( output , target ) loss . backward () optimizer . step () scheduler1 . step () scheduler2 . step () In many places in the documentation, we will use the following template to refer to schedulers
algorithms. >>> scheduler = ... >>> for epoch in range ( 100 ): >>> train ( ... ) >>> validate ( ... ) >>> scheduler . step () Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before
the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use
the learning rate scheduler (calling scheduler.step() ) before the optimizer’s update
(calling optimizer.step() ), this will skip the first value of the learning rate schedule.
If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check
if you are calling scheduler.step() at the wrong time. lr_scheduler.LRScheduler Adjusts the learning rate during optimization. lr_scheduler.LambdaLR Sets the initial learning rate. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ConstantLR Multiply the learning rate of each parameter group by a small constant factor. lr_scheduler.LinearLR Decays the learning rate of each parameter group by linearly changing small multiplicative factor. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.PolynomialLR Decays the learning rate of each parameter group using a polynomial function in the given total_iters. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule. lr_scheduler.ChainedScheduler Chains a list of learning rate schedulers. lr_scheduler.SequentialLR Contains a list of schedulers expected to be called sequentially during the optimization process. lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule. How to utilize named parameters to load optimizer state dict ¶ The function load_state_dict() stores the optional param_names content from the
loaded state dict if present. However, the process of loading the optimizer state is not affected,
as the order of the parameters matters to maintain compatibility (in case of different ordering).
To utilize the loaded parameters names from the loaded state dict, a custom register_load_state_dict_pre_hook needs to be implemented according to the desired behavior. This can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to
remain unchanged. The following example demonstrates how to implement this customization. Example: class OneLayerModel ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc = nn . Linear ( 3 , 4 ) def forward ( self , x ): return self . fc ( x ) model = OneLayerModel () optimizer = optim . SGD ( model . named_parameters (), lr = 0.01 , momentum = 0.9 ) # training.. torch . save ( optimizer . state_dict (), PATH ) Let’s say that model implements an expert (MoE), and we want to duplicate it and resume training
for two experts, both initialized the same way as the fc layer. For the following model2 we create two layers identical to fc and resume training by loading the model weights and optimizer states from model into both fc1 and fc2 of model2 (and adjust them accordingly): class TwoLayerModel ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc1 = nn . Linear ( 3 , 4 ) self . fc2 = nn . Linear ( 3 , 4 ) def forward ( self , x ): return ( self . fc1 ( x ) + self . fc2 ( x )) / 2 model2 = TwoLayerModel () # adapt and load model weights.. optimizer2 = optim . SGD ( model2 . named_parameters (), lr = 0.01 , momentum = 0.9 ) To load the state dict for optimizer2 with the state dict of the previous optimizer such that both fc1 and fc2 will be initialized with a copy of fc optimizer states
(to resume training for each layer from fc ), we can use the following hook: def adapt_state_dict_ids ( optimizer , state_dict ): adapted_state_dict = deepcopy ( optimizer . state_dict ()) # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. for k , v in state_dict [ 'param_groups' ][ 0 ] . items (): if k not in [ 'params' , 'param_names' ]: adapted_state_dict [ 'param_groups' ][ 0 ][ k ] = v lookup_dict = { 'fc1.weight' : 'fc.weight' , 'fc1.bias' : 'fc.bias' , 'fc2.weight' : 'fc.weight' , 'fc2.bias' : 'fc.bias' } clone_deepcopy = lambda d : { k : ( v . clone () if isinstance ( v , torch . Tensor ) else deepcopy ( v )) for k , v in d . items ()} for param_id , param_name in zip ( optimizer . state_dict ()[ 'param_groups' ][ 0 ][ 'params' ], optimizer . state_dict ()[ 'param_groups' ][ 0 ][ 'param_names' ]): name_in_loaded = lookup_dict [ param_name ] index_in_loaded_list = state_dict [ 'param_groups' ][ 0 ][ 'param_names' ] . index ( name_in_loaded ) id_in_loaded = state_dict [ 'param_groups' ][ 0 ][ 'params' ][ index_in_loaded_list ] # Copy the state of the corresponding parameter if id_in_loaded in state_dict [ 'state' ]: adapted_state_dict [ 'state' ][ param_id ] = clone_deepcopy ( state_dict [ 'state' ][ id_in_loaded ]) return adapted_state_dict optimizer2 . register_load_state_dict_pre_hook ( adapt_state_dict_ids ) optimizer2 . load_state_dict ( torch . load ( PATH )) # The previous optimizer saved state_dict This ensures that the adapted state_dict with the correct states for the layers of model2 will be used
during model loading.
Note that this code is designed specifically for this example (e.g., assuming a single parameter group),
and other cases might require different adaptations. The following example shows how to handle missing parameters in a loaded state dict when the model structure changes.
The Model_bypass adds a new bypass layer, which is not present in the original Model1 .
To resume training, a custom adapt_state_dict_missing_param hook is used to adapt the optimizer’s state_dict ,
ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged
(as initialized in this example).
This approach enables smooth loading and resuming of the optimizer state despite model changes.
The new bypass layer will be trained from scratch: class Model1 ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc = nn . Linear ( 5 , 5 ) def forward ( self , x ): return self . fc ( x ) + x model = Model1 () optimizer = optim . SGD ( model . named_parameters (), lr = 0.01 , momentum = 0.9 ) # training.. torch . save ( optimizer . state_dict (), PATH ) class Model_bypass ( nn . Module ): def __init__ ( self ): super () . __init__ () self . fc = nn . Linear ( 5 , 5 ) self . bypass = nn . Linear ( 5 , 5 , bias = False ) torch . nn . init . eye_ ( self . bypass . weight ) def forward ( self , x ): return self . fc ( x ) + self . bypass ( x ) model2 = Model_bypass () optimizer2 = optim . SGD ( model2 . named_parameters (), lr = 0.01 , momentum = 0.9 ) def adapt_state_dict_missing_param ( optimizer , state_dict ): adapted_state_dict = deepcopy ( optimizer . state_dict ()) # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict. for k , v in state_dict [ 'param_groups' ][ 0 ] . items (): if k not in [ 'params' , 'param_names' ]: adapted_state_dict [ 'param_groups' ][ 0 ][ k ] = v lookup_dict = { 'fc.weight' : 'fc.weight' , 'fc.bias' : 'fc.bias' , 'bypass.weight' : None , } clone_deepcopy = lambda d : { k : ( v . clone () if isinstance ( v , torch . Tensor ) else deepcopy ( v )) for k , v in d . items ()} for param_id , param_name in zip ( optimizer . state_dict ()[ 'param_groups' ][ 0 ][ 'params' ], optimizer . state_dict ()[ 'param_groups' ][ 0 ][ 'param_names' ]): name_in_loaded = lookup_dict [ param_name ] if name_in_loaded in state_dict [ 'param_groups' ][ 0 ][ 'param_names' ]: index_in_loaded_list = state_dict [ 'param_groups' ][ 0 ][ 'param_names' ] . index ( name_in_loaded ) id_in_loaded = state_dict [ 'param_groups' ][ 0 ][ 'params' ][ index_in_loaded_list ] # Copy the state of the corresponding parameter if id_in_loaded in state_dict [ 'state' ]: adapted_state_dict [ 'state' ][ param_id ] = clone_deepcopy ( state_dict [ 'state' ][ id_in_loaded ]) return adapted_state_dict optimizer2 . register_load_state_dict_pre_hook ( adapt_state_dict_ids ) optimizer2 . load_state_dict ( torch . load ( PATH )) # The previous optimizer saved state_dict As a third example, instead of loading a state according to the order of parameters (the default approach),
this hook can be used to load according to the parameters’ names: def names_matching ( optimizer , state_dict ): assert len ( state_dict [ 'param_groups' ]) == len ( optimizer . state_dict ()[ 'param_groups' ]) adapted_state_dict = deepcopy ( optimizer . state_dict ()) for g_ind in range ( len ( state_dict [ 'param_groups' ])): assert len ( state_dict [ 'param_groups' ][ g_ind ][ 'params' ]) == len ( optimizer . state_dict ()[ 'param_groups' ][ g_ind ][ 'params' ]) for k , v in state_dict [ 'param_groups' ][ g_ind ] . items (): if k not in [ 'params' , 'param_names' ]: adapted_state_dict [ 'param_groups' ][ g_ind ][ k ] = v for param_id , param_name in zip ( optimizer . state_dict ()[ 'param_groups' ][ g_ind ][ 'params' ], optimizer . state_dict ()[ 'param_groups' ][ g_ind ][ 'param_names' ]): index_in_loaded_list = state_dict [ 'param_groups' ][ g_ind ][ 'param_names' ] . index ( param_name ) id_in_loaded = state_dict [ 'param_groups' ][ g_ind ][ 'params' ][ index_in_loaded_list ] # Copy the state of the corresponding parameter if id_in_loaded in state_dict [ 'state' ]: adapted_state_dict [ 'state' ][ param_id ] = deepcopy ( state_dict [ 'state' ][ id_in_loaded ]) return adapted_state_dict Weight Averaging (SWA and EMA) ¶ torch.optim.swa_utils.AveragedModel implements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), torch.optim.swa_utils.SWALR implements the SWA learning rate scheduler and torch.optim.swa_utils.update_bn() is a utility function used to update SWA/EMA batch
normalization statistics at the end of training. SWA has been proposed in Averaging Weights Leads to Wider Optima and Better Generalization . EMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of Polyak averaging , but using exponential weights instead of equal weights across iterations. Constructing averaged models ¶ The AveragedModel class serves to compute the weights of the SWA or EMA model. You can create an SWA averaged model by running: >>> averaged_model = AveragedModel ( model ) EMA models are constructed by specifying the multi_avg_fn argument as follows: >>> decay = 0.999 >>> averaged_model = AveragedModel ( model , multi_avg_fn = get_ema_multi_avg_fn ( decay )) Decay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to torch.optim.swa_utils.get_ema_multi_avg_fn() , the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues. torch.optim.swa_utils.get_ema_multi_avg_fn() returns a function that applies the following EMA equation to the weights: W t + 1 EMA = α W t EMA + ( 1 − α ) W t model W^\textrm{EMA}_{t+1} = \alpha W^\textrm{EMA}_{t} + (1 - \alpha) W^\textrm{model}_t W t + 1 EMA ​ = α W t EMA ​ + ( 1 − α ) W t model ​ where alpha is the EMA decay. Here the model model can be an arbitrary torch.nn.Module object. averaged_model will keep track of the running averages of the parameters of the model . To update these
averages, you should use the update_parameters() function after the optimizer.step() : >>> averaged_model . update_parameters ( model ) For SWA and EMA, this call is usually done right after the optimizer step() . In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training. Custom averaging strategies ¶ By default, torch.optim.swa_utils.AveragedModel computes a running equal average of
the parameters that you provide, but you can also use custom averaging functions with the avg_fn or multi_avg_fn parameters: avg_fn allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter. multi_avg_fn allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the torch._foreach* functions. This function must update the averaged parameters in-place. In the following example ema_model computes an exponential moving average using the avg_fn parameter: >>> ema_avg = lambda averaged_model_parameter , model_parameter , num_averaged : \ >>> 0.9 * averaged_model_parameter + 0.1 * model_parameter >>> ema_model = torch . optim . swa_utils . AveragedModel ( model , avg_fn = ema_avg ) In the following example ema_model computes an exponential moving average using the more efficient multi_avg_fn parameter: >>> ema_model = AveragedModel ( model , multi_avg_fn = get_ema_multi_avg_fn ( 0.9 )) SWA learning rate schedules ¶ Typically, in SWA the learning rate is set to a high constant value. SWALR is a
learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it
constant. For example, the following code creates a scheduler that linearly anneals the
learning rate from its initial value to 0.05 in 5 epochs within each parameter group: >>> swa_scheduler = torch . optim . swa_utils . SWALR ( optimizer , \ >>> anneal_strategy = ""linear"" , anneal_epochs = 5 , swa_lr = 0.05 ) You can also use cosine annealing to a fixed value instead of linear annealing by setting anneal_strategy=""cos"" . Taking care of batch normalization ¶ update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model
on a given dataloader loader at the end of training: >>> torch . optim . swa_utils . update_bn ( loader , swa_model ) update_bn() applies the swa_model to every element in the dataloader and computes the activation
statistics for each batch normalization layer in the model. Warning update_bn() assumes that each batch in the dataloader loader is either a tensors or a list of
tensors where the first element is the tensor that the network swa_model should be applied to.
If your dataloader has a different structure, you can update the batch normalization statistics of the swa_model by doing a forward pass with the swa_model on each element of the dataset. Putting it all together: SWA ¶ In the example below, swa_model is the SWA model that accumulates the averages of the weights.
We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule
and start to collect SWA averages of the parameters at epoch 160: >>> loader , optimizer , model , loss_fn = ... >>> swa_model = torch . optim . swa_utils . AveragedModel ( model ) >>> scheduler = torch . optim . lr_scheduler . CosineAnnealingLR ( optimizer , T_max = 300 ) >>> swa_start = 160 >>> swa_scheduler = SWALR ( optimizer , swa_lr = 0.05 ) >>> >>> for epoch in range ( 300 ): >>> for input , target in loader : >>> optimizer . zero_grad () >>> loss_fn ( model ( input ), target ) . backward () >>> optimizer . step () >>> if epoch > swa_start : >>> swa_model . update_parameters ( model ) >>> swa_scheduler . step () >>> else : >>> scheduler . step () >>> >>> # Update bn statistics for the swa_model at the end >>> torch . optim . swa_utils . update_bn ( loader , swa_model ) >>> # Use swa_model to make predictions on test data >>> preds = swa_model ( test_input ) Putting it all together: EMA ¶ In the example below, ema_model is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999.
We train the model for a total of 300 epochs and start to collect EMA averages immediately. >>> loader , optimizer , model , loss_fn = ... >>> ema_model = torch . optim . swa_utils . AveragedModel ( model , \ >>> multi_avg_fn = torch . optim . swa_utils . get_ema_multi_avg_fn ( 0.999 )) >>> >>> for epoch in range ( 300 ): >>> for input , target in loader : >>> optimizer . zero_grad () >>> loss_fn ( model ( input ), target ) . backward () >>> optimizer . step () >>> ema_model . update_parameters ( model ) >>> >>> # Update bn statistics for the ema_model at the end >>> torch . optim . swa_utils . update_bn ( loader , ema_model ) >>> # Use ema_model to make predictions on test data >>> preds = ema_model ( test_input ) swa_utils.AveragedModel Implements averaged model for Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA). swa_utils.SWALR Anneals the learning rate in each parameter group to a fixed value. torch.optim.swa_utils. get_ema_multi_avg_fn ( decay = 0.999 ) [source] [source] ¶ Get the function applying exponential moving average (EMA) across multiple params. torch.optim.swa_utils. update_bn ( loader , model , device = None ) [source] [source] ¶ Update BatchNorm running_mean, running_var buffers in the model. It performs one pass over data in loader to estimate the activation
statistics for BatchNorm layers in the model. Parameters loader ( torch.utils.data.DataLoader ) – dataset loader to compute the
activation statistics on. Each data batch should be either a
tensor, or a list/tuple whose first element is a tensor
containing data. model ( torch.nn.Module ) – model for which we seek to update BatchNorm
statistics. device ( torch.device , optional ) – If set, data will be transferred to device before being passed into model . Example >>> loader , model = ... >>> torch . optim . swa_utils . update_bn ( loader , model ) Note The update_bn utility assumes that each data batch in loader is either a tensor or a list or tuple of tensors; in the latter case it
is assumed that model.forward() should be called on the first
element of the list or tuple corresponding to the data batch.",29170,24,21,"optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)
---
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([('layer0', var1), ('layer1', var2)], lr=0.0001)
---
optim.SGD([
                {'params': model.base.parameters(), 'lr': 1e-2},
                {'params': model.classifier.parameters()}
            ], lr=1e-3, momentum=0.9)

optim.SGD([
                {'params': model.base.named_parameters(), 'lr': 1e-2},
                {'params': model.classifier.named_parameters()}
            ], lr=1e-3, momentum=0.9)
---
bias_params = [p for name, p in self.named_parameters() if 'bias' in name]
others = [p for name, p in self.named_parameters() if 'bias' not in name]

optim.SGD([
                {'params': others},
                {'params': bias_params, 'weight_decay': 0}
            ], weight_decay=1e-2, lr=1e-2)
---
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
---
for input, target in dataset:
    def closure():
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        return loss
    optimizer.step(closure)
---
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()
---
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler1 = ExponentialLR(optimizer, gamma=0.9)
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler1.step()
    scheduler2.step()
---
>>> scheduler = ...
>>> for epoch in range(100):
>>>     train(...)
>>>     validate(...)
>>>     scheduler.step()
---
class OneLayerModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(3, 4)

    def forward(self, x):
        return self.fc(x)

model = OneLayerModel()
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
# training..
torch.save(optimizer.state_dict(), PATH)
---
class TwoLayerModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(3, 4)
        self.fc2 = nn.Linear(3, 4)

    def forward(self, x):
        return (self.fc1(x) + self.fc2(x)) / 2

model2 = TwoLayerModel()
# adapt and load model weights..
optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)
---
def adapt_state_dict_ids(optimizer, state_dict):
    adapted_state_dict = deepcopy(optimizer.state_dict())
    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
    for k, v in state_dict['param_groups'][0].items():
        if k not in ['params', 'param_names']:
            adapted_state_dict['param_groups'][0][k] = v

    lookup_dict = {
        'fc1.weight': 'fc.weight',
        'fc1.bias': 'fc.bias',
        'fc2.weight': 'fc.weight',
        'fc2.bias': 'fc.bias'
    }
    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
    for param_id, param_name in zip(
            optimizer.state_dict()['param_groups'][0]['params'],
            optimizer.state_dict()['param_groups'][0]['param_names']):
        name_in_loaded = lookup_dict[param_name]
        index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
        id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
        # Copy the state of the corresponding parameter
        if id_in_loaded in state_dict['state']:
            adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict

optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict
---
class Model1(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(5, 5)

    def forward(self, x):
        return self.fc(x) + x


model = Model1()
optimizer = optim.SGD(model.named_parameters(), lr=0.01, momentum=0.9)
# training..
torch.save(optimizer.state_dict(), PATH)

class Model_bypass(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(5, 5)
        self.bypass = nn.Linear(5, 5, bias=False)
        torch.nn.init.eye_(self.bypass.weight)

    def forward(self, x):
        return self.fc(x) + self.bypass(x)

model2 = Model_bypass()
optimizer2 = optim.SGD(model2.named_parameters(), lr=0.01, momentum=0.9)

def adapt_state_dict_missing_param(optimizer, state_dict):
    adapted_state_dict = deepcopy(optimizer.state_dict())
    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.
    for k, v in state_dict['param_groups'][0].items():
        if k not in ['params', 'param_names']:
            adapted_state_dict['param_groups'][0][k] = v

    lookup_dict = {
        'fc.weight': 'fc.weight',
        'fc.bias': 'fc.bias',
        'bypass.weight': None,
    }

    clone_deepcopy = lambda d: {k: (v.clone() if isinstance(v, torch.Tensor) else deepcopy(v)) for k, v in d.items()}
    for param_id, param_name in zip(
            optimizer.state_dict()['param_groups'][0]['params'],
            optimizer.state_dict()['param_groups'][0]['param_names']):
        name_in_loaded = lookup_dict[param_name]
        if name_in_loaded in state_dict['param_groups'][0]['param_names']:
            index_in_loaded_list = state_dict['param_groups'][0]['param_names'].index(name_in_loaded)
            id_in_loaded = state_dict['param_groups'][0]['params'][index_in_loaded_list]
            # Copy the state of the corresponding parameter
            if id_in_loaded in state_dict['state']:
                adapted_state_dict['state'][param_id] = clone_deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict

optimizer2.register_load_state_dict_pre_hook(adapt_state_dict_ids)
optimizer2.load_state_dict(torch.load(PATH)) # The previous optimizer saved state_dict
---
def names_matching(optimizer, state_dict):
    assert len(state_dict['param_groups']) == len(optimizer.state_dict()['param_groups'])
    adapted_state_dict = deepcopy(optimizer.state_dict())
    for g_ind in range(len(state_dict['param_groups'])):
        assert len(state_dict['param_groups'][g_ind]['params']) == len(
            optimizer.state_dict()['param_groups'][g_ind]['params'])

        for k, v in state_dict['param_groups'][g_ind].items():
            if k not in ['params', 'param_names']:
                adapted_state_dict['param_groups'][g_ind][k] = v

        for param_id, param_name in zip(
                optimizer.state_dict()['param_groups'][g_ind]['params'],
                optimizer.state_dict()['param_groups'][g_ind]['param_names']):
            index_in_loaded_list = state_dict['param_groups'][g_ind]['param_names'].index(param_name)
            id_in_loaded = state_dict['param_groups'][g_ind]['params'][index_in_loaded_list]
            # Copy the state of the corresponding parameter
            if id_in_loaded in state_dict['state']:
                adapted_state_dict['state'][param_id] = deepcopy(state_dict['state'][id_in_loaded])

    return adapted_state_dict
---
>>> averaged_model = AveragedModel(model)
---
>>> decay = 0.999
>>> averaged_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(decay))
---
>>> averaged_model.update_parameters(model)
---
>>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\
>>>         0.9 * averaged_model_parameter + 0.1 * model_parameter
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)
---
>>> ema_model = AveragedModel(model, multi_avg_fn=get_ema_multi_avg_fn(0.9))
---
>>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \
>>>         anneal_strategy=""linear"", anneal_epochs=5, swa_lr=0.05)
---
>>> torch.optim.swa_utils.update_bn(loader, swa_model)
---
>>> loader, optimizer, model, loss_fn = ...
>>> swa_model = torch.optim.swa_utils.AveragedModel(model)
>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)
>>> swa_start = 160
>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)
>>>
>>> for epoch in range(300):
>>>       for input, target in loader:
>>>           optimizer.zero_grad()
>>>           loss_fn(model(input), target).backward()
>>>           optimizer.step()
>>>       if epoch > swa_start:
>>>           swa_model.update_parameters(model)
>>>           swa_scheduler.step()
>>>       else:
>>>           scheduler.step()
>>>
>>> # Update bn statistics for the swa_model at the end
>>> torch.optim.swa_utils.update_bn(loader, swa_model)
>>> # Use swa_model to make predictions on test data
>>> preds = swa_model(test_input)
---
>>> loader, optimizer, model, loss_fn = ...
>>> ema_model = torch.optim.swa_utils.AveragedModel(model, \
>>>             multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(0.999))
>>>
>>> for epoch in range(300):
>>>       for input, target in loader:
>>>           optimizer.zero_grad()
>>>           loss_fn(model(input), target).backward()
>>>           optimizer.step()
>>>           ema_model.update_parameters(model)
>>>
>>> # Update bn statistics for the ema_model at the end
>>> torch.optim.swa_utils.update_bn(loader, ema_model)
>>> # Use ema_model to make predictions on test data
>>> preds = ema_model(test_input)
---
>>> loader, model = ...
>>> torch.optim.swa_utils.update_bn(loader, model)",1752175618.1272886
https://pytorch.org/docs/stable/notes/windows.html,Windows FAQ — PyTorch 2.7 documentation,"Windows FAQ ¶ Building from source ¶ Include optional components ¶ There are two supported components for Windows PyTorch:
MKL and MAGMA. Here are the steps to build with them. REM Make sure you have 7z and curl installed. REM Download MKL files curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O
7z x -aoa mkl_2020.2.254.7z -omkl REM Download MAGMA files REM version available: REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release) REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release) REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release) REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release) set CUDA_PREFIX = cuda102 set CONFIG = release
curl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_ %CUDA_PREFIX% _ %CONFIG% .7z -o magma.7z
7z x -aoa magma.7z -omagma REM Setting essential environment variables set ""CMAKE_INCLUDE_PATH= %cd% \mkl\include"" set ""LIB= %cd% \mkl\lib; %LIB% "" set ""MAGMA_HOME= %cd% \magma"" Speeding CUDA build for Windows ¶ Visual Studio doesn’t support parallel custom task currently.
As an alternative, we can use Ninja to parallelize CUDA
build tasks. It can be used by typing only a few lines of code. REM Let's install ninja first. pip install ninja REM Set it as the cmake generator set CMAKE_GENERATOR = Ninja One key install script ¶ You can take a look at this set of scripts .
It will lead the way for you. Extension ¶ CFFI Extension ¶ The support for CFFI Extension is very experimental. You must specify
additional libraries in Extension object to make it build on
Windows. ffi = create_extension ( '_ext.my_lib' , headers = headers , sources = sources , define_macros = defines , relative_to = __file__ , with_cuda = with_cuda , extra_compile_args = [ ""-std=c99"" ], libraries = [ 'ATen' , '_C' ] # Append cuda libraries when necessary, like cudart ) Cpp Extension ¶ This type of extension has better support compared with
the previous one. However, it still needs some manual
configuration. First, you should open the x86_x64 Cross Tools Command Prompt for VS 2017 .
And then, you can start your compiling process. Installation ¶ Package not found in win-32 channel. ¶ Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

- pytorch

Current channels:
- https://conda.anaconda.org/pytorch/win-32
- https://conda.anaconda.org/pytorch/noarch
- https://repo.continuum.io/pkgs/main/win-32
- https://repo.continuum.io/pkgs/main/noarch
- https://repo.continuum.io/pkgs/free/win-32
- https://repo.continuum.io/pkgs/free/noarch
- https://repo.continuum.io/pkgs/r/win-32
- https://repo.continuum.io/pkgs/r/noarch
- https://repo.continuum.io/pkgs/pro/win-32
- https://repo.continuum.io/pkgs/pro/noarch
- https://repo.continuum.io/pkgs/msys2/win-32
- https://repo.continuum.io/pkgs/msys2/noarch PyTorch doesn’t work on 32-bit system. Please use Windows and
Python 64-bit version. Import error ¶ from torch._C import * ImportError : DLL load failed : The specified module could not be found . The problem is caused by the missing of the essential files. Actually,
we include almost all the essential files that PyTorch need for the conda
package except VC2017 redistributable and some mkl libraries.
You can resolve this by typing the following command. conda install -c peterjc123 vc vs2017_runtime
conda install mkl_fft intel_openmp numpy mkl As for the wheels package, since we didn’t pack some libraries and VS2017
redistributable files in, please make sure you install them manually.
The VS 2017 redistributable installer can be downloaded.
And you should also pay attention to your installation of Numpy. Make sure it
uses MKL instead of OpenBLAS. You may type in the following command. pip install numpy mkl intel-openmp mkl_fft Another possible cause may be you are using GPU version without NVIDIA
graphics cards. Please replace your GPU package with the CPU one. from torch._C import * ImportError : DLL load failed : The operating system cannot run % 1. This is actually an upstream issue of Anaconda. When you initialize your
environment with conda-forge channel, this issue will emerge. You may fix
the intel-openmp libraries through this command. conda install -c defaults intel-openmp -f Usage (multiprocessing) ¶ Multiprocessing error without if-clause protection ¶ RuntimeError : An attempt has been made to start a new process before the current process has finished its bootstrapping phase . This probably means that you are not using fork to start your child processes and you have forgotten to use the proper idiom in the main module : if __name__ == '__main__' : freeze_support () ... The ""freeze_support()"" line can be omitted if the program is not going to be frozen to produce an executable . The implementation of multiprocessing is different on Windows, which
uses spawn instead of fork . So we have to wrap the code with an
if-clause to protect the code from executing multiple times. Refactor
your code into the following structure. import torch def main () for i , data in enumerate ( dataloader ): # do something here if __name__ == '__main__' : main () Multiprocessing error “Broken pipe” ¶ ForkingPickler ( file , protocol ) . dump ( obj ) BrokenPipeError : [ Errno 32 ] Broken pipe This issue happens when the child process ends before the parent process
finishes sending data. There may be something wrong with your code. You
can debug your code by reducing the num_worker of DataLoader to zero and see if the issue persists. Multiprocessing error “driver shut down” ¶ Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\lib\TH\THAllocator.c:154

[windows] driver shut down Please update your graphics driver. If this persists, this may be that your
graphics card is too old or the calculation is too heavy for your card. Please
update the TDR settings according to this post . CUDA IPC operations ¶ THCudaCheck FAIL file = torch \ csrc \ generic \ StorageSharing . cpp line = 252 error = 63 : OS call failed or operation not supported on this OS They are not supported on Windows. Something like doing multiprocessing on CUDA
tensors cannot succeed, there are two alternatives for this. 1. Don’t use multiprocessing . Set the num_worker of DataLoader to zero. 2. Share CPU tensors instead. Make sure your custom DataSet returns CPU tensors.",6322,14,19,"REM Make sure you have 7z and curl installed.

REM Download MKL files
curl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O
7z x -aoa mkl_2020.2.254.7z -omkl

REM Download MAGMA files
REM version available:
REM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)
REM 2.5.3 (CUDA 10.1 10.2 11.0) x (Debug Release)
REM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
REM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)
set CUDA_PREFIX=cuda102
set CONFIG=release
curl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4_%CUDA_PREFIX%_%CONFIG%.7z -o magma.7z
7z x -aoa magma.7z -omagma

REM Setting essential environment variables
set ""CMAKE_INCLUDE_PATH=%cd%\mkl\include""
set ""LIB=%cd%\mkl\lib;%LIB%""
set ""MAGMA_HOME=%cd%\magma""
---
REM Let's install ninja first.
pip install ninja

REM Set it as the cmake generator
set CMAKE_GENERATOR=Ninja
---
ffi = create_extension(
    '_ext.my_lib',
    headers=headers,
    sources=sources,
    define_macros=defines,
    relative_to=__file__,
    with_cuda=with_cuda,
    extra_compile_args=[""-std=c99""],
    libraries=['ATen', '_C'] # Append cuda libraries when necessary, like cudart
)
---
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

- pytorch

Current channels:
- https://conda.anaconda.org/pytorch/win-32
- https://conda.anaconda.org/pytorch/noarch
- https://repo.continuum.io/pkgs/main/win-32
- https://repo.continuum.io/pkgs/main/noarch
- https://repo.continuum.io/pkgs/free/win-32
- https://repo.continuum.io/pkgs/free/noarch
- https://repo.continuum.io/pkgs/r/win-32
- https://repo.continuum.io/pkgs/r/noarch
- https://repo.continuum.io/pkgs/pro/win-32
- https://repo.continuum.io/pkgs/pro/noarch
- https://repo.continuum.io/pkgs/msys2/win-32
- https://repo.continuum.io/pkgs/msys2/noarch
---
from torch._C import *

ImportError: DLL load failed: The specified module could not be found.
---
conda install -c peterjc123 vc vs2017_runtime
conda install mkl_fft intel_openmp numpy mkl
---
pip install numpy mkl intel-openmp mkl_fft
---
from torch._C import *

ImportError: DLL load failed: The operating system cannot run %1.
---
conda install -c defaults intel-openmp -f
---
RuntimeError:
       An attempt has been made to start a new process before the
       current process has finished its bootstrapping phase.

   This probably means that you are not using fork to start your
   child processes and you have forgotten to use the proper idiom
   in the main module:

       if __name__ == '__main__':
           freeze_support()
           ...

   The ""freeze_support()"" line can be omitted if the program
   is not going to be frozen to produce an executable.
---
import torch

def main()
    for i, data in enumerate(dataloader):
        # do something here

if __name__ == '__main__':
    main()
---
ForkingPickler(file, protocol).dump(obj)

BrokenPipeError: [Errno 32] Broken pipe
---
Couldn’t open shared file mapping: <torch_14808_1591070686>, error code: <1455> at torch\lib\TH\THAllocator.c:154

[windows] driver shut down
---
THCudaCheck FAIL file=torch\csrc\generic\StorageSharing.cpp line=252 error=63 : OS call failed or operation not supported on this OS",1752175619.186152
