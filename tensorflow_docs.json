{
  "https://www.tensorflow.org/tfx/tutorials": "These tutorials will get you started, and help you learn a few different ways of working with TFX for production workflows and deployments. In particular, you'll learn the two main styles of developing a TFX pipeline:\n  * Using the `InteractiveContext` to develop a pipeline in a notebook, working with one component at a time. This style makes development easier and more Pythonic.\n  * Defining an entire pipeline and executing it with a runner. This is what your pipelines will look like when you deploy them.\n\n\n### [1. Starter Pipeline ](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple) Probably the simplest pipeline you can build, to help you get started. Click the _Run in Google Colab_ button. \n### [2. Adding Data Validation ](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv) Building on the simple pipeline to add data validation components. \n### [3. Adding Feature Engineering ](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft) Building on the data validation pipeline to add a feature engineering component. \n### [4. Adding Model Analysis ](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma) Building on the simple pipeline to add a model analysis component. \n### [Running on Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple) Running pipelines on a managed pipeline service, Vertex Pipelines. \n### [Read data from BigQuery ](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_bq) Using BigQuery as a data source of ML pipelines. \n### [Vertex AI Training and Serving](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training) Using cloud resources for ML training and serving with Vertex AI. \n### [TFX on Cloud AI Platform Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines) An introduction to using TFX and Cloud AI Platform Pipelines. \nOnce you have a basic understanding of TFX, check these additional tutorials and guides. And don't forget to read the [TFX User Guide](https://www.tensorflow.org/tfx/guide). \n### [Complete Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras) A component-by-component introduction to TFX, including the _interactive context_ , a very useful development tool. Click the _Run in Google Colab_ button. \n### [Custom Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component) A tutorial showing how to develop your own custom TFX components. \n### [Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic) This Google Colab notebook demonstrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize a dataset, including generating descriptive statistics, inferring a schema, and finding anomalies. \n### [Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic) This Google Colab notebook demonstrates how TensorFlow Model Analysis (TFMA) can be used to investigate and visualize the characteristics of a dataset and evaluate the performance of a model along several axes of accuracy. \n### [Serve a Model](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple) This tutorial demonstrates how TensorFlow Serving can be used to serve a model using a simple REST API. \nTF Dev Summit 2020 \nTF World 2019 \nGOTO Copenhagen 2019 \n",
  "https://www.tensorflow.org/tfx/guide": "## Introduction\nTFX is a Google-production-scale machine learning (ML) platform based on TensorFlow. It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.\n## TFX 1.0\nWe are happy to announce the availability of the \n## Installation\n```\npip\n```\n\n### Nightly Packages\nTFX also hosts nightly packages at <https://pypi-nightly.tensorflow.org> on Google Cloud. To install the latest nightly package, please use the following command:\n```\npipinstall--extra-index-urlhttps://pypi-nightly.tensorflow.org/simple --pre tfx\n\n```\n\nThis will install the nightly packages for the major dependencies of TFX such as TensorFlow Model Analysis (TFMA), TensorFlow Data Validation (TFDV), TensorFlow Transform (TFT), TFX Basic Shared Libraries (TFX-BSL), ML Metadata (MLMD).\n## About TFX\nTFX is a platform for building and managing ML workflows in a production environment. TFX provides the following:\n  * A toolkit for building ML pipelines. TFX pipelines let you orchestrate your ML workflow on several platforms, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines.\n[Learn more about TFX pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines).\n  * A set of standard components that you can use as a part of a pipeline, or as a part of your ML training script. TFX standard components provide proven functionality to help you get started building an ML process easily.\n[Learn more about TFX standard components](https://www.tensorflow.org/tfx/guide#tfx_standard_components).\n  * Libraries which provide the base functionality for many of the standard components. You can use the TFX libraries to add this functionality to your own custom components, or use them separately.\n[Learn more about the TFX libraries](https://www.tensorflow.org/tfx/guide#tfx_libraries).\n\n\nTFX is a Google-production-scale machine learning toolkit based on TensorFlow. It provides a configuration framework and shared libraries to integrate common components needed to define, launch, and monitor your machine learning system.\n## TFX Standard Components\nA TFX pipeline is a sequence of components that implement an \nA TFX pipeline typically includes the following components:\n  * [**ExampleGen**](https://www.tensorflow.org/tfx/guide/examplegen) is the initial input component of a pipeline that ingests and optionally splits the input dataset.\n  * [**StatisticsGen**](https://www.tensorflow.org/tfx/guide/statsgen) calculates statistics for the dataset.\n  * [**SchemaGen**](https://www.tensorflow.org/tfx/guide/schemagen) examines the statistics and creates a data schema.\n  * [**ExampleValidator**](https://www.tensorflow.org/tfx/guide/exampleval) looks for anomalies and missing values in the dataset.\n  * [**Transform**](https://www.tensorflow.org/tfx/guide/transform) performs feature engineering on the dataset.\n  * [**Trainer**](https://www.tensorflow.org/tfx/guide/trainer) trains the model.\n  * [**Tuner**](https://www.tensorflow.org/tfx/guide/tuner) tunes the hyperparameters of the model.\n  * [**Evaluator**](https://www.tensorflow.org/tfx/guide/evaluator) performs deep analysis of the training results and helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.\n  * [**InfraValidator**](https://www.tensorflow.org/tfx/guide/infra_validator) checks the model is actually servable from the infrastructure, and prevents bad model from being pushed.\n  * [**Pusher**](https://www.tensorflow.org/tfx/guide/pusher) deploys the model on a serving infrastructure.\n  * [**BulkInferrer**](https://www.tensorflow.org/tfx/guide/bulkinferrer) performs batch processing on a model with unlabelled inference requests.\n\n\nThis diagram illustrates the flow of data between these components:\n## TFX Libraries\nTFX includes both libraries and pipeline components. This diagram illustrates the relationships between TFX libraries and pipeline components:\nTFX provides several Python packages that are the libraries which are used to create pipeline components. You'll use these libraries to create the components of your pipelines so that your code can focus on the unique aspects of your pipeline.\nTFX libraries include:\n  * [**TensorFlow Data Validation (TFDV)**](https://www.tensorflow.org/tfx/guide/tfdv) is a library for analyzing and validating machine learning data. It is designed to be highly scalable and to work well with TensorFlow and TFX. TFDV includes:\n    * Scalable calculation of summary statistics of training and test data.\n    * Integration with a viewer for data distributions and statistics, as well as faceted comparison of pairs of datasets (Facets).\n    * Automated data-schema generation to describe expectations about data like required values, ranges, and vocabularies.\n    * A schema viewer to help you inspect the schema.\n    * Anomaly detection to identify anomalies, such as missing features, out-of- range values, or wrong feature types, to name a few.\n    * An anomalies viewer so that you can see what features have anomalies and learn more in order to correct them.\n  * [**TensorFlow Transform (TFT)**](https://www.tensorflow.org/tfx/guide/tft) is a library for preprocessing data with TensorFlow. TensorFlow Transform is useful for data that requires a full- pass, such as:\n    * Normalize an input value by mean and standard deviation.\n    * Convert strings to integers by generating a vocabulary over all input values.\n    * Convert floats to integers by assigning them to buckets based on the observed data distribution.\n  * [**TensorFlow**](https://www.tensorflow.org/tfx/guide/train) is used for training models with TFX. It ingests training data and modeling code and creates a SavedModel result. It also integrates a feature engineering pipeline created by TensorFlow Transform for preprocessing input data.\n[KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) is used for tuning hyperparameters for model.\n  * [**TensorFlow Model Analysis (TFMA)**](https://www.tensorflow.org/tfx/guide/tfma) is a library for evaluating TensorFlow models. It is used along with TensorFlow to create an EvalSavedModel, which becomes the basis for its analysis. It allows users to evaluate their models on large amounts of data in a distributed manner, using the same metrics defined in their trainer. These metrics can be computed over different slices of data and visualized in Jupyter notebooks.\n  *     * A schema describing tabular data (e.g., tf.Examples).\n    * A collection of summary statistics over such datasets.\n  * [**ML Metadata (MLMD)**](https://www.tensorflow.org/tfx/guide/mlmd) is a library for recording and retrieving metadata associated with ML developer and data scientist workflows. Most often the metadata uses TFMD representations. MLMD manages persistence using \n\n\n### Supporting Technologies\n#### Required\n  * [**Apache Beam**](https://www.tensorflow.org/tfx/guide/beam) is an open source, unified model for defining both batch and streaming data-parallel processing pipelines. TFX uses Apache Beam to implement data-parallel pipelines. The pipeline is then executed by one of Beam's supported distributed processing back-ends, which include Apache Flink, Apache Spark, \n\n\n#### Optional\nOrchestrators such as Apache Airflow and Kubeflow make configuring, operating, monitoring, and maintaining an ML pipeline easier.\n### Portability and Interoperability\nTFX is designed to be portable to multiple environments and orchestration frameworks, including [Apache Airflow](https://www.tensorflow.org/tfx/guide/airflow), [Apache Beam](https://www.tensorflow.org/tfx/guide/beam_orchestrator) and [Kubeflow](https://www.tensorflow.org/tfx/guide/kubeflow) . It is also portable to different computing platforms, including on-premise, and cloud platforms such as the \n### Model vs. SavedModel\n#### Model\nA model is the output of the training process. It is the serialized record of the weights that have been learned during the training process. These weights can be subsequently used to compute predictions for new input examples. For TFX and TensorFlow, 'model' refers to the checkpoints containing the weights learned up to that point.\nNote that 'model' might also refer to the definition of the TensorFlow computation graph (i.e. a Python file) that expresses how a prediction will be computed. The two senses may be used interchangeably based on context.\n#### SavedModel\n  * **What is a[SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model)**: a universal, language-neutral, hermetic, recoverable serialization of a TensorFlow model.\n  * **Why is it important** : It enables higher-level systems to produce, transform, and consume TensorFlow models using a single abstraction.\n\n\nSavedModel is the recommended serialization format for serving a TensorFlow model in production, or exporting a trained model for a native mobile or JavaScript application. For example, to turn a model into a REST service for making predictions, you can serialize the model as a SavedModel and serve it using TensorFlow Serving. See [Serving a TensorFlow Model](https://www.tensorflow.org/tfx/serving/serving_basic) for more information.\n### Schema\nSome TFX components use a description of your input data called a _schema_. The schema is an instance of \nHere's an excerpt from a schema protobuf:\n```\n...\nfeature{\nname:\"age\"\nvalue_count{\nmin:1\nmax:1\n}\ntype:FLOAT\npresence{\nmin_fraction:1\nmin_count:1\n}\n}\nfeature{\nname:\"capital-gain\"\nvalue_count{\nmin:1\nmax:1\n}\ntype:FLOAT\npresence{\nmin_fraction:1\nmin_count:1\n}\n}\n...\n\n```\n\nThe following components use the schema:\n  * TensorFlow Data Validation\n  * TensorFlow Transform\n\n\nIn a typical TFX pipeline TensorFlow Data Validation generates a schema, which is consumed by the other components.\n## Developing with TFX\nTFX provides a powerful platform for every phase of a machine learning project, from research, experimentation, and development on your local machine, through deployment. In order to avoid code duplication and eliminate the potential for [training/serving skew](https://www.tensorflow.org/tfx/guide/tfdv#training-serving_skew_detection) it is strongly recommended to implement your TFX pipeline for both model training and deployment of trained models, and use [Transform](https://www.tensorflow.org/tfx/guide/transform) components which leverage the [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) library for both training and inference. By doing so you will use the same preprocessing and analysis code consistently, and avoid differences between data used for training and data fed to your trained models in production, as well as benefitting from writing that code once.\n### Data Exploration, Visualization, and Cleaning\nTFX pipelines typically begin with an [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component, which accepts input data and formats it as tf.Examples. Often this is done after the data has been split into training and evaluation datasets so that there are actually two copies of ExampleGen components, one each for training and evaluation. This is typically followed by a [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) component and a [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component, which will examine your data and infer a data schema and statistics. The schema and statistics will be consumed by an [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component, which will look for anomalies, missing values, and incorrect data types in your data. All of these components leverage the capabilities of the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) library.\n[TensorFlow Data Validation (TFDV)](https://www.tensorflow.org/tfx/guide/tfdv) is a valuable tool when doing initial exploration, visualization, and cleaning of your dataset. TFDV examines your data and infers the data types, categories, and ranges, and then automatically helps identify anomalies and missing values. It also provides visualization tools that can help you examine and understand your dataset. After your pipeline completes you can read metadata from [MLMD](https://www.tensorflow.org/tfx/guide/mlmd) and use the visualization tools of TFDV in a Jupyter notebook to analyze your data.\nFollowing your initial model training and deployment, TFDV can be used to monitor new data from inference requests to your deployed models, and look for anomalies and/or drift. This is especially useful for time series data that changes over time as a result of trend or seasonality, and can help inform when there are data problems or when models need to be retrained on new data.\n### Data Visualization\nAfter you have completed your first run of your data through the section of your pipeline that uses TFDV (typically StatisticsGen, SchemaGen, and ExampleValidator) you can visualize the results in a Jupyter style notebook. For additional runs you can compare these results as you make adjustments, until your data is optimal for your model and application.\nYou will first query [**ML Metadata (MLMD)**](https://www.tensorflow.org/tfx/guide/mlmd) to locate the results of these executions of these components, and then use the visualization support API in TFDV to create the visualizations in your notebook. This includes [tfdv.load_statistics()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/load_statistics) and [tfdv.visualize_statistics()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics) Using this visualization you can better understand the characteristics of your dataset, and if necessary modify as required.\n### Developing and Training Models\nA typical TFX pipeline will include a [Transform](https://www.tensorflow.org/tfx/guide/transform) component, which will perform feature engineering by leveraging the capabilities of the [TensorFlow Transform (TFT)](https://www.tensorflow.org/tfx/guide/tft) library. A Transform component consumes the schema created by a SchemaGen component, and applies [data transformations](https://www.tensorflow.org/tfx/tutorials/transform/simple) to create, combine, and transform the features that will be used to train your model. Cleanup of missing values and conversion of types should also be done in the Transform component if there is ever a possibility that these will also be present in data sent for inference requests. [There are some important considerations](https://www.tensorflow.org/tfx/guide/train) when designing TensorFlow code for training in TFX.\nThe result of a Transform component is a SavedModel which will be imported and used in your modeling code in TensorFlow, during a [Trainer](https://www.tensorflow.org/tfx/guide/trainer) component. This SavedModel includes all of the data engineering transformations that were created in the Transform component, so that the identical transforms are performed using the exact same code during both training and inference. Using the modeling code, including the SavedModel from the Transform component, you can consume your training and evaluation data and train your model.\nWhen working with Estimator based models, the last section of your modeling code should save your model as both a SavedModel and an EvalSavedModel. Saving as an EvalSavedModel ensures the metrics used at training time are also available during evaluation (note that this is not required for keras based models). Saving an EvalSavedModel requires that you import the [TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/guide/tfma) library in your Trainer component.\n```\nimporttensorflow_model_analysisastfma\n...\n\ntfma.export.export_eval_savedmodel(\n        estimator=estimator,\n        export_dir_base=eval_model_dir,\n        eval_input_receiver_fn=receiver_fn)\n\n```\n\nAn optional [Tuner](https://www.tensorflow.org/tfx/guide/tuner) component can be added before Trainer to tune the hyperparameters (e.g., number of layers) for the model. With the given model and hyperparameters' search space, tuning algorithm will find the best hyperparameters based on the objective.\n### Analyzing and Understanding Model Performance\nFollowing initial model development and training it's important to analyze and really understand your model's performance. A typical TFX pipeline will include an [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) component, which leverages the capabilities of the [TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/guide/tfma) library, which provides a power toolset for this phase of development. An Evaluator component consumes the model that you exported above, and allows you to specify a list of [`tfma.SlicingSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec) that you can use when visualizing and analyzing your model's performance. Each `SlicingSpec` defines a slice of your training data that you want to examine, such as particular categories for categorical features, or particular ranges for numerical features.\nFor example, this would be important for trying to understand your model's performance for different segments of your customers, which could be segmented by annual purchases, geographical data, age group, or gender. This can be especially important for datasets with long tails, where the performance of a dominant group may mask unacceptable performance for important, yet smaller groups. For example, your model may perform well for average employees but fail miserably for executive staff, and it might be important to you to know that.\n### Model Analysis and Visualization\nAfter you have completed your first run of your data through training your model and running the [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) component (which leverages [TFMA](https://www.tensorflow.org/tfx/guide/tfma)) on the training results, you can visualize the results in a Jupyter style notebook. For additional runs you can compare these results as you make adjustments, until your results are optimal for your model and application.\nYou will first query [**ML Metadata (MLMD)**](https://www.tensorflow.org/tfx/guide/mlmd) to locate the results of these executions of these components, and then use the visualization support API in TFMA to create the visualizations in your notebook. This includes [tfma.load_eval_results](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_eval_results) and [tfma.view.render_slicing_metrics](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_slicing_metrics) Using this visualization you can better understand the characteristics of your model, and if necessary modify as required.\n### Validating Model Performance\nAs part of analyzing a model's performance you might want to validate the performance against a baseline (such as the currently serving model). Model validation is performed by passing both a candidate and baseline model to the [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) component. The Evaluator computes metrics (e.g. AUC, loss) for both the candidate and baseline along with a corresponding set of diff metrics. Thresholds may then be applied and used to gate pushing your models to production.\n### Validating That A Model Can Be Served\nBefore deploying the trained model, you might want to validate whether the model is really servable in the serving infrastructure. This is especially important in production environments to ensure that the newly published model does not prevent the system from serving predictions. The [InfraValidator](https://www.tensorflow.org/tfx/guide/infra_validator) component will make a canary deployment of your model in a sandboxed environment, and optionally send real requests to check that your model works correctly.\n## Deployment Targets\nOnce you have developed and trained a model that you're happy with, it's now time to deploy it to one or more deployment target(s) where it will receive inference requests. TFX supports deployment to three classes of deployment targets. Trained models which have been exported as SavedModels can be deployed to any or all of these deployment targets.\n### Inference: TensorFlow Serving\n[TensorFlow Serving (TFS)](https://www.tensorflow.org/tfx/guide/serving) is a flexible, high-performance serving system for machine learning models, designed for production environments. It consumes a SavedModel and will accept inference requests over either REST or gRPC interfaces. It runs as a set of processes on one or more network servers, using one of several advanced architectures to handle synchronization and distributed computation. See the [TFS documentation](https://www.tensorflow.org/tfx/guide/serving) for more information on developing and deploying TFS solutions.\nIn a typical pipeline, a SavedModel which has been trained in a [Trainer](https://www.tensorflow.org/tfx/guide/trainer) component would first be infra-validated in an [InfraValidator](https://www.tensorflow.org/tfx/guide/infra_validator) component. InfraValidator launches a canary TFS model server to actually serve the SavedModel. If validation has passed, a [Pusher](https://www.tensorflow.org/tfx/guide/pusher) component will finally deploy the SavedModel to your TFS infrastructure. This includes handling multiple versions and model updates.\n### Inference in Native Mobile and IoT Applications: TensorFlow Lite\n[TensorFlow Lite](https://www.tensorflow.org/lite) is a suite of tools which is dedicated to help developers use their trained TensorFlow Models in native mobile and IoT applications. It consumes the same SavedModels as TensorFlow Serving, and applies optimizations such as quantization and pruning to optimize the size and performance of the resulting models for the challenges of running on mobile and IoT devices. See the TensorFlow Lite documentation for more information on using TensorFlow Lite.\n### Inference in JavaScript: TensorFlow JS\n[TensorFlow JS](https://js.tensorflow.org/) is a JavaScript library for training and deploying ML models in the browser and on Node.js. It consumes the same SavedModels as TensorFlow Serving and TensorFlow Lite, and converts them to the TensorFlow.js Web format. See the TensorFlow JS documentation for more details on using TensorFlow JS.\n## Creating a TFX Pipeline With Airflow\nCheck [airflow workshop](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop/) for details\n## Creating a TFX Pipeline With Kubeflow\n### Setup\nKubeflow requires a Kubernetes cluster to run the pipelines at scale. See the Kubeflow deployment guideline that guide through the options for \n### Configure and run TFX pipeline\nPlease follow the [TFX on Cloud AI Platform Pipeline tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines) to run the TFX example pipeline on Kubeflow. TFX components have been containerized to compose the Kubeflow pipeline and the sample illustrates the ability to configure the pipeline to read large public dataset and execute training and data processing steps at scale in the cloud.\n## Command line interface for pipeline actions\nTFX provides a unified CLI which helps the perform full range of pipeline actions such as create, update, run, list, and delete pipelines on various orchestrators including Apache Airflow, Apache Beam, and Kubeflow. For details, please follow \n",
  "https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic": "**_An Example of a Key Component of TensorFlow Extended_**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)  \n---  \nThis example colab notebook illustrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset. That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset. It's important to understand your dataset's characteristics, including how it might change over time in your production pipeline. It's also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they're consistent.\nWe'll use data from the \nThe columns in the dataset are: \npickup_community_area | fare | trip_start_month  \n---|---|---  \ntrip_start_hour | trip_start_day | trip_start_timestamp  \npickup_latitude | pickup_longitude | dropoff_latitude  \ndropoff_longitude | trip_miles | pickup_census_tract  \ndropoff_census_tract | payment_type | company  \ntrip_seconds | dropoff_community_area | tips  \n## Install and import packages\nInstall the packages for TensorFlow Data Validation.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install Data Validation packages\nInstall the TensorFlow Data Validation packages and dependencies, which takes a few minutes. You may see warnings and errors regarding incompatible dependency versions, which you will resolve in the next section.\n```\nprint('Installing TensorFlow Data Validation')\n!pip install --upgrade 'tensorflow_data_validation[visualization]<2'\n\n```\n\n### Import TensorFlow and reload updated packages\nThe prior step updates the default packages in the Gooogle Colab environment, so you must reload the package resources to resolve the new dependencies.\n```\nimportpkg_resources\nimportimportlib\nimportlib.reload(pkg_resources)\n\n```\n```\n/tmpfs/tmp/ipykernel_183404/3239164719.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n<module 'pkg_resources' from '/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/pkg_resources/__init__.py'>\n\n```\n\nCheck the versions of TensorFlow and the Data Validation before proceeding. \n```\nimporttensorflowastf\nimporttensorflow_data_validationastfdv\nprint('TF version:', tf.__version__)\nprint('TFDV version:', tfdv.version.__version__)\n\n```\n```\n2024-04-30 10:45:22.158704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:45:22.158751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:45:22.160265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTF version: 2.15.1\nTFDV version: 1.15.1\n\n```\n\n## Load the dataset\nWe will download our dataset from Google Cloud Storage.\n```\nimportos\nimporttempfile,urllib,zipfile\n\n# Set up some globals for our file paths\nBASE_DIR = tempfile.mkdtemp()\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nOUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output')\nTRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\nEVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\nSERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n\n# Download the zip file from GCP and unzip it\nzip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\nzipfile.ZipFile(zip).extractall(BASE_DIR)\nzipfile.ZipFile(zip).close()\n\nprint(\"Here's what we downloaded:\")\n!ls -R {os.path.join(BASE_DIR, 'data')}\n\n```\n```\nHere's what we downloaded:\n/tmpfs/tmp/tmp7em4lo4u/data:\neval  serving  train\n\n/tmpfs/tmp/tmp7em4lo4u/data/eval:\ndata.csv\n\n/tmpfs/tmp/tmp7em4lo4u/data/serving:\ndata.csv\n\n/tmpfs/tmp/tmp7em4lo4u/data/train:\ndata.csv\n\n```\n\n## Compute and visualize statistics\nFirst we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\nTFDV can compute descriptive \nInternally, TFDV uses \n```\ntrain_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\n\n```\n\nNow let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses \n  * Notice that numeric features and catagorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n  * Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features. The percentage is the percentage of examples that have missing or zero values for that feature.\n  * Notice that there are no examples with values for `pickup_census_tract`. This is an opportunity for dimensionality reduction!\n  * Try clicking \"expand\" above the charts to change the display\n  * Try hovering over bars in the charts to display bucket ranges and counts\n  * Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n  * Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages\n\n```\n# docs-infra: no-execute\ntfdv.visualize_statistics(train_stats)\n\n```\n\n## Infer a schema\nNow let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data. A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data. For categorical features the schema also defines the domain - the list of acceptable values. Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\nGetting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct. The schema also provides documentation for the data, and so is useful when different developers work on the same data. Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it.\n```\nschema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n\n```\n\n## Check evaluation data for errors\nSo far we've only been looking at the training data. It's important that our evaluation data is consistent with our training data, including that it uses the same schema. It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training. The same is true for categorical features. Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.\n  * Notice that each feature now includes statistics for both the training and evaluation datasets.\n  * Notice that the charts now have both the training and evaluation datasets overlaid, making it easy to compare them.\n  * Notice that the charts now include a percentages view, which can be combined with log or the default linear scales.\n  * Notice that the mean and median for `trip_miles` are different for the training versus the evaluation datasets. Will that cause problems?\n  * Wow, the max `tips` is very different for the training versus the evaluation datasets. Will that cause problems?\n  * Click expand on the Numeric Features chart, and select the log scale. Review the `trip_seconds` feature, and notice the difference in the max. Will evaluation miss parts of the loss surface?\n\n```\n# Compute stats for evaluation data\neval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)\n\n```\n```\n# docs-infra: no-execute\n# Compare evaluation data with training data\ntfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n\n```\n\n## Check for evaluation anomalies\nDoes our evaluation dataset match the schema from our training dataset? This is especially important for categorical features, where we want to identify the range of acceptable values.\n```\n# Check eval data for errors by validating the eval data stats using the previously inferred schema.\nanomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\ntfdv.display_anomalies(anomalies)\n\n```\n\n## Fix evaluation anomalies in the schema\nOops! It looks like we have some new values for `company` in our evaluation data, that we didn't have in our training data. We also have a new value for `payment_type`. These should be considered anomalies, but what we decide to do about them depends on our domain knowledge of the data. If an anomaly truly indicates a data error, then the underlying data should be fixed. Otherwise, we can simply update the schema to include the values in the eval dataset.\nUnless we change our evaluation dataset we can't fix everything, but we can fix things in the schema that we're comfortable accepting. That includes relaxing our view of what is and what is not an anomaly for particular features, as well as updating our schema to include missing values for categorical features. TFDV has enabled us to discover what we need to fix.\nLet's make those fixes now, and then review one more time.\n```\n# Relax the minimum fraction of values that must come from the domain for feature company.\ncompany = tfdv.get_feature(schema, 'company')\ncompany.distribution_constraints.min_domain_mass = 0.9\n\n# Add new value to the domain of feature payment_type.\npayment_type_domain = tfdv.get_domain(schema, 'payment_type')\npayment_type_domain.value.append('Prcard')\n\n# Validate eval stats after updating the schema \nupdated_anomalies = tfdv.validate_statistics(eval_stats, schema)\ntfdv.display_anomalies(updated_anomalies)\n\n```\n\nHey, look at that! We verified that the training and evaluation data are now consistent! Thanks TFDV ;)\n## Schema Environments\nWe also split off a 'serving' dataset for this example, so we should check that too. By default all datasets in a pipeline should use the same schema, but there are often exceptions. For example, in supervised learning we need to include labels in our dataset, but when we serve the model for inference the labels will not be included. In some cases introducing slight schema variations is necessary.\n**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using `default_environment`, `in_environment` and `not_in_environment`.\nFor example, in this dataset the `tips` feature is included as the label for training, but it's missing in the serving data. Without environment specified, it will show up as an anomaly.\n```\nserving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n\n```\n\nWe'll deal with the `tips` feature below. We also have an INT value in our trip seconds, where our schema expected a FLOAT. By making us aware of that difference, TFDV helps uncover inconsistencies in the way the data is generated for training and serving. It's very easy to be unaware of problems like that until model performance suffers, sometimes catastrophically. It may or may not be a significant issue, but in any case this should be cause for further investigation.\nIn this case, we can safely convert INT values to FLOATs, so we want to tell TFDV to use our schema to infer the type. Let's do that now.\n```\noptions = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\nserving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n\n```\n\nNow we just have the `tips` feature (which is our label) showing up as an anomaly ('Column dropped'). Of course we don't expect to have labels in our serving data, so let's tell TFDV to ignore that.\n```\n# All features are by default in both TRAINING and SERVING environments.\nschema.default_environment.append('TRAINING')\nschema.default_environment.append('SERVING')\n\n# Specify that 'tips' feature is not in SERVING environment.\ntfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n\nserving_anomalies_with_env = tfdv.validate_statistics(\n    serving_stats, schema, environment='SERVING')\n\ntfdv.display_anomalies(serving_anomalies_with_env)\n\n```\n\n## Check for drift and skew\nIn addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew. TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema.\n### Drift\nDrift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of \n### Skew\nTFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew.\n#### Schema Skew\nSchema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n#### Feature Skew\nFeature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:\n  * A data source that provides some feature values is modified between training and serving time\n  * There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code paths.\n\n\n#### Distribution Skew\nDistribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on.\n```\n# Add skew comparator for 'payment_type' feature.\npayment_type = tfdv.get_feature(schema, 'payment_type')\npayment_type.skew_comparator.infinity_norm.threshold = 0.01\n\n# Add drift comparator for 'company' feature.\ncompany=tfdv.get_feature(schema, 'company')\ncompany.drift_comparator.infinity_norm.threshold = 0.001\n\nskew_anomalies = tfdv.validate_statistics(train_stats, schema,\n                                          previous_statistics=eval_stats,\n                                          serving_statistics=serving_stats)\n\ntfdv.display_anomalies(skew_anomalies)\n\n```\n\nIn this example we do see some drift, but it is well below the threshold that we've set.\n## Freeze the schema\nNow that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state.\n```\nfromtensorflow.python.lib.ioimport file_io\nfromgoogle.protobufimport text_format\n\nfile_io.recursive_create_dir(OUTPUT_DIR)\nschema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\ntfdv.write_schema_text(schema, schema_file)\n\n!cat {schema_file}\n\n```\n```\nfeature {\n  name: \"pickup_community_area\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"fare\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_month\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_hour\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_day\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_timestamp\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_latitude\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_longitude\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"dropoff_latitude\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"dropoff_longitude\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"trip_miles\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_census_tract\"\n  type: BYTES\n  presence {\n    min_count: 0\n  }\n}\nfeature {\n  name: \"dropoff_census_tract\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: INT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"payment_type\"\n  type: BYTES\n  domain: \"payment_type\"\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  skew_comparator {\n    infinity_norm {\n      threshold: 0.01\n    }\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"company\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: BYTES\n  domain: \"company\"\n  presence {\n    min_count: 1\n  }\n  distribution_constraints {\n    min_domain_mass: 0.9\n  }\n  drift_comparator {\n    infinity_norm {\n      threshold: 0.001\n    }\n  }\n}\nfeature {\n  name: \"trip_seconds\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"dropoff_community_area\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: INT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"tips\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  not_in_environment: \"SERVING\"\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nstring_domain {\n  name: \"payment_type\"\n  value: \"Cash\"\n  value: \"Credit Card\"\n  value: \"Dispute\"\n  value: \"No Charge\"\n  value: \"Pcard\"\n  value: \"Unknown\"\n  value: \"Prcard\"\n}\nstring_domain {\n  name: \"company\"\n  value: \"0118 - 42111 Godfrey S.Awir\"\n  value: \"0694 - 59280 Chinesco Trans Inc\"\n  value: \"1085 - 72312 N and W Cab Co\"\n  value: \"2733 - 74600 Benny Jona\"\n  value: \"2809 - 95474 C & D Cab Co Inc.\"\n  value: \"3011 - 66308 JBL Cab Inc.\"\n  value: \"3152 - 97284 Crystal Abernathy\"\n  value: \"3201 - C&D Cab Co Inc\"\n  value: \"3201 - CID Cab Co Inc\"\n  value: \"3253 - 91138 Gaither Cab Co.\"\n  value: \"3385 - 23210 Eman Cab\"\n  value: \"3623 - 72222 Arrington Enterprises\"\n  value: \"3897 - Ilie Malec\"\n  value: \"4053 - Adwar H. Nikola\"\n  value: \"4197 - 41842 Royal Star\"\n  value: \"4615 - 83503 Tyrone Henderson\"\n  value: \"4615 - Tyrone Henderson\"\n  value: \"4623 - Jay Kim\"\n  value: \"5006 - 39261 Salifu Bawa\"\n  value: \"5006 - Salifu Bawa\"\n  value: \"5074 - 54002 Ahzmi Inc\"\n  value: \"5074 - Ahzmi Inc\"\n  value: \"5129 - 87128\"\n  value: \"5129 - 98755 Mengisti Taxi\"\n  value: \"5129 - Mengisti Taxi\"\n  value: \"5724 - KYVI Cab Inc\"\n  value: \"585 - Valley Cab Co\"\n  value: \"5864 - 73614 Thomas Owusu\"\n  value: \"5864 - Thomas Owusu\"\n  value: \"5874 - 73628 Sergey Cab Corp.\"\n  value: \"5997 - 65283 AW Services Inc.\"\n  value: \"5997 - AW Services Inc.\"\n  value: \"6488 - 83287 Zuha Taxi\"\n  value: \"6743 - Luhak Corp\"\n  value: \"Blue Ribbon Taxi Association Inc.\"\n  value: \"C & D Cab Co Inc\"\n  value: \"Chicago Elite Cab Corp.\"\n  value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\n  value: \"Chicago Medallion Leasing INC\"\n  value: \"Chicago Medallion Management\"\n  value: \"Choice Taxi Association\"\n  value: \"Dispatch Taxi Affiliation\"\n  value: \"KOAM Taxi Association\"\n  value: \"Northwest Management LLC\"\n  value: \"Taxi Affiliation Services\"\n  value: \"Top Cab Affiliation\"\n}\ndefault_environment: \"TRAINING\"\ndefault_environment: \"SERVING\"\n\n```\n\n## When to use TFDV\nIt's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here's a few more:\n  * Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n  * Validating new data for inference to make sure that our model has trained on that part of the decision surface\n  * Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)) to make sure we haven't done something wrong\n\n\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple)  \n---  \nThis notebook-based tutorial will create a simple TFX pipeline and run it using Google Cloud Vertex Pipelines. This notebook is based on the TFX pipeline we built in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple). If you are not familiar with TFX and you have not read that tutorial yet, you should read it before proceeding with this notebook.\nGoogle Cloud Vertex Pipelines helps you to automate, monitor, and govern your ML systems by orchestrating your ML workflow in a serverless manner. You can define your ML pipelines using Python with TFX, and then execute your pipelines on Google Cloud. See \nThis notebook is intended to be run on \n## Set up\nBefore you run this notebook, ensure that you have following:\n  * A \n  * A \n  * Enable \n\n\nPlease see \n### Install python packages\nWe will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.\n```\n# Use the latest version of pip.\npip\npip\"tfx[kfp]<2\"\n```\n\n#### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nIf you are not on Colab, you can restart runtime with following cell.\n```\n# docs_infra: no_execute\nimportsys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  importIPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n\n```\n\n### Login in to Google for this notebook\nIf you are running this notebook on Colab, authenticate with your user account:\n```\nimportsys\nif 'google.colab' in sys.modules:\n  fromgoogle.colabimport auth\n  auth.authenticate_user()\n\n```\n\n**If you are on AI Platform Notebooks** , authenticate with Google Cloud before running the next section, by running\n```\ngcloud\n```\n\n**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance.\nCheck the package versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimportkfp\nprint('KFP version: {}'.format(kfp.__version__))\n\n```\n```\n2024-05-08 10:08:36.306387: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 10:08:36.306437: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 10:08:36.307973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\nKFP version: 1.8.22\n\n```\n\n### Set up variables\nWe will set up some variables used to customize the pipelines below. Following information is required:\n  * GCP Project id. See \n  * GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the \n  * Google Cloud Storage Bucket to store pipeline outputs.\n\n\n**Enter required values in the cell below before running it**.\n```\nGOOGLE_CLOUD_PROJECT = ''     # <--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''      # <--- ENTER THIS\nGCS_BUCKET_NAME = ''          # <--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    fromabslimport logging\n    logging.error('Please set all required parameters.')\n\n```\n```\nERROR:absl:Please set all required parameters.\n\n```\n\nSet `gcloud` to use your project.\n```\ngcloudset{GOOGLE_CLOUD_PROJECT}\n```\n```\nERROR: (gcloud.config.set) argument VALUE: Must be specified.\nUsage: gcloud config set SECTION/PROPERTY VALUE [optional flags]\n  optional flags may be  --help | --installation\n\nFor detailed information on this command and its flags, run:\n  gcloud config set --help\n\n```\n```\nPIPELINE_NAME = 'penguin-vertex-pipelines'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for input data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# This is the path where your model will be pushed for serving.\nSERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n\n```\n```\nPIPELINE_ROOT: gs:///pipeline_root/penguin-vertex-pipelines\n\n```\n\n### Prepare example data\nWe will use the same [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nThere are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the `species` of penguins.\nWe need to make our own copy of the dataset. Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it on GCS.\n```\ngsutil{DATA_ROOT}/\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///data/penguin-vertex-pipelines/\".\n\n```\n\nTake a quick look at the CSV file.\n```\ngsutil{DATA_ROOT}/penguins_processed.csv|\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///data/penguin-vertex-pipelines/penguins_processed.csv\".\n\n```\n\n## Create a pipeline\nTFX pipelines are defined using Python APIs. We will define a pipeline which consists of three components, CsvExampleGen, Trainer and Pusher. The pipeline and model definition is almost the same as [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nThe only difference is that we don't need to set `metadata_connection_config` which is used to locate [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) database. Because Vertex Pipelines uses a managed metadata service, users don't need to care of it, and we don't need to specify the parameter.\nBefore actually define the pipeline, we need to write a model code for the Trainer component first.\n### Write model code.\nWe will use the same model code as in the [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_transform.tf_metadataimport schema_utils\n\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\n\nfromtensorflow_metadata.proto.v0import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_make_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _make_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\nCopy the module file to GCS which can be accessed from the pipeline components. Because model training happens on GCP, we need to upload this model definition. \nOtherwise, you might want to build a container image including the module file and use the image to run the pipeline.\n```\ngsutil{_trainer_module_file}{MODULE_ROOT}/\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///pipeline_module/penguin-vertex-pipelines/\".\n\n```\n\n### Write a pipeline definition\nWe will define a function to create a TFX pipeline.\n```\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n# slightly modified because we don't need `metadata_path` argument.\n\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     ) -> tfx.dsl.Pipeline:\n\"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  # Following three components will be included in the pipeline.\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components)\n\n```\n\n## Run the pipeline on Vertex Pipelines.\nWe used `LocalDagRunner` which runs on local environment in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple). TFX provides multiple orchestrators to run your pipeline. In this tutorial we will use the Vertex Pipelines together with the Kubeflow V2 dag runner.\nWe need to define a runner to actually run the pipeline. You will compile your pipeline into our pipeline definition format using TFX APIs.\n```\n# docs_infra: no_execute\nimportos\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n# Following function will write the pipeline definition to PIPELINE_DEFINITION_FILE.\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        serving_model_dir=SERVING_MODEL_DIR))\n\n```\n\nThe generated definition file can be submitted using kfp client.\n```\n# docs_infra: no_execute\nfromgoogle.cloudimport aiplatform\nfromgoogle.cloud.aiplatformimport pipeline_jobs\nimportlogging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n\n```\n\nNow you can visit the link in the output above or visit 'Vertex AI > Pipelines' in \n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple": "**_A Short tutorial to run a simple TFX pipeline._**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple)  \n---  \nIn this notebook-based tutorial, we will create and run a TFX pipeline for a simple classification model. The pipeline will consist of three essential TFX components: ExampleGen, Trainer and Pusher. The pipeline includes the most minimal ML workflow like importing data, training a model and exporting the trained model.\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n## Set Up\nWe first need to install the TFX Python package and download the dataset which we will use for our model.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TFX\n```\npip\n```\n\n### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nCheck the TensorFlow and TFX versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n\n```\n```\n2024-05-08 09:40:19.025902: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:40:19.025947: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:40:19.027624: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\n\n```\n\n### Set up variables\nThere are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.\n```\nimportos\n\nPIPELINE_NAME = \"penguin-simple\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfromabslimport logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n\n```\n\n### Prepare example data\nWe will download the example dataset for use in our TFX pipeline. The dataset we are using is \nThere are four numeric features in this dataset:\n  * culmen_length_mm\n  * culmen_depth_mm\n  * flipper_length_mm\n  * body_mass_g\n\n\nAll features were already normalized to have range [0,1]. We will build a classification model which predicts the `species` of penguins.\nBecause TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it.\n```\nimporturllib.request\nimporttempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n\n```\n```\n('/tmpfs/tmp/tfx-dataftg7q8n1/data.csv',\n <http.client.HTTPMessage at 0x7f30e73d94c0>)\n\n```\n\nTake a quick look at the CSV file.\n```\nhead{_data_filepath}\n```\n```\nspecies,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n\n```\n\nYou should be able to see five values. `species` is one of 0, 1 or 2, and all other features should have values between 0 and 1.\n## Create a pipeline\nTFX pipelines are defined using Python APIs. We will define a pipeline which consists of following three components.\n  * CsvExampleGen: Reads in data files and convert them to TFX internal format for further processing. There are multiple [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)s for various formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n  * Trainer: Trains an ML model. [Trainer component](https://www.tensorflow.org/tfx/guide/trainer) requires a model definition code from users. You can use TensorFlow APIs to specify how to train a model and save it in a _saved _model_ format.\n  * Pusher: Copies the trained model outside of the TFX pipeline. [Pusher component](https://www.tensorflow.org/tfx/guide/pusher) can be thought of as a deployment process of the trained ML model.\n\n\nBefore actually define the pipeline, we need to write a model code for the Trainer component first.\n### Write model training code\nWe will create a simple DNN model for classification using TensorFlow Keras API. This model training code will be saved to a separate file.\nIn this tutorial we will use [Generic Trainer](https://www.tensorflow.org/tfx/guide/trainer#generic_trainer) of TFX which support Keras-based models. You need to write a Python file containing `run_fn` function, which is the entrypoint for the `Trainer` component.\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\nfromtensorflow_metadata.proto.v0import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_build_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\nNow you have completed all preparation steps to build a TFX pipeline.\n### Write a pipeline definition\nWe define a function to create a TFX pipeline. A `Pipeline` object represents a TFX pipeline which can be run using one of the pipeline orchestration systems that TFX supports.\n```\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     metadata_path: str) -> tfx.dsl.Pipeline:\n\"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  # Following three components will be included in the pipeline.\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n\n```\n\n## Run the pipeline\nTFX supports multiple orchestrators to run pipelines. In this tutorial we will use `LocalDagRunner` which is included in the TFX Python package and runs pipelines on local environment. We often call TFX pipelines \"DAGs\" which stands for directed acyclic graph.\n`LocalDagRunner` provides fast iterations for development and debugging. TFX also supports other orchestrators including Kubeflow Pipelines and Apache Airflow which are suitable for production use cases.\nSee [TFX on Cloud AI Platform Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines) or [TFX Airflow Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop) to learn more about other orchestration systems.\nNow we create a `LocalDagRunner` and pass a `Pipeline` object created from the function we already defined.\nThe pipeline runs directly and you can see logs for the progress of the pipeline including ML model training.\n```\ntfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n\n```\n```\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/penguin_trainer.py' (including modules: ['penguin_trainer']).\nINFO:absl:User module package has hash fingerprint version a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmpqkkf3qo2/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmpw3g3t_53', '--dist-dir', '/tmpfs/tmp/tmp806yzscl']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'; target user module is 'penguin_trainer'.\nINFO:absl:Full user module path is 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'\nINFO:absl:Using deployment config:\n executor_specs {\n  key: \"CsvExampleGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"Pusher\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.pusher.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"Trainer\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n    }\n  }\n}\ncustom_driver_specs {\n  key: \"CsvExampleGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n    }\n  }\n}\nmetadata_connection_config {\n  database_connection_config {\n    sqlite {\n      filename_uri: \"metadata/penguin-simple/metadata.db\"\n      connection_mode: READWRITE_OPENCREATE\n    }\n  }\n}\n\nINFO:absl:Using connection config:\n sqlite {\n  filename_uri: \"metadata/penguin-simple/metadata.db\"\n  connection_mode: READWRITE_OPENCREATE\n}\n\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataftg7q8n1\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:[CsvExampleGen] Resolved inputs: ({},)\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying penguin_trainer.py -> build/lib\ninstalling to /tmpfs/tmp/tmpw3g3t_53\nrunning install\nrunning install_lib\ncopying build/lib/penguin_trainer.py -> /tmpfs/tmp/tmpw3g3t_53\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Trainer.egg-info\nwriting tfx_user_code_Trainer.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nCopying tfx_user_code_Trainer.egg-info to /tmpfs/tmp/tmpw3g3t_53/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmpw3g3t_53/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmp806yzscl/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl' and adding '/tmpfs/tmp/tmpw3g3t_53' to it\nadding 'penguin_trainer.py'\nadding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/METADATA'\nadding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/WHEEL'\nadding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/top_level.txt'\nadding 'tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc.dist-info/RECORD'\nremoving /tmpfs/tmp/tmpw3g3t_53\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 1\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715161224,sum_checksum:1715161224\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}), exec_properties={'output_data_format': 6, 'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_base': '/tmpfs/tmp/tfx-dataftg7q8n1', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715161224,sum_checksum:1715161224'}, execution_output_uri='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/CsvExampleGen/.system/stateful_working_dir/e523b127-d674-462b-92e0-59a13ec52729', tmp_dir='pipelines/penguin-simple/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataftg7q8n1\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-simple\"\n, pipeline_run_id='2024-05-08T09:40:25.257891', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating examples.\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-dataftg7q8n1/* to TFExample.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\nINFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 1 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/CsvExampleGen/.system/stateful_working_dir/e523b127-d674-462b-92e0-59a13ec52729\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715161224,sum_checksum:1715161224\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}) for execution 1\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\nINFO:absl:Component Trainer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:40:25.257891\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715161224,sum_checksum:1715161224\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715161226444\nlast_update_time_since_epoch: 1715161226444\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 2\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-simple/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715161224,sum_checksum:1715161224\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715161226444\nlast_update_time_since_epoch: 1715161226444\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/2\"\n, artifact_type: name: \"ModelRun\"\n)], 'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/2\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)]}), exec_properties={'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 5\\n}'}, execution_output_uri='pipelines/penguin-simple/Trainer/.system/executor_execution/2/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Trainer/.system/stateful_working_dir/a07264a3-5d92-474f-9d8f-edd50602a2cd', tmp_dir='pipelines/penguin-simple/Trainer/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:40:25.257891\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-simple\"\n, pipeline_run_id='2024-05-08T09:40:25.257891', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Train on the 'train' split when train_args.splits is not set.\nINFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\nINFO:absl:udf_utils.get_fn {'module_path': 'penguin_trainer@pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'custom_config': 'null', 'eval_args': '{\\n  \"num_steps\": 5\\n}'} 'run_fn'\nINFO:absl:Installing 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmpyb97hv_s', 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl']\nProcessing ./pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-simple/_wheels/tfx_user_code_Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc-py3-none-any.whl'.\nINFO:absl:Training model.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Trainer\nSuccessfully installed tfx-user-code-Trainer-0.0+a7e2e8dccbb913b74904edeec5549d868a2ea392bcd84fbc1965aba698dce3fc\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Model: \"model\"\nINFO:absl:__________________________________________________________________________________________________\nINFO:absl: Layer (type)                Output Shape                 Param #   Connected to                  \nINFO:absl:==================================================================================================\nINFO:absl: culmen_length_mm (InputLay  [(None, 1)]                  0         []                            \nINFO:absl: er)                                                                                              \nINFO:absl:                                                                                                  \nINFO:absl: culmen_depth_mm (InputLaye  [(None, 1)]                  0         []                            \nINFO:absl: r)                                                                                               \nINFO:absl:                                                                                                  \nINFO:absl: flipper_length_mm (InputLa  [(None, 1)]                  0         []                            \nINFO:absl: yer)                                                                                             \nINFO:absl:                                                                                                  \nINFO:absl: body_mass_g (InputLayer)    [(None, 1)]                  0         []                            \nINFO:absl:                                                                                                  \nINFO:absl: concatenate (Concatenate)   (None, 4)                    0         ['culmen_length_mm[0][0]',    \nINFO:absl:                                                                     'culmen_depth_mm[0][0]',     \nINFO:absl:                                                                     'flipper_length_mm[0][0]',   \nINFO:absl:                                                                     'body_mass_g[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense (Dense)               (None, 8)                    40        ['concatenate[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense_1 (Dense)             (None, 8)                    72        ['dense[0][0]']               \nINFO:absl:                                                                                                  \nINFO:absl: dense_2 (Dense)             (None, 3)                    27        ['dense_1[0][0]']             \nINFO:absl:                                                                                                  \nINFO:absl:==================================================================================================\nINFO:absl:Total params: 139 (556.00 Byte)\nINFO:absl:Trainable params: 139 (556.00 Byte)\nINFO:absl:Non-trainable params: 0 (0.00 Byte)\nINFO:absl:__________________________________________________________________________________________________\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715161233.929065   32337 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100/100 [==============================] - 2s 5ms/step - loss: 0.4152 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.1404 - val_sparse_categorical_accuracy: 0.9600\nINFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to model_dense_2_biasadd_readvariableop_resource in the SavedModel.\nINFO:tensorflow:Assets written to: pipelines/penguin-simple/Trainer/model/2/Format-Serving/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-simple/Trainer/model/2/Format-Serving/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-simple/Trainer/model/2/Format-Serving/fingerprint.pb\nINFO:absl:Training complete. Model written to pipelines/penguin-simple/Trainer/model/2/Format-Serving. ModelRun written to pipelines/penguin-simple/Trainer/model_run/2\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 2 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/Trainer/.system/stateful_working_dir/a07264a3-5d92-474f-9d8f-edd50602a2cd\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model_run/2\"\n, artifact_type: name: \"ModelRun\"\n)], 'model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Trainer/model/2\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)]}) for execution 2\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Trainer is finished.\nINFO:absl:Component Pusher is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:40:25.257891\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 3\ntype_id: 18\nuri: \"pipelines/penguin-simple/Trainer/model/2\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715161235608\nlast_update_time_since_epoch: 1715161235608\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 3\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'model': [Artifact(artifact: id: 3\ntype_id: 18\nuri: \"pipelines/penguin-simple/Trainer/model/2\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715161235608\nlast_update_time_since_epoch: 1715161235608\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/3\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-simple\"\\n  }\\n}'}, execution_output_uri='pipelines/penguin-simple/Pusher/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-simple/Pusher/.system/stateful_working_dir/5f68650b-1da8-445f-a6d7-8e4a7418f5c4', tmp_dir='pipelines/penguin-simple/Pusher/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:40:25.257891\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-simple.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:40:25.257891\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-simple.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-simple\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-simple\"\n, pipeline_run_id='2024-05-08T09:40:25.257891', top_level_pipeline_run_id=None, frontend_url=None)\nWARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\nINFO:absl:Model version: 1715161235\nINFO:absl:Model written to serving path serving_model/penguin-simple/1715161235.\nINFO:absl:Model pushed to pipelines/penguin-simple/Pusher/pushed_model/3.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 3 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-simple/Pusher/.system/stateful_working_dir/5f68650b-1da8-445f-a6d7-8e4a7418f5c4\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-simple/Pusher/pushed_model/3\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}) for execution 3\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Pusher is finished.\n\n```\n\nYou should see \"INFO:absl:Component Pusher is finished.\" at the end of the logs if the pipeline finished successfully. Because `Pusher` component is the last component of the pipeline.\nThe pusher component pushes the trained model to the `SERVING_MODEL_DIR` which is the `serving_model/penguin-simple` directory if you did not change the variables in the previous steps. You can see the result from the file browser in the left-side panel in Colab, or using the following command:\n```\n# List files in created model directory.\nfind{SERVING_MODEL_DIR}\n```\n```\nserving_model/penguin-simple\nserving_model/penguin-simple/1715161235\nserving_model/penguin-simple/1715161235/variables\nserving_model/penguin-simple/1715161235/variables/variables.index\nserving_model/penguin-simple/1715161235/variables/variables.data-00000-of-00001\nserving_model/penguin-simple/1715161235/assets\nserving_model/penguin-simple/1715161235/keras_metadata.pb\nserving_model/penguin-simple/1715161235/fingerprint.pb\nserving_model/penguin-simple/1715161235/saved_model.pb\n\n```\n\n## Next steps\nYou can find more resources on <https://www.tensorflow.org/tfx/tutorials>\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/components_keras": "**_A Component-by-Component Introduction to TensorFlow Extended (TFX)_**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)  \n---  \nThis Colab-based tutorial will interactively walk through each built-in component of TensorFlow Extended (TFX).\nIt covers every step in an end-to-end machine learning pipeline, from data ingestion to pushing a model to serving.\nWhen you're done, the contents of this notebook can be automatically exported as TFX pipeline source code, which you can orchestrate with Apache Airflow and Apache Beam.\n## Background\nThis notebook demonstrates how to use TFX in a Jupyter/Colab environment. Here, we walk through the Chicago Taxi example in an interactive notebook.\nWorking in an interactive notebook is a useful way to become familiar with the structure of a TFX pipeline. It's also useful when doing development of your own pipelines as a lightweight development environment, but you should be aware that there are differences in the way interactive notebooks are orchestrated, and how they access metadata artifacts.\n### Orchestration\nIn a production deployment of TFX, you will use an orchestrator such as Apache Airflow, Kubeflow Pipelines, or Apache Beam to orchestrate a pre-defined pipeline graph of TFX components. In an interactive notebook, the notebook itself is the orchestrator, running each TFX component as you execute the notebook cells.\n### Metadata\nIn a production deployment of TFX, you will access metadata through the ML Metadata (MLMD) API. MLMD stores metadata properties in a database such as MySQL or SQLite, and stores the metadata payloads in a persistent store such as on your filesystem. In an interactive notebook, both properties and payloads are stored in an ephemeral SQLite database in the `/tmp` directory on the Jupyter notebook or Colab server.\n## Setup\nFirst, we install and import the necessary packages, set up paths, and download data.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.\n```\nimportsys\nif 'google.colab' in sys.modules:\n  !pip install --upgrade pip\n\n```\n\n### Install TFX\n```\npip\n```\n\n## Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime > Restart runtime ...). This is because of the way that Colab loads packages.\n### Import packages\nWe import necessary packages, including standard TFX component classes.\n```\nimportos\nimportpprint\nimporttempfile\nimporturllib\n\nimportabsl\nimporttensorflowastf\nimporttensorflow_model_analysisastfma\ntf.get_logger().propagate = False\npp = pprint.PrettyPrinter()\n\nfromtfximport v1 as tfx\nfromtfx.orchestration.experimental.interactive.interactive_contextimport InteractiveContext\n\n%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip\n\n```\n```\n2024-08-02 09:25:16.953664: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-02 09:25:16.953708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-02 09:25:16.955358: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\nLet's check the library versions.\n```\nprint('TensorFlow version: {}'.format(tf.__version__))\nprint('TFX version: {}'.format(tfx.__version__))\n\n```\n```\nTensorFlow version: 2.15.1\nTFX version: 1.15.1\n\n```\n\n### Set up pipeline paths\n```\n# This is the root directory for your TFX pip package installation.\n_tfx_root = tfx.__path__[0]\n\n# This is the directory containing the TFX Chicago Taxi Pipeline example.\n_taxi_root = os.path.join(_tfx_root, 'examples/chicago_taxi_pipeline')\n\n# This is the path where your model will be pushed for serving.\n_serving_model_dir = os.path.join(\n    tempfile.mkdtemp(), 'serving_model/taxi_simple')\n\n# Set up logging.\nabsl.logging.set_verbosity(absl.logging.INFO)\n\n```\n\n### Download example data\nWe download the example dataset for use in our TFX pipeline.\nThe dataset we're using is the \npickup_community_area | fare | trip_start_month  \n---|---|---  \ntrip_start_hour | trip_start_day | trip_start_timestamp  \npickup_latitude | pickup_longitude | dropoff_latitude  \ndropoff_longitude | trip_miles | pickup_census_tract  \ndropoff_census_tract | payment_type | company  \ntrip_seconds | dropoff_community_area | tips  \nWith this dataset, we will build a model that predicts the `tips` of a trip.\n```\n_data_root = tempfile.mkdtemp(prefix='tfx-data')\nDATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/chicago_taxi_pipeline/data/simple/data.csv'\n_data_filepath = os.path.join(_data_root, \"data.csv\")\nurllib.request.urlretrieve(DATA_PATH, _data_filepath)\n\n```\n```\n('/tmpfs/tmp/tfx-datajp95savt/data.csv',\n <http.client.HTTPMessage at 0x7faad452acd0>)\n\n```\n\nTake a quick look at the CSV file.\n```\nhead{_data_filepath}\n```\n```\npickup_community_area,fare,trip_start_month,trip_start_hour,trip_start_day,trip_start_timestamp,pickup_latitude,pickup_longitude,dropoff_latitude,dropoff_longitude,trip_miles,pickup_census_tract,dropoff_census_tract,payment_type,company,trip_seconds,dropoff_community_area,tips\n,12.45,5,19,6,1400269500,,,,,0.0,,,Credit Card,Chicago Elite Cab Corp. (Chicago Carriag,0,,0.0\n,0,3,19,5,1362683700,,,,,0,,,Unknown,Chicago Elite Cab Corp.,300,,0\n60,27.05,10,2,3,1380593700,41.836150155,-87.648787952,,,12.6,,,Cash,Taxi Affiliation Services,1380,,0.0\n10,5.85,10,1,2,1382319000,41.985015101,-87.804532006,,,0.0,,,Cash,Taxi Affiliation Services,180,,0.0\n14,16.65,5,7,5,1369897200,41.968069,-87.721559063,,,0.0,,,Cash,Dispatch Taxi Affiliation,1080,,0.0\n13,16.45,11,12,3,1446554700,41.983636307,-87.723583185,,,6.9,,,Cash,,780,,0.0\n16,32.05,12,1,1,1417916700,41.953582125,-87.72345239,,,15.4,,,Cash,,1200,,0.0\n30,38.45,10,10,5,1444301100,41.839086906,-87.714003807,,,14.6,,,Cash,,2580,,0.0\n11,14.65,1,1,3,1358213400,41.978829526,-87.771166703,,,5.81,,,Cash,,1080,,0.0\n\n```\n\n_Disclaimer: This site provides applications using data that has been modified for use from its original source, www.cityofchicago.org, the official website of the City of Chicago. The City of Chicago makes no claims as to the content, accuracy, timeliness, or completeness of any of the data provided at this site. The data provided at this site is subject to change at any time. It is understood that the data provided at this site is being used at one’s own risk._\n### Create the InteractiveContext\nLast, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook.\n```\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\ncontext = InteractiveContext()\n\n```\n```\nWARNING:absl:InteractiveContext pipeline_root argument not provided: using temporary directory /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle as root for pipeline outputs.\nWARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/metadata.sqlite.\n\n```\n\n## Run TFX components interactively\nIn the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts.\n### ExampleGen\nThe `ExampleGen` component is usually at the start of a TFX pipeline. It will:\n  1. Split data into training and evaluation sets (by default, 2/3 training + 1/3 eval)\n  2. Convert data into the `tf.Example` format (learn more [here](https://www.tensorflow.org/tutorials/load_data/tfrecord))\n  3. Copy data into the `_tfx_root` directory for other components to access\n\n\n`ExampleGen` takes as input the path to your data source. In our case, this is the `_data_root` path that contains the downloaded CSV.\n#### Enabling the Cache\nWhen using the `InteractiveContext` in a notebook to develop a pipeline you can control when individual components will cache their outputs. Set `enable_cache` to `True` when you want to reuse the previous output artifacts that the component generated. Set `enable_cache` to `False` when you want to recompute the output artifacts for a component, if you are making changes to the code for example.\n```\nexample_gen = tfx.components.CsvExampleGen(input_base=_data_root)\ncontext.run(example_gen, enable_cache=True)\n\n```\n```\nINFO:absl:Running driver for CsvExampleGen\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:Running executor for CsvExampleGen\nINFO:absl:Generating examples.\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-datajp95savt/* to TFExample.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Running publisher for CsvExampleGen\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nLet's examine the output artifacts of `ExampleGen`. This component produces two artifacts, training examples and evaluation examples:\n```\nartifact = example_gen.outputs['examples'].get()[0]\nprint(artifact.split_names, artifact.uri)\n\n```\n```\n[\"train\", \"eval\"] /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/CsvExampleGen/examples/1\n\n```\n\nWe can also take a look at the first three training examples:\n```\n# Get the URI of the output artifact representing the training examples, which is a directory\ntrain_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n```\n```\nfeatures {\n  feature {\n    key: \"company\"\n    value {\n      bytes_list {\n        value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"fare\"\n    value {\n      float_list {\n        value: 12.449999809265137\n      }\n    }\n  }\n  feature {\n    key: \"payment_type\"\n    value {\n      bytes_list {\n        value: \"Credit Card\"\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day\"\n    value {\n      int64_list {\n        value: 6\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour\"\n    value {\n      int64_list {\n        value: 19\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month\"\n    value {\n      int64_list {\n        value: 5\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_timestamp\"\n    value {\n      int64_list {\n        value: 1400269500\n      }\n    }\n  }\n}\n\nfeatures {\n  feature {\n    key: \"company\"\n    value {\n      bytes_list {\n        value: \"Taxi Affiliation Services\"\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"fare\"\n    value {\n      float_list {\n        value: 27.049999237060547\n      }\n    }\n  }\n  feature {\n    key: \"payment_type\"\n    value {\n      bytes_list {\n        value: \"Cash\"\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area\"\n    value {\n      int64_list {\n        value: 60\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude\"\n    value {\n      float_list {\n        value: 41.836151123046875\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude\"\n    value {\n      float_list {\n        value: -87.64878845214844\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles\"\n    value {\n      float_list {\n        value: 12.600000381469727\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds\"\n    value {\n      int64_list {\n        value: 1380\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day\"\n    value {\n      int64_list {\n        value: 3\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour\"\n    value {\n      int64_list {\n        value: 2\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month\"\n    value {\n      int64_list {\n        value: 10\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_timestamp\"\n    value {\n      int64_list {\n        value: 1380593700\n      }\n    }\n  }\n}\n\nfeatures {\n  feature {\n    key: \"company\"\n    value {\n      bytes_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude\"\n    value {\n      float_list {\n      }\n    }\n  }\n  feature {\n    key: \"fare\"\n    value {\n      float_list {\n        value: 16.450000762939453\n      }\n    }\n  }\n  feature {\n    key: \"payment_type\"\n    value {\n      bytes_list {\n        value: \"Cash\"\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract\"\n    value {\n      int64_list {\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area\"\n    value {\n      int64_list {\n        value: 13\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude\"\n    value {\n      float_list {\n        value: 41.98363494873047\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude\"\n    value {\n      float_list {\n        value: -87.72357940673828\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles\"\n    value {\n      float_list {\n        value: 6.900000095367432\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds\"\n    value {\n      int64_list {\n        value: 780\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day\"\n    value {\n      int64_list {\n        value: 3\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour\"\n    value {\n      int64_list {\n        value: 12\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month\"\n    value {\n      int64_list {\n        value: 11\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_timestamp\"\n    value {\n      int64_list {\n        value: 1446554700\n      }\n    }\n  }\n}\n\n```\n\nNow that `ExampleGen` has finished ingesting the data, the next step is data analysis.\n### StatisticsGen\nThe `StatisticsGen` component computes statistics over your dataset for data analysis, as well as for use in downstream components. It uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen`.\n```\nstatistics_gen = tfx.components.StatisticsGen(\n    examples=example_gen.outputs['examples'])\ncontext.run(statistics_gen, enable_cache=True)\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Running driver for StatisticsGen\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for StatisticsGen\nINFO:absl:Generating statistics for split train.\nINFO:absl:Statistics for split train written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/StatisticsGen/statistics/2/Split-train.\nINFO:absl:Generating statistics for split eval.\nINFO:absl:Statistics for split eval written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/StatisticsGen/statistics/2/Split-eval.\nINFO:absl:Running publisher for StatisticsGen\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nAfter `StatisticsGen` finishes running, we can visualize the outputted statistics. Try playing with the different plots!\n```\ncontext.show(statistics_gen.outputs['statistics'])\n\n```\n\n### SchemaGen\nThe `SchemaGen` component generates a schema based on your data statistics. (A schema defines the expected bounds, types, and properties of the features in your dataset.) It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default.\n```\nschema_gen = tfx.components.SchemaGen(\n    statistics=statistics_gen.outputs['statistics'],\n    infer_feature_shape=False)\ncontext.run(schema_gen, enable_cache=True)\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Running driver for SchemaGen\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for SchemaGen\nINFO:absl:Processing schema from statistics for split train.\nINFO:absl:Processing schema from statistics for split eval.\nINFO:absl:Schema written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/SchemaGen/schema/3/schema.pbtxt.\nINFO:absl:Running publisher for SchemaGen\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nAfter `SchemaGen` finishes running, we can visualize the generated schema as a table.\n```\ncontext.show(schema_gen.outputs['schema'])\n\n```\n\nEach feature in your dataset shows up as a row in the schema table, alongside its properties. The schema also captures all the values that a categorical feature takes on, denoted as its domain.\nTo learn more about schemas, see [the SchemaGen documentation](https://www.tensorflow.org/tfx/guide/schemagen).\n### ExampleValidator\nThe `ExampleValidator` component detects anomalies in your data, based on the expectations defined by the schema. It also uses the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started) library.\n`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`.\n```\nexample_validator = tfx.components.ExampleValidator(\n    statistics=statistics_gen.outputs['statistics'],\n    schema=schema_gen.outputs['schema'])\ncontext.run(example_validator, enable_cache=True)\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Running driver for ExampleValidator\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for ExampleValidator\nINFO:absl:Validating schema against the computed statistics for split train.\nINFO:absl:Anomalies alerts created for split train.\nINFO:absl:Validation complete for split train. Anomalies written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/ExampleValidator/anomalies/4/Split-train.\nINFO:absl:Validating schema against the computed statistics for split eval.\nINFO:absl:Anomalies alerts created for split eval.\nINFO:absl:Validation complete for split eval. Anomalies written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/ExampleValidator/anomalies/4/Split-eval.\nINFO:absl:Running publisher for ExampleValidator\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nAfter `ExampleValidator` finishes running, we can visualize the anomalies as a table.\n```\ncontext.show(example_validator.outputs['anomalies'])\n\n```\n\nIn the anomalies table, we can see that there are no anomalies. This is what we'd expect, since this the first dataset that we've analyzed and the schema is tailored to it. You should review this schema -- anything unexpected means an anomaly in the data. Once reviewed, the schema can be used to guard future data, and anomalies produced here can be used to debug model performance, understand how your data evolves over time, and identify data errors.\n### Transform\nThe `Transform` component performs feature engineering for both training and serving. It uses the [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) library.\n`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module that contains user-defined Transform code.\nLet's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, [see the tutorial](https://www.tensorflow.org/tfx/tutorials/transform/simple)). First, we define a few constants for feature engineering:\n```\n_taxi_constants_module_file = 'taxi_constants.py'\n\n```\n```\n%%writefile {_taxi_constants_module_file}\n\nNUMERICAL_FEATURES = ['trip_miles', 'fare', 'trip_seconds']\n\nBUCKET_FEATURES = [\n    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n    'dropoff_longitude'\n]\n# Number of buckets used by tf.transform for encoding each feature.\nFEATURE_BUCKET_COUNT = 10\n\nCATEGORICAL_NUMERICAL_FEATURES = [\n    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n    'dropoff_community_area'\n]\n\nCATEGORICAL_STRING_FEATURES = [\n    'payment_type',\n    'company',\n]\n\n# Number of vocabulary terms used for encoding categorical features.\nVOCAB_SIZE = 1000\n\n# Count of out-of-vocab buckets in which unrecognized categorical are hashed.\nOOV_SIZE = 10\n\n# Keys\nLABEL_KEY = 'tips'\nFARE_KEY = 'fare'\n\ndeft_name(key):\n\"\"\"\n  Rename the feature keys so that they don't clash with the raw keys when\n  running the Evaluator component.\n  Args:\n    key: The original feature key\n  Returns:\n    key with '_xf' appended\n  \"\"\"\n  return key + '_xf'\n\n```\n```\nWriting taxi_constants.py\n\n```\n\nNext, we write a `preprocessing_fn` that takes in raw data as input, and returns transformed features that our model can train on:\n```\n_taxi_transform_module_file = 'taxi_transform.py'\n\n```\n```\n%%writefile {_taxi_transform_module_file}\n\nimporttensorflowastf\nimporttensorflow_transformastft\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimporttaxi_constants\nimportsys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  importimportlib\n  importlib.reload(taxi_constants)\n\n_NUMERICAL_FEATURES = taxi_constants.NUMERICAL_FEATURES\n_BUCKET_FEATURES = taxi_constants.BUCKET_FEATURES\n_FEATURE_BUCKET_COUNT = taxi_constants.FEATURE_BUCKET_COUNT\n_CATEGORICAL_NUMERICAL_FEATURES = taxi_constants.CATEGORICAL_NUMERICAL_FEATURES\n_CATEGORICAL_STRING_FEATURES = taxi_constants.CATEGORICAL_STRING_FEATURES\n_VOCAB_SIZE = taxi_constants.VOCAB_SIZE\n_OOV_SIZE = taxi_constants.OOV_SIZE\n_FARE_KEY = taxi_constants.FARE_KEY\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n\ndef_make_one_hot(x, key):\n\"\"\"Make a one-hot tensor to encode categorical features.\n  Args:\n    X: A dense tensor\n    key: A string key for the feature in the input\n  Returns:\n    A dense one-hot tensor as a float list\n  \"\"\"\n  integerized = tft.compute_and_apply_vocabulary(x,\n          top_k=_VOCAB_SIZE,\n          num_oov_buckets=_OOV_SIZE,\n          vocab_filename=key, name=key)\n  depth = (\n      tft.experimental.get_vocabulary_size_by_name(key) + _OOV_SIZE)\n  one_hot_encoded = tf.one_hot(\n      integerized,\n      depth=tf.cast(depth, tf.int32),\n      on_value=1.0,\n      off_value=0.0)\n  return tf.reshape(one_hot_encoded, [-1, depth])\n\n\ndef_fill_in_missing(x):\n\"\"\"Replace missing values in a SparseTensor.\n  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n  Args:\n    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n      in the second dimension.\n  Returns:\n    A rank 1 tensor where missing values of `x` have been filled in.\n  \"\"\"\n  if not isinstance(x, tf.sparse.SparseTensor):\n    return x\n\n  default_value = '' if x.dtype == tf.string else 0\n  return tf.squeeze(\n      tf.sparse.to_dense(\n          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n          default_value),\n      axis=1)\n\n\ndefpreprocessing_fn(inputs):\n\"\"\"tf.transform's callback function for preprocessing inputs.\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _NUMERICAL_FEATURES:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[taxi_constants.t_name(key)] = tft.scale_to_z_score(\n        _fill_in_missing(inputs[key]), name=key)\n\n  for key in _BUCKET_FEATURES:\n    outputs[taxi_constants.t_name(key)] = tf.cast(tft.bucketize(\n            _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT, name=key),\n            dtype=tf.float32)\n\n  for key in _CATEGORICAL_STRING_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(_fill_in_missing(inputs[key]), key)\n\n  for key in _CATEGORICAL_NUMERICAL_FEATURES:\n    outputs[taxi_constants.t_name(key)] = _make_one_hot(tf.strings.strip(\n        tf.strings.as_string(_fill_in_missing(inputs[key]))), key)\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_LABEL_KEY] = tf.where(\n      tf.math.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n```\n```\nWriting taxi_transform.py\n\n```\n\nNow, we pass in this feature engineering code to the `Transform` component and run it to transform your data.\n```\ntransform = tfx.components.Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\ncontext.run(transform, enable_cache=True)\n\n```\n```\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/taxi_transform.py' (including modules: ['taxi_transform', 'taxi_constants']).\nINFO:absl:User module package has hash fingerprint version d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmpu8kpouyg/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmp6787jn2x', '--dist-dir', '/tmpfs/tmp/tmpj98iwvji']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl'; target user module is 'taxi_transform'.\nINFO:absl:Full user module path is 'taxi_transform@/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl'\nINFO:absl:Running driver for Transform\nINFO:absl:MetadataStore with DB connection initialized\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying taxi_transform.py -> build/lib\ncopying taxi_constants.py -> build/lib\ninstalling to /tmpfs/tmp/tmp6787jn2x\nrunning install\nrunning install_lib\ncopying build/lib/taxi_transform.py -> /tmpfs/tmp/tmp6787jn2x\ncopying build/lib/taxi_constants.py -> /tmpfs/tmp/tmp6787jn2x\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Transform.egg-info\nwriting tfx_user_code_Transform.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Transform.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nCopying tfx_user_code_Transform.egg-info to /tmpfs/tmp/tmp6787jn2x/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmp6787jn2x/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmpj98iwvji/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl' and adding '/tmpfs/tmp/tmp6787jn2x' to it\nadding 'taxi_constants.py'\nadding 'taxi_transform.py'\nadding 'tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.dist-info/METADATA'\nadding 'tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.dist-info/WHEEL'\nadding 'tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.dist-info/top_level.txt'\nadding 'tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d.dist-info/RECORD'\nremoving /tmpfs/tmp/tmp6787jn2x\nINFO:absl:Running executor for Transform\nINFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\nINFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'taxi_transform@/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\nINFO:absl:Installing '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmp_7nbz18y', '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl']\nProcessing /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl\nINFO:absl:Successfully installed '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl'.\nINFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'taxi_transform@/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\nINFO:absl:Installing '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmp8mngt494', '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl']\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d\nProcessing /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl\nINFO:absl:Successfully installed '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl'.\nINFO:absl:Installing '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmpe376fviy', '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl']\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d\nProcessing /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl\nINFO:absl:Successfully installed '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d-py3-none-any.whl'.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+d7f32accc04453d93cd29bae5b4d879eb83d8a54c7e01d354a58158f2f84251d\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: payment_type/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: company/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_hour/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_day/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_month/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: pickup_census_tract/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: dropoff_census_tract/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: pickup_community_area/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: dropoff_community_area/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: payment_type/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: company/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_hour/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_day/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: trip_start_month/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: pickup_census_tract/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: dropoff_census_tract/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: pickup_community_area/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: dropoff_community_area/apply_vocab/text_file_init/InitializeTableFromTextFileV2\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Transform/transform_graph/5/.temp_path/tftransform_tmp/c804114a115a454e9dc24ba23291fe73/assets\nINFO:absl:Writing fingerprint to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Transform/transform_graph/5/.temp_path/tftransform_tmp/c804114a115a454e9dc24ba23291fe73/fingerprint.pb\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Transform/transform_graph/5/.temp_path/tftransform_tmp/089f1b2bd7b8429fb4979023e5e36d13/assets\nINFO:absl:Writing fingerprint to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Transform/transform_graph/5/.temp_path/tftransform_tmp/089f1b2bd7b8429fb4979023e5e36d13/fingerprint.pb\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:absl:Running publisher for Transform\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nLet's examine the output artifacts of `Transform`. This component produces two types of outputs:\n  * `transform_graph` is the graph that can perform the preprocessing operations (this graph will be included in the serving and evaluation models).\n  * `transformed_examples` represents the preprocessed training and evaluation data.\n\n```\ntransform.outputs\n\n```\n```\n{'transform_graph': OutputChannel(artifact_type=TransformGraph, producer_component_id=Transform, output_key=transform_graph, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'transformed_examples': OutputChannel(artifact_type=Examples, producer_component_id=Transform, output_key=transformed_examples, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'updated_analyzer_cache': OutputChannel(artifact_type=TransformCache, producer_component_id=Transform, output_key=updated_analyzer_cache, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'pre_transform_schema': OutputChannel(artifact_type=Schema, producer_component_id=Transform, output_key=pre_transform_schema, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'pre_transform_stats': OutputChannel(artifact_type=ExampleStatistics, producer_component_id=Transform, output_key=pre_transform_stats, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'post_transform_schema': OutputChannel(artifact_type=Schema, producer_component_id=Transform, output_key=post_transform_schema, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'post_transform_stats': OutputChannel(artifact_type=ExampleStatistics, producer_component_id=Transform, output_key=post_transform_stats, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'post_transform_anomalies': OutputChannel(artifact_type=ExampleAnomalies, producer_component_id=Transform, output_key=post_transform_anomalies, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False)}\n\n```\n\nTake a peek at the `transform_graph` artifact. It points to a directory containing three subdirectories.\n```\ntrain_uri = transform.outputs['transform_graph'].get()[0].uri\nos.listdir(train_uri)\n\n```\n```\n['metadata', 'transformed_metadata', 'transform_fn']\n\n```\n\nThe `transformed_metadata` subdirectory contains the schema of the preprocessed data. The `transform_fn` subdirectory contains the actual preprocessing graph. The `metadata` subdirectory contains the schema of the original data.\nWe can also take a look at the first three transformed examples:\n```\n# Get the URI of the output artifact representing the transformed examples, which is a directory\ntrain_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n\n# Get the list of files in this directory (all compressed TFRecord files)\ntfrecord_filenames = [os.path.join(train_uri, name)\n                      for name in os.listdir(train_uri)]\n\n# Create a `TFRecordDataset` to read these files\ndataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n\n# Iterate over the first 3 records and decode them.\nfor tfrecord in dataset.take(3):\n  serialized_example = tfrecord.numpy()\n  example = tf.train.Example()\n  example.ParseFromString(serialized_example)\n  pp.pprint(example)\n\n```\n```\nfeatures {\n  feature {\n    key: \"company_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude_xf\"\n    value {\n      float_list {\n        value: 9.0\n      }\n    }\n  }\n  feature {\n    key: \"fare_xf\"\n    value {\n      float_list {\n        value: 0.061060599982738495\n      }\n    }\n  }\n  feature {\n    key: \"payment_type_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude_xf\"\n    value {\n      float_list {\n        value: 9.0\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles_xf\"\n    value {\n      float_list {\n        value: -0.15886741876602173\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds_xf\"\n    value {\n      float_list {\n        value: -0.7118487358093262\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n}\n\nfeatures {\n  feature {\n    key: \"company_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude_xf\"\n    value {\n      float_list {\n        value: 9.0\n      }\n    }\n  }\n  feature {\n    key: \"fare_xf\"\n    value {\n      float_list {\n        value: 1.2521240711212158\n      }\n    }\n  }\n  feature {\n    key: \"payment_type_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude_xf\"\n    value {\n      float_list {\n        value: 3.0\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles_xf\"\n    value {\n      float_list {\n        value: 0.532160758972168\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds_xf\"\n    value {\n      float_list {\n        value: 0.5509493350982666\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n}\n\nfeatures {\n  feature {\n    key: \"company_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_latitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"dropoff_longitude_xf\"\n    value {\n      float_list {\n        value: 9.0\n      }\n    }\n  }\n  feature {\n    key: \"fare_xf\"\n    value {\n      float_list {\n        value: 0.3873794376850128\n      }\n    }\n  }\n  feature {\n    key: \"payment_type_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_census_tract_xf\"\n    value {\n      float_list {\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_community_area_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_latitude_xf\"\n    value {\n      float_list {\n        value: 9.0\n      }\n    }\n  }\n  feature {\n    key: \"pickup_longitude_xf\"\n    value {\n      float_list {\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"tips\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"trip_miles_xf\"\n    value {\n      float_list {\n        value: 0.21955277025699615\n      }\n    }\n  }\n  feature {\n    key: \"trip_seconds_xf\"\n    value {\n      float_list {\n        value: 0.0019067146349698305\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_day_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_hour_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n  feature {\n    key: \"trip_start_month_xf\"\n    value {\n      float_list {\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 1.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n        value: 0.0\n      }\n    }\n  }\n}\n\n```\n\nAfter the `Transform` component has transformed your data into features, and the next step is to train a model.\n### Trainer\nThe `Trainer` component will train a model that you define in TensorFlow.\n`Trainer` takes as input the schema from `SchemaGen`, the transformed data and graph from `Transform`, training parameters, as well as a module that contains user-defined model code.\nLet's see an example of user-defined model code below (for an introduction to the TensorFlow Keras APIs, [see the tutorial](https://www.tensorflow.org/guide/keras)):\n```\n_taxi_trainer_module_file = 'taxi_trainer.py'\n\n```\n```\n%%writefile {_taxi_trainer_module_file}\n\nfromtypingimport Dict, List, Text\n\nimportos\nimportglob\nfromabslimport logging\n\nimportdatetime\nimporttensorflowastf\nimporttensorflow_transformastft\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\nfromtensorflow_transformimport TFTransformOutput\n\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimporttaxi_constants\nimportsys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  importimportlib\n  importlib.reload(taxi_constants)\n\n_LABEL_KEY = taxi_constants.LABEL_KEY\n\n_BATCH_SIZE = 40\n\n\ndef_input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n\"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      tf_transform_output.transformed_metadata.schema)\n\ndef_get_tf_examples_serving_signature(model, tf_transform_output):\n\"\"\"Returns a serving signature that accepts `tensorflow.Example`.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_inference = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  defserve_tf_examples_fn(serialized_tf_example):\n\"\"\"Returns the output to be used in the serving signature.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    # Remove label feature since these will not be present at serving time.\n    raw_feature_spec.pop(_LABEL_KEY)\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_inference(raw_features)\n    logging.info('serve_transformed_features = %s', transformed_features)\n\n    outputs = model(transformed_features)\n    # TODO(b/154085620): Convert the predicted labels from the model using a\n    # reverse-lookup (opposite of transform.py).\n    return {'outputs': outputs}\n\n  return serve_tf_examples_fn\n\n\ndef_get_transform_features_signature(model, tf_transform_output):\n\"\"\"Returns a serving signature that applies tf.Transform to features.\"\"\"\n\n  # We need to track the layers in the model in order to save it.\n  # TODO(b/162357359): Revise once the bug is resolved.\n  model.tft_layer_eval = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  deftransform_features_fn(serialized_tf_example):\n\"\"\"Returns the transformed_features to be fed as input to evaluator.\"\"\"\n    raw_feature_spec = tf_transform_output.raw_feature_spec()\n    raw_features = tf.io.parse_example(serialized_tf_example, raw_feature_spec)\n    transformed_features = model.tft_layer_eval(raw_features)\n    logging.info('eval_transformed_features = %s', transformed_features)\n    return transformed_features\n\n  return transform_features_fn\n\n\ndefexport_serving_model(tf_transform_output, model, output_dir):\n\"\"\"Exports a keras model for serving.\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    model: A keras model to export for serving.\n    output_dir: A directory where the model will be exported to.\n  \"\"\"\n  # The layer has to be saved to the model for keras tracking purpases.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  signatures = {\n      'serving_default':\n          _get_tf_examples_serving_signature(model, tf_transform_output),\n      'transform_features':\n          _get_transform_features_signature(model, tf_transform_output),\n  }\n\n  model.save(output_dir, save_format='tf', signatures=signatures)\n\n\ndef_build_keras_model(tf_transform_output: TFTransformOutput\n                       ) -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying taxi data.\n\n  Args:\n    tf_transform_output: [TFTransformOutput], the outputs from Transform\n\n  Returns:\n    A keras Model.\n  \"\"\"\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(_LABEL_KEY)\n\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf.keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      # TODO(b/208879020): Move into schema such that spec.shape is [1] and not\n      # [] for scalars.\n      inputs[key] = tf.keras.layers.Input(\n          shape=spec.shape or [1], name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n\n  output = tf.keras.layers.Concatenate()(tf.nest.flatten(inputs))\n  output = tf.keras.layers.Dense(100, activation='relu')(output)\n  output = tf.keras.layers.Dense(70, activation='relu')(output)\n  output = tf.keras.layers.Dense(50, activation='relu')(output)\n  output = tf.keras.layers.Dense(20, activation='relu')(output)\n  output = tf.keras.layers.Dense(1)(output)\n  return tf.keras.Model(inputs=inputs, outputs=output)\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor,\n                            tf_transform_output, _BATCH_SIZE)\n  eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor,\n                           tf_transform_output, _BATCH_SIZE)\n\n  model = _build_keras_model(tf_transform_output)\n\n  model.compile(\n      loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n      metrics=[tf.keras.metrics.BinaryAccuracy()])\n\n  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n      log_dir=fn_args.model_run_dir, update_freq='batch')\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps,\n      callbacks=[tensorboard_callback])\n\n  # Export the model.\n  export_serving_model(tf_transform_output, model, fn_args.serving_model_dir)\n\n```\n```\nWriting taxi_trainer.py\n\n```\n\nNow, we pass in this model code to the `Trainer` component and run it to train the model.\n```\ntrainer = tfx.components.Trainer(\n    module_file=os.path.abspath(_taxi_trainer_module_file),\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    train_args=tfx.proto.TrainArgs(num_steps=10000),\n    eval_args=tfx.proto.EvalArgs(num_steps=5000))\ncontext.run(trainer, enable_cache=True)\n\n```\n```\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/taxi_trainer.py' (including modules: ['taxi_trainer', 'taxi_transform', 'taxi_constants']).\nINFO:absl:User module package has hash fingerprint version c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmpjusxyuw6/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmpv4l9jvk4', '--dist-dir', '/tmpfs/tmp/tmpd232wkwu']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl'; target user module is 'taxi_trainer'.\nINFO:absl:Full user module path is 'taxi_trainer@/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl'\nINFO:absl:Running driver for Trainer\nINFO:absl:MetadataStore with DB connection initialized\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying taxi_trainer.py -> build/lib\ncopying taxi_transform.py -> build/lib\ncopying taxi_constants.py -> build/lib\ninstalling to /tmpfs/tmp/tmpv4l9jvk4\nrunning install\nrunning install_lib\ncopying build/lib/taxi_trainer.py -> /tmpfs/tmp/tmpv4l9jvk4\ncopying build/lib/taxi_transform.py -> /tmpfs/tmp/tmpv4l9jvk4\ncopying build/lib/taxi_constants.py -> /tmpfs/tmp/tmpv4l9jvk4\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Trainer.egg-info\nwriting tfx_user_code_Trainer.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nCopying tfx_user_code_Trainer.egg-info to /tmpfs/tmp/tmpv4l9jvk4/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmpv4l9jvk4/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmpd232wkwu/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl' and adding '/tmpfs/tmp/tmpv4l9jvk4' to it\nadding 'taxi_constants.py'\nadding 'taxi_trainer.py'\nadding 'taxi_transform.py'\nadding 'tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.dist-info/METADATA'\nadding 'tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.dist-info/WHEEL'\nadding 'tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.dist-info/top_level.txt'\nadding 'tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d.dist-info/RECORD'\nremoving /tmpfs/tmp/tmpv4l9jvk4\nINFO:absl:Running executor for Trainer\nINFO:absl:Train on the 'train' split when train_args.splits is not set.\nINFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\nWARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\nWARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\nWARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\nINFO:absl:udf_utils.get_fn {'train_args': '{\\n  \"num_steps\": 10000\\n}', 'eval_args': '{\\n  \"num_steps\": 5000\\n}', 'module_file': None, 'run_fn': None, 'trainer_fn': None, 'custom_config': 'null', 'module_path': 'taxi_trainer@/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl'} 'run_fn'\nINFO:absl:Installing '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmpkzpl88qp', '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl']\nProcessing /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl\nINFO:absl:Successfully installed '/tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/_wheels/tfx_user_code_Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d-py3-none-any.whl'.\nINFO:absl:Training model.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Trainer\nSuccessfully installed tfx-user-code-Trainer-0.0+c83184ecc7def0038911b1171ce3fafec7757168a9f4d647f3c3b86b5467668d\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nINFO:absl:Feature company_xf has a shape dim {\n  size: 55\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_census_tract_xf has a shape dim {\n  size: 216\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_community_area_xf has a shape dim {\n  size: 79\n}\n. Setting to DenseTensor.\nINFO:absl:Feature dropoff_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature dropoff_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature fare_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature payment_type_xf has a shape dim {\n  size: 16\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_census_tract_xf has a shape dim {\n  size: 11\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_community_area_xf has a shape dim {\n  size: 66\n}\n. Setting to DenseTensor.\nINFO:absl:Feature pickup_latitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature pickup_longitude_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature tips has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_miles_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_seconds_xf has a shape . Setting to DenseTensor.\nINFO:absl:Feature trip_start_day_xf has a shape dim {\n  size: 17\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_hour_xf has a shape dim {\n  size: 34\n}\n. Setting to DenseTensor.\nINFO:absl:Feature trip_start_month_xf has a shape dim {\n  size: 22\n}\n. Setting to DenseTensor.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1722590785.122239   22788 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n10000/10000 [==============================] - 78s 7ms/step - loss: 0.0824 - binary_accuracy: 0.9607 - val_loss: 1.0096 - val_binary_accuracy: 0.8731\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:serve_transformed_features = {'trip_miles_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:12' shape=(None,) dtype=float32>, 'pickup_community_area_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:8' shape=(None, 66) dtype=float32>, 'dropoff_longitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:4' shape=(None,) dtype=float32>, 'payment_type_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:6' shape=(None, 16) dtype=float32>, 'fare_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:5' shape=(None,) dtype=float32>, 'pickup_longitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:10' shape=(None,) dtype=float32>, 'trip_seconds_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:13' shape=(None,) dtype=float32>, 'dropoff_census_tract_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:1' shape=(None, 216) dtype=float32>, 'pickup_census_tract_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:7' shape=(None, 11) dtype=float32>, 'trip_start_day_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:14' shape=(None, 17) dtype=float32>, 'dropoff_community_area_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:2' shape=(None, 79) dtype=float32>, 'pickup_latitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:9' shape=(None,) dtype=float32>, 'dropoff_latitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:3' shape=(None,) dtype=float32>, 'trip_start_month_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:16' shape=(None, 22) dtype=float32>, 'trip_start_hour_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:15' shape=(None, 34) dtype=float32>, 'company_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:0' shape=(None, 55) dtype=float32>}\nINFO:absl:Function `serve_tf_examples_fn` contains input name(s) 327971, 327981, 327991, 328001, 328011, 328021, 328031, 328041, 328051, resource with unsupported characters which will be renamed to transform_features_layer_327971, transform_features_layer_327981, transform_features_layer_327991, transform_features_layer_328001, transform_features_layer_328011, transform_features_layer_328021, transform_features_layer_328031, transform_features_layer_328041, transform_features_layer_328051, model_dense_4_biasadd_readvariableop_resource in the SavedModel.\nINFO:absl:Feature company has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature dropoff_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature fare has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature payment_type has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_census_tract has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_community_area has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_latitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature pickup_longitude has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature tips has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_miles has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_seconds has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_day has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_hour has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_month has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:Feature trip_start_timestamp has no shape. Setting to varlen_sparse_tensor.\nINFO:absl:eval_transformed_features = {'trip_miles_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:12' shape=(None,) dtype=float32>, 'pickup_community_area_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:8' shape=(None, 66) dtype=float32>, 'dropoff_longitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:4' shape=(None,) dtype=float32>, 'payment_type_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:6' shape=(None, 16) dtype=float32>, 'fare_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:5' shape=(None,) dtype=float32>, 'pickup_longitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:10' shape=(None,) dtype=float32>, 'trip_seconds_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:13' shape=(None,) dtype=float32>, 'dropoff_census_tract_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:1' shape=(None, 216) dtype=float32>, 'pickup_census_tract_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:7' shape=(None, 11) dtype=float32>, 'trip_start_day_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:14' shape=(None, 17) dtype=float32>, 'dropoff_community_area_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:2' shape=(None, 79) dtype=float32>, 'pickup_latitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:9' shape=(None,) dtype=float32>, 'dropoff_latitude_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:3' shape=(None,) dtype=float32>, 'trip_start_month_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:16' shape=(None, 22) dtype=float32>, 'trip_start_hour_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:15' shape=(None, 34) dtype=float32>, 'company_xf': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:0' shape=(None, 55) dtype=float32>, 'tips': <tf.Tensor 'transform_features_layer/StatefulPartitionedCall:11' shape=(None,) dtype=int64>}\nINFO:absl:Function `transform_features_fn` contains input name(s) 328346, 328356, 328366, 328376, 328386, 328396, 328406, 328416, 328426 with unsupported characters which will be renamed to transform_features_layer_328346, transform_features_layer_328356, transform_features_layer_328366, transform_features_layer_328376, transform_features_layer_328386, transform_features_layer_328396, transform_features_layer_328406, transform_features_layer_328416, transform_features_layer_328426 in the SavedModel.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Trainer/model/6/Format-Serving/assets\nINFO:absl:Writing fingerprint to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Trainer/model/6/Format-Serving/fingerprint.pb\nINFO:absl:Training complete. Model written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Trainer/model/6/Format-Serving. ModelRun written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Trainer/model_run/6\nINFO:absl:Running publisher for Trainer\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\n#### Analyze Training with TensorBoard\nTake a peek at the trainer artifact. It points to a directory containing the model subdirectories.\n```\nmodel_artifact_dir = trainer.outputs['model'].get()[0].uri\npp.pprint(os.listdir(model_artifact_dir))\nmodel_dir = os.path.join(model_artifact_dir, 'Format-Serving')\npp.pprint(os.listdir(model_dir))\n\n```\n```\n['Format-Serving']\n['saved_model.pb', 'variables', 'assets', 'fingerprint.pb', 'keras_metadata.pb']\n\n```\n\nOptionally, we can connect TensorBoard to the Trainer to analyze our model's training curves.\n```\nmodel_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n\n%load_ext tensorboard\n%tensorboard --logdir {model_run_artifact_dir}\n\n```\n\n### Evaluator\nThe `Evaluator` component computes model performance metrics over the evaluation set. It uses the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) library. The `Evaluator` can also optionally validate that a newly trained model is better than the previous model. This is useful in a production pipeline setting where you may automatically train and validate a model every day. In this notebook, we only train one model, so the `Evaluator` automatically will label the model as \"good\".\n`Evaluator` will take as input the data from `ExampleGen`, the trained model from `Trainer`, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values (e.g. how does your model perform on taxi trips that start at 8am versus 8pm?). See an example of this configuration below:\n```\n# Imported files such as taxi_constants are normally cached, so changes are\n# not honored after the first import.  Normally this is good for efficiency, but\n# during development when we may be iterating code it can be a problem. To\n# avoid this problem during development, reload the file.\nimporttaxi_constants\nimportsys\nif 'google.colab' in sys.modules:  # Testing to see if we're doing development\n  importimportlib\n  importlib.reload(taxi_constants)\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name: 'eval' and\n        # remove the label_key.\n        tfma.ModelSpec(\n            signature_name='serving_default',\n            label_key=taxi_constants.LABEL_KEY,\n            preprocessing_function_names=['transform_features'],\n            )\n        ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            # To add validation thresholds for metrics saved with the model,\n            # add them keyed by metric name to the thresholds map.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(class_name='BinaryAccuracy',\n                  threshold=tfma.MetricThreshold(\n                      value_threshold=tfma.GenericValueThreshold(\n                          lower_bound={'value': 0.5}),\n                      # Change threshold will be ignored if there is no\n                      # baseline model resolved from MLMD (first run).\n                      change_threshold=tfma.GenericChangeThreshold(\n                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                          absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(\n            feature_keys=['trip_start_hour'])\n    ])\n\n```\n\nNext, we give this configuration to `Evaluator` and run it.\n```\n# Use TFMA to compute a evaluation statistics over features of a model and\n# validate them against a baseline.\n\n# The model resolver is only required if performing model validation in addition\n# to evaluation. In this case we validate against the latest blessed model. If\n# no model has been blessed before (as in this case) the evaluator will make our\n# candidate the first blessed model.\nmodel_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\ncontext.run(model_resolver, enable_cache=True)\n\nevaluator = tfx.components.Evaluator(\n    examples=example_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=eval_config)\ncontext.run(evaluator, enable_cache=True)\n\n```\n```\nINFO:absl:Running driver for latest_blessed_model_resolver\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running publisher for latest_blessed_model_resolver\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running driver for Evaluator\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for Evaluator\nINFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"tips\",\\n      \"preprocessing_function_names\": [\\n        \"transform_features\"\\n      ],\\n      \"signature_name\": \"serving_default\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"trip_start_hour\"\\n      ]\\n    }\\n  ]\\n}', 'feature_slicing_spec': None, 'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'module_file': None, 'module_path': None} 'custom_eval_shared_model'\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  signature_name: \"serving_default\"\n  label_key: \"tips\"\n  preprocessing_function_names: \"transform_features\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"trip_start_hour\"\n}\nmetrics_specs {\n  metrics {\n    class_name: \"ExampleCount\"\n  }\n  metrics {\n    class_name: \"BinaryAccuracy\"\n    threshold {\n      value_threshold {\n        lower_bound {\n          value: 0.5\n        }\n      }\n    }\n  }\n}\n\nINFO:absl:Using /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Trainer/model/6/Format-Serving as  model.\nINFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\nINFO:absl:Evaluating model.\nINFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": -1e-10,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"tips\",\\n      \"preprocessing_function_names\": [\\n        \"transform_features\"\\n      ],\\n      \"signature_name\": \"serving_default\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"trip_start_hour\"\\n      ]\\n    }\\n  ]\\n}', 'feature_slicing_spec': None, 'fairness_indicator_thresholds': 'null', 'example_splits': 'null', 'module_file': None, 'module_path': None} 'custom_extractors'\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  signature_name: \"serving_default\"\n  label_key: \"tips\"\n  preprocessing_function_names: \"transform_features\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"trip_start_hour\"\n}\nmetrics_specs {\n  metrics {\n    class_name: \"ExampleCount\"\n  }\n  metrics {\n    class_name: \"BinaryAccuracy\"\n    threshold {\n      value_threshold {\n        lower_bound {\n          value: 0.5\n        }\n      }\n    }\n  }\n  model_names: \"\"\n}\n\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  signature_name: \"serving_default\"\n  label_key: \"tips\"\n  preprocessing_function_names: \"transform_features\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"trip_start_hour\"\n}\nmetrics_specs {\n  metrics {\n    class_name: \"ExampleCount\"\n  }\n  metrics {\n    class_name: \"BinaryAccuracy\"\n    threshold {\n      value_threshold {\n        lower_bound {\n          value: 0.5\n        }\n      }\n    }\n  }\n  model_names: \"\"\n}\n\nINFO:absl:eval_shared_models have model_types: {'tf_keras'}\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  signature_name: \"serving_default\"\n  label_key: \"tips\"\n  preprocessing_function_names: \"transform_features\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"trip_start_hour\"\n}\nmetrics_specs {\n  metrics {\n    class_name: \"ExampleCount\"\n  }\n  metrics {\n    class_name: \"BinaryAccuracy\"\n    threshold {\n      value_threshold {\n        lower_bound {\n          value: 0.5\n        }\n      }\n    }\n  }\n  model_names: \"\"\n}\n\nINFO:absl:Evaluation complete. Results written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Evaluator/evaluation/8.\nINFO:absl:Checking validation results.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nINFO:absl:Blessing result True written to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Evaluator/blessing/8.\nINFO:absl:Running publisher for Evaluator\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nNow let's examine the output artifacts of `Evaluator`.\n```\nevaluator.outputs\n\n```\n```\n{'evaluation': OutputChannel(artifact_type=ModelEvaluation, producer_component_id=Evaluator, output_key=evaluation, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False),\n 'blessing': OutputChannel(artifact_type=ModelBlessing, producer_component_id=Evaluator, output_key=blessing, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False)}\n\n```\n\nUsing the `evaluation` output we can show the default visualization of global metrics on the entire evaluation set.\n```\ncontext.show(evaluator.outputs['evaluation'])\n\n```\n```\nSlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'Overall', 'metrics':…\n\n```\n\nTo see the visualization for sliced evaluation metrics, we can directly call the TensorFlow Model Analysis library.\n```\nimporttensorflow_model_analysisastfma\n\n# Get the TFMA output result path and load the result.\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\ntfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n\n# Show data sliced along feature column trip_start_hour.\ntfma.view.render_slicing_metrics(\n    tfma_result, slicing_column='trip_start_hour')\n\n```\n```\nSlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'trip_start_hour:19',…\n\n```\n\nThis visualization shows the same metrics, but computed at every feature value of `trip_start_hour` instead of on the entire evaluation set.\nTensorFlow Model Analysis supports many other visualizations, such as Fairness Indicators and plotting a time series of model performance. To learn more, see [the tutorial](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic).\nSince we added thresholds to our config, validation output is also available. The precence of a `blessing` artifact indicates that our model passed validation. Since this is the first validation being performed the candidate is automatically blessed.\n```\nblessing_uri = evaluator.outputs['blessing'].get()[0].uri\n!ls -l {blessing_uri}\n\n```\n```\ntotal 0\n-rw-rw-r-- 1 kbuilder kbuilder 0 Aug  2 09:28 BLESSED\n\n```\n\nNow can also verify the success by loading the validation result record:\n```\nPATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\nprint(tfma.load_validation_result(PATH_TO_RESULT))\n\n```\n```\nvalidation_ok: true\nvalidation_details {\n  slicing_details {\n    slicing_spec {\n    }\n    num_matching_slices: 25\n  }\n}\n\n```\n\n### Pusher\nThe `Pusher` component is usually at the end of a TFX pipeline. It checks whether a model has passed validation, and if so, exports the model to `_serving_model_dir`.\n```\npusher = tfx.components.Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(\n        filesystem=tfx.proto.PushDestination.Filesystem(\n            base_directory=_serving_model_dir)))\ncontext.run(pusher, enable_cache=True)\n\n```\n```\nINFO:absl:Running driver for Pusher\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Running executor for Pusher\nINFO:absl:Model version: 1722590889\nINFO:absl:Model written to serving path /tmpfs/tmp/tmpyd4qezdk/serving_model/taxi_simple/1722590889.\nINFO:absl:Model pushed to /tmpfs/tmp/tfx-interactive-2024-08-02T09_25_22.884962-abik3cle/Pusher/pushed_model/9.\nINFO:absl:Running publisher for Pusher\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nLet's examine the output artifacts of `Pusher`.\n```\npusher.outputs\n\n```\n```\n{'pushed_model': OutputChannel(artifact_type=PushedModel, producer_component_id=Pusher, output_key=pushed_model, additional_properties={}, additional_custom_properties={}, _input_trigger=None, _is_async=False)}\n\n```\n\nIn particular, the Pusher will export your model in the SavedModel format, which looks like this:\n```\npush_uri = pusher.outputs['pushed_model'].get()[0].uri\nmodel = tf.saved_model.load(push_uri)\n\nfor item in model.signatures.items():\n  pp.pprint(item)\n\n```\n```\n('serving_default',\n <ConcreteFunction (*, examples: TensorSpec(shape=(None,), dtype=tf.string, name='examples')) -> Dict[['outputs', TensorSpec(shape=(None, 1), dtype=tf.float32, name='outputs')]] at 0x7FA96012E850>)\n('transform_features',\n <ConcreteFunction (*, examples: TensorSpec(shape=(None,), dtype=tf.string, name='examples')) -> Dict[['pickup_longitude_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='pickup_longitude_xf')], ['trip_miles_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='trip_miles_xf')], ['dropoff_community_area_xf', TensorSpec(shape=(None, 79), dtype=tf.float32, name='dropoff_community_area_xf')], ['trip_start_day_xf', TensorSpec(shape=(None, 17), dtype=tf.float32, name='trip_start_day_xf')], ['tips', TensorSpec(shape=(None,), dtype=tf.int64, name='tips')], ['trip_start_hour_xf', TensorSpec(shape=(None, 34), dtype=tf.float32, name='trip_start_hour_xf')], ['dropoff_longitude_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='dropoff_longitude_xf')], ['company_xf', TensorSpec(shape=(None, 55), dtype=tf.float32, name='company_xf')], ['dropoff_latitude_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='dropoff_latitude_xf')], ['payment_type_xf', TensorSpec(shape=(None, 16), dtype=tf.float32, name='payment_type_xf')], ['pickup_community_area_xf', TensorSpec(shape=(None, 66), dtype=tf.float32, name='pickup_community_area_xf')], ['fare_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='fare_xf')], ['pickup_census_tract_xf', TensorSpec(shape=(None, 11), dtype=tf.float32, name='pickup_census_tract_xf')], ['dropoff_census_tract_xf', TensorSpec(shape=(None, 216), dtype=tf.float32, name='dropoff_census_tract_xf')], ['pickup_latitude_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='pickup_latitude_xf')], ['trip_seconds_xf', TensorSpec(shape=(None,), dtype=tf.float32, name='trip_seconds_xf')], ['trip_start_month_xf', TensorSpec(shape=(None, 22), dtype=tf.float32, name='trip_start_month_xf')]] at 0x7FA9600DDDC0>)\n\n```\n\nWe're finished our tour of built-in TFX components!\n",
  "https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)  \n---  \n**_An Example of a Key Component of TensorFlow Extended (TFX)_**\n[TensorFlow Model Analysis (TFMA)](https://www.tensorflow.org/tfx/guide/tfma) is a library for performing model evaluation across different slices of data. TFMA performs its computations in a distributed manner over large amounts of data using \nThis example colab notebook illustrates how TFMA can be used to investigate and visualize the performance of a model with respect to characteristics of the dataset. We'll use a model that we trained previously, and now you get to play with the results! The model we trained was for the \nAs a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about \nThe columns in the dataset are: \npickup_community_area | fare | trip_start_month  \n---|---|---  \ntrip_start_hour | trip_start_day | trip_start_timestamp  \npickup_latitude | pickup_longitude | dropoff_latitude  \ndropoff_longitude | trip_miles | pickup_census_tract  \ndropoff_census_tract | payment_type | company  \ntrip_seconds | dropoff_community_area | tips  \n## Install Jupyter Extensions\n```\njupyterenableenable\n```\n\n## Install TensorFlow Model Analysis (TFMA)\nThis will pull in all the dependencies, and will take a minute.\n```\n# Upgrade pip to the latest, and install TFMA.\npip\npip\n```\n\n**Now you must restart the runtime before running the cells below.**\n```\n# This setup was tested with TF 2.10 and TFMA 0.41 (using colab), but it should\n# also work with the latest release.\nimportsys\n\n# Confirm that we're using Python 3\nassert sys.version_info.major==3, 'This notebook must be run using Python 3.'\n\nimporttensorflowastf\nprint('TF version: {}'.format(tf.__version__))\nimportapache_beamasbeam\nprint('Beam version: {}'.format(beam.__version__))\nimporttensorflow_model_analysisastfma\nprint('TFMA version: {}'.format(tfma.__version__))\n\n```\n```\n2024-04-30 10:58:28.448131: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:58:28.448179: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:58:28.449816: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTF version: 2.15.1\nBeam version: 2.55.1\nTFMA version: 0.46.0\n\n```\n\n## Load The Files\nWe'll download a tar file that has everything we need. That includes:\n  * Training and evaluation datasets\n  * Data schema\n  * Training and serving saved models (keras and estimator) and eval saved models (estimator).\n\n```\n# Download the tar file from GCP and extract it\nimportio,os,tempfile\nTAR_NAME = 'saved_models-2.2'\nBASE_DIR = tempfile.mkdtemp()\nDATA_DIR = os.path.join(BASE_DIR, TAR_NAME, 'data')\nMODELS_DIR = os.path.join(BASE_DIR, TAR_NAME, 'models')\nSCHEMA = os.path.join(BASE_DIR, TAR_NAME, 'schema.pbtxt')\nOUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n\n!curl -O https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/{TAR_NAME}.tar\n!tar xf {TAR_NAME}.tar\n!mv {TAR_NAME} {BASE_DIR}\n!rm {TAR_NAME}.tar\n\nprint(\"Here's what we downloaded:\")\n!ls -R {BASE_DIR}\n\n```\n```\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 6800k  100 6800k    0     0  26.9M      0 --:--:-- --:--:-- --:--:-- 26.9M\nHere's what we downloaded:\n/tmpfs/tmp/tmpo4hj24ht:\nsaved_models-2.2\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2:\ndata  models  schema.pbtxt\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/data:\neval  train\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/data/eval:\ndata.csv\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/data/train:\ndata.csv\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models:\nestimator  keras\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator:\neval_model_dir  serving_model_dir\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/eval_model_dir:\n1591221811\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/eval_model_dir/1591221811:\nsaved_model.pb  tmp.pbtxt  variables\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/eval_model_dir/1591221811/variables:\nvariables.data-00000-of-00001  variables.index\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir:\ncheckpoint\neval_chicago-taxi-eval\nevents.out.tfevents.1591221780.my-pipeline-b57vp-237544850\nexport\ngraph.pbtxt\nmodel.ckpt-100.data-00000-of-00001\nmodel.ckpt-100.index\nmodel.ckpt-100.meta\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir/eval_chicago-taxi-eval:\nevents.out.tfevents.1591221799.my-pipeline-b57vp-237544850\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir/export:\nchicago-taxi\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir/export/chicago-taxi:\n1591221801\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir/export/chicago-taxi/1591221801:\nsaved_model.pb  variables\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/serving_model_dir/export/chicago-taxi/1591221801/variables:\nvariables.data-00000-of-00001  variables.index\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras:\n0  1  2\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/0:\nsaved_model.pb  variables\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/0/variables:\nvariables.data-00000-of-00001  variables.index\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/1:\nsaved_model.pb  variables\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/1/variables:\nvariables.data-00000-of-00001  variables.index\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/2:\nsaved_model.pb  variables\n\n/tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/keras/2/variables:\nvariables.data-00000-of-00001  variables.index\n\n```\n\n## Parse the Schema\nAmong the things we downloaded was a schema for our data that was created by [TensorFlow Data Validation](https://www.tensorflow.org/tfx/data_validation/get_started/). Let's parse that now so that we can use it with TFMA.\n```\nimporttensorflowastf\nfromgoogle.protobufimport text_format\nfromtensorflow.python.lib.ioimport file_io\nfromtensorflow_metadata.proto.v0import schema_pb2\nfromtensorflow.core.exampleimport example_pb2\n\nschema = schema_pb2.Schema()\ncontents = file_io.read_file_to_string(SCHEMA)\nschema = text_format.Parse(contents, schema)\n\n```\n\n## Use the Schema to Create TFRecords\nWe need to give TFMA access to our dataset, so let's create a TFRecords file. We can use our schema to create it, since it gives us the correct type for each feature.\n```\nimportcsv\n\ndatafile = os.path.join(DATA_DIR, 'eval', 'data.csv')\nreader = csv.DictReader(open(datafile, 'r'))\nexamples = []\nfor line in reader:\n  example = example_pb2.Example()\n  for feature in schema.feature:\n    key = feature.name\n    if feature.type == schema_pb2.FLOAT:\n      example.features.feature[key].float_list.value[:] = (\n          [float(line[key])] if len(line[key]) > 0 else [])\n    elif feature.type == schema_pb2.INT:\n      example.features.feature[key].int64_list.value[:] = (\n          [int(line[key])] if len(line[key]) > 0 else [])\n    elif feature.type == schema_pb2.BYTES:\n      example.features.feature[key].bytes_list.value[:] = (\n          [line[key].encode('utf8')] if len(line[key]) > 0 else [])\n  # Add a new column 'big_tipper' that indicates if tips was > 20% of the fare. \n  # TODO(b/157064428): Remove after label transformation is supported for Keras.\n  big_tipper = float(line['tips']) > float(line['fare']) * 0.2\n  example.features.feature['big_tipper'].float_list.value[:] = [big_tipper]\n  examples.append(example)\n\ntfrecord_file = os.path.join(BASE_DIR, 'train_data.rio')\nwith tf.io.TFRecordWriter(tfrecord_file) as writer:\n  for example in examples:\n    writer.write(example.SerializeToString())\n\n!ls {tfrecord_file}\n\n```\n```\n/tmpfs/tmp/tmpo4hj24ht/train_data.rio\n\n```\n\n## Setup and Run TFMA\nTFMA supports a number of different model types including TF keras models, models based on generic TF2 signature APIs, as well TF estimator based models. The [get_started](https://www.tensorflow.org/tfx/model_analysis/get_started) guide has the full list of model types supported and any restrictions. For this example we are going to show how to configure a keras based model as well as an estimator based model that was saved as an [`EvalSavedModel`](https://www.tensorflow.org/tfx/model_analysis/eval_saved_model). See the [FAQ](https://www.tensorflow.org/tfx/model_analysis/faq) for examples of other configurations.\nTFMA provides support for calculating metrics that were used at training time (i.e. built-in metrics) as well metrics defined after the model was saved as part of the TFMA configuration settings. For our keras [setup](https://www.tensorflow.org/tfx/model_analysis/setup) we will demonstrate adding our metrics and plots manually as part of our configuration (see the [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) guide for information on the metrics and plots that are supported). For the estimator setup we will use the built-in metrics that were saved with the model. Our setups also include a number of slicing specs which are discussed in more detail in the following sections.\nAfter creating a [`tfma.EvalConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig) and [`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalSharedModel) we can then run TFMA using [`tfma.run_model_analysis`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis). This will create a [`tfma.EvalResult`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult) which we can use later for rendering our metrics and plots.\n### Keras\n```\nimporttensorflow_model_analysisastfma\n\n# Setup tfma.EvalConfig settings\nkeras_eval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # For keras (and serving models) we need to add a `label_key`.\n    label_key: \"big_tipper\"\n  }\n\n  ## Post training metric information. These will be merged with any built-in\n  ## metrics from training.\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"Precision\" }\n    metrics { class_name: \"Recall\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"CalibrationPlot\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_values: {\n      key: \"trip_start_month\"\n      value: \"1\"\n    }\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create a tfma.EvalSharedModel that points at our keras model.\nkeras_model_path = os.path.join(MODELS_DIR, 'keras', '2')\nkeras_eval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=keras_model_path,\n    eval_config=keras_eval_config)\n\nkeras_output_path = os.path.join(OUTPUT_DIR, 'keras')\n\n# Run TFMA\nkeras_eval_result = tfma.run_model_analysis(\n    eval_shared_model=keras_eval_shared_model,\n    eval_config=keras_eval_config,\n    data_location=tfrecord_file,\n    output_path=keras_output_path)\n\n```\n```\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\n\n```\n\n### Estimator\n```\nimporttensorflow_model_analysisastfma\n\n# Setup tfma.EvalConfig settings\nestimator_eval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # To use EvalSavedModel set `signature_name` to \"eval\".\n    signature_name: \"eval\"\n  }\n\n  ## Post training metric information. These will be merged with any built-in\n  ## metrics from training.\n  metrics_specs {\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_values: {\n      key: \"trip_start_month\"\n      value: \"1\"\n    }\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create a tfma.EvalSharedModel that points at our eval saved model.\nestimator_base_model_path = os.path.join(\n    MODELS_DIR, 'estimator', 'eval_model_dir')\nestimator_model_path = os.path.join(\n    estimator_base_model_path, os.listdir(estimator_base_model_path)[0])\nestimator_eval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path=estimator_model_path,\n    eval_config=estimator_eval_config)\n\nestimator_output_path = os.path.join(OUTPUT_DIR, 'estimator')\n\n# Run TFMA\nestimator_eval_result = tfma.run_model_analysis(\n    eval_shared_model=estimator_eval_shared_model,\n    eval_config=estimator_eval_config,\n    data_location=tfrecord_file,\n    output_path=estimator_output_path)\n\n```\n```\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/eval_saved_model/load.py:163: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.saved_model.load` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/eval_saved_model/load.py:163: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.saved_model.load` instead.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/eval_model_dir/1591221811/variables/variables\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpo4hj24ht/saved_models-2.2/models/estimator/eval_model_dir/1591221811/variables/variables\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:184: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n2024-04-30 10:58:52.926791: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_INT64\n    }\n  }\n}\n is neither a subtype nor a supertype of the combined inputs preceding it:\ntype_id: TFT_OPTIONAL\nargs {\n  type_id: TFT_PRODUCT\n  args {\n    type_id: TFT_TENSOR\n    args {\n      type_id: TFT_INT32\n    }\n  }\n}\n\n    for Tuple type infernce function 0\n    while inferring type of node 'dnn/zero_fraction/cond/output/_9'\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/eval_saved_model/graph_ref.py:184: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n2024-04-30 10:58:53.077553: W tensorflow/c/c_api.cc:305] Operation '{name:'head/metrics/true_positives_1/Assign' id:674 op device:{requested: '', assigned: ''} def:{ { {node head/metrics/true_positives_1/Assign} } = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](head/metrics/true_positives_1, head/metrics/true_positives_1/Initializer/zeros)} }' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n2024-04-30 10:58:53.204776: W tensorflow/c/c_api.cc:305] Operation '{name:'head/metrics/true_positives_1/Assign' id:674 op device:{requested: '', assigned: ''} def:{ { {node head/metrics/true_positives_1/Assign} } = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](head/metrics/true_positives_1, head/metrics/true_positives_1/Initializer/zeros)} }' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n\n```\n\n## Visualizing Metrics and Plots\nNow that we've run the evaluation, let's take a look at our visualizations using TFMA. For the following examples, we will visualize the results from running the evaluation on the keras model. To view the estimator based model update the `eval_result_path` to point at our `estimator_output_path` variable.\n```\neval_result_path = keras_output_path\n# eval_result_path = estimator_output_path\n\neval_result = keras_eval_result\n# eval_result = estimator_eval_result\n\n```\n\n### Rendering Metrics\nTFMA provides dataframe APIs in [`tfma.experimental.dataframe`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/experimental) to load the materalized output as `metrics_as_dataframes(tfma.load_metrics(eval_path))`, which returns an object which potentially contains several DataFrames, one for each metric value type (`double_value`, `confusion_matrix_at_thresholds`, `bytes_value`, and `array_value`). The specific DataFrames populated depends on the eval result. Here, we show the `double_value` DataFrame as an example.\n```\nimporttensorflow_model_analysis.experimental.dataframeastfma_dataframe\ndfs = tfma_dataframe.metrics_as_dataframes(\n  tfma.load_metrics(eval_result_path))\n\ndisplay(dfs.double_value.head())\n\n```\n\nEach of the DataFrames has a column multi-index with the top-level columns: `slices`, `metric_keys`, and `metric_values`. The exact columns of each group can change according to the payload. we can use `DataFrame.columns` API to inspect all the multi-index columns. For example, the slices columns are 'Overall', 'trip_start_day', 'trip_start_hour', and 'trip_start_month', which is configured by the `slicing_specs` in the `eval_config`.\n```\nprint(dfs.double_value.columns)\n\n```\n```\nMultiIndex([(       'slices',  'trip_start_hour'),\n            (       'slices',          'Overall'),\n            (       'slices',   'trip_start_day'),\n            (       'slices', 'trip_start_month'),\n            (  'metric_keys',             'name'),\n            (  'metric_keys',       'model_name'),\n            (  'metric_keys',      'output_name'),\n            (  'metric_keys', 'example_weighted'),\n            (  'metric_keys',          'is_diff'),\n            ('metric_values',     'double_value')],\n           )\n\n```\n\n### Auto pivoting\nThe DataFrame is verbose by design so that there is no loss of information from the payload. However, sometimes, for direct consumption, we might want to organize the information in a more concise but lossy form: slices as rows and metrics as columns. TFMA provides an `auto_pivot` API for this purpose. The util pivots on all of the non-unique columns inside `metric_keys`, and condenses all the slices into one `stringified_slices` column by default.\n```\ntfma_dataframe.auto_pivot(dfs.double_value).head()\n\n```\n\n### Filtering slices\nSince the outputs are DataFrames, any native DataFrame APIs can be used to slice and dice the DataFrame. For example, if we are only interested in `trip_start_hour` of 1, 3, 5, 7 and not in `trip_start_day`, we can use DataFrame's `.loc` filtering logic. Again, we use the `auto_pivot` function to re-organize the DataFrame in the slice vs. metrics view after the filtering is performed.\n```\ndf_double = dfs.double_value\ndf_filtered = (df_double\n  .loc[df_double.slices.trip_start_hour.isin([1,3,5,7])]\n)\ndisplay(tfma_dataframe.auto_pivot(df_filtered))\n\n```\n\n### Sorting by metric values\nWe can also sort slices by metrics value. As an example, we show how to sort slices in the above DataFrame by ascending AUC, so that we can find poorly performing slices. This involves two steps: auto-pivoting so that slices are represented as rows and columns are metrics, and then sorting the pivoted DataFrame by the AUC column.\n```\n# Pivoted table sorted by AUC in ascending order.\ndf_sorted = (\n    tfma_dataframe.auto_pivot(df_double)\n    .sort_values(by='auc', ascending=True)\n    )\ndisplay(df_sorted.head())\n\n```\n\n### Rendering Plots\nAny plots that were added to the [`tfma.EvalConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig) as post training `metric_specs` can be displayed using [`tfma.view.render_plot`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_plot).\nAs with metrics, plots can be viewed by slice. Unlike metrics, only plots for a particular slice value can be displayed so the [`tfma.SlicingSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec) must be used and it must specify both a slice feature name and value. If no slice is provided then the plots for the `Overall` slice is used.\nIn the example below we are displaying the `CalibrationPlot` and `ConfusionMatrixPlot` plots that were computed for the `trip_start_hour:1` slice.\n```\ntfma.view.render_plot(\n    eval_result,\n    tfma.SlicingSpec(feature_values={'trip_start_hour': '1'}))\n\n```\n```\nPlotViewer(config={'sliceName': 'trip_start_hour:1', 'metricKeys': {'calibrationPlot': {'metricName': 'calibra…\n\n```\n\n## Tracking Model Performance Over Time\nYour training dataset will be used for training your model, and will hopefully be representative of your test dataset and the data that will be sent to your model in production. However, while the data in inference requests may remain the same as your training data, in many cases it will start to change enough so that the performance of your model will change.\nThat means that you need to monitor and measure your model's performance on an ongoing basis, so that you can be aware of and react to changes. Let's take a look at how TFMA can help.\nLet's load 3 different model runs and use TFMA to see how they compare using [`render_time_series`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_time_series).\n```\n# Note this re-uses the EvalConfig from the keras setup.\n\n# Run eval on each saved model\noutput_paths = []\nfor i in range(3):\n  # Create a tfma.EvalSharedModel that points at our saved model.\n  eval_shared_model = tfma.default_eval_shared_model(\n      eval_saved_model_path=os.path.join(MODELS_DIR, 'keras', str(i)),\n      eval_config=keras_eval_config)\n\n  output_path = os.path.join(OUTPUT_DIR, 'time_series', str(i))\n  output_paths.append(output_path)\n\n  # Run TFMA\n  tfma.run_model_analysis(eval_shared_model=eval_shared_model,\n                          eval_config=keras_eval_config,\n                          data_location=tfrecord_file,\n                          output_path=output_path)\n\n```\n```\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\n\n```\n\nFirst, we'll imagine that we've trained and deployed our model yesterday, and now we want to see how it's doing on the new data coming in today. The visualization will start by displaying AUC. From the UI you can:\n  * Add other metrics using the \"Add metric series\" menu.\n  * Close unwanted graphs by clicking on x\n  * Hover over data points (the ends of line segments in the graph) to get more details\n\n```\neval_results_from_disk = tfma.load_eval_results(output_paths[:2])\n\ntfma.view.render_time_series(eval_results_from_disk)\n\n```\n```\nTimeSeriesViewer(config={'isModelCentric': True}, data=[{'metrics': {'': {'': {'example_count': {'doubleValue'…\n\n```\n\nNow we'll imagine that another day has passed and we want to see how it's doing on the new data coming in today, compared to the previous two days:\n```\neval_results_from_disk = tfma.load_eval_results(output_paths)\n\ntfma.view.render_time_series(eval_results_from_disk)\n\n```\n```\nTimeSeriesViewer(config={'isModelCentric': True}, data=[{'metrics': {'': {'': {'example_count': {'doubleValue'…\n\n```\n\n## Model Validation\nTFMA can be configured to evaluate multiple models at the same time. Typically this is done to compare a new model against a baseline (such as the currently serving model) to determine what the performance differences in metrics (e.g. AUC, etc) are relative to the baseline. When [thresholds](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricThreshold) are configured, TFMA will produce a [`tfma.ValidationResult`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ValidationResult) record indicating whether the performance matches expecations.\nLet's re-configure our keras evaluation to compare two models: a candidate and a baseline. We will also validate the candidate's performance against the baseline by setting a [`tmfa.MetricThreshold`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricThreshold) on the AUC metric.\n```\n# Setup tfma.EvalConfig setting\neval_config_with_thresholds = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    name: \"candidate\"\n    # For keras we need to add a `label_key`.\n    label_key: \"big_tipper\"\n  }\n  model_specs {\n    name: \"baseline\"\n    # For keras we need to add a `label_key`.\n    label_key: \"big_tipper\"\n    is_baseline: true\n  }\n\n  ## Post training metric information\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"BinaryAccuracy\" }\n    metrics { class_name: \"BinaryCrossentropy\" }\n    metrics {\n      class_name: \"AUC\"\n      threshold {\n        # Ensure that AUC is always > 0.9\n        value_threshold {\n          lower_bound { value: 0.9 }\n\n        # Ensure that AUC does not drop by more than a small epsilon\n        # e.g. (candidate - baseline) > -1e-10 or candidate > baseline - 1e-10\n        change_threshold {\n          direction: HIGHER_IS_BETTER\n          absolute { value: -1e-10 }\n\n\n    }\n    metrics { class_name: \"AUCPrecisionRecall\" }\n    metrics { class_name: \"Precision\" }\n    metrics { class_name: \"Recall\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"CalibrationPlot\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    # ... add additional metrics and plots ...\n  }\n\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_day\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_month\"]\n  }\n  slicing_specs {\n    feature_keys: [\"trip_start_hour\", \"trip_start_day\"]\n  }\n\"\"\", tfma.EvalConfig())\n\n# Create tfma.EvalSharedModels that point at our keras models.\ncandidate_model_path = os.path.join(MODELS_DIR, 'keras', '2')\nbaseline_model_path = os.path.join(MODELS_DIR, 'keras', '1')\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path=candidate_model_path,\n      eval_config=eval_config_with_thresholds),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path=baseline_model_path,\n      eval_config=eval_config_with_thresholds),\n]\n\nvalidation_output_path = os.path.join(OUTPUT_DIR, 'validation')\n\n# Run TFMA\neval_result_with_validation = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config_with_thresholds,\n    data_location=tfrecord_file,\n    output_path=validation_output_path)\n\n```\n```\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Tensorflow version (2.15.1) found. Note that TFMA support for TF 2.0 is currently in beta\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py:528: RuntimeWarning: invalid value encountered in divide\n  prec_slope = dtp / np.maximum(dp, 0)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py:532: RuntimeWarning: divide by zero encountered in divide\n  p[:num_thresholds - 1] / np.maximum(p[1:], 0), np.ones_like(p[1:]))\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py:532: RuntimeWarning: invalid value encountered in divide\n  p[:num_thresholds - 1] / np.maximum(p[1:], 0), np.ones_like(p[1:]))\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:152: RuntimeWarning: invalid value encountered in divide\n  f1 = 2 * precision * recall / (precision + recall)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/binary_confusion_matrices.py:155: RuntimeWarning: invalid value encountered in divide\n  false_omission_rate = fn / predicated_negatives\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py:539: RuntimeWarning: invalid value encountered in divide\n  recall = tp / (tp + fn)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/metrics/confusion_matrix_metrics.py:534: RuntimeWarning: invalid value encountered in divide\n  prec_slope * (dtp + intercept * np.log(safe_p_ratio)) /\n\n```\n\nWhen running evaluations with one or more models against a baseline, TFMA automatically adds diff metrics for all of the metrics computed during the evaluation. These metrics are named after the corresponding metric but with `_diff` appended to the metric name.\nLet's take a look at the metrics produced by our run:\n```\ntfma.view.render_time_series(eval_result_with_validation)\n\n```\n```\nTimeSeriesViewer(config={'isModelCentric': True}, data=[{'metrics': {'': {'': {'binary_crossentropy': {'double…\n\n```\n\nNow let's look at the output from our validation checks. To view the validation results we use [`tfma.load_validator_result`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_validation_result). For our example, the validation fails because AUC is below the threshold.\n```\nvalidation_result = tfma.load_validation_result(validation_output_path)\nprint(validation_result.validation_ok)\n\n```\n```\nFalse\n\n```\n\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines": "## Introduction\nThis tutorial is designed to introduce [TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx) and \nAt the end of this tutorial, you will have created and run an ML Pipeline, hosted on Google Cloud. You'll be able to visualize the results of each run, and view the lineage of the created artifacts.\nYou'll follow a typical ML development process, starting by examining the dataset, and ending up with a complete working pipeline. Along the way you'll explore ways to debug and update your pipeline, and measure performance.\n### Chicago Taxi Dataset\nYou're using the \nYou can \n#### Model Goal - Binary classification\nWill the customer tip more or less than 20%?\n## 1. Set up a Google Cloud project\n### 1.a Set up your environment on Google Cloud\nTo get started, you need a Google Cloud Account. If you already have one, skip ahead to [Create New Project](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines#create_project).\n  1. Go to the \n  2. Agree to Google Cloud terms and conditions\n  3. If you would like to start with a free trial account, click on \n    1. Select your country.\n    2. Agree to the terms of service.\n    3. Enter billing details.\nYou will not be charged at this point. If you have no other Google Cloud projects, you can complete this tutorial without exceeding the \n\n\n### 1.b Create a new project. \n  1. From the **Google Cloud Platform** header, and select **New Project**.\n  2. Give your project a name and enter other project details\n  3. **Once you have created a project, make sure to select it from the project drop-down.**\n\n\n## 2. Set up and deploy an AI Platform Pipeline on a new Kubernetes cluster\n  1. Go to the \nUnder the Main Navigation Menu: ≡ > AI Platform > Pipelines\n  2. Click **+ New Instance** to create a new cluster.\n  3. On the **Kubeflow Pipelines** overview page, click **Configure**.\n  4. Click \"Enable\" to enable the Kubernetes Engine API\n  5. On the **Deploy Kubeflow Pipelines** page:\n    1. Select a \n    2. **IMPORTANT** Check the box labeled _Allow access to the following cloud APIs_. (This is required for this cluster to access the other pieces of your project. If you miss this step, fixing it later is a bit tricky.)\n    3. Click **Create New Cluster** , and wait several minutes until the cluster has been created. This will take a few minutes. When it completes you will see a message like:\n> Cluster \"cluster-1\" successfully created in zone \"us-central1-a\".\n    4. Select a namespace and instance name (using the defaults is fine). For the purposes of this tutorial do not check _executor.emissary_ or _managedstorage.enabled_.\n    5. Click **Deploy** , and wait several moments until the pipeline has been deployed. By deploying Kubeflow Pipelines, you accept the Terms of Service.\n\n\n## 3. Set up Cloud AI Platform Notebook instance.\n  1. Go to the \nUnder the Main Navigation Menu: ≡ -> Vertex AI -> Workbench\n  2. If prompted, enable the Compute Engine API.\n  3. Create a **New Notebook** with TensorFlow Enterprise 2.7 (or above) installed.\nNew Notebook -> TensorFlow Enterprise 2.7 -> Without GPU\nSelect a region and zone, and give the notebook instance a name.\nTo stay within the Free Tier limits, you may need to change the default settings here to reduce the number of vCPUs available to this instance from 4 to 2:\n    1. Select **Advanced Options** at the bottom of the **New notebook** form.\n    2. Under **Machine configuration** you may want to select a configuration with 1 or 2 vCPUs if you need to stay in the free tier.\n    3. Wait for the new notebook to be created, and then click **Enable Notebooks API**\n\n\n## 4. Launch the Getting Started Notebook\n  1. Go to the \nUnder the Main Navigation Menu: ≡ -> AI Platform -> Pipelines\n  2. On the line for the cluster you are using in this tutorial, click **Open Pipelines Dashboard**.\n  3. On the **Getting Started** page, click **Open a Cloud AI Platform Notebook on Google Cloud**.\n  4. Select the Notebook instance you are using for this tutorial and **Continue** , and then **Confirm**.\n\n\n## 5. Continue working in the Notebook\n### Install\nThe Getting Started Notebook starts by installing [TFX](https://www.tensorflow.org/tfx) and \nIt then checks which version of TFX is installed, does an import, and sets and prints the Project ID:\n### Connect with your Google Cloud services\nThe pipeline configuration needs your project ID, which you can get through the notebook and set as an environmental variable.\n```\n# Read GCP project id from env.\nshell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\nGCP_PROJECT_ID=shell_output[0]\nprint(\"GCP project ID:\" + GCP_PROJECT_ID)\n\n```\n\nNow set your KFP cluster endpoint.\nThis can be found from the URL of the Pipelines dashboard. Go to the Kubeflow Pipeline dashboard and look at the URL. The endpoint is everything in the URL _starting with_ the `https://`, _up to, and including_ , `googleusercontent.com`.\n```\nENDPOINT='' # Enter YOUR ENDPOINT here.\n\n```\n\nThe notebook then sets a unique name for the custom Docker image:\n```\n# Docker image name for the pipeline image\nCUSTOM_TFX_IMAGE='gcr.io/' + GCP_PROJECT_ID + '/tfx-pipeline'\n\n```\n\n## 6. Copy a template into your project directory\nEdit the next notebook cell to set a name for your pipeline. In this tutorial we will use `my_pipeline`.\n```\nPIPELINE_NAME=\"my_pipeline\"\nPROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"imported\",PIPELINE_NAME)\n\n```\n\nThe notebook then uses the `tfx` CLI to copy the pipeline template. This tutorial uses the Chicago Taxi dataset to perform binary classification, so the template sets the model to `taxi`:\n```\n!tfx template copy \\\n  --pipeline-name={PIPELINE_NAME} \\\n  --destination-path={PROJECT_DIR} \\\n  --model=taxi\n\n```\n\nThe notebook then changes its CWD context to the project directory:\n```\n%cd{PROJECT_DIR}\n\n```\n\n### Browse the pipeline files\nOn the left-hand side of the Cloud AI Platform Notebook, you should see a file browser. There should be a directory with your pipeline name (`my_pipeline`). Open it and view the files. (You'll be able to open them and edit from the notebook environment as well.)\n```\n# You can also list the files from the shell\n\n```\n\nThe `tfx template copy` command above created a basic scaffold of files that build a pipeline. These include Python source codes, sample data, and Jupyter notebooks. These are meant for this particular example. For your own pipelines these would be the supporting files that your pipeline requires.\nHere is brief description of the Python files.\n  * `pipeline` - This directory contains the definition of the pipeline \n    * `configs.py` — defines common constants for pipeline runners\n    * `pipeline.py` — defines TFX components and a pipeline\n  * `models` - This directory contains ML model definitions. \n    * `features.py` `features_test.py` — defines features for the model\n    * `preprocessing.py` / `preprocessing_test.py` — defines preprocessing jobs using `tf::Transform`\n    * `estimator` - This directory contains an Estimator based model. \n      * `constants.py` — defines constants of the model\n      * `model.py` / `model_test.py` — defines DNN model using TF estimator\n    * `keras` - This directory contains a Keras based model. \n      * `constants.py` — defines constants of the model\n      * `model.py` / `model_test.py` — defines DNN model using Keras\n  * `beam_runner.py` / `kubeflow_runner.py` — define runners for each orchestration engine\n\n\n## 7. Run your first TFX pipeline on Kubeflow\nThe notebook will run the pipeline using the `tfx run` CLI command.\n### Connect to storage\nRunning pipelines create artifacts which have to be stored in `<your-project-id>-kubeflowpipelines-default`.\n### Create the pipeline\nThe notebook will upload our sample data to GCS bucket so that we can use it in our pipeline later.\n```\ngsutil{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/tfx-template/data/taxi/data.csv\n```\n\nThe notebook then uses the `tfx pipeline create` command to create the pipeline.\n```\n!tfx pipeline create  \\\n--pipeline-path=kubeflow_runner.py \\\n--endpoint={ENDPOINT} \\\n--build-image\n\n```\n\nWhile creating a pipeline, `Dockerfile` will be generated to build a Docker image. Don't forget to add these files to your source control system (for example, git) along with other source files.\n### Run the pipeline\nThe notebook then uses the `tfx run create` command to start an execution run of your pipeline. You will also see this run listed under Experiments in the Kubeflow Pipelines Dashboard.\n```\ntfx={PIPELINE_NAME}={ENDPOINT}\n```\n\nYou can view your pipeline from the Kubeflow Pipelines Dashboard.\n## 8. Validate your data\nThe first task in any data science or ML project is to understand and clean the data.\n  * Understand the data types for each feature\n  * Look for anomalies and missing values\n  * Understand the distributions for each feature\n\n\n### Components\n  * [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) ingests and splits the input dataset.\n  * [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) calculates statistics for the dataset.\n  * [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) SchemaGen examines the statistics and creates a data schema.\n  * [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) looks for anomalies and missing values in the dataset.\n\n\n### In Jupyter lab file editor:\nIn `pipeline`/`pipeline.py`, uncomment the lines which append these components to your pipeline:\n```\n# components.append(statistics_gen)\n# components.append(schema_gen)\n# components.append(example_validator)\n\n```\n\n(`ExampleGen` was already enabled when the template files were copied.)\n### Update the pipeline and re-run it\n```\n# Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n\n```\n\n### Check the pipeline\nFor Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.\n### More advanced example\nThe example presented here is really only meant to get you started. For a more advanced example see the [TensorFlow Data Validation Colab](https://www.tensorflow.org/tfx/tutorials/data_validation/chicago_taxi).\nFor more information on using TFDV to explore and validate a dataset, [see the examples on tensorflow.org](https://www.tensorflow.org/tfx/data_validation).\n## 9. Feature engineering\nYou can increase the predictive quality of your data and/or reduce dimensionality with feature engineering.\n  * Feature crosses\n  * Vocabularies\n  * Embeddings\n  * PCA\n  * Categorical encoding\n\n\nOne of the benefits of using TFX is that you will write your transformation code once, and the resulting transforms will be consistent between training and serving.\n### Components\n  * [Transform](https://www.tensorflow.org/tfx/guide/transform) performs feature engineering on the dataset.\n\n\n### In Jupyter lab file editor:\nIn `pipeline`/`pipeline.py`, find and uncomment the line which appends [Transform](https://www.tensorflow.org/tfx/guide/transform) to the pipeline.\n```\n# components.append(transform)\n\n```\n\n### Update the pipeline and re-run it\n```\n# Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n\n```\n\n### Check pipeline outputs\nFor Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.\n### More advanced example\nThe example presented here is really only meant to get you started. For a more advanced example see the [TensorFlow Transform Colab](https://www.tensorflow.org/tfx/tutorials/transform/census).\n## 10. Training\nTrain a TensorFlow model with your nice, clean, transformed data.\n  * Include the transformations from the previous step so that they are applied consistently\n  * Save the results as a SavedModel for production\n  * Visualize and explore the training process using TensorBoard\n  * Also save an EvalSavedModel for analysis of model performance\n\n\n### Components\n  * [Trainer](https://www.tensorflow.org/tfx/guide/trainer) trains a TensorFlow model.\n\n\n### In Jupyter lab file editor:\nIn `pipeline`/`pipeline.py`, find and uncomment the which appends Trainer to the pipeline:\n```\n# components.append(trainer)\n\n```\n\n### Update the pipeline and re-run it\n```\n# Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n\n```\n\n### Check pipeline outputs\nFor Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.\n### More advanced example\nThe example presented here is really only meant to get you started. For a more advanced example see the [TensorBoard Tutorial](https://www.tensorflow.org/tensorboard/get_started).\n## 11. Analyzing model performance\nUnderstanding more than just the top level metrics.\n  * Users experience model performance for their queries only\n  * Poor performance on slices of data can be hidden by top level metrics\n  * Model fairness is important\n  * Often key subsets of users or data are very important, and may be small \n    * Performance in critical but unusual conditions\n    * Performance for key audiences such as influencers\n  * If you’re replacing a model that is currently in production, first make sure that the new one is better\n\n\n### Components\n  * [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) performs deep analysis of the training results.\n\n\n### In Jupyter lab file editor:\nIn `pipeline`/`pipeline.py`, find and uncomment the line which appends Evaluator to the pipeline:\n```\ncomponents.append(evaluator)\n\n```\n\n### Update the pipeline and re-run it\n```\n# Update the pipeline\n! tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n! tfx run create --pipeline-name \"{PIPELINE_NAME}\"\n\n```\n\n### Check pipeline outputs\nFor Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.\n## 12. Serving the model\nIf the new model is ready, make it so.\n  * Pusher deploys SavedModels to well-known locations\n\n\nDeployment targets receive new models from well-known locations\n  * TensorFlow Serving\n  * TensorFlow Lite\n  * TensorFlow JS\n  * TensorFlow Hub\n\n\n### Components\n  * [Pusher](https://www.tensorflow.org/tfx/guide/pusher) deploys the model to a serving infrastructure.\n\n\n### In Jupyter lab file editor:\nIn `pipeline`/`pipeline.py`, find and uncomment the line that appends Pusher to the pipeline:\n```\n# components.append(pusher)\n\n```\n\n### Check pipeline outputs\nFor Kubeflow Orchestrator, visit KFP dashboard and find pipeline outputs in the page for your pipeline run. Click \"Experiments\" tab on the left, and \"All runs\" in the Experiments page. You should be able to find the run with the name of your pipeline.\n### Available deployment targets\nYou have now trained and validated your model, and your model is now ready for production. You can now deploy your model to any of the TensorFlow deployment targets, including:\n  * [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving), for serving your model on a server or server farm and processing REST and/or gRPC inference requests.\n  * [TensorFlow Lite](https://www.tensorflow.org/lite), for including your model in an Android or iOS native mobile application, or in a Raspberry Pi, IoT, or microcontroller application.\n  * [TensorFlow.js](https://www.tensorflow.org/js), for running your model in a web browser or Node.JS application.\n\n\n## More advanced examples\nThe example presented above is really only meant to get you started. Below are some examples of integration with other Cloud services.\n### Kubeflow Pipelines resource considerations\nDepending on the requirements of your workload, the default configuration for your Kubeflow Pipelines deployment may or may not meet your needs. You can customize your resource configurations using `pipeline_operator_funcs` in your call to `KubeflowDagRunnerConfig`.\n`pipeline_operator_funcs` is a list of `OpFunc` items, which transforms all the generated `ContainerOp` instances in the KFP pipeline spec which is compiled from `KubeflowDagRunner`.\nFor example, to configure memory we can use `set_memory_request` and use it to add to to the list of pipeline `OpFunc`s:\n```\ndefrequest_more_memory():\n  def_set_memory_spec(container_op):\n    container_op.set_memory_request('32G')\n  return _set_memory_spec\n\n# Then use this opfunc in KubeflowDagRunner\npipeline_op_funcs = kubeflow_dag_runner.get_default_pipeline_operator_funcs()\npipeline_op_funcs.append(request_more_memory())\nconfig = KubeflowDagRunnerConfig(\n    pipeline_operator_funcs=pipeline_op_funcs,\n    ...\n)\nkubeflow_dag_runner.KubeflowDagRunner(config=config).run(pipeline)\n\n```\n\nSimilar resource configuration functions include:\n  * `set_memory_limit`\n  * `set_cpu_request`\n  * `set_cpu_limit`\n  * `set_gpu_limit`\n\n\n### Try `BigQueryExampleGen`\n`BigQueryExampleGen` to the pipeline.\n#### In Jupyter lab file editor:\n**Double-click to open`pipeline.py`**. Comment out `CsvExampleGen` and uncomment the line which creates an instance of `BigQueryExampleGen`. You also need to uncomment the `query` argument of the `create_pipeline` function.\nWe need to specify which GCP project to use for BigQuery, and this is done by setting `--project` in `beam_pipeline_args` when creating a pipeline.\n**Double-click to open`configs.py`**. Uncomment the definition of `BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS` and `BIG_QUERY_QUERY`. You should replace the project id and the region value in this file with the correct values for your GCP project.\n**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change the pipeline name.\n**Double-click to open`kubeflow_runner.py`**. Uncomment two arguments, `query` and `beam_pipeline_args`, for the `create_pipeline` function.\nNow the pipeline is ready to use BigQuery as an example source. Update the pipeline as before and create a new execution run as we did in step 5 and 6.\n#### Update the pipeline and re-run it\n```\n# Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n\n```\n\n### Try Dataflow\nSeveral [TFX Components use Apache Beam](https://www.tensorflow.org/tfx/guide/beam) to implement data-parallel pipelines, and it means that you can distribute data processing workloads using \n```\n# Select your project:\ngcloudset# Get a list of services that you can enable in your project:\ngcloud|# If you don't see dataflow.googleapis.com listed, that means you haven't been\n# granted access to enable the Dataflow API.  See your account adminstrator.\n\n# Enable the Dataflow service:\n\ngcloudenable\n```\n\n**Double-click`pipeline` to change directory, and double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, and `DATAFLOW_BEAM_PIPELINE_ARGS`.\n**Change directory one level up.** Click the name of the directory above the file list. The name of the directory is the name of the pipeline which is `my_pipeline` if you didn't change.\n**Double-click to open`kubeflow_runner.py`**. Uncomment `beam_pipeline_args`. (Also make sure to comment out current `beam_pipeline_args` that you added in Step 7.)\n#### Update the pipeline and re-run it\n```\n# Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n\n```\n\nYou can find your Dataflow jobs in \n### Try Cloud AI Platform Training and Prediction with KFP\nTFX interoperates with several managed GCP services, such as `Trainer` component to use Cloud AI Platform Training, a managed service for training ML models. Moreover, when your model is built and ready to be served, you can _push_ your model to Cloud AI Platform Prediction for serving. In this step, we will set our `Trainer` and `Pusher` component to use Cloud AI Platform services.\nBefore editing files, you might first have to enable _AI Platform Training & Prediction API_.\n**Double-click`pipeline` to change directory, and double-click to open `configs.py`**. Uncomment the definition of `GOOGLE_CLOUD_REGION`, `GCP_AI_PLATFORM_TRAINING_ARGS` and `GCP_AI_PLATFORM_SERVING_ARGS`. We will use our custom built container image to train a model in Cloud AI Platform Training, so we should set `masterConfig.imageUri` in `GCP_AI_PLATFORM_TRAINING_ARGS` to the same value as `CUSTOM_TFX_IMAGE` above.\n**Change directory one level up, and double-click to open`kubeflow_runner.py`**. Uncomment `ai_platform_training_args` and `ai_platform_serving_args`.\n#### Update the pipeline and re-run it\n```\n# Update the pipeline\n!tfx pipeline update \\\n  --pipeline-path=kubeflow_runner.py \\\n  --endpoint={ENDPOINT}\n\n!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}\n\n```\n\nYou can find your training jobs in \n## 14. Use your own data\nIn this tutorial, you made a pipeline for a model using the Chicago Taxi dataset. Now try putting your own data into the pipeline. Your data can be stored anywhere the pipeline can access it, including Google Cloud Storage, BigQuery, or CSV files.\nYou need to modify the pipeline definition to accommodate your data.\n### If your data is stored in files\n  1. Modify `DATA_PATH` in `kubeflow_runner.py`, indicating the location.\n\n\n### If your data is stored in BigQuery\n  1. Modify `BIG_QUERY_QUERY` in configs.py to your query statement.\n  2. Add features in `models`/`features.py`.\n  3. Modify `models`/`preprocessing.py` to [transform input data for training](https://www.tensorflow.org/tfx/guide/transform).\n  4. Modify `models`/`keras`/`model.py` and `models`/`keras`/`constants.py` to [describe your ML model](https://www.tensorflow.org/tfx/guide/trainer).\n\n\n### Learn more about Trainer\nSee [Trainer component guide](https://www.tensorflow.org/tfx/guide/trainer) for more details on Training pipelines.\n## Cleaning up\nTo clean up all Google Cloud resources used in this project, you can \nAlternatively, you can clean up individual resources by visiting each consoles: - \n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)  \n---  \nThis notebook-based tutorial will create and run a TFX pipeline which trains an ML model using Vertex AI Training service and publishes it to Vertex AI for serving.\nThis notebook is based on the TFX pipeline we built in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple). If you have not read that tutorial yet, you should read it before proceeding with this notebook.\nYou can train models on Vertex AI using AutoML, or use custom training. In custom training, you can select many different machine types to power your training jobs, enable distributed training, use hyperparameter tuning, and accelerate with GPUs.\nYou can also serve prediction requests by deploying the trained model to Vertex AI Models and creating an endpoint.\nIn this tutorial, we will use Vertex AI Training with custom jobs to train a model in a TFX pipeline. We will also deploy the model to serve prediction request using Vertex AI.\nThis notebook is intended to be run on \n## Set up\nIf you have completed [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple), you will have a working GCP project and a GCS bucket and that is all we need for this tutorial. Please read the preliminary tutorial first if you missed it.\n### Install python packages\nWe will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.\n```\n# Use the latest version of pip.\npip\npip\"tfx[kfp]<2\"\n```\n\n#### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nIf you are not on Colab, you can restart runtime with following cell.\n```\n# docs_infra: no_execute\nimportsys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  importIPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n\n```\n\n### Login in to Google for this notebook\nIf you are running this notebook on Colab, authenticate with your user account:\n```\nimportsys\nif 'google.colab' in sys.modules:\n  fromgoogle.colabimport auth\n  auth.authenticate_user()\n\n```\n\n**If you are on AI Platform Notebooks** , authenticate with Google Cloud before running the next section, by running\n```\ngcloud\n```\n\n**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance.\nCheck the package versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimportkfp\nprint('KFP version: {}'.format(kfp.__version__))\n\n```\n```\n2024-05-08 09:16:21.420852: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:16:21.420896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:16:21.422493: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\nKFP version: 1.8.22\n\n```\n\n### Set up variables\nWe will set up some variables used to customize the pipelines below. Following information is required:\n  * GCP Project id. See \n  * GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the \n  * Google Cloud Storage Bucket to store pipeline outputs.\n\n\n**Enter required values in the cell below before running it**.\n```\nGOOGLE_CLOUD_PROJECT = ''     # <--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''      # <--- ENTER THIS\nGCS_BUCKET_NAME = ''          # <--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    fromabslimport logging\n    logging.error('Please set all required parameters.')\n\n```\n```\nERROR:absl:Please set all required parameters.\n\n```\n\nSet `gcloud` to use your project.\n```\ngcloudset{GOOGLE_CLOUD_PROJECT}\n```\n```\nERROR: (gcloud.config.set) argument VALUE: Must be specified.\nUsage: gcloud config set SECTION/PROPERTY VALUE [optional flags]\n  optional flags may be  --help | --installation\n\nFor detailed information on this command and its flags, run:\n  gcloud config set --help\n\n```\n```\nPIPELINE_NAME = 'penguin-vertex-training'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Name of Vertex AI Endpoint.\nENDPOINT_NAME = 'prediction-' + PIPELINE_NAME\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n\n```\n```\nPIPELINE_ROOT: gs:///pipeline_root/penguin-vertex-training\n\n```\n\n### Prepare example data\nWe will use the same [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nThere are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the `species` of penguins.\nWe need to make our own copy of the dataset. Because TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it on GCS.\n```\ngsutil{DATA_ROOT}/\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///data/penguin-vertex-training/\".\n\n```\n\nTake a quick look at the CSV file.\n```\ngsutil{DATA_ROOT}/penguins_processed.csv|\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///data/penguin-vertex-training/penguins_processed.csv\".\n\n```\n\n## Create a pipeline\nOur pipeline will be very similar to the pipeline we created in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple). The pipeline will consists of three components, CsvExampleGen, Trainer and Pusher. But we will use a special Trainer and Pusher component. The Trainer component will move training workloads to Vertex AI, and the Pusher component will publish the trained ML model to Vertex AI instead of a filesystem.\nTFX provides a special `Trainer` to submit training jobs to Vertex AI Training service. All we have to do is use `Trainer` in the extension module instead of the standard `Trainer` component along with some required GCP parameters.\nIn this tutorial, we will run Vertex AI Training jobs only using CPUs first and then with a GPU.\nTFX also provides a special `Pusher` to upload the model to _Vertex AI Models_. `Pusher` will create _Vertex AI Endpoint_ resource to serve online perdictions, too. See \n### Write model code.\nThe model itself is almost similar to the model in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nWe will add `_get_distribution_strategy()` function which creates a [TensorFlow distribution strategy](https://www.tensorflow.org/guide/distributed_training) and it is used in `run_fn` to use MirroredStrategy if GPU is available.\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple and\n# slightly modified run_fn() to add distribution_strategy.\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_metadata.proto.v0import schema_pb2\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n        for feature in _FEATURE_KEYS\n    }, _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_make_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# NEW: Read `use_gpu` from the custom_config of the Trainer.\n#      if it uses GPU, enable MirroredStrategy.\ndef_get_distribution_strategy(fn_args: tfx.components.FnArgs):\n  if fn_args.custom_config.get('use_gpu', False):\n    logging.info('Using MirroredStrategy with one GPU.')\n    return tf.distribute.MirroredStrategy(devices=['device:GPU:0'])\n  return None\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  # NEW: If we have a distribution strategy, build a model in a strategy scope.\n  strategy = _get_distribution_strategy(fn_args)\n  if strategy is None:\n    model = _make_keras_model()\n  else:\n    with strategy.scope():\n      model = _make_keras_model()\n\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\nCopy the module file to GCS which can be accessed from the pipeline components.\nOtherwise, you might want to build a container image including the module file and use the image to run the pipeline and AI Platform Training jobs.\n```\ngsutil{_trainer_module_file}{MODULE_ROOT}/\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///pipeline_module/penguin-vertex-training/\".\n\n```\n\n### Write a pipeline definition\nWe will define a function to create a TFX pipeline. It has the same three Components as in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple), but we use a `Trainer` and `Pusher` component in the GCP extension module.\n`tfx.extensions.google_cloud_ai_platform.Trainer` behaves like a regular `Trainer`, but it just moves the computation for the model training to cloud. It launches a custom job in Vertex AI Training service and the trainer component in the orchestration system will just wait until the Vertex AI Training job completes.\n`tfx.extensions.google_cloud_ai_platform.Pusher` creates a Vertex AI Model and a Vertex AI Endpoint using the trained model.\n```\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, endpoint_name: str, project_id: str,\n                     region: str, use_gpu: bool) -> tfx.dsl.Pipeline:\n\"\"\"Implements the penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline or otherwise joins/converts training data.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # NEW: Configuration for Vertex AI Training.\n  # This dictionary will be passed as `CustomJobSpec`.\n  vertex_job_spec = {\n      'project': project_id,\n      'worker_pool_specs': [{\n          'machine_spec': {\n              'machine_type': 'n1-standard-4',\n          },\n          'replica_count': 1,\n          'container_spec': {\n              'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n          },\n      }],\n  }\n  if use_gpu:\n    # See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec#acceleratortype\n    # for available machine types.\n    vertex_job_spec['worker_pool_specs'][0]['machine_spec'].update({\n        'accelerator_type': 'NVIDIA_TESLA_K80',\n        'accelerator_count': 1\n    })\n\n  # Trains a model using Vertex AI Training.\n  # NEW: We need to specify a Trainer for GCP with related configs.\n  trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5),\n      custom_config={\n          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n              True,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n              region,\n          tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n              vertex_job_spec,\n          'use_gpu':\n              use_gpu,\n      })\n\n  # NEW: Configuration for pusher.\n  vertex_serving_spec = {\n      'project_id': project_id,\n      'endpoint_name': endpoint_name,\n      # Remaining argument is passed to aiplatform.Model.deploy()\n      # See https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api#deploy_the_model\n      # for the detail.\n      #\n      # Machine type is the compute resource to serve prediction requests.\n      # See https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types\n      # for available machine types and acccerators.\n      'machine_type': 'n1-standard-4',\n  }\n\n  # Vertex AI provides pre-built containers with various configurations for\n  # serving.\n  # See https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n  # for available container images.\n  serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-6:latest'\n  if use_gpu:\n    vertex_serving_spec.update({\n        'accelerator_type': 'NVIDIA_TESLA_K80',\n        'accelerator_count': 1\n    })\n    serving_image = 'us-docker.pkg.dev/vertex-ai/prediction/tf2-gpu.2-6:latest'\n\n  # NEW: Pushes the model to Vertex AI.\n  pusher = tfx.extensions.google_cloud_ai_platform.Pusher(\n      model=trainer.outputs['model'],\n      custom_config={\n          tfx.extensions.google_cloud_ai_platform.ENABLE_VERTEX_KEY:\n              True,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_REGION_KEY:\n              region,\n          tfx.extensions.google_cloud_ai_platform.VERTEX_CONTAINER_IMAGE_URI_KEY:\n              serving_image,\n          tfx.extensions.google_cloud_ai_platform.SERVING_ARGS_KEY:\n            vertex_serving_spec,\n      })\n\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components)\n\n```\n\n## Run the pipeline on Vertex Pipelines.\nWe will use Vertex Pipelines to run the pipeline as we did in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple).\n```\n# docs_infra: no_execute\nimportos\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        endpoint_name=ENDPOINT_NAME,\n        project_id=GOOGLE_CLOUD_PROJECT,\n        region=GOOGLE_CLOUD_REGION,\n        # We will use CPUs only for now.\n        use_gpu=False))\n\n```\n\nThe generated definition file can be submitted using Google Cloud aiplatform client in `google-cloud-aiplatform` package.\n```\n# docs_infra: no_execute\nfromgoogle.cloudimport aiplatform\nfromgoogle.cloud.aiplatformimport pipeline_jobs\nimportlogging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n\n```\n\nNow you can visit the link in the output above or visit 'Vertex AI > Pipelines' in \n## Test with a prediction request\nOnce the pipeline completes, you will find a _deployed_ model at the one of the endpoints in 'Vertex AI > Endpoints'. We need to know the id of the endpoint to send a prediction request to the new endpoint. This is different from the _endpoint name_ we entered above. You can find the id at the `Google Cloud Console`, it looks like a very long number.\n**Set ENDPOINT_ID below before running it.**\n```\nENDPOINT_ID=''     # <--- ENTER THIS\nif not ENDPOINT_ID:\n    fromabslimport logging\n    logging.error('Please set the endpoint id.')\n\n```\n```\nERROR:absl:Please set the endpoint id.\n\n```\n\nWe use the same aiplatform client to send a request to the endpoint. We will send a prediction request for Penguin species classification. The input is the four features that we used, and the model will return three values, because our model outputs one value for each species.\nFor example, the following specific example has the largest value at index '2' and will print '2'.\n```\n# docs_infra: no_execute\nimportnumpyasnp\n\n# The AI Platform services require regional API endpoints.\nclient_options = {\n    'api_endpoint': GOOGLE_CLOUD_REGION + '-aiplatform.googleapis.com'\n    }\n# Initialize client that will be used to create and send requests.\nclient = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n\n# Set data values for the prediction request.\n# Our model expects 4 feature inputs and produces 3 output values for each\n# species. Note that the output is logit value rather than probabilities.\n# See the model code to understand input / output structure.\ninstances = [{\n    'culmen_length_mm':[0.71],\n    'culmen_depth_mm':[0.38],\n    'flipper_length_mm':[0.98],\n    'body_mass_g': [0.78],\n}]\n\nendpoint = client.endpoint_path(\n    project=GOOGLE_CLOUD_PROJECT,\n    location=GOOGLE_CLOUD_REGION,\n    endpoint=ENDPOINT_ID,\n)\n# Send a prediction request and get response.\nresponse = client.predict(endpoint=endpoint, instances=instances)\n\n# Uses argmax to find the index of the maximum value.\nprint('species:', np.argmax(response.predictions[0]))\n\n```\n\nFor detailed information about online prediction, please visit the `Google Cloud Console`. you can find a guide on sending sample requests and links to more resources.\n## Run the pipeline using a GPU\nVertex AI supports training using various machine types including support for GPUs. See \nWe already defined our pipeline to support GPU training. All we need to do is setting `use_gpu` flag to True. Then a pipeline will be created with a machine spec including one NVIDIA_TESLA_K80 and our model training code will use [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\nNote that `use_gpu` flag is not a part of the Vertex or TFX API. It is just used to control the training code in this tutorial.\n```\n# docs_infra: no_execute\nrunner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        data_root=DATA_ROOT,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        endpoint_name=ENDPOINT_NAME,\n        project_id=GOOGLE_CLOUD_PROJECT,\n        region=GOOGLE_CLOUD_REGION,\n        # Updated: Use GPUs. We will use a NVIDIA_TESLA_K80 and \n        # the model code will use tf.distribute.MirroredStrategy.\n        use_gpu=True))\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n\n```\n\nNow you can visit the link in the output above or visit 'Vertex AI > Pipelines' in \n## Cleaning up\nYou have created a Vertex AI Model and Endpoint in this tutorial. Please delete these resources to avoid any unwanted charges by going to _undeploying_ the model from the endpoint first. Then you can delete the endpoint and the model separately.\n",
  "https://www.tensorflow.org/tfx/tutorials/serving/rest_simple": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple)  \n---  \nThis guide trains a neural network model to classify [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving). The focus is on TensorFlow Serving, rather than the modeling and training in TensorFlow, so for a complete example which focuses on the modeling and training see the \nThis guide uses \n```\nimportsys\n\n# Confirm that we're using Python 3\nassert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n\n```\n```\n# TensorFlow and tf.keras\nprint(\"Installing dependencies for Colab environment\")\n!pip install -Uq grpcio==1.26.0\n\nimporttensorflowastf\nfromtensorflowimport keras\n\n# Helper libraries\nimportnumpyasnp\nimportmatplotlib.pyplotasplt\nimportos\nimportsubprocess\n\nprint('TensorFlow version: {}'.format(tf.__version__))\n\n```\n```\n2024-04-30 10:41:08.716822: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:41:08.716875: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:41:08.718684: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\n## Create your model\n### Import the Fashion MNIST dataset\nThis guide uses the \n**Figure 1.**  \n---  \nFashion MNIST is intended as a drop-in replacement for the classic \n```\nfashion_mnist = keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# scale the values to 0.0 to 1.0\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\n# reshape for feeding into the model\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n\nclass_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\nprint('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))\n\n```\n```\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 0s 0us/step\n\ntrain_images.shape: (60000, 28, 28, 1), of float64\ntest_images.shape: (10000, 28, 28, 1), of float64\n\n```\n\n### Train and evaluate your model\nLet's use the simplest possible CNN, since we're not focused on the modeling part.\n```\nmodel = keras.Sequential([\n  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n                      strides=2, activation='relu', name='Conv1'),\n  keras.layers.Flatten(),\n  keras.layers.Dense(10, name='Dense')\n])\nmodel.summary()\n\ntesting = False\nepochs = 5\n\nmodel.compile(optimizer='adam', \n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.SparseCategoricalAccuracy()])\nmodel.fit(train_images, train_labels, epochs=epochs)\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint('\\nTest accuracy: {}'.format(test_acc))\n\n```\n```\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n Conv1 (Conv2D)              (None, 13, 13, 8)         80        \n                                                                 \n flatten (Flatten)           (None, 1352)              0         \n                                                                 \n Dense (Dense)               (None, 10)                13530     \n                                                                 \n=================================================================\nTotal params: 13610 (53.16 KB)\nTrainable params: 13610 (53.16 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/5\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1714473676.987867  178558 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1875/1875 [==============================] - 6s 2ms/step - loss: 0.5614 - sparse_categorical_accuracy: 0.8059\nEpoch 2/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.4274 - sparse_categorical_accuracy: 0.8499\nEpoch 3/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3859 - sparse_categorical_accuracy: 0.8654\nEpoch 4/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3561 - sparse_categorical_accuracy: 0.8740\nEpoch 5/5\n1875/1875 [==============================] - 4s 2ms/step - loss: 0.3328 - sparse_categorical_accuracy: 0.8818\n313/313 [==============================] - 1s 2ms/step - loss: 0.3635 - sparse_categorical_accuracy: 0.8706\n\nTest accuracy: 0.8705999851226807\n\n```\n\n## Save your model\nTo load our trained model into TensorFlow Serving we first need to save it in [SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model) format. This will create a protobuf file in a well-defined directory hierarchy, and will include a version number. [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) allows us to select which version of a model, or \"servable\" we want to use when we make inference requests. Each version will be exported to a different sub-directory under the given path.\n```\n# Fetch the Keras session and save the model\n# The signature definition is defined by the input and output tensors,\n# and stored with the default serving key\nimporttempfile\n\nMODEL_DIR = tempfile.gettempdir()\nversion = 1\nexport_path = os.path.join(MODEL_DIR, str(version))\nprint('export_path = {}\\n'.format(export_path))\n\ntf.keras.models.save_model(\n    model,\n    export_path,\n    overwrite=True,\n    include_optimizer=True,\n    save_format=None,\n    signatures=None,\n    options=None\n)\n\nprint('\\nSaved model:')\n!ls -l {export_path}\n\n```\n```\nexport_path = /tmpfs/tmp/1\n\nINFO:tensorflow:Assets written to: /tmpfs/tmp/1/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/1/assets\nSaved model:\ntotal 104\ndrwxr-xr-x 2 kbuilder kbuilder  4096 Apr 30 10:41 assets\n-rw-rw-r-- 1 kbuilder kbuilder    54 Apr 30 10:41 fingerprint.pb\n-rw-rw-r-- 1 kbuilder kbuilder  8798 Apr 30 10:41 keras_metadata.pb\n-rw-rw-r-- 1 kbuilder kbuilder 78816 Apr 30 10:41 saved_model.pb\ndrwxr-xr-x 2 kbuilder kbuilder  4096 Apr 30 10:41 variables\n\n```\n\n## Examine your saved model\nWe'll use the command line utility `saved_model_cli` to look at the [MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef) (the models) and [SignatureDefs](https://www.tensorflow.org/tfx/serving/signature_defs) (the methods you can call) in our SavedModel. See \n```\nsaved_model_cli{export_path}\n```\n```\n2024-04-30 10:41:41.354485: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:41:41.354542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:41:41.355947: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['__saved_model_init_op']:\n  The given SavedModel SignatureDef contains the following input(s):\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['__saved_model_init_op'] tensor_info:\n        dtype: DT_INVALID\n        shape: unknown_rank\n        name: NoOp\n  Method name is: \n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['Conv1_input'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 28, 28, 1)\n        name: serving_default_Conv1_input:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['Dense'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 10)\n        name: StatefulPartitionedCall:0\n  Method name is: tensorflow/serving/predict\nThe MetaGraph with tag set ['serve'] contains the following ops: {'SaveV2', 'Pack', 'ShardedFilename', 'NoOp', 'ReadVariableOp', 'Placeholder', 'Reshape', 'RestoreV2', 'StaticRegexFullMatch', 'Const', 'Identity', 'Select', 'Conv2D', 'AssignVariableOp', 'Relu', 'BiasAdd', 'MergeV2Checkpoints', 'DisableCopyOnRead', 'StringJoin', 'StatefulPartitionedCall', 'VarHandleOp', 'MatMul'}\n\nConcrete Functions:\n  Function Name: '__call__'\n    Option #1\n      Callable with:\n        Argument #1\n          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n  Function Name: '_default_save_signature'\n    Option #1\n      Callable with:\n        Argument #1\n          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n\n  Function Name: 'call_and_return_all_conditional_losses'\n    Option #1\n      Callable with:\n        Argument #1\n          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n        Argument #2\n          DType: bool\n          Value: False\n        Argument #3\n          DType: NoneType\n          Value: None\n    Option #2\n      Callable with:\n        Argument #1\n          Conv1_input: TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='Conv1_input')\n        Argument #2\n          DType: bool\n          Value: True\n        Argument #3\n          DType: NoneType\n          Value: None\n\n```\n\nThat tells us a lot about our model! In this case we just trained our model, so we already know the inputs and outputs, but if we didn't this would be important information. It doesn't tell us everything, like the fact that this is grayscale image data for example, but it's a great start.\n## Serve your model with TensorFlow Serving\n### Add TensorFlow Serving distribution URI as a package source:\nWe're preparing to install TensorFlow Serving using `tensorflow-model-server` package to the list of packages that Aptitude knows about. Note that we're running as root.\n```\nimportsys\n# We need sudo prefix if not on a Google Colab.\nif 'google.colab' not in sys.modules:\n  SUDO_IF_NEEDED = 'sudo'\nelse:\n  SUDO_IF_NEEDED = ''\n\n```\n```\n# This is the same as you would do from your command line, but without the [arch=amd64], and no sudo\n# You would instead do:\n# echo \"deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\n# curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -\n\n!echo \"deb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\" | {SUDO_IF_NEEDED} tee /etc/apt/sources.list.d/tensorflow-serving.list && \\\ncurl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | {SUDO_IF_NEEDED} apt-key add -\n!{SUDO_IF_NEEDED} apt update\n\n```\n```\ndeb http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  2943  100  2943    0     0  52553      0 --:--:-- --:--:-- --:--:-- 53509\nOK\nHit:1 http://us-central1.gce.archive.ubuntu.com/ubuntu focal InRelease\nHit:2 http://us-central1.gce.archive.ubuntu.com/ubuntu focal-updates InRelease\nHit:3 http://us-central1.gce.archive.ubuntu.com/ubuntu focal-backports InRelease\nHit:4 https://download.docker.com/linux/ubuntu focal InRelease\nGet:5 https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease [1484 B]\nHit:6 https://nvidia.github.io/nvidia-container-runtime/stable/ubuntu18.04/amd64  InRelease\nHit:7 https://nvidia.github.io/nvidia-docker/ubuntu18.04/amd64  InRelease\nHit:8 http://security.ubuntu.com/ubuntu focal-security InRelease\nGet:9 http://storage.googleapis.com/tensorflow-serving-apt stable InRelease [3026 B]\nHit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\nHit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\nHit:13 http://ppa.launchpad.net/longsleep/golang-backports/ubuntu focal InRelease\nHit:14 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu focal InRelease\nHit:12 https://apt.llvm.org/focal llvm-toolchain-focal-17 InRelease\nGet:15 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server amd64 Packages [341 B]\nGet:16 http://storage.googleapis.com/tensorflow-serving-apt stable/tensorflow-model-server-universal amd64 Packages [349 B]\nFetched 5200 B in 2s (3446 B/s)\n\n\n\n235 packages can be upgraded. Run 'apt list --upgradable' to see them.\n\n```\n\n### Install TensorFlow Serving\nThis is all you need - one command line!\n```\n# TODO: Use the latest model server version when colab supports it.\n#!{SUDO_IF_NEEDED} apt-get install tensorflow-model-server\n# We need to install Tensorflow Model server 2.8 instead of latest version\n# Tensorflow Serving >2.9.0 required `GLIBC_2.29` and `GLIBCXX_3.4.26`. Currently colab environment doesn't support latest version of`GLIBC`,so workaround is to use specific version of Tensorflow Serving `2.8.0` to mitigate issue.\nwget'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb'\ndpkg\npip3==2.8.0\n```\n```\n--2024-04-30 10:41:50--  http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-2.8.0/t/tensorflow-model-server/tensorflow-model-server_2.8.0_all.deb\nResolving storage.googleapis.com (storage.googleapis.com)... 142.251.172.207, 142.251.180.207, 142.251.183.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|142.251.172.207|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 340152790 (324M) [application/x-debian-package]\nSaving to: ‘tensorflow-model-server_2.8.0_all.deb’\n\ntensorflow-model-se 100%[===================>] 324.39M   223MB/s    in 1.5s    \n\n2024-04-30 10:41:51 (223 MB/s) - ‘tensorflow-model-server_2.8.0_all.deb’ saved [340152790/340152790]\n\ndpkg: error: requested operation requires superuser privilege\nCollecting tensorflow-serving-api==2.8.0\n  Downloading tensorflow_serving_api-2.8.0-py2.py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: grpcio<2,>=1.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow-serving-api==2.8.0) (1.26.0)\nRequirement already satisfied: protobuf>=3.6.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow-serving-api==2.8.0) (3.20.3)\nRequirement already satisfied: tensorflow<3,>=2.8.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow-serving-api==2.8.0) (2.15.1)\nRequirement already satisfied: six>=1.5.2 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from grpcio<2,>=1.0->tensorflow-serving-api==2.8.0) (1.16.0)\nRequirement already satisfied: absl-py>=1.0.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.11.0)\nRequirement already satisfied: libclang>=13.0.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.3.2)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.3.0)\nRequirement already satisfied: packaging in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (24.0)\nRequirement already satisfied: setuptools in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (69.5.1)\nRequirement already satisfied: termcolor>=1.1.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (4.11.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.36.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.15.2)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.15.0)\nRequirement already satisfied: keras<2.16,>=2.15.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.43.0)\nCollecting grpcio<2,>=1.0 (from tensorflow-serving-api==2.8.0)\n  Downloading grpcio-1.62.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.29.0)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.6)\nRequirement already satisfied: requests<3,>=2.21.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (5.3.3)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.4.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.0.0)\nRequirement already satisfied: importlib-metadata>=4.4 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (7.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (2.1.5)\nRequirement already satisfied: zipp>=0.5 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.18.1)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (0.6.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /tmpfs/src/tf_docs_env/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.8.0->tensorflow-serving-api==2.8.0) (3.2.2)\nDownloading tensorflow_serving_api-2.8.0-py2.py3-none-any.whl (37 kB)\nDownloading grpcio-1.62.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\nInstalling collected packages: grpcio, tensorflow-serving-api\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.26.0\n    Uninstalling grpcio-1.26.0:\n      Successfully uninstalled grpcio-1.26.0\n  Attempting uninstall: tensorflow-serving-api\n    Found existing installation: tensorflow-serving-api 2.15.1\n    Uninstalling tensorflow-serving-api-2.15.1:\n      Successfully uninstalled tensorflow-serving-api-2.15.1\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.15.1 which is incompatible.\ntfx 1.15.0 requires tensorflow-serving-api<2.16,>=2.15, but you have tensorflow-serving-api 2.8.0 which is incompatible.\ntfx-bsl 1.15.1 requires tensorflow-serving-api<3,>=2.13.0, but you have tensorflow-serving-api 2.8.0 which is incompatible.\nSuccessfully installed grpcio-1.62.2 tensorflow-serving-api-2.8.0\n\n```\n\n### Start running TensorFlow Serving\nThis is where we start running TensorFlow Serving and load our model. After it loads we can start making inference requests using REST. There are some important parameters:\n  * `rest_api_port`: The port that you'll use for REST requests.\n  * `model_name`: You'll use this in the URL of REST requests. It can be anything.\n  * `model_base_path`: This is the path to the directory where you've saved your model.\n\n```\nos.environ[\"MODEL_DIR\"] = MODEL_DIR\n\n```\n```\nnohup\\\n=8501\\\n=fashion_model\\\n=\"${MODEL_DIR}\"2>&1\n```\n```\ntail\n```\n```\nnohup: failed to run command 'tensorflow_model_server': No such file or directory\n\n```\n\n## Make a request to your model in TensorFlow Serving\nFirst, let's take a look at a random example from our test data.\n```\ndefshow(idx, title):\n  plt.figure()\n  plt.imshow(test_images[idx].reshape(28,28))\n  plt.axis('off')\n  plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n\nimportrandom\nrando = random.randint(0,len(test_images)-1)\nshow(rando, 'An Example Image: {}'.format(class_names[test_labels[rando]]))\n\n```\n\nOk, that looks interesting. How hard is that for you to recognize? Now let's create the JSON object for a batch of three inference requests, and see how well our model recognizes things:\n```\nimportjson\ndata = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\nprint('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n\n```\n```\nData: {\"signature_name\": \"serving_default\", \"instances\": ...  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]}\n\n```\n\n### Make REST requests\n#### Newest version of the servable\nWe'll send a predict request as a POST to our server's REST endpoint, and pass it three examples. We'll ask our server to give us the latest version of our servable by not specifying a particular version.\n```\n# docs_infra: no_execute\n!pip install -q requests\n\nimportrequests\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/fashion_model:predict', data=data, headers=headers)\npredictions = json.loads(json_response.text)['predictions']\n\nshow(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))\n\n```\n\n#### A particular version of the servable\nNow let's specify a particular version of our servable. Since we only have one, let's select version 1. We'll also look at all three results.\n```\n# docs_infra: no_execute\nheaders = {\"content-type\": \"application/json\"}\njson_response = requests.post('http://localhost:8501/v1/models/fashion_model/versions/1:predict', data=data, headers=headers)\npredictions = json.loads(json_response.text)['predictions']\n\nfor i in range(0,3):\n  show(i, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n    class_names[np.argmax(predictions[i])], np.argmax(predictions[i]), class_names[test_labels[i]], test_labels[i]))\n\n```\n\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_bq": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_bq)  \n---  \nThis notebook-based tutorial will use \nThis notebook is based on the TFX pipeline we built in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple). If you have not read that tutorial yet, you should read it before proceeding with this notebook.\n[publish the trained model](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/Pusher) to BigQuery.\nIn this tutorial, we will use the `BigQueryExampleGen` component which reads data from BigQuery to TFX pipelines.\nThis notebook is intended to be run on \n## Set up\nIf you have completed [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple), you will have a working GCP project and a GCS bucket and that is all we need for this tutorial. Please read the preliminary tutorial first if you missed it.\n### Install python packages\nWe will install required Python packages including TFX and KFP to author ML pipelines and submit jobs to Vertex Pipelines.\n```\n# Use the latest version of pip.\npip\npip\"tfx[kfp]<2\"\n```\n\n#### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nIf you are not on Colab, you can restart runtime with following cell.\n```\n# docs_infra: no_execute\nimportsys\nif not 'google.colab' in sys.modules:\n  # Automatically restart kernel after installs\n  importIPython\n  app = IPython.Application.instance()\n  app.kernel.do_shutdown(True)\n\n```\n\n### Login in to Google for this notebook\nIf you are running this notebook on Colab, authenticate with your user account:\n```\nimportsys\nif 'google.colab' in sys.modules:\n  fromgoogle.colabimport auth\n  auth.authenticate_user()\n\n```\n\n**If you are on AI Platform Notebooks** , authenticate with Google Cloud before running the next section, by running\n```\ngcloud\n```\n\n**in the Terminal window** (which you can open via **File** > **New** in the menu). You only need to do this once per notebook instance.\nCheck the package versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\nimportkfp\nprint('KFP version: {}'.format(kfp.__version__))\n\n```\n```\n2024-05-08 09:24:47.541894: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:24:47.541946: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:24:47.543649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\nKFP version: 1.8.22\n\n```\n\n### Set up variables\nWe will set up some variables used to customize the pipelines below. Following information is required:\n  * GCP Project id and number. See \n  * GCP Region to run pipelines. For more information about the regions that Vertex Pipelines is available in, see the \n  * Google Cloud Storage Bucket to store pipeline outputs.\n\n\n**Enter required values in the cell below before running it**.\n```\nGOOGLE_CLOUD_PROJECT = ''         # <--- ENTER THIS\nGOOGLE_CLOUD_PROJECT_NUMBER = ''  # <--- ENTER THIS\nGOOGLE_CLOUD_REGION = ''          # <--- ENTER THIS\nGCS_BUCKET_NAME = ''              # <--- ENTER THIS\n\nif not (GOOGLE_CLOUD_PROJECT and  GOOGLE_CLOUD_PROJECT_NUMBER and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n    fromabslimport logging\n    logging.error('Please set all required parameters.')\n\n```\n```\nERROR:absl:Please set all required parameters.\n\n```\n\nSet `gcloud` to use your project.\n```\ngcloudset{GOOGLE_CLOUD_PROJECT}\n```\n```\nERROR: (gcloud.config.set) argument VALUE: Must be specified.\nUsage: gcloud config set SECTION/PROPERTY VALUE [optional flags]\n  optional flags may be  --help | --installation\n\nFor detailed information on this command and its flags, run:\n  gcloud config set --help\n\n```\n```\nPIPELINE_NAME = 'penguin-bigquery'\n\n# Path to various pipeline artifact.\nPIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' Python module.\nMODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# Paths for users' data.\nDATA_ROOT = 'gs://{}/data/{}'.format(GCS_BUCKET_NAME, PIPELINE_NAME)\n\n# This is the path where your model will be pushed for serving.\nSERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n    GCS_BUCKET_NAME, PIPELINE_NAME)\n\nprint('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))\n\n```\n```\nPIPELINE_ROOT: gs:///pipeline_root/penguin-bigquery\n\n```\n\nBy default the Vertex Pipelines uses the default GCE VM service account of format `[project-number]-compute@developer.gserviceaccount.com`. We need to give a permission to use BigQuery to this account to access BigQuery in the pipeline. We will add 'BigQuery User' role to the account.\n```\n!gcloud projects add-iam-policy-binding {GOOGLE_CLOUD_PROJECT} \\\n  --member=serviceAccount:{GOOGLE_CLOUD_PROJECT_NUMBER}-compute@developer.gserviceaccount.com \\\n  --role=roles/bigquery.user\n\n```\n```\nERROR: (gcloud.projects.add-iam-policy-binding) argument PROJECT_ID: Must be specified.\nUsage: gcloud projects add-iam-policy-binding PROJECT_ID --member=PRINCIPAL --role=ROLE [optional flags]\n  optional flags may be  --condition | --condition-from-file | --help\n\nFor detailed information on this command and its flags, run:\n  gcloud projects add-iam-policy-binding --help\n\n```\n\nPlease see \n## Create a pipeline\nTFX pipelines are defined using Python APIs as we did in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple). We previously used `CsvExampleGen` which reads data from a CSV file. In this tutorial, we will use [`BigQueryExampleGen`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/BigQueryExampleGen) component which reads data from BigQuery.\n### Prepare BigQuery query\nWe will use the same `tfx-oss-public.palmer_penguins.palmer_penguins` which is populated using the same CSV file.\nIf you are using Google Colab, you can examine the content of the BigQuery table directly.\n```\n# docs_infra: no_execute\n%%bigquery --project {GOOGLE_CLOUD_PROJECT}\nSELECT *\nFROM `tfx-oss-public.palmer_penguins.palmer_penguins`\nLIMIT 5\n\n```\n\nAll features were already normalized to 0~1 except `species` which is the label. We will build a classification model which predicts the `species` of penguins.\n`BigQueryExampleGen` requires a query to specify which data to fetch. Because we will use all the fields of all rows in the table, the query is quite simple. You can also specify field names and add `WHERE` conditions as needed according to the \n```\nQUERY = \"SELECT * FROM `tfx-oss-public.palmer_penguins.palmer_penguins`\"\n\n```\n\n### Write model code.\nWe will use the same model code as in the [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\n\nfromtensorflow_metadata.proto.v0import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_make_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _make_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\nCopy the module file to GCS which can be accessed from the pipeline components. Because model training happens on GCP, we need to upload this model definition.\nOtherwise, you might want to build a container image including the module file and use the image to run the pipeline.\n```\ngsutil{_trainer_module_file}{MODULE_ROOT}/\n```\n```\nInvalidUrlError: Cloud URL scheme should be followed by colon and two slashes: \"://\". Found: \"gs:///pipeline_module/penguin-bigquery/\".\n\n```\n\n### Write a pipeline definition\nWe will define a function to create a TFX pipeline. We need to use `BigQueryExampleGen` which takes `query` as an argument. One more change from the previous tutorial is that we need to pass `beam_pipeline_args` which is passed to components when they are executed. We will use `beam_pipeline_args` to pass additional parameters to BigQuery.\n```\nfromtypingimport List, Optional\n\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, query: str,\n                     module_file: str, serving_model_dir: str,\n                     beam_pipeline_args: Optional[List[str]],\n                     ) -> tfx.dsl.Pipeline:\n\"\"\"Creates a TFX pipeline using BigQuery.\"\"\"\n\n  # NEW: Query data in BigQuery as a data source.\n  example_gen = tfx.extensions.google_cloud_big_query.BigQueryExampleGen(\n      query=query)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a file destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      components=components,\n      # NEW: `beam_pipeline_args` is required to use BigQueryExampleGen.\n      beam_pipeline_args=beam_pipeline_args)\n\n```\n\n## Run the pipeline on Vertex Pipelines.\nWe will use Vertex Pipelines to run the pipeline as we did in [Simple TFX Pipeline for Vertex Pipelines Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_simple).\nWe also need to pass `beam_pipeline_args` for the BigQueryExampleGen. It includes configs like the name of the GCP project and the temporary storage for the BigQuery execution.\n```\n# docs_infra: no_execute\nimportos\n\n# We need to pass some GCP related configs to BigQuery. This is currently done\n# using `beam_pipeline_args` parameter.\nBIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS = [\n   '--project=' + GOOGLE_CLOUD_PROJECT,\n   '--temp_location=' + os.path.join('gs://', GCS_BUCKET_NAME, 'tmp'),\n   ]\n\nPIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n\nrunner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(),\n    output_filename=PIPELINE_DEFINITION_FILE)\n_ = runner.run(\n    _create_pipeline(\n        pipeline_name=PIPELINE_NAME,\n        pipeline_root=PIPELINE_ROOT,\n        query=QUERY,\n        module_file=os.path.join(MODULE_ROOT, _trainer_module_file),\n        serving_model_dir=SERVING_MODEL_DIR,\n        beam_pipeline_args=BIG_QUERY_WITH_DIRECT_RUNNER_BEAM_PIPELINE_ARGS))\n\n```\n\nThe generated definition file can be submitted using kfp client.\n```\n# docs_infra: no_execute\nfromgoogle.cloudimport aiplatform\nfromgoogle.cloud.aiplatformimport pipeline_jobs\nimportlogging\nlogging.getLogger().setLevel(logging.INFO)\n\naiplatform.init(project=GOOGLE_CLOUD_PROJECT, location=GOOGLE_CLOUD_REGION)\n\njob = pipeline_jobs.PipelineJob(template_path=PIPELINE_DEFINITION_FILE,\n                                display_name=PIPELINE_NAME)\njob.submit()\n\n```\n\nNow you can visit the link in the output above or visit 'Vertex AI > Pipelines' in \n",
  "https://pypi-nightly.tensorflow.org": "\n",
  "https://js.tensorflow.org": "#  TensorFlow.js is a library for machine learning in JavaScript \nDevelop ML models in JavaScript, and use ML directly in the browser or in Node.js.\n[ See tutorials ](https://www.tensorflow.org/js/tutorials)\nTutorials show you how to use TensorFlow.js with complete, end-to-end examples. \n[ See models ](https://www.tensorflow.org/js/models)\nPre-trained, out-of-the-box models for common use cases. \n[ See demos ](https://www.tensorflow.org/js/demos)\nLive demos and examples run in your browser using TensorFlow.js. \n##  How it works \n####  Run existing models \nUse off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js.\n[ Use official TensorFlow.js models ](https://www.tensorflow.org/js/models) [ Convert Python models ](https://www.tensorflow.org/js/tutorials#convert_pretained_models_to_tensorflowjs)\n####  Retrain existing models \nRetrain pre-existing ML models using your own data.\n[ Use Transfer Learning to customize models ](https://www.tensorflow.org/js/tutorials/transfer/what_is_transfer_learning)\n####  Develop ML with JavaScript \nBuild and train models directly in JavaScript using flexible and intuitive APIs.\n[ Get started with TensorFlow.js ](https://www.tensorflow.org/js/tutorials)\n##  Demos \n[ Performance RNN ](https://magenta.tensorflow.org/demos/performance_rnn/index.html#2%7C2,0,1,0,1,1,0,1,0,1,0,1%7C1,1,1,1,1,1,1,1,1,1,1,1%7C1,1,1,1,1,1,1,1,1,1,1,1%7Cfalse)\nEnjoy a real-time piano performance by a neural network.\nPlay Pac-Man using images trained in your browser.\nTransport yourself to a tropical beach, outer space, and elsewhere with the power of web ML.\n##  News & announcements \nCheck out our [blog](https://blog.tensorflow.org/search?label=TensorFlow.js&max-results=20) for additional updates, and subscribe to our TensorFlow newsletter to get the latest announcements sent directly to your inbox.\nTensorFlow Blog\n### [How Adobe used Web ML with TensorFlow.js to enhance Photoshop for web](https://blog.tensorflow.org/2023/03/how-adobe-used-web-ml-with-tensorflowjs-to-enhance-photoshop-for-web.html)\nPhotoshop Web Beta is a browser-based version of the popular desktop image editing software, Adobe Photoshop. This online tool offers a wide range of features and capabilities for editing, enhancing, and manipulating images, all through a web\nTensorFlow\nMarch 30, 2023\nTensorFlow Blog\n### [Content moderation using machine learning: the server-side part](https://blog.tensorflow.org/2022/09/content-moderation-using-machine-learning-the-server-side-part.html)\ncd text - moderation / functions const functions = require ( 'firebase-functions' ); const toxicity = require ( '@tensorflow-models/toxicity' ); exports. moderator = functions. database. ref ( '/messages/{messageId}' ). onCreate ( async ( snapshot,\nTensorFlow\nSeptember 8, 2022\nTensorFlow Blog\n### [JAX on the Web with TensorFlow.js](https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html)\nIn this blog post we demonstrate how to convert and run Python-based JAX functions and Flax machine learning models in the browser using TensorFlow.js. We have produced three examples of JAX-to-TensorFlow.js conversion each with increasing\nTensorFlow\nAugust 31, 2022\nTensorFlow Blog\n### [Content moderation using machine learning: a dual approach](https://blog.tensorflow.org/2022/08/content-moderation-using-machine-learning-a-dual-approach.html)\nI've often wondered why anonymity drives people to say things that they'd never dare say in person, and it’s unfortunate that comment sections for videos and articles are so often toxic! If you’re interested in content moderation, you can use machine\nTensorFlow\nAugust 19, 2022\n##  Community participation \nSee more ways to participate in the TensorFlow community.\n##  Get started with TensorFlow.js \n[ Explore tutorials ](https://www.tensorflow.org/js/tutorials)\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy": "Synchronous training across multiple replicas on one machine.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.MirroredStrategy(\n    devices=None, cross_device_ops=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Random number generation](https://www.tensorflow.org/guide/random_numbers)\n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n  * [Migrate single-worker multiple-GPU training](https://www.tensorflow.org/guide/migrate/mirrored_strategy)\n\n| \n  * [Save and load a model using a distribution strategy](https://www.tensorflow.org/tutorials/distribute/save_and_load)\n  * [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training)\n  * [Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras)\n  * [TFP Release Notes notebook (0.13.0)](https://www.tensorflow.org/probability/examples/TFP_Release_Notebook_0_13_0)\n\n  \nThis strategy is typically used for training on one machine with multiple GPUs. For TPUs, use [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy). To use `MirroredStrategy` with multiple workers, please refer to [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy).\nFor example, a variable created under a `MirroredStrategy` is a `MirroredVariable`. If no devices are specified in the constructor argument of the strategy then it will use all the available GPUs. If no GPUs are found, it will use the available CPUs. Note that TensorFlow treats all CPUs on a machine as a single device, and uses threads internally for parallelism.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\nwith strategy.scope():\n  x = tf.Variable(1.)\n\nMirroredVariable:{\n  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>\n\n```\n\nWhile using distribution strategies, all the variable creation should be done within the strategy's scope. This will replicate the variables across all the replicas and keep them in sync using an all-reduce algorithm.\nVariables created inside a `MirroredStrategy` which is wrapped with a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) are still `MirroredVariables`.\n```\nx = []\n@tf.function  # Wrap the function with tf.function.\ndefcreate_variable():\n  if not x:\n    x.append(tf.Variable(1.))\n  return x[0]\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\nwith strategy.scope():\n  _ = create_variable()\n  print(x[0])\nMirroredVariable:{\n  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>\n\n```\n\n`experimental_distribute_dataset` can be used to distribute the dataset across the replicas when writing your own training loop. If you are using `.fit` and `.compile` methods available in [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras), then [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) will handle the distribution for you.\n#### For example:\n```\nmy_strategy = tf.distribute.MirroredStrategy()\nwith my_strategy.scope():\n  @tf.function\n  defdistribute_train_epoch(dataset):\n    defreplica_fn(input):\n      # process input and return result\n      return result\n\n    total_result = 0\n    for x in dataset:\n      per_replica_result = my_strategy.run(replica_fn, args=(x,))\n      total_result += my_strategy.reduce(tf.distribute.ReduceOp.SUM,\n                                         per_replica_result, axis=None)\n    return total_result\n\n  dist_dataset = my_strategy.experimental_distribute_dataset(dataset)\n  for _ in range(EPOCHS):\n    train_result = distribute_train_epoch(dist_dataset)\n\n```\n\n## Args  \n---  \n`devices` |  a list of device strings such as `['/gpu:0', '/gpu:1']`. If `None`, all available GPUs are used. If no GPUs are found, CPU is used.   \n`cross_device_ops` |  optional, a descendant of `CrossDeviceOps`. If this is not set, `NcclAllReduce()` will be used by default. One would customize this if NCCL isn't available or if a special implementation that exploits the particular hardware is available.   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.In general, when using a multi-worker [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), there is a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) must set the relevant attribute, or override this property; otherwise, `None` is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver), and in those cases this property will return `None`. The [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, ```\n\nos.environ['TF_CONFIG'] = json.dumps({\n  'cluster': {\n      'worker': [\"localhost:12345\", \"localhost:23456\"],\n      'ps': [\"localhost:34567\"]\n  },\n  'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. Since we set this\n  # as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. Since we\n  # set this as a worker above, this block will not run on this particular\n  # instance.\n\n```\nFor more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s API docstring.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\n\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nInvokes `fn` on each replica, with the given arguments.\nThis method is the primary way to distribute your computation with a tf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` is invoked under a replica context. `fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`. Please see the module-level docstring of tf.distribute for the concept of replica context.\nAll arguments in `args` or `kwargs` can be a nested structure of tensors, e.g. a list of tensors, in which case `args` and `kwargs` will be passed to the `fn` invoked on each replica. Or `args` or `kwargs` can be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors, i.e. [`tf.compat.v1.TensorInfo.CompositeTensor`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor), in which case each `fn` call will get the component of a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) corresponding to its replica. Note that arbitrary Python values that are not of the types above are not supported.\n#### Example usage:\n  1. Constant tensor input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndefreplica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n      1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n\n\n```\n\n  2. DistributedValues input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  defreplica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n    <tf.Tensor: shape=(), dtype=int32, numpy=4>\n\n```\n\n  3. Use [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) to allreduce values.\n```\nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndefrun():\n   defvalue_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   defreplica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\n         \"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n      1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n```\n\n\n\nArgs  \n---  \n`fn` |  The function to run on each replica.   \n`args` |  Optional positional arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`kwargs` |  Optional keyword arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`options` |  An optional instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/guide/keras": "Keras is the high-level API of the TensorFlow platform. It provides an approachable, highly-productive interface for solving machine learning (ML) problems, with a focus on modern deep learning. Keras covers every step of the machine learning workflow, from data processing to hyperparameter tuning to deployment. It was developed with a focus on enabling fast experimentation.\nWith Keras, you have full access to the scalability and cross-platform capabilities of TensorFlow. You can run Keras on a TPU Pod or large clusters of GPUs, and you can export Keras models to run in the browser or on mobile devices. You can also serve Keras models via a web API.\nKeras is designed to reduce cognitive load by achieving the following goals:\n  * Offer simple, consistent interfaces.\n  * Minimize the number of actions required for common use cases.\n  * Provide clear, actionable error messages.\n  * Follow the principle of progressive disclosure of complexity: It's easy to get started, and you can complete advanced workflows by learning as you go.\n  * Help you write concise, readable code.\n\n\n## Who should use Keras\nThe short answer is that every TensorFlow user should use the Keras APIs by default. Whether you're an engineer, a researcher, or an ML practitioner, you should start with Keras.\nThere are a few use cases (for example, building tools on top of TensorFlow or developing your own high-performance platform) that require the low-level [TensorFlow Core APIs](https://www.tensorflow.org/guide/core). But if your use case doesn't fall into one of the [Core API applications](https://www.tensorflow.org/guide/core#core_api_applications), you should prefer Keras.\n## Keras API components\nThe core data structures of Keras are \n### Layers\nThe `tf.keras.layers.Layer` class is the fundamental abstraction in Keras. A `Layer` encapsulates a state (weights) and some computation (defined in the `tf.keras.layers.Layer.call` method).\nWeights created by layers can be trainable or non-trainable. Layers are recursively composable: If you assign a layer instance as an attribute of another layer, the outer layer will start tracking the weights created by the inner layer.\nYou can also use layers to handle data preprocessing tasks like normalization and text vectorization. Preprocessing layers can be included directly into a model, either during or after training, which makes the model portable.\n### Models\nA model is an object that groups layers together and that can be trained on data.\nThe simplest type of model is the [`Sequential` model](https://www.tensorflow.org/guide/keras/sequential_model), which is a linear stack of layers. For more complex architectures, you can either use the [Keras functional API](https://www.tensorflow.org/guide/keras/functional_api), which lets you build arbitrary graphs of layers, or [use subclassing to write models from scratch](https://www.tensorflow.org/guide/keras/making_new_layers_and_models_via_subclassing).\nThe `tf.keras.Model` class features built-in training and evaluation methods:\n  * `tf.keras.Model.fit`: Trains the model for a fixed number of epochs.\n  * `tf.keras.Model.predict`: Generates output predictions for the input samples.\n  * `tf.keras.Model.evaluate`: Returns the loss and metrics values for the model; configured via the `tf.keras.Model.compile` method.\n\n\nThese methods give you access to the following built-in training features:\n  * [Callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). You can leverage built-in callbacks for early stopping, model checkpointing, and [TensorBoard](https://www.tensorflow.org/tensorboard) monitoring. You can also [implement custom callbacks](https://www.tensorflow.org/guide/keras/writing_your_own_callbacks).\n  * [Distributed training](https://www.tensorflow.org/guide/keras/distributed_training). You can easily scale up your training to multiple GPUs, TPUs, or devices.\n  * Step fusing. With the `steps_per_execution` argument in `tf.keras.Model.compile`, you can process multiple batches in a single `tf.function` call, which greatly improves device utilization on TPUs.\n\n\nFor a detailed overview of how to use `fit`, see the [training and evaluation guide](https://www.tensorflow.org/guide/keras/training_with_built_in_methods). To learn how to customize the built-in training and evaluation loops, see [Customizing what happens in `fit()`](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\n### Other APIs and tools\nKeras provides many other APIs and tools for deep learning, including:\nFor a full list of available APIs, see the \n## Next steps\nTo get started using Keras with TensorFlow, check out the following topics:\n  * [The Sequential model](https://www.tensorflow.org/guide/keras/sequential_model)\n  * [The Functional API](https://www.tensorflow.org/guide/keras/functional)\n  * [Training & evaluation with the built-in methods](https://www.tensorflow.org/guide/keras/training_with_built_in_methods)\n  * [Making new layers and models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\n  * [Serialization and saving](https://www.tensorflow.org/guide/keras/save_and_serialize)\n  * [Working with preprocessing layers](https://www.tensorflow.org/guide/keras/preprocessing_layers)\n  * [Customizing what happens in fit()](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)\n  * [Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n  * [Working with RNNs](https://www.tensorflow.org/guide/keras/rnn)\n  * [Understanding masking & padding](https://www.tensorflow.org/guide/keras/masking_and_padding)\n  * [Writing your own callbacks](https://www.tensorflow.org/guide/keras/custom_callback)\n  * [Transfer learning & fine-tuning](https://www.tensorflow.org/guide/keras/transfer_learning)\n  * [Multi-GPU and distributed training](https://www.tensorflow.org/guide/keras/distributed_training)\n\n\nTo learn more about Keras, see the following topics at \n",
  "https://www.tensorflow.org/js": "#  TensorFlow.js is a library for machine learning in JavaScript \nDevelop ML models in JavaScript, and use ML directly in the browser or in Node.js.\n[ See tutorials ](https://www.tensorflow.org/js/tutorials)\nTutorials show you how to use TensorFlow.js with complete, end-to-end examples. \n[ See models ](https://www.tensorflow.org/js/models)\nPre-trained, out-of-the-box models for common use cases. \n[ See demos ](https://www.tensorflow.org/js/demos)\nLive demos and examples run in your browser using TensorFlow.js. \n##  How it works \n####  Run existing models \nUse off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js.\n[ Use official TensorFlow.js models ](https://www.tensorflow.org/js/models) [ Convert Python models ](https://www.tensorflow.org/js/tutorials#convert_pretained_models_to_tensorflowjs)\n####  Retrain existing models \nRetrain pre-existing ML models using your own data.\n[ Use Transfer Learning to customize models ](https://www.tensorflow.org/js/tutorials/transfer/what_is_transfer_learning)\n####  Develop ML with JavaScript \nBuild and train models directly in JavaScript using flexible and intuitive APIs.\n[ Get started with TensorFlow.js ](https://www.tensorflow.org/js/tutorials)\n##  Demos \n[ Performance RNN ](https://magenta.tensorflow.org/demos/performance_rnn/index.html#2%7C2,0,1,0,1,1,0,1,0,1,0,1%7C1,1,1,1,1,1,1,1,1,1,1,1%7C1,1,1,1,1,1,1,1,1,1,1,1%7Cfalse)\nEnjoy a real-time piano performance by a neural network.\nPlay Pac-Man using images trained in your browser.\nTransport yourself to a tropical beach, outer space, and elsewhere with the power of web ML.\n##  News & announcements \nCheck out our [blog](https://blog.tensorflow.org/search?label=TensorFlow.js&max-results=20) for additional updates, and subscribe to our TensorFlow newsletter to get the latest announcements sent directly to your inbox.\nTensorFlow Blog\n### [How Adobe used Web ML with TensorFlow.js to enhance Photoshop for web](https://blog.tensorflow.org/2023/03/how-adobe-used-web-ml-with-tensorflowjs-to-enhance-photoshop-for-web.html)\nPhotoshop Web Beta is a browser-based version of the popular desktop image editing software, Adobe Photoshop. This online tool offers a wide range of features and capabilities for editing, enhancing, and manipulating images, all through a web\nTensorFlow\nMarch 30, 2023\nTensorFlow Blog\n### [Content moderation using machine learning: the server-side part](https://blog.tensorflow.org/2022/09/content-moderation-using-machine-learning-the-server-side-part.html)\ncd text - moderation / functions const functions = require ( 'firebase-functions' ); const toxicity = require ( '@tensorflow-models/toxicity' ); exports. moderator = functions. database. ref ( '/messages/{messageId}' ). onCreate ( async ( snapshot,\nTensorFlow\nSeptember 8, 2022\nTensorFlow Blog\n### [JAX on the Web with TensorFlow.js](https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html)\nIn this blog post we demonstrate how to convert and run Python-based JAX functions and Flax machine learning models in the browser using TensorFlow.js. We have produced three examples of JAX-to-TensorFlow.js conversion each with increasing\nTensorFlow\nAugust 31, 2022\nTensorFlow Blog\n### [Content moderation using machine learning: a dual approach](https://blog.tensorflow.org/2022/08/content-moderation-using-machine-learning-a-dual-approach.html)\nI've often wondered why anonymity drives people to say things that they'd never dare say in person, and it’s unfortunate that comment sections for videos and articles are so often toxic! If you’re interested in content moderation, you can use machine\nTensorFlow\nAugust 19, 2022\n##  Community participation \nSee more ways to participate in the TensorFlow community.\n##  Get started with TensorFlow.js \n[ Explore tutorials ](https://www.tensorflow.org/js/tutorials)\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma)  \n---  \nIn this notebook-based tutorial, we will create and run a TFX pipeline which creates a simple classification model and analyzes its performance across multiple runs. This notebook is based on the TFX pipeline we built in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple). If you have not read that tutorial yet, you should read it before proceeding with this notebook.\nAs you tweak your model or train it with a new dataset, you need to check whether your model has improved or become worse. Just checking top-level metrics like accuracy might not be enough. Every trained model should be evaluated before it is pushed to production.\nWe will add an `Evaluator` component to the pipeline created in the previous tutorial. The Evaluator component performs deep analysis for your models and compare the new model against a baseline to determine they are \"good enough\". It is implemented using the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) library.\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n## Set Up\nThe Set up process is the same as the previous tutorial.\nWe first need to install the TFX Python package and download the dataset which we will use for our model.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TFX\n```\npip\n```\n\n### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nCheck the TensorFlow and TFX versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n\n```\n```\n2024-05-08 09:12:22.461208: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:12:22.461259: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:12:22.462861: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\n\n```\n\n### Set up variables\nThere are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.\n```\nimportos\n\nPIPELINE_NAME = \"penguin-tfma\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfromabslimport logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n\n```\n\n### Prepare example data\nWe will use the same \nThere are four numeric features in this dataset which were already normalized to have range [0,1]. We will build a classification model which predicts the `species` of penguins.\nBecause TFX ExampleGen reads inputs from a directory, we need to create a directory and copy dataset to it.\n```\nimporturllib.request\nimporttempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n\n```\n```\n('/tmpfs/tmp/tfx-datakcma5ryu/data.csv',\n <http.client.HTTPMessage at 0x7fc5d80acb80>)\n\n```\n\n## Create a pipeline\nWe will add an [`Evaluator`](https://www.tensorflow.org/tfx/guide/evaluator) component to the pipeline we created in the [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nAn Evaluator component requires input data from an `ExampleGen` component and a model from a `Trainer` component and a [`tfma.EvalConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig) object. We can optionally supply a baseline model which can be used to compare metrics with the newly trained model.\nAn evaluator creates two kinds of output artifacts, `ModelEvaluation` and `ModelBlessing`. ModelEvaluation contains the detailed evaluation result which can be investigated and visualized further with TFMA library. ModelBlessing contains a boolean result whether the model passed given criteria and can be used in later components like a Pusher as a signal.\n### Write model training code\nWe will use the same model code as in the [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\n# Copied from https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfx.components.trainer.executorimport TrainerFnArgs\nfromtfx.components.trainer.fn_args_utilsimport DataAccessor\nfromtfx_bsl.tfxioimport dataset_options\nfromtensorflow_metadata.proto.v0import schema_pb2\n\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n# Since we're not generating or creating a schema, we will instead create\n# a feature spec.  Since there are a fairly small number of features this is\n# manageable for this dataset.\n_FEATURE_SPEC = {\n    **{\n        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n           for feature in _FEATURE_KEYS\n       },\n    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n}\n\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      dataset_options.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_build_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: TrainerFnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # This schema is usually either an output of SchemaGen or a manually-curated\n  # version provided by pipeline author. A schema can also derived from TFT\n  # graph if a Transform component is used. In the case when either is missing,\n  # `schema_from_feature_spec` could be used to generate schema from very simple\n  # feature_spec, but the schema returned would be very primitive.\n  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\n### Write a pipeline definition\nWe will define a function to create a TFX pipeline. In addition to the Evaluator component we mentioned above, we will add one more node called [`Resolver`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/Resolver). To check a new model is getting better than previous model, we need to compare it against a previous published model, called baseline. [ML Metadata(MLMD)](https://www.tensorflow.org/tfx/guide/mlmd) tracks all previous artifacts of the pipeline and `Resolver` can find what was the latest _blessed_ model -- a model passed Evaluator successfully -- from MLMD using a strategy class called `LatestBlessedModelStrategy`.\n```\nimporttensorflow_model_analysisastfma\n\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     module_file: str, serving_model_dir: str,\n                     metadata_path: str) -> tfx.dsl.Pipeline:\n\"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # NEW: Get the latest blessed model for Evaluator.\n  model_resolver = tfx.dsl.Resolver(\n      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n      model_blessing=tfx.dsl.Channel(\n          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n              'latest_blessed_model_resolver')\n\n  # NEW: Uses TFMA to compute evaluation statistics over features of a model and\n  #   perform quality validation of a candidate model (compared to a baseline).\n\n  eval_config = tfma.EvalConfig(\n      model_specs=[tfma.ModelSpec(label_key='species')],\n      slicing_specs=[\n          # An empty slice spec means the overall slice, i.e. the whole dataset.\n          tfma.SlicingSpec(),\n          # Calculate metrics for each penguin species.\n          tfma.SlicingSpec(feature_keys=['species']),\n          ],\n      metrics_specs=[\n          tfma.MetricsSpec(per_slice_thresholds={\n              'sparse_categorical_accuracy':\n                  tfma.PerSliceMetricThresholds(thresholds=[\n                      tfma.PerSliceMetricThreshold(\n                          slicing_specs=[tfma.SlicingSpec()],\n                          threshold=tfma.MetricThreshold(\n                              value_threshold=tfma.GenericValueThreshold(\n                                   lower_bound={'value': 0.6}),\n                              # Change threshold will be ignored if there is no\n                              # baseline model resolved from MLMD (first run).\n                              change_threshold=tfma.GenericChangeThreshold(\n                                  direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                                  absolute={'value': -1e-10}))\n                       )]),\n          })],\n      )\n  evaluator = tfx.components.Evaluator(\n      examples=example_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      baseline_model=model_resolver.outputs['model'],\n      eval_config=eval_config)\n\n  # Checks whether the model passed the validation steps and pushes the model\n  # to a file destination if check passed.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      model_blessing=evaluator.outputs['blessing'], # Pass an evaluation result.\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      trainer,\n\n      # Following two components were added to the pipeline.\n      model_resolver,\n      evaluator,\n\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n\n```\n\nWe need to supply the following information to the Evaluator via `eval_config`:\n  * Additional metrics to configure (if want more metrics than defined in model).\n  * Slices to configure\n  * Model validations thresholds to verify if validation to be included\n\n\nBecause `SparseCategoricalAccuracy` was already included in the `model.compile()` call, it will be included in the analysis automatically. So we do not add any additional metrics here. `SparseCategoricalAccuracy` will be used to decide whether the model is good enough, too.\nWe compute the metrics for the whole dataset and for each penguin species. `SlicingSpec` specifies how we aggregate the declared metrics.\nThere are two thresholds that a new model should pass, one is an absolute threshold of 0.6 and the other is a relative threshold that it should be higher than the baseline model. When you run the pipeline for the first time, the `change_threshold` will be ignored and only the value_threshold will be checked. If you run the pipeline more than once, the `Resolver` will find a model from the previous run and it will be used as a baseline model for the comparison.\nSee [Evaluator component guide](https://www.tensorflow.org/tfx/guide/evaluator#using_the_evaluator_component) for more information.\n## Run the pipeline\nWe will use `LocalDagRunner` as in the previous tutorial.\n```\ntfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n\n```\n```\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/penguin_trainer.py' (including modules: ['penguin_trainer']).\nINFO:absl:User module package has hash fingerprint version 1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmprw4uskdx/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmp380pw4r5', '--dist-dir', '/tmpfs/tmp/tmp3ooau66m']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl'; target user module is 'penguin_trainer'.\nINFO:absl:Full user module path is 'penguin_trainer@pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl'\nINFO:absl:Using deployment config:\n executor_specs {\n  key: \"CsvExampleGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"Evaluator\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.evaluator.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"Pusher\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.pusher.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"Trainer\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n    }\n  }\n}\ncustom_driver_specs {\n  key: \"CsvExampleGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n    }\n  }\n}\nmetadata_connection_config {\n  database_connection_config {\n    sqlite {\n      filename_uri: \"metadata/penguin-tfma/metadata.db\"\n      connection_mode: READWRITE_OPENCREATE\n    }\n  }\n}\n\nINFO:absl:Using connection config:\n sqlite {\n  filename_uri: \"metadata/penguin-tfma/metadata.db\"\n  connection_mode: READWRITE_OPENCREATE\n}\n\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-datakcma5ryu\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"Evaluator\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying penguin_trainer.py -> build/lib\ninstalling to /tmpfs/tmp/tmp380pw4r5\nrunning install\nrunning install_lib\ncopying build/lib/penguin_trainer.py -> /tmpfs/tmp/tmp380pw4r5\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Trainer.egg-info\nwriting tfx_user_code_Trainer.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nCopying tfx_user_code_Trainer.egg-info to /tmpfs/tmp/tmp380pw4r5/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmp380pw4r5/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmp3ooau66m/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl' and adding '/tmpfs/tmp/tmp380pw4r5' to it\nadding 'penguin_trainer.py'\nadding 'tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.dist-info/METADATA'\nadding 'tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.dist-info/WHEEL'\nadding 'tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.dist-info/top_level.txt'\nadding 'tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703.dist-info/RECORD'\nremoving /tmpfs/tmp/tmp380pw4r5\nINFO:absl:[CsvExampleGen] Resolved inputs: ({},)\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 1\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}), exec_properties={'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_base': '/tmpfs/tmp/tfx-datakcma5ryu', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548'}, execution_output_uri='pipelines/penguin-tfma/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-tfma/CsvExampleGen/.system/stateful_working_dir/71dcf2a2-5c57-4eda-b836-d76ab760acc0', tmp_dir='pipelines/penguin-tfma/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-datakcma5ryu\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"Evaluator\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfma\"\n, pipeline_run_id='2024-05-08T09:12:28.606391', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating examples.\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-datakcma5ryu/* to TFExample.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\nINFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 1 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfma/CsvExampleGen/.system/stateful_working_dir/71dcf2a2-5c57-4eda-b836-d76ab760acc0\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}) for execution 1\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\nINFO:absl:Component latest_blessed_model_resolver is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.dsl.components.common.resolver.Resolver\"\n  }\n  id: \"latest_blessed_model_resolver\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"_generated_model_3\"\n    value {\n      channels {\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n      }\n      hidden: true\n    }\n  }\n  inputs {\n    key: \"_generated_modelblessing_4\"\n    value {\n      channels {\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ModelBlessing\"\n          }\n        }\n      }\n      hidden: true\n    }\n  }\n  inputs {\n    key: \"model\"\n    value {\n      input_graph_ref {\n        graph_id: \"graph_1\"\n        key: \"model\"\n      }\n    }\n  }\n  inputs {\n    key: \"model_blessing\"\n    value {\n      input_graph_ref {\n        graph_id: \"graph_1\"\n        key: \"model_blessing\"\n      }\n    }\n  }\n  input_graphs {\n    key: \"graph_1\"\n    value {\n      nodes {\n        key: \"dict_2\"\n        value {\n          output_data_type: ARTIFACT_MULTIMAP\n          dict_node {\n            node_ids {\n              key: \"model\"\n              value: \"input_3\"\n            }\n            node_ids {\n              key: \"model_blessing\"\n              value: \"input_4\"\n            }\n          }\n        }\n      }\n      nodes {\n        key: \"input_3\"\n        value {\n          output_data_type: ARTIFACT_LIST\n          input_node {\n            input_key: \"_generated_model_3\"\n          }\n        }\n      }\n      nodes {\n        key: \"input_4\"\n        value {\n          output_data_type: ARTIFACT_LIST\n          input_node {\n            input_key: \"_generated_modelblessing_4\"\n          }\n        }\n      }\n      nodes {\n        key: \"op_1\"\n        value {\n          output_data_type: ARTIFACT_MULTIMAP\n          op_node {\n            op_type: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n            args {\n              node_id: \"dict_2\"\n            }\n          }\n        }\n      }\n      result_node: \"op_1\"\n    }\n  }\n}\ndownstream_nodes: \"Evaluator\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:Running as an resolver node.\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:[latest_blessed_model_resolver] Resolved inputs: ({'model_blessing': [], 'model': []},)\nINFO:absl:Component latest_blessed_model_resolver is finished.\nINFO:absl:Component Trainer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"Evaluator\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715159549765\nlast_update_time_since_epoch: 1715159549765\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 3\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715159549765\nlast_update_time_since_epoch: 1715159549765\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Trainer/model/3\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Trainer/model_run/3\"\n, artifact_type: name: \"ModelRun\"\n)]}), exec_properties={'module_path': 'penguin_trainer@pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}'}, execution_output_uri='pipelines/penguin-tfma/Trainer/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-tfma/Trainer/.system/stateful_working_dir/cc01f07b-5e10-46fd-89d4-e0e7d78fb6fe', tmp_dir='pipelines/penguin-tfma/Trainer/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"Evaluator\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfma\"\n, pipeline_run_id='2024-05-08T09:12:28.606391', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Train on the 'train' split when train_args.splits is not set.\nINFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\nINFO:absl:udf_utils.get_fn {'module_path': 'penguin_trainer@pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl', 'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}'} 'run_fn'\nINFO:absl:Installing 'pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmpflpa4y_q', 'pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl']\nProcessing ./pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-tfma/_wheels/tfx_user_code_Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703-py3-none-any.whl'.\nINFO:absl:Training model.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Trainer\nSuccessfully installed tfx-user-code-Trainer-0.0+1e19049dced0ccb21e0af60dae1c6e0ef09b63d1ff0e370d7f699920c2735703\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Model: \"model\"\nINFO:absl:__________________________________________________________________________________________________\nINFO:absl: Layer (type)                Output Shape                 Param #   Connected to                  \nINFO:absl:==================================================================================================\nINFO:absl: culmen_length_mm (InputLay  [(None, 1)]                  0         []                            \nINFO:absl: er)                                                                                              \nINFO:absl:                                                                                                  \nINFO:absl: culmen_depth_mm (InputLaye  [(None, 1)]                  0         []                            \nINFO:absl: r)                                                                                               \nINFO:absl:                                                                                                  \nINFO:absl: flipper_length_mm (InputLa  [(None, 1)]                  0         []                            \nINFO:absl: yer)                                                                                             \nINFO:absl:                                                                                                  \nINFO:absl: body_mass_g (InputLayer)    [(None, 1)]                  0         []                            \nINFO:absl:                                                                                                  \nINFO:absl: concatenate (Concatenate)   (None, 4)                    0         ['culmen_length_mm[0][0]',    \nINFO:absl:                                                                     'culmen_depth_mm[0][0]',     \nINFO:absl:                                                                     'flipper_length_mm[0][0]',   \nINFO:absl:                                                                     'body_mass_g[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense (Dense)               (None, 8)                    40        ['concatenate[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense_1 (Dense)             (None, 8)                    72        ['dense[0][0]']               \nINFO:absl:                                                                                                  \nINFO:absl: dense_2 (Dense)             (None, 3)                    27        ['dense_1[0][0]']             \nINFO:absl:                                                                                                  \nINFO:absl:==================================================================================================\nINFO:absl:Total params: 139 (556.00 Byte)\nINFO:absl:Trainable params: 139 (556.00 Byte)\nINFO:absl:Non-trainable params: 0 (0.00 Byte)\nINFO:absl:__________________________________________________________________________________________________\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715159557.217578   10799 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100/100 [==============================] - 2s 5ms/step - loss: 0.4620 - sparse_categorical_accuracy: 0.8455 - val_loss: 0.1694 - val_sparse_categorical_accuracy: 0.9400\nINFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to model_dense_2_biasadd_readvariableop_resource in the SavedModel.\nINFO:tensorflow:Assets written to: pipelines/penguin-tfma/Trainer/model/3/Format-Serving/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-tfma/Trainer/model/3/Format-Serving/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-tfma/Trainer/model/3/Format-Serving/fingerprint.pb\nINFO:absl:Training complete. Model written to pipelines/penguin-tfma/Trainer/model/3/Format-Serving. ModelRun written to pipelines/penguin-tfma/Trainer/model_run/3\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 3 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfma/Trainer/.system/stateful_working_dir/cc01f07b-5e10-46fd-89d4-e0e7d78fb6fe\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Trainer/model/3\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Trainer/model_run/3\"\n, artifact_type: name: \"ModelRun\"\n)]}) for execution 3\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Trainer is finished.\nINFO:absl:Component Evaluator is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.evaluator.component.Evaluator\"\n    base_type: EVALUATE\n  }\n  id: \"Evaluator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Evaluator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"baseline_model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"latest_blessed_model_resolver\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"blessing\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelBlessing\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"evaluation\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelEvaluation\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"eval_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"example_splits\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"fairness_indicator_thresholds\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"Trainer\"\nupstream_nodes: \"latest_blessed_model_resolver\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Evaluator] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715159549765\nlast_update_time_since_epoch: 1715159549765\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)], 'model': [Artifact(artifact: id: 2\ntype_id: 18\nuri: \"pipelines/penguin-tfma/Trainer/model/3\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715159558908\nlast_update_time_since_epoch: 1715159558908\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)], 'baseline_model': []},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 4\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfma/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715159548,sum_checksum:1715159548\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715159549765\nlast_update_time_since_epoch: 1715159549765\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)], 'model': [Artifact(artifact: id: 2\ntype_id: 18\nuri: \"pipelines/penguin-tfma/Trainer/model/3\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715159558908\nlast_update_time_since_epoch: 1715159558908\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)], 'baseline_model': []}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Evaluator/evaluation/4\"\n, artifact_type: name: \"ModelEvaluation\"\n)], 'blessing': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Evaluator/blessing/4\"\n, artifact_type: name: \"ModelBlessing\"\n)]}), exec_properties={'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'}, execution_output_uri='pipelines/penguin-tfma/Evaluator/.system/executor_execution/4/executor_output.pb', stateful_working_dir='pipelines/penguin-tfma/Evaluator/.system/stateful_working_dir/156cd629-f1d0-4e6d-8519-c9ad5128ceba', tmp_dir='pipelines/penguin-tfma/Evaluator/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.evaluator.component.Evaluator\"\n    base_type: EVALUATE\n  }\n  id: \"Evaluator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Evaluator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"baseline_model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"latest_blessed_model_resolver\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.latest_blessed_model_resolver\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"blessing\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelBlessing\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"evaluation\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelEvaluation\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"eval_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"per_slice_thresholds\\\": {\\n        \\\"sparse_categorical_accuracy\\\": {\\n          \\\"thresholds\\\": [\\n            {\\n              \\\"slicing_specs\\\": [\\n                {}\\n              ],\\n              \\\"threshold\\\": {\\n                \\\"change_threshold\\\": {\\n                  \\\"absolute\\\": -1e-10,\\n                  \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n                },\\n                \\\"value_threshold\\\": {\\n                  \\\"lower_bound\\\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"species\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"species\\\"\\n      ]\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"example_splits\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"fairness_indicator_thresholds\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"Trainer\"\nupstream_nodes: \"latest_blessed_model_resolver\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfma\"\n, pipeline_run_id='2024-05-08T09:12:28.606391', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'} 'custom_eval_shared_model'\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  label_key: \"species\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"species\"\n}\nmetrics_specs {\n  per_slice_thresholds {\n    key: \"sparse_categorical_accuracy\"\n    value {\n      thresholds {\n        slicing_specs {\n        }\n        threshold {\n          value_threshold {\n            lower_bound {\n              value: 0.6\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nINFO:absl:Using pipelines/penguin-tfma/Trainer/model/3/Format-Serving as  model.\nINFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\nINFO:absl:Evaluating model.\nINFO:absl:udf_utils.get_fn {'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"per_slice_thresholds\": {\\n        \"sparse_categorical_accuracy\": {\\n          \"thresholds\": [\\n            {\\n              \"slicing_specs\": [\\n                {}\\n              ],\\n              \"threshold\": {\\n                \"change_threshold\": {\\n                  \"absolute\": -1e-10,\\n                  \"direction\": \"HIGHER_IS_BETTER\"\\n                },\\n                \"value_threshold\": {\\n                  \"lower_bound\": 0.6\\n                }\\n              }\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"species\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {},\\n    {\\n      \"feature_keys\": [\\n        \"species\"\\n      ]\\n    }\\n  ]\\n}', 'example_splits': 'null', 'fairness_indicator_thresholds': 'null'} 'custom_extractors'\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  label_key: \"species\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"species\"\n}\nmetrics_specs {\n  model_names: \"\"\n  per_slice_thresholds {\n    key: \"sparse_categorical_accuracy\"\n    value {\n      thresholds {\n        slicing_specs {\n        }\n        threshold {\n          value_threshold {\n            lower_bound {\n              value: 0.6\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  label_key: \"species\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"species\"\n}\nmetrics_specs {\n  model_names: \"\"\n  per_slice_thresholds {\n    key: \"sparse_categorical_accuracy\"\n    value {\n      thresholds {\n        slicing_specs {\n        }\n        threshold {\n          value_threshold {\n            lower_bound {\n              value: 0.6\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nINFO:absl:eval_shared_models have model_types: {'tf_keras'}\nINFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\nmodel_specs {\n  label_key: \"species\"\n}\nslicing_specs {\n}\nslicing_specs {\n  feature_keys: \"species\"\n}\nmetrics_specs {\n  model_names: \"\"\n  per_slice_thresholds {\n    key: \"sparse_categorical_accuracy\"\n    value {\n      thresholds {\n        slicing_specs {\n        }\n        threshold {\n          value_threshold {\n            lower_bound {\n              value: 0.6\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\nINFO:absl:Evaluation complete. Results written to pipelines/penguin-tfma/Evaluator/evaluation/4.\nINFO:absl:Checking validation results.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nINFO:absl:Blessing result True written to pipelines/penguin-tfma/Evaluator/blessing/4.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 4 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfma/Evaluator/.system/stateful_working_dir/156cd629-f1d0-4e6d-8519-c9ad5128ceba\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Evaluator/evaluation/4\"\n, artifact_type: name: \"ModelEvaluation\"\n)], 'blessing': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Evaluator/blessing/4\"\n, artifact_type: name: \"ModelBlessing\"\n)]}) for execution 4\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Evaluator is finished.\nINFO:absl:Component Pusher is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n  inputs {\n    key: \"model_blessing\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Evaluator\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Evaluator\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ModelBlessing\"\n          }\n        }\n        output_key: \"blessing\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-tfma\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Evaluator\"\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Pusher] Resolved inputs: ({'model_blessing': [Artifact(artifact: id: 5\ntype_id: 22\nuri: \"pipelines/penguin-tfma/Evaluator/blessing/4\"\ncustom_properties {\n  key: \"blessed\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"current_model\"\n  value {\n    string_value: \"pipelines/penguin-tfma/Trainer/model/3\"\n  }\n}\ncustom_properties {\n  key: \"current_model_id\"\n  value {\n    int_value: 2\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ModelBlessing\"\ncreate_time_since_epoch: 1715159563475\nlast_update_time_since_epoch: 1715159563475\n, artifact_type: id: 22\nname: \"ModelBlessing\"\n)], 'model': [Artifact(artifact: id: 2\ntype_id: 18\nuri: \"pipelines/penguin-tfma/Trainer/model/3\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715159558908\nlast_update_time_since_epoch: 1715159558908\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 5\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'model_blessing': [Artifact(artifact: id: 5\ntype_id: 22\nuri: \"pipelines/penguin-tfma/Evaluator/blessing/4\"\ncustom_properties {\n  key: \"blessed\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"current_model\"\n  value {\n    string_value: \"pipelines/penguin-tfma/Trainer/model/3\"\n  }\n}\ncustom_properties {\n  key: \"current_model_id\"\n  value {\n    int_value: 2\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ModelBlessing\"\ncreate_time_since_epoch: 1715159563475\nlast_update_time_since_epoch: 1715159563475\n, artifact_type: id: 22\nname: \"ModelBlessing\"\n)], 'model': [Artifact(artifact: id: 2\ntype_id: 18\nuri: \"pipelines/penguin-tfma/Trainer/model/3\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715159558908\nlast_update_time_since_epoch: 1715159558908\n, artifact_type: id: 18\nname: \"Model\"\nbase_type: MODEL\n)]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Pusher/pushed_model/5\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-tfma\"\\n  }\\n}'}, execution_output_uri='pipelines/penguin-tfma/Pusher/.system/executor_execution/5/executor_output.pb', stateful_working_dir='pipelines/penguin-tfma/Pusher/.system/stateful_working_dir/12b47904-285c-43d9-bb2e-9b1b59dce2f0', tmp_dir='pipelines/penguin-tfma/Pusher/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:12:28.606391\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfma.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n  inputs {\n    key: \"model_blessing\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Evaluator\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:12:28.606391\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfma.Evaluator\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ModelBlessing\"\n          }\n        }\n        output_key: \"blessing\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-tfma\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Evaluator\"\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfma\"\n, pipeline_run_id='2024-05-08T09:12:28.606391', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Model version: 1715159563\nINFO:absl:Model written to serving path serving_model/penguin-tfma/1715159563.\nINFO:absl:Model pushed to pipelines/penguin-tfma/Pusher/pushed_model/5.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 5 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfma/Pusher/.system/stateful_working_dir/12b47904-285c-43d9-bb2e-9b1b59dce2f0\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-tfma/Pusher/pushed_model/5\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}) for execution 5\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Pusher is finished.\n\n```\n\nWhen the pipeline completed, you should be able to see something like following:\n```\nINFO:absl:BlessingresultTruewrittentopipelines/penguin-tfma/Evaluator/blessing/4.\n\n```\n\nOr you can also check manually the output directory where the generated artifacts are stored. If you visit `pipelines/penguin-tfma/Evaluator/blessing/` with a file broswer, you can see a file with a name `BLESSED` or `NOT_BLESSED` according to the evaluation result.\nIf the blessing result is `False`, Pusher will refuse to push the model to the `serving_model_dir`, because the model is not good enough to be used in production.\nYou can run the pipeline again possibly with different evaluation configs. Even if you run the pipeline with the exact same config and dataset, the trained model might be slightly different due to the inherent randomness of the model training which can lead to a `NOT_BLESSED` model.\n### Examine outputs of the pipeline\nYou can use TFMA to investigate and visualize the evaluation result in ModelEvaluation artifact.\n#### Get analysis result from output artifacts\nYou can use MLMD APIs to locate these outputs programatically. First, we will define some utility functions to search for the output artifacts that were just produced.\n```\nfromml_metadata.protoimport metadata_store_pb2\n# Non-public APIs, just for showcase.\nfromtfx.orchestration.portable.mlmdimport execution_lib\n\n# TODO(b/171447278): Move these functions into the TFX library.\n\ndefget_latest_artifacts(metadata, pipeline_name, component_id):\n\"\"\"Output artifacts of the latest run of the component.\"\"\"\n  context = metadata.store.get_context_by_type_and_name(\n      'node', f'{pipeline_name}.{component_id}')\n  executions = metadata.store.get_executions_by_context(context.id)\n  latest_execution = max(executions,\n                         key=lambda e:e.last_update_time_since_epoch)\n  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n\n```\n\nWe can find the latest execution of the `Evaluator` component and get output artifacts of it.\n```\n# Non-public APIs, just for showcase.\nfromtfx.orchestration.metadataimport Metadata\nfromtfx.typesimport standard_component_specs\n\nmetadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  # Find output artifacts from MLMD.\n  evaluator_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n                                          'Evaluator')\n  eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0]\n\n```\n```\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\n`Evaluator` always returns one evaluation artifact, and we can visualize it using TensorFlow Model Analysis library. For example, following code will render the accuracy metrics for each penguin species.\n```\nimporttensorflow_model_analysisastfma\n\neval_result = tfma.load_eval_result(eval_artifact.uri)\ntfma.view.render_slicing_metrics(eval_result, slicing_column='species')\n\n```\n```\nSlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'species:0', 'metrics…\n\n```\n\nIf you choose 'sparse_categorical_accuracy' in `Show` drop-down list, you can see the accuracy values per species. You might want to add more slices and check whether your model is good for all distribution and if there is any possible bias.\n## Next steps\nLearn more on model analysis at [TensorFlow Model Analysis library tutorial](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic).\nYou can find more resources on <https://www.tensorflow.org/tfx/tutorials>\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft": "**_Transform input data and train a model with a TFX pipeline._**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)  \n---  \nIn this notebook-based tutorial, we will create and run a TFX pipeline to ingest raw input data and preprocess it appropriately for ML training. This notebook is based on the TFX pipeline we built in [Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv). If you have not read that one yet, you should read it before proceeding with this notebook.\nYou can increase the predictive quality of your data and/or reduce dimensionality with feature engineering. One of the benefits of using TFX is that you will write your transformation code once, and the resulting transforms will be consistent between training and serving in order to avoid training/serving skew.\nWe will add a `Transform` component to the pipeline. The Transform component is implemented using the [tf.transform](https://www.tensorflow.org/tfx/transform/get_started) library.\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n## Set Up\nWe first need to install the TFX Python package and download the dataset which we will use for our model.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TFX\n```\npip\n```\n\n### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nCheck the TensorFlow and TFX versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n\n```\n```\n2024-05-08 09:20:08.101132: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:20:08.101177: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:20:08.102632: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\n\n```\n\n### Set up variables\nThere are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.\n```\nimportos\n\nPIPELINE_NAME = \"penguin-transform\"\n\n# Output directory to store artifacts generated from the pipeline.\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfromabslimport logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n\n```\n\n### Prepare example data\nWe will download the example dataset for use in our TFX pipeline. The dataset we are using is \nHowever, unlike previous tutorials which used an already preprocessed dataset, we will use the **raw** Palmer Penguins dataset.\nBecause the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it.\n```\nimporturllib.request\nimporttempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_path, _data_filepath)\n\n```\n```\n('/tmpfs/tmp/tfx-data244l5nap/data.csv',\n <http.client.HTTPMessage at 0x7f9eee0ce2b0>)\n\n```\n\nTake a quick look at what the raw data looks like.\n```\nhead{_data_filepath}\n```\n```\nspecies,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,MALE\nAdelie,Torgersen,39.5,17.4,186,3800,FEMALE\nAdelie,Torgersen,40.3,18,195,3250,FEMALE\nAdelie,Torgersen,NA,NA,NA,NA,NA\nAdelie,Torgersen,36.7,19.3,193,3450,FEMALE\nAdelie,Torgersen,39.3,20.6,190,3650,MALE\nAdelie,Torgersen,38.9,17.8,181,3625,FEMALE\nAdelie,Torgersen,39.2,19.6,195,4675,MALE\nAdelie,Torgersen,34.1,18.1,193,3475,NA\n\n```\n\nThere are some entries with missing values which are represented as `NA`. We will just delete those entries in this tutorial.\n```\nsed'/\\bNA\\b/d'{_data_filepath}\nhead{_data_filepath}\n```\n```\nspecies,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,MALE\nAdelie,Torgersen,39.5,17.4,186,3800,FEMALE\nAdelie,Torgersen,40.3,18,195,3250,FEMALE\nAdelie,Torgersen,36.7,19.3,193,3450,FEMALE\nAdelie,Torgersen,39.3,20.6,190,3650,MALE\nAdelie,Torgersen,38.9,17.8,181,3625,FEMALE\nAdelie,Torgersen,39.2,19.6,195,4675,MALE\nAdelie,Torgersen,41.1,17.6,182,3200,FEMALE\nAdelie,Torgersen,38.6,21.2,191,3800,MALE\n\n```\n\nYou should be able to see seven features which describe penguins. We will use the same set of features as the previous tutorials - 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' - and will predict the 'species' of a penguin.\n**The only difference will be that the input data is not preprocessed.** Note that we will not use other features like 'island' or 'sex' in this tutorial.\n### Prepare a schema file\nAs described in [Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv), we need a schema file for the dataset. Because the dataset is different from the previous tutorial we need to generate it again. In this tutorial, we will skip those steps and just use a prepared schema file.\n```\nimportshutil\n\nSCHEMA_PATH = 'schema'\n\n_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n_schema_filename = 'schema.pbtxt'\n_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n\nos.makedirs(SCHEMA_PATH, exist_ok=True)\nurllib.request.urlretrieve(_schema_uri, _schema_filepath)\n\n```\n```\n('schema/schema.pbtxt', <http.client.HTTPMessage at 0x7fa026398e80>)\n\n```\n\nThis schema file was created with the same pipeline as in the previous tutorial without any manual changes.\n## Create a pipeline\nTFX pipelines are defined using Python APIs. We will add `Transform` component to the pipeline we created in the [Data Validation tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv).\nA Transform component requires input data from an `ExampleGen` component and a schema from a `SchemaGen` component, and produces a \"transform graph\". The output will be used in a `Trainer` component. Transform can optionally produce \"transformed data\" in addition, which is the materialized data after transformation. However, we will transform data during training in this tutorial without materialization of the intermediate transformed data.\nOne thing to note is that we need to define a Python function, `preprocessing_fn` to describe how input data should be transformed. This is similar to a Trainer component which also requires user code for model definition.\n### Write preprocessing and training code\nWe need to define two Python functions. One for Transform and one for Trainer.\n#### preprocessing_fn\nThe Transform component will find a function named `preprocessing_fn` in the given module file as we did for `Trainer` component. You can also specify a specific function using the \nIn this example, we will do two kinds of transformation. For continuous numeric features like `culmen_length_mm` and `body_mass_g`, we will normalize these values using the [tft.scale_to_z_score](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score) function. For the label feature, we need to convert string labels into numeric index values. We will use [`tf.lookup.StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable) for conversion.\n#### run_fn\nThe model itself is almost the same as in the previous tutorials, but this time we will transform the input data using the transform graph from the Transform component.\nOne more important difference compared to the previous tutorial is that now we export a model for serving which includes not only the computation graph of the model, but also the transform graph for preprocessing, which is generated in Transform component. We need to define a separate function which will be used for serving incoming requests. You can see that the same function `_apply_preprocessing` was used for both of the training data and the serving request.\n```\n_module_file = 'penguin_utils.py'\n\n```\n```\n%%writefile {_module_file}\n\n\nfromtypingimport List, Text\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_metadata.proto.v0import schema_pb2\nimporttensorflow_transformastft\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\n\n# Specify features that we will use.\n_FEATURE_KEYS = [\n    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n]\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\n\n# NEW: TFX Transform will call this function.\ndefpreprocessing_fn(inputs):\n\"\"\"tf.transform's callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature.\n  \"\"\"\n  outputs = {}\n\n  # Uses features defined in _FEATURE_KEYS only.\n  for key in _FEATURE_KEYS:\n    # tft.scale_to_z_score computes the mean and variance of the given feature\n    # and scales the output based on the result.\n    outputs[key] = tft.scale_to_z_score(inputs[key])\n\n  # For the label column we provide the mapping from string to index.\n  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n  # compute the vocabulary dynamically and perform a lookup.\n  # Since in this example there are only 3 possible values, we use a hard-coded\n  # table for simplicity.\n  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n  initializer = tf.lookup.KeyValueTensorInitializer(\n      keys=table_keys,\n      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n      key_dtype=tf.string,\n      value_dtype=tf.int64)\n  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n\n  return outputs\n\n\n# NEW: This function will apply the same transform operation to training data\n#      and serving requests.\ndef_apply_preprocessing(raw_features, tft_layer):\n  transformed_features = tft_layer(raw_features)\n  if _LABEL_KEY in raw_features:\n    transformed_label = transformed_features.pop(_LABEL_KEY)\n    return transformed_features, transformed_label\n  else:\n    return transformed_features, None\n\n\n# NEW: This function will create a handler function which gets a serialized\n#      tf.example, preprocess and run an inference with it.\ndef_get_serve_tf_examples_fn(model, tf_transform_output):\n  # We must save the tft_layer to the model to ensure its assets are kept and\n  # tracked.\n  model.tft_layer = tf_transform_output.transform_features_layer()\n\n  @tf.function(input_signature=[\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n  ])\n  defserve_tf_examples_fn(serialized_tf_examples):\n    # Expected input is a string which is serialized tf.Example format.\n    feature_spec = tf_transform_output.raw_feature_spec()\n    # Because input schema includes unnecessary fields like 'species' and\n    # 'island', we filter feature_spec to include required keys only.\n    required_feature_spec = {\n        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n    }\n    parsed_features = tf.io.parse_example(serialized_tf_examples,\n                                          required_feature_spec)\n\n    # Preprocess parsed input with transform operation defined in\n    # preprocessing_fn().\n    transformed_features, _ = _apply_preprocessing(parsed_features,\n                                                   model.tft_layer)\n    # Run inference with ML model.\n    return model(transformed_features)\n\n  return serve_tf_examples_fn\n\n\ndef_input_fn(file_pattern: List[Text],\n              data_accessor: tfx.components.DataAccessor,\n              tf_transform_output: tft.TFTransformOutput,\n              batch_size: int = 200) -> tf.data.Dataset:\n\"\"\"Generates features and label for tuning/training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    tf_transform_output: A TFTransformOutput.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  dataset = data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n      schema=tf_transform_output.raw_metadata.schema)\n\n  transform_layer = tf_transform_output.transform_features_layer()\n  defapply_transform(raw_features):\n    return _apply_preprocessing(raw_features, transform_layer)\n\n  return dataset.map(apply_transform).repeat()\n\n\ndef_build_keras_model() -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n  inputs = [\n      keras.layers.Input(shape=(1,), name=key)\n      for key in _FEATURE_KEYS\n  ]\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      tf_transform_output,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      tf_transform_output,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model()\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # NEW: Save a computation graph including transform layer.\n  signatures = {\n      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n  }\n  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)\n\n```\n```\nWriting penguin_utils.py\n\n```\n\nNow you have completed all of the preparation steps to build a TFX pipeline.\n### Write a pipeline definition\nWe define a function to create a TFX pipeline. A `Pipeline` object represents a TFX pipeline, which can be run using one of the pipeline orchestration systems that TFX supports.\n```\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     schema_path: str, module_file: str, serving_model_dir: str,\n                     metadata_path: str) -> tfx.dsl.Pipeline:\n\"\"\"Implements the penguin pipeline with TFX.\"\"\"\n  # Brings data into the pipeline or otherwise joins/converts training data.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Computes statistics over data for visualization and example validation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # Import the schema.\n  schema_importer = tfx.dsl.Importer(\n      source_uri=schema_path,\n      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n          'schema_importer')\n\n  # Performs anomaly detection based on statistics and data schema.\n  example_validator = tfx.components.ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_importer.outputs['result'])\n\n  # NEW: Transforms input data using preprocessing_fn in the 'module_file'.\n  transform = tfx.components.Transform(\n      examples=example_gen.outputs['examples'],\n      schema=schema_importer.outputs['result'],\n      materialize=False,\n      module_file=module_file)\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n\n      # NEW: Pass transform_graph to the trainer.\n      transform_graph=transform.outputs['transform_graph'],\n\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n      statistics_gen,\n      schema_importer,\n      example_validator,\n\n      transform,  # NEW: Transform component was added to the pipeline.\n\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n\n```\n\n## Run the pipeline\nWe will use `LocalDagRunner` as in the previous tutorial.\n```\ntfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      schema_path=SCHEMA_PATH,\n      module_file=_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/penguin_utils.py' (including modules: ['penguin_utils']).\nINFO:absl:User module package has hash fingerprint version a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmpkolg3eiy/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmp1yspllww', '--dist-dir', '/tmpfs/tmp/tmp0f9tyt4n']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'; target user module is 'penguin_utils'.\nINFO:absl:Full user module path is 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/penguin_utils.py' (including modules: ['penguin_utils']).\nINFO:absl:User module package has hash fingerprint version a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmphxgs65wf/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmphmpuvvpe', '--dist-dir', '/tmpfs/tmp/tmpmyjp2nkq']\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying penguin_utils.py -> build/lib\ninstalling to /tmpfs/tmp/tmp1yspllww\nrunning install\nrunning install_lib\ncopying build/lib/penguin_utils.py -> /tmpfs/tmp/tmp1yspllww\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Transform.egg-info\nwriting tfx_user_code_Transform.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Transform.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\nCopying tfx_user_code_Transform.egg-info to /tmpfs/tmp/tmp1yspllww/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmp1yspllww/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmp0f9tyt4n/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' and adding '/tmpfs/tmp/tmp1yspllww' to it\nadding 'penguin_utils.py'\nadding 'tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/METADATA'\nadding 'tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/WHEEL'\nadding 'tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/top_level.txt'\nadding 'tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/RECORD'\nremoving /tmpfs/tmp/tmp1yspllww\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'; target user module is 'penguin_utils'.\nINFO:absl:Full user module path is 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'\nINFO:absl:Using deployment config:\n executor_specs {\n  key: \"CsvExampleGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"ExampleValidator\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_validator.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"Pusher\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.pusher.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"StatisticsGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"Trainer\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"Transform\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.transform.executor.Executor\"\n      }\n    }\n  }\n}\ncustom_driver_specs {\n  key: \"CsvExampleGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n    }\n  }\n}\nmetadata_connection_config {\n  database_connection_config {\n    sqlite {\n      filename_uri: \"metadata/penguin-transform/metadata.db\"\n      connection_mode: READWRITE_OPENCREATE\n    }\n  }\n}\n\nINFO:absl:Using connection config:\n sqlite {\n  filename_uri: \"metadata/penguin-transform/metadata.db\"\n  connection_mode: READWRITE_OPENCREATE\n}\n\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-data244l5nap\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\ndownstream_nodes: \"Trainer\"\ndownstream_nodes: \"Transform\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying penguin_utils.py -> build/lib\ninstalling to /tmpfs/tmp/tmphmpuvvpe\nrunning install\nrunning install_lib\ncopying build/lib/penguin_utils.py -> /tmpfs/tmp/tmphmpuvvpe\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Trainer.egg-info\nwriting tfx_user_code_Trainer.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nCopying tfx_user_code_Trainer.egg-info to /tmpfs/tmp/tmphmpuvvpe/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmphmpuvvpe/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmpmyjp2nkq/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' and adding '/tmpfs/tmp/tmphmpuvvpe' to it\nadding 'penguin_utils.py'\nadding 'tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/METADATA'\nadding 'tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/WHEEL'\nadding 'tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/top_level.txt'\nadding 'tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9.dist-info/RECORD'\nremoving /tmpfs/tmp/tmphmpuvvpe\nINFO:absl:[CsvExampleGen] Resolved inputs: ({},)\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 1\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}), exec_properties={'input_base': '/tmpfs/tmp/tfx-data244l5nap', 'output_data_format': 6, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013'}, execution_output_uri='pipelines/penguin-transform/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/CsvExampleGen/.system/stateful_working_dir/2a68db53-a342-4b70-b5ef-4bea08945c28', tmp_dir='pipelines/penguin-transform/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-data244l5nap\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\ndownstream_nodes: \"Trainer\"\ndownstream_nodes: \"Transform\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating examples.\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-data244l5nap/* to TFExample.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\nINFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 1 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/CsvExampleGen/.system/stateful_working_dir/2a68db53-a342-4b70-b5ef-4bea08945c28\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}) for execution 1\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\nINFO:absl:Component schema_importer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.dsl.components.common.importer.Importer\"\n  }\n  id: \"schema_importer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.schema_importer\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"result\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"artifact_uri\"\n    value {\n      field_value {\n        string_value: \"schema\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_key\"\n    value {\n      field_value {\n        string_value: \"result\"\n      }\n    }\n  }\n  parameters {\n    key: \"reimport\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n}\ndownstream_nodes: \"ExampleValidator\"\ndownstream_nodes: \"Transform\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:Running as an importer node.\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Processing source uri: schema, properties: {}, custom_properties: {}\nINFO:absl:Component schema_importer is finished.\nINFO:absl:Component StatisticsGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"ExampleValidator\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 3\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-transform/StatisticsGen/statistics/3\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-transform/StatisticsGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/StatisticsGen/.system/stateful_working_dir/87ad10ec-9fbd-4d39-bccf-4085439aebed', tmp_dir='pipelines/penguin-transform/StatisticsGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"ExampleValidator\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating statistics for split train.\nINFO:absl:Statistics for split train written to pipelines/penguin-transform/StatisticsGen/statistics/3/Split-train.\nINFO:absl:Generating statistics for split eval.\nINFO:absl:Statistics for split eval written to pipelines/penguin-transform/StatisticsGen/statistics/3/Split-eval.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 3 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/StatisticsGen/.system/stateful_working_dir/87ad10ec-9fbd-4d39-bccf-4085439aebed\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-transform/StatisticsGen/statistics/3\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}) for execution 3\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component StatisticsGen is finished.\nINFO:absl:Component Transform is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.transform.component.Transform\"\n    base_type: TRANSFORM\n  }\n  id: \"Transform\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Transform\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"post_transform_anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"post_transform_schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"post_transform_stats\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"pre_transform_schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"pre_transform_stats\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"transform_graph\"\n    value {\n      artifact_spec {\n        type {\n          name: \"TransformGraph\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"updated_analyzer_cache\"\n    value {\n      artifact_spec {\n        type {\n          name: \"TransformCache\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"disable_statistics\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n  parameters {\n    key: \"force_tf_compat_v1\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"schema_importer\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Transform] Resolved inputs: ({'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160016371\nlast_update_time_since_epoch: 1715160016371\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 4\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160016371\nlast_update_time_since_epoch: 1715160016371\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'post_transform_schema': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_schema/4\"\n, artifact_type: name: \"Schema\"\n)], 'updated_analyzer_cache': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/updated_analyzer_cache/4\"\n, artifact_type: name: \"TransformCache\"\n)], 'pre_transform_stats': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/pre_transform_stats/4\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'transform_graph': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/transform_graph/4\"\n, artifact_type: name: \"TransformGraph\"\n)], 'post_transform_stats': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_stats/4\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'post_transform_anomalies': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_anomalies/4\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)], 'pre_transform_schema': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/pre_transform_schema/4\"\n, artifact_type: name: \"Schema\"\n)]}), exec_properties={'force_tf_compat_v1': 0, 'module_path': 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl', 'disable_statistics': 0, 'custom_config': 'null'}, execution_output_uri='pipelines/penguin-transform/Transform/.system/executor_execution/4/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/Transform/.system/stateful_working_dir/353ff412-c22b-442d-86fa-d000785c15c6', tmp_dir='pipelines/penguin-transform/Transform/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.transform.component.Transform\"\n    base_type: TRANSFORM\n  }\n  id: \"Transform\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Transform\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"post_transform_anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"post_transform_schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"post_transform_stats\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"pre_transform_schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"pre_transform_stats\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"transform_graph\"\n    value {\n      artifact_spec {\n        type {\n          name: \"TransformGraph\"\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"updated_analyzer_cache\"\n    value {\n      artifact_spec {\n        type {\n          name: \"TransformCache\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"disable_statistics\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n  parameters {\n    key: \"force_tf_compat_v1\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"schema_importer\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\nINFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\nINFO:absl:Installing 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmp0mjyiork', 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl']\nProcessing ./pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'.\nINFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\nINFO:absl:Installing 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmp_hkp2fs0', 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl']\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9\nProcessing ./pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'.\nINFO:absl:Installing 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmprvrky6ts', 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl']\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9\nProcessing ./pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-transform/_wheels/tfx_user_code_Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Transform\nSuccessfully installed tfx-user-code-Transform-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/c08ebbeca8814f288e2437f43b1224e5/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/c08ebbeca8814f288e2437f43b1224e5/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/c08ebbeca8814f288e2437f43b1224e5/fingerprint.pb\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/922454c8ed954a439ec7388ab1479458/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/922454c8ed954a439ec7388ab1479458/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-transform/Transform/transform_graph/4/.temp_path/tftransform_tmp/922454c8ed954a439ec7388ab1479458/fingerprint.pb\nWARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: key_value_init/LookupTableImportV2\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 4 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/Transform/.system/stateful_working_dir/353ff412-c22b-442d-86fa-d000785c15c6\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'post_transform_schema': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_schema/4\"\n, artifact_type: name: \"Schema\"\n)], 'updated_analyzer_cache': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/updated_analyzer_cache/4\"\n, artifact_type: name: \"TransformCache\"\n)], 'pre_transform_stats': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/pre_transform_stats/4\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'transform_graph': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/transform_graph/4\"\n, artifact_type: name: \"TransformGraph\"\n)], 'post_transform_stats': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_stats/4\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'post_transform_anomalies': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/post_transform_anomalies/4\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)], 'pre_transform_schema': [Artifact(artifact: uri: \"pipelines/penguin-transform/Transform/pre_transform_schema/4\"\n, artifact_type: name: \"Schema\"\n)]}) for execution 4\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Transform is finished.\nINFO:absl:Component ExampleValidator is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_validator.component.ExampleValidator\"\n  }\n  id: \"ExampleValidator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.ExampleValidator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nupstream_nodes: \"schema_importer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[ExampleValidator] Resolved inputs: ({'statistics': [Artifact(artifact: id: 3\ntype_id: 19\nuri: \"pipelines/penguin-transform/StatisticsGen/statistics/3\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160019591\nlast_update_time_since_epoch: 1715160019591\n, artifact_type: id: 19\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160016371\nlast_update_time_since_epoch: 1715160016371\n, artifact_type: id: 17\nname: \"Schema\"\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 5\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'statistics': [Artifact(artifact: id: 3\ntype_id: 19\nuri: \"pipelines/penguin-transform/StatisticsGen/statistics/3\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160019591\nlast_update_time_since_epoch: 1715160019591\n, artifact_type: id: 19\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)], 'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160016371\nlast_update_time_since_epoch: 1715160016371\n, artifact_type: id: 17\nname: \"Schema\"\n)]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"pipelines/penguin-transform/ExampleValidator/anomalies/5\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-transform/ExampleValidator/.system/executor_execution/5/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/ExampleValidator/.system/stateful_working_dir/5c7a9496-8531-48e8-9884-9db5c9dd4bb2', tmp_dir='pipelines/penguin-transform/ExampleValidator/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_validator.component.ExampleValidator\"\n  }\n  id: \"ExampleValidator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.ExampleValidator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nupstream_nodes: \"schema_importer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Validating schema against the computed statistics for split train.\nINFO:absl:Anomalies alerts created for split train.\nINFO:absl:Validation complete for split train. Anomalies written to pipelines/penguin-transform/ExampleValidator/anomalies/5/Split-train.\nINFO:absl:Validating schema against the computed statistics for split eval.\nINFO:absl:Anomalies alerts created for split eval.\nINFO:absl:Validation complete for split eval. Anomalies written to pipelines/penguin-transform/ExampleValidator/anomalies/5/Split-eval.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 5 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/ExampleValidator/.system/stateful_working_dir/5c7a9496-8531-48e8-9884-9db5c9dd4bb2\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"pipelines/penguin-transform/ExampleValidator/anomalies/5\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)]}) for execution 5\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component ExampleValidator is finished.\nINFO:absl:Component Trainer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"transform_graph\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Transform\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.Transform\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"TransformGraph\"\n          }\n        }\n        output_key: \"transform_graph\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"Transform\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)], 'transform_graph': [Artifact(artifact: id: 7\ntype_id: 22\nuri: \"pipelines/penguin-transform/Transform/transform_graph/4\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"TransformGraph\"\ncreate_time_since_epoch: 1715160039921\nlast_update_time_since_epoch: 1715160039921\n, artifact_type: id: 22\nname: \"TransformGraph\"\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 6\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-transform/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:13161,xor_checksum:1715160013,sum_checksum:1715160013\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160016351\nlast_update_time_since_epoch: 1715160016351\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)], 'transform_graph': [Artifact(artifact: id: 7\ntype_id: 22\nuri: \"pipelines/penguin-transform/Transform/transform_graph/4\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"TransformGraph\"\ncreate_time_since_epoch: 1715160039921\nlast_update_time_since_epoch: 1715160039921\n, artifact_type: id: 22\nname: \"TransformGraph\"\n)]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-transform/Trainer/model/6\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-transform/Trainer/model_run/6\"\n, artifact_type: name: \"ModelRun\"\n)]}), exec_properties={'train_args': '{\\n  \"num_steps\": 100\\n}', 'custom_config': 'null', 'module_path': 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl', 'eval_args': '{\\n  \"num_steps\": 5\\n}'}, execution_output_uri='pipelines/penguin-transform/Trainer/.system/executor_execution/6/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/Trainer/.system/stateful_working_dir/04611601-4dbf-412d-8c6c-4e4864bb335a', tmp_dir='pipelines/penguin-transform/Trainer/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"transform_graph\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Transform\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.Transform\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"TransformGraph\"\n          }\n        }\n        output_key: \"transform_graph\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"Transform\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Train on the 'train' split when train_args.splits is not set.\nINFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\nINFO:absl:udf_utils.get_fn {'train_args': '{\\n  \"num_steps\": 100\\n}', 'custom_config': 'null', 'module_path': 'penguin_utils@pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl', 'eval_args': '{\\n  \"num_steps\": 5\\n}'} 'run_fn'\nINFO:absl:Installing 'pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmpwu08fhdc', 'pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl']\nProcessing ./pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-transform/_wheels/tfx_user_code_Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9-py3-none-any.whl'.\nINFO:absl:Training model.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Trainer\nSuccessfully installed tfx-user-code-Trainer-0.0+a5e9139bd7facf5026b5306a6aea534f89db0dea58ebe1bb1fb5ebb9df5fdea9\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Model: \"model\"\nINFO:absl:__________________________________________________________________________________________________\nINFO:absl: Layer (type)                Output Shape                 Param #   Connected to                  \nINFO:absl:==================================================================================================\nINFO:absl: culmen_length_mm (InputLay  [(None, 1)]                  0         []                            \nINFO:absl: er)                                                                                              \nINFO:absl:                                                                                                  \nINFO:absl: culmen_depth_mm (InputLaye  [(None, 1)]                  0         []                            \nINFO:absl: r)                                                                                               \nINFO:absl:                                                                                                  \nINFO:absl: flipper_length_mm (InputLa  [(None, 1)]                  0         []                            \nINFO:absl: yer)                                                                                             \nINFO:absl:                                                                                                  \nINFO:absl: body_mass_g (InputLayer)    [(None, 1)]                  0         []                            \nINFO:absl:                                                                                                  \nINFO:absl: concatenate (Concatenate)   (None, 4)                    0         ['culmen_length_mm[0][0]',    \nINFO:absl:                                                                     'culmen_depth_mm[0][0]',     \nINFO:absl:                                                                     'flipper_length_mm[0][0]',   \nINFO:absl:                                                                     'body_mass_g[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense (Dense)               (None, 8)                    40        ['concatenate[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense_1 (Dense)             (None, 8)                    72        ['dense[0][0]']               \nINFO:absl:                                                                                                  \nINFO:absl: dense_2 (Dense)             (None, 3)                    27        ['dense_1[0][0]']             \nINFO:absl:                                                                                                  \nINFO:absl:==================================================================================================\nINFO:absl:Total params: 139 (556.00 Byte)\nINFO:absl:Trainable params: 139 (556.00 Byte)\nINFO:absl:Non-trainable params: 0 (0.00 Byte)\nINFO:absl:__________________________________________________________________________________________________\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715160045.949607   17085 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100/100 [==============================] - 2s 6ms/step - loss: 0.2421 - sparse_categorical_accuracy: 0.9250 - val_loss: 0.0074 - val_sparse_categorical_accuracy: 1.0000\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature island has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature sex has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Function `serve_tf_examples_fn` contains input name(s) 5332, resource with unsupported characters which will be renamed to transform_features_layer_5332, model_dense_2_biasadd_readvariableop_resource in the SavedModel.\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Trainer/model/6/Format-Serving/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-transform/Trainer/model/6/Format-Serving/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-transform/Trainer/model/6/Format-Serving/fingerprint.pb\nINFO:absl:Training complete. Model written to pipelines/penguin-transform/Trainer/model/6/Format-Serving. ModelRun written to pipelines/penguin-transform/Trainer/model_run/6\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 6 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/Trainer/.system/stateful_working_dir/04611601-4dbf-412d-8c6c-4e4864bb335a\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-transform/Trainer/model/6\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-transform/Trainer/model_run/6\"\n, artifact_type: name: \"ModelRun\"\n)]}) for execution 6\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Trainer is finished.\nINFO:absl:Component Pusher is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-transform\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 12\ntype_id: 26\nuri: \"pipelines/penguin-transform/Trainer/model/6\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715160050020\nlast_update_time_since_epoch: 1715160050020\n, artifact_type: id: 26\nname: \"Model\"\nbase_type: MODEL\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 7\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'model': [Artifact(artifact: id: 12\ntype_id: 26\nuri: \"pipelines/penguin-transform/Trainer/model/6\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715160050020\nlast_update_time_since_epoch: 1715160050020\n, artifact_type: id: 26\nname: \"Model\"\nbase_type: MODEL\n)]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-transform/Pusher/pushed_model/7\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}), exec_properties={'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-transform\"\\n  }\\n}', 'custom_config': 'null'}, execution_output_uri='pipelines/penguin-transform/Pusher/.system/executor_execution/7/executor_output.pb', stateful_working_dir='pipelines/penguin-transform/Pusher/.system/stateful_working_dir/b2701c8d-b1b7-4c37-a72d-ac97a07c8706', tmp_dir='pipelines/penguin-transform/Pusher/.system/executor_execution/7/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:20:15.209892\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-transform.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:20:15.209892\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-transform.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-transform\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-transform\"\n, pipeline_run_id='2024-05-08T09:20:15.209892', top_level_pipeline_run_id=None, frontend_url=None)\nWARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\nINFO:absl:Model version: 1715160050\nINFO:absl:Model written to serving path serving_model/penguin-transform/1715160050.\nINFO:absl:Model pushed to pipelines/penguin-transform/Pusher/pushed_model/7.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 7 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-transform/Pusher/.system/stateful_working_dir/b2701c8d-b1b7-4c37-a72d-ac97a07c8706\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-transform/Pusher/pushed_model/7\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}) for execution 7\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Pusher is finished.\n\n```\n\nYou should see \"INFO:absl:Component Pusher is finished.\" if the pipeline finished successfully.\nThe pusher component pushes the trained model to the `SERVING_MODEL_DIR` which is the `serving_model/penguin-transform` directory if you did not change the variables in the previous steps. You can see the result from the file browser in the left-side panel in Colab, or using the following command:\n```\n# List files in created model directory.\nfind{SERVING_MODEL_DIR}\n```\n```\nserving_model/penguin-transform\nserving_model/penguin-transform/1715160050\nserving_model/penguin-transform/1715160050/variables\nserving_model/penguin-transform/1715160050/variables/variables.index\nserving_model/penguin-transform/1715160050/variables/variables.data-00000-of-00001\nserving_model/penguin-transform/1715160050/assets\nserving_model/penguin-transform/1715160050/keras_metadata.pb\nserving_model/penguin-transform/1715160050/fingerprint.pb\nserving_model/penguin-transform/1715160050/saved_model.pb\n\n```\n\nYou can also check the signature of the generated model using the [`saved_model_cli` tool](https://www.tensorflow.org/guide/saved_model#show_command).\n```\nsaved_model_cli{SERVING_MODEL_DIR}/$(ls{SERVING_MODEL_DIR}||)\n```\n```\n2024-05-08 09:20:50.856062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:20:50.856134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:20:50.857634: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nThe given SavedModel SignatureDef contains the following input(s):\n  inputs['examples'] tensor_info:\n      dtype: DT_STRING\n      shape: (-1)\n      name: serving_default_examples:0\nThe given SavedModel SignatureDef contains the following output(s):\n  outputs['output_0'] tensor_info:\n      dtype: DT_FLOAT\n      shape: (-1, 3)\n      name: StatefulPartitionedCall_1:0\nMethod name is: tensorflow/serving/predict\n\n```\n\nBecause we defined `serving_default` with our own `serve_tf_examples_fn` function, the signature shows that it takes a single string. This string is a serialized string of tf.Examples and will be parsed with the [tf.io.parse_example()](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) function as we defined earlier (learn more about tf.Examples [here](https://www.tensorflow.org/tutorials/load_data/tfrecord)).\nWe can load the exported model and try some inferences with a few examples.\n```\n# Find a model with the latest timestamp.\nmodel_dirs = (item for item in os.scandir(SERVING_MODEL_DIR) if item.is_dir())\nmodel_path = max(model_dirs, key=lambda i: int(i.name)).path\n\nloaded_model = tf.keras.models.load_model(model_path)\ninference_fn = loaded_model.signatures['serving_default']\n\n```\n```\n# Prepare an example and run inference.\nfeatures = {\n  'culmen_length_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[49.9])),\n  'culmen_depth_mm': tf.train.Feature(float_list=tf.train.FloatList(value=[16.1])),\n  'flipper_length_mm': tf.train.Feature(int64_list=tf.train.Int64List(value=[213])),\n  'body_mass_g': tf.train.Feature(int64_list=tf.train.Int64List(value=[5400])),\n}\nexample_proto = tf.train.Example(features=tf.train.Features(feature=features))\nexamples = example_proto.SerializeToString()\n\nresult = inference_fn(examples=tf.constant([examples]))\nprint(result['output_0'].numpy())\n\n```\n```\n[[-2.76344   -0.5130405  7.046433 ]]\n\n```\n\nThe third element, which corresponds to 'Gentoo' species, is expected to be the largest among three.\n## Next steps\nIf you want to learn more about Transform component, see [Transform Component guide](https://www.tensorflow.org/tfx/guide/transform). You can find more resources on <https://www.tensorflow.org/tfx/tutorials>\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component)  \n---  \nThis notebook contains an examples on how to author and run Python function components within the TFX InteractiveContext and in a locally-orchestrated TFX pipeline.\nFor more context and information, see the [Custom Python function components](https://www.tensorflow.org/tfx/guide/custom_function_component) page on the TFX documentation site.\n## Setup\nWe will first install TFX and import necessary modules. TFX requires Python 3.\n### Check the system Python version\n```\nimportsys\nsys.version\n\n```\n```\n'3.9.19 (main, Apr  6 2024, 17:57:55) \\n[GCC 9.4.0]'\n\n```\n\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TFX\n```\npip\n```\n\n## Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime > Restart runtime ...). This is because of the way that Colab loads packages.\n### Import packages\nWe import TFX and check its version.\n```\n# Check version\nfromtfximport v1 as tfx\ntfx.__version__\n\n```\n```\n2024-05-08 09:54:22.424419: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:54:22.424465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:54:22.426007: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n'1.15.0'\n\n```\n\n## Custom Python function components\nIn this section, we will create components from Python functions. We will not be doing any real ML problem — these simple functions are just used to illustrate the Python function component development process.\nSee [Python function based component guide](https://www.tensorflow.org/tfx/guide/custom_function_component) for more documentation.\n### Create Python custom components\nWe begin by writing a function that generate some dummy data. This is written to its own Python module file.\n```\n%%writefile my_generator.py\n\nimportos\nimporttensorflowastf  # Used for writing files.\n\nfromtfximport v1 as tfx\n\n# Non-public APIs, just for showcase.\nfromtfx.types.experimental.simple_artifactsimport Dataset\n\n@tfx.dsl.components.component\ndefMyGenerator(data: tfx.dsl.components.OutputArtifact[Dataset]):\n\"\"\"Create a file with dummy data in the output artifact.\"\"\"\n  with tf.io.gfile.GFile(os.path.join(data.uri, 'data_file.txt'), 'w') as f:\n    f.write('Dummy data')\n\n  # Set metadata and ensure that it gets passed to downstream components.\n  data.set_string_custom_property('my_custom_field', 'my_custom_value')\n\n```\n```\nWriting my_generator.py\n\n```\n\nNext, we write a second component that uses the dummy data produced. We will just calculate hash of the data and return it.\n```\n%%writefile my_consumer.py\n\nimporthashlib\nimportos\nimporttensorflowastf\n\nfromtfximport v1 as tfx\n\n# Non-public APIs, just for showcase.\nfromtfx.types.experimental.simple_artifactsimport Dataset\nfromtfx.types.standard_artifactsimport String\n\n@tfx.dsl.components.component\ndefMyConsumer(data: tfx.dsl.components.InputArtifact[Dataset],\n               hash: tfx.dsl.components.OutputArtifact[String],\n               algorithm: tfx.dsl.components.Parameter[str] = 'sha256'):\n\"\"\"Reads the contents of data and calculate.\"\"\"\n  with tf.io.gfile.GFile(\n      os.path.join(data.uri, 'data_file.txt'), 'r') as f:\n    contents = f.read()\n  h = hashlib.new(algorithm)\n  h.update(tf.compat.as_bytes(contents))\n  hash.value = h.hexdigest()\n\n  # Read a custom property from the input artifact and set to the output.\n  custom_value = data.get_string_custom_property('my_custom_field')\n  hash.set_string_custom_property('input_custom_field', custom_value)\n\n```\n```\nWriting my_consumer.py\n\n```\n\n### Run in-notebook with the InteractiveContext\nNow, we will demonstrate usage of our new components in the TFX InteractiveContext.\nFor more information on what you can do with the TFX notebook InteractiveContext, see the in-notebook [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras).\n```\nfrommy_generatorimport MyGenerator\nfrommy_consumerimport MyConsumer\n\n```\n\n#### Construct the InteractiveContext\n```\n# Here, we create an InteractiveContext using default parameters. This will\n# use a temporary directory with an ephemeral ML Metadata database instance.\n# To use your own pipeline root or database, the optional properties\n# `pipeline_root` and `metadata_connection_config` may be passed to\n# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n# notebook.\nfromtfx.orchestration.experimental.interactive.interactive_contextimport InteractiveContext\ncontext = InteractiveContext()\n\n```\n```\nWARNING:absl:InteractiveContext pipeline_root argument not provided: using temporary directory /tmpfs/tmp/tfx-interactive-2024-05-08T09_54_27.937649-5yvglrqg as root for pipeline outputs.\nWARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at /tmpfs/tmp/tfx-interactive-2024-05-08T09_54_27.937649-5yvglrqg/metadata.sqlite.\n\n```\n\n#### Run your component interactively with `context.run()`\nNext, we run our components interactively within the notebook with `context.run()`. Our consumer component uses the outputs of the generator component.\n```\ngenerator = MyGenerator()\ncontext.run(generator)\n\n```\n```\nconsumer = MyConsumer(\n    data=generator.outputs['data'],\n    algorithm='md5')\ncontext.run(consumer)\n\n```\n\nAfter execution, we can inspect the contents of the \"hash\" output artifact of the consumer component on disk.\n```\ntail{consumer.outputs['hash'].get()[0].uri}\n```\n```\n==> /tmpfs/tmp/tfx-interactive-2024-05-08T09_54_27.937649-5yvglrqg/MyConsumer/hash/2/value <==\n0015fe7975d1a2794b59aa12635703f1\n\n```\n\nThat's it, and you've now written and executed your own custom components!\n### Write a pipeline definition\nNext, we will author a pipeline using these same components. While using the `InteractiveContext` within a notebook works well for experimentation, defining a pipeline lets you deploy your pipeline on local or remote runners for production usage.\nHere, we will demonstrate usage of the LocalDagRunner running locally on your machine. For production execution, the Airflow or Kubeflow runners may be more suitable.\n#### Construct a pipeline\n```\nimportos\nimporttempfile\nfromtfximport v1 as tfx\n\n# Select a persistent TFX root directory to store your output artifacts.\n# For demonstration purposes only, we use a temporary directory.\nPIPELINE_ROOT = tempfile.mkdtemp()\n# Select a pipeline name so that multiple runs of the same logical pipeline\n# can be grouped.\nPIPELINE_NAME = \"function-based-pipeline\"\n# We use a ML Metadata configuration that uses a local SQLite database in\n# the pipeline root directory. Other backends for ML Metadata are available\n# for production usage.\nMETADATA_CONNECTION_CONFIG = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    os.path.join(PIPELINE_ROOT, 'metadata.sqlite'))\n\ndeffunction_based_pipeline():\n  # Here, we construct our generator and consumer components in the same way.\n  generator = MyGenerator()\n  consumer = MyConsumer(\n      data=generator.outputs['data'],\n      algorithm='md5')\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      components=[generator, consumer],\n      metadata_connection_config=METADATA_CONNECTION_CONFIG)\n\nmy_pipeline = function_based_pipeline()\n\n```\n\n#### Run your pipeline with the `LocalDagRunner`\n```\ntfx.orchestration.LocalDagRunner().run(my_pipeline)\n\n```\n```\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\n\n```\n\nWe can inspect the output artifacts generated by this pipeline execution.\n```\nfind{PIPELINE_ROOT}\n```\n```\n/tmpfs/tmp/tmpcu4s98j0\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/data\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/data/1\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/data/1/data_file.txt\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/.system\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/.system/stateful_working_dir\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/.system/executor_execution\n/tmpfs/tmp/tmpcu4s98j0/MyGenerator/.system/executor_execution/1\n/tmpfs/tmp/tmpcu4s98j0/metadata.sqlite\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/.system\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/.system/stateful_working_dir\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/.system/executor_execution\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/.system/executor_execution/2\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/hash\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/hash/2\n/tmpfs/tmp/tmpcu4s98j0/MyConsumer/hash/2/value\n\n```\n\nYou have now written your own custom components and orchestrated their execution on the LocalDagRunner! For next steps, check out additional tutorials and guides on the [TFX website](https://www.tensorflow.org/tfx).\n",
  "https://www.tensorflow.org/guide/distributed_training": "[View on TensorFlow.org](https://www.tensorflow.org/guide/distributed_training)  \n---  \n## Overview\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) has been designed with these key goals in mind:\n  * Easy to use and support multiple user segments, including researchers, machine learning engineers, etc.\n  * Provide good performance out of the box.\n  * Easy switching between strategies.\n\n\nYou can distribute training using [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with a high-level API like Keras [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), as well as [custom training loops](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch) (and, in general, any computation using TensorFlow).\nIn TensorFlow 2.x, you can execute your programs eagerly, or in a graph using [`tf.function`](https://www.tensorflow.org/guide/function). [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) intends to support both these modes of execution, but works best with [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Eager mode is only recommended for debugging purposes and not supported for [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy). Although training is the focus of this guide, this API can also be used for distributing evaluation and prediction on different platforms.\nYou can use [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with very few changes to your code, because the underlying components of TensorFlow have been changed to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.\nIn this guide, you will learn about various types of strategies and how you can use them in different situations. To learn how to debug performance issues, check out the [Optimize TensorFlow GPU performance](https://www.tensorflow.org/guide/gpu_performance_analysis) guide.\n## Set up TensorFlow\n```\nimporttensorflowastf\n\n```\n```\n2024-10-25 03:10:09.809713: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1729825809.832772  192915 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1729825809.839425  192915 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\n## Types of strategies\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) intends to cover a number of use cases along different axes. Some of these combinations are currently supported and others will be added in the future. Some of these axes are:\n  * _Synchronous vs asynchronous training:_ These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n  * _Hardware platform:_ You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n\n\nIn order to support these use cases, TensorFlow has `MirroredStrategy`, `TPUStrategy`, `MultiWorkerMirroredStrategy`, `ParameterServerStrategy`, `CentralStorageStrategy`, as well as other strategies available. The next section explains which of these are supported in which scenarios in TensorFlow. Here is a quick overview:\nTraining API | `MirroredStrategy` | `TPUStrategy` | `MultiWorkerMirroredStrategy` | `CentralStorageStrategy` | `ParameterServerStrategy`  \n---|---|---|---|---|---  \n**Keras[`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)** | Supported | Supported | Supported | Experimental support | Experimental support  \n**Custom training loop** | Supported | Supported | Supported | Experimental support | Experimental support  \n**Estimator API** | Limited Support | Not supported | Limited Support | Limited Support | Limited Support  \n### MirroredStrategy\n[`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) supports synchronous distributed training on multiple GPUs on one machine. It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. Together, these variables form a single conceptual variable called `MirroredVariable`. These variables are kept in sync with each other by applying identical updates.\nEfficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. It’s a fused algorithm that is very efficient and can reduce the overhead of synchronization significantly. There are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, it uses the NVIDIA Collective Communication Library (\nHere is the simplest way of creating `MirroredStrategy`:\n```\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\nW0000 00:00:1729825812.490898  192915 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\nSkipping registering GPU devices...\n\n```\n\nThis will create a `MirroredStrategy` instance, which will use all the GPUs that are visible to TensorFlow, and NCCL—as the cross-device communication.\nIf you wish to use only some of the GPUs on your machine, you can do so like this:\n```\nmirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n\n```\n\nIf you wish to override the cross device communication, you can do so using the `cross_device_ops` argument by supplying an instance of [`tf.distribute.CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps). Currently, [`tf.distribute.HierarchicalCopyAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce) and [`tf.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice) are two options other than [`tf.distribute.NcclAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce), which is the default.\n```\nmirrored_strategy = tf.distribute.MirroredStrategy(\n    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n\n```\n\n### TPUStrategy\n[`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) lets you run your TensorFlow training on [Tensor Processing Units (TPUs)](https://www.tensorflow.org/guide/tpu). TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads. They are available on \nIn terms of distributed training architecture, `TPUStrategy` is the same `MirroredStrategy`—it implements synchronous distributed training. TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in `TPUStrategy`.\nHere is how you would instantiate `TPUStrategy`:\n```\ncluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n    tpu=tpu_address)\ntf.config.experimental_connect_to_cluster(cluster_resolver)\ntf.tpu.experimental.initialize_tpu_system(cluster_resolver)\ntpu_strategy = tf.distribute.TPUStrategy(cluster_resolver)\n\n```\n\nThe `TPUClusterResolver` instance helps locate the TPUs. In Colab, you don't need to specify any arguments to it.\nIf you want to use this for Cloud TPUs:\n  * You must specify the name of your TPU resource in the `tpu` argument.\n  * You must initialize the TPU system explicitly at the _start_ of the program. This is required before TPUs can be used for computation. Initializing the TPU system also wipes out the TPU memory, so it's important to complete this step first in order to avoid losing state.\n\n\n### MultiWorkerMirroredStrategy\n[`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy) is very similar to `MirroredStrategy`. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy), it creates copies of all variables in the model on each device across all workers.\nHere is the simplest way of creating `MultiWorkerMirroredStrategy`:\n```\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\n```\n```\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\nINFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.AUTO\n\n```\n\n`MultiWorkerMirroredStrategy` has two implementations for cross-device communications. [`CommunicationImplementation.RING`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation#RING) is [`CommunicationImplementation.NCCL`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation#NCCL) uses NCCL and provides state-of-art performance on GPUs but it doesn't support CPUs. [`CollectiveCommunication.AUTO`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation#AUTO) defers the choice to Tensorflow. You can specify them in the following way:\n```\ncommunication_options = tf.distribute.experimental.CommunicationOptions(\n    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\nstrategy = tf.distribute.MultiWorkerMirroredStrategy(\n    communication_options=communication_options)\n\n```\n```\nWARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\nINFO:tensorflow:Using MirroredStrategy with devices ('/device:CPU:0',)\nWARNING:tensorflow:Enabled NCCL communication but no GPUs detected/specified.\nINFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:CPU:0',), communication = CommunicationImplementation.NCCL\n\n```\n\nOne of the key differences to get multi worker training going, as compared to multi-GPU training, is the multi-worker setup. The `'TF_CONFIG'` environment variable is the standard way in TensorFlow to specify the cluster configuration to each worker that is part of the cluster. Learn more in the [setting up TF_CONFIG section](https://www.tensorflow.org/guide/distributed_training#TF_CONFIG) of this document.\nFor more details about `MultiWorkerMirroredStrategy`, consider the following tutorials:\n  * [Multi-worker training with Keras Model.fit](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\n  * [Multi-worker training with a custom training loop](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n\n\n### ParameterServerStrategy\nParameter server training is a common data-parallel method to scale up model training on multiple machines. A parameter server training cluster consists of workers and parameter servers. Variables are created on parameter servers and they are read and updated by workers in each step. Check out the [Parameter server training](https://www.tensorflow.org/tutorials/distribute/parameter_server_training) tutorial for details.\nIn TensorFlow 2, parameter server training uses a central coordinator-based architecture via the [`tf.distribute.experimental.coordinator.ClusterCoordinator`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator) class.\nIn this implementation, the `worker` and `parameter server` tasks run [`tf.distribute.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server)s that listen for tasks from the coordinator. The coordinator creates resources, dispatches training tasks, writes checkpoints, and deals with task failures.\nIn the programming running on the coordinator, you will use a `ParameterServerStrategy` object to define a training step and use a `ClusterCoordinator` to dispatch training steps to remote workers. Here is the simplest way to create them:\n```\nstrategy = tf.distribute.experimental.ParameterServerStrategy(\n    tf.distribute.cluster_resolver.TFConfigClusterResolver(),\n    variable_partitioner=variable_partitioner)\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy)\n\n```\n\nTo learn more about `ParameterServerStrategy`, check out the [Parameter server training with Keras Model.fit and a custom training loop](https://www.tensorflow.org/tutorials/distribute/parameter_server_training) tutorial.\nIn TensorFlow 1, `ParameterServerStrategy` is available only with an Estimator via [`tf.compat.v1.distribute.experimental.ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/distribute/experimental/ParameterServerStrategy) symbol.\n### CentralStorageStrategy\n[`tf.distribute.experimental.CentralStorageStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CentralStorageStrategy) does synchronous training as well. Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU.\nCreate an instance of `CentralStorageStrategy` by:\n```\ncentral_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()\n\n```\n```\nINFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ['/job:localhost/replica:0/task:0/device:CPU:0'], variable_device = '/job:localhost/replica:0/task:0/device:CPU:0'\n\n```\n\nThis will create a `CentralStorageStrategy` instance which will use all visible GPUs and CPU. Update to variables on replicas will be aggregated before being applied to variables.\n### Other strategies\nIn addition to the above strategies, there are two other strategies which might be useful for prototyping and debugging when using [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) APIs.\n#### Default Strategy\nThe Default Strategy is a distribution strategy which is present when no explicit distribution strategy is in scope. It implements the [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) interface but is a pass-through and provides no actual distribution. For instance, [`Strategy.run(fn)`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run) will simply call `fn`. Code written using this strategy should behave exactly as code written without any strategy. You can think of it as a \"no-op\" strategy.\nThe Default Strategy is a singleton—and one cannot create more instances of it. It can be obtained using [`tf.distribute.get_strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) outside any explicit strategy's scope (the same API that can be used to get the current strategy inside an explicit strategy's scope).\n```\ndefault_strategy = tf.distribute.get_strategy()\n\n```\n\nThis strategy serves two main purposes:\n  * It allows writing distribution-aware library code unconditionally. For example, in [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) you can use [`tf.distribute.get_strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) and use that strategy for reducing gradients—it will always return a strategy object on which you can call the [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) API.\n\n```\n# In optimizer or other library code\n# Get currently active strategy\nstrategy = tf.distribute.get_strategy()\nstrategy.reduce(\"SUM\", 1., axis=None)  # reduce some values\n\n```\n```\n1.0\n\n```\n\n  * Similar to library code, it can be used to write end users' programs to work with and without distribution strategy, without requiring conditional logic. Here's a sample code snippet illustrating this:\n\n```\nif tf.config.list_physical_devices('GPU'):\n  strategy = tf.distribute.MirroredStrategy()\nelse:  # Use the Default Strategy\n  strategy = tf.distribute.get_strategy()\n\nwith strategy.scope():\n  # Do something interesting\n  print(tf.Variable(1.))\n\n```\n```\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n\n```\n\n#### OneDeviceStrategy\n[`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) is a strategy to place all variables and computation on a single specified device.\n```\nstrategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n\n```\n\nThis strategy is distinct from the Default Strategy in a number of ways. In the Default Strategy, the variable placement logic remains unchanged when compared to running TensorFlow without any distribution strategy. But when using `OneDeviceStrategy`, all variables created in its scope are explicitly placed on the specified device. Moreover, any functions called via [`OneDeviceStrategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy#run) will also be placed on the specified device.\nInput distributed through this strategy will be prefetched to the specified device. In the Default Strategy, there is no input distribution.\nSimilar to the Default Strategy, this strategy could also be used to test your code before switching to other strategies which actually distribute to multiple devices/machines. This will exercise the distribution strategy machinery somewhat more than the Default Strategy, but not to the full extent of using, for example, `MirroredStrategy` or `TPUStrategy`. If you want code that behaves as if there is no strategy, then use the Default Strategy.\nSo far you've learned about different strategies and how you can instantiate them. The next few sections show the different ways in which you can use them to distribute your training.\n## Use tf.distribute.Strategy with Keras Model.fit\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) is integrated into [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras), which is TensorFlow's implementation of the [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) is a high-level API to build and train models. By integrating into the [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) backend, it's seamless for you to distribute your training written in the Keras training framework [using Model.fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit).\nHere's what you need to change in your code:\n  1. Create an instance of the appropriate [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy).\n  2. Move the creation of Keras model, optimizer and metrics inside `strategy.scope`. Thus the code in the model's `call()`, `train_step()`, and `test_step()` methods will all be distributed and executed on the accelerator(s).\n\n\nTensorFlow distribution strategies support all types of Keras models—[Sequential](https://www.tensorflow.org/guide/keras/sequential_model), [Functional](https://www.tensorflow.org/guide/keras/functional), and [subclassed](https://www.tensorflow.org/guide/keras/custom_layers_and_models)\nHere is a snippet of code to do this for a very simple Keras model with one `Dense` layer:\n```\nmirrored_strategy = tf.distribute.MirroredStrategy()\n\nwith mirrored_strategy.scope():\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(1, input_shape=(1,),\n                            kernel_regularizer=tf.keras.regularizers.L2(1e-4))])\n  model.compile(loss='mse', optimizer='sgd')\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n\n```\n\nThis example uses `MirroredStrategy`, so you can run this on a machine with multiple GPUs. `strategy.scope()` indicates to Keras which strategy to use to distribute the training. Creating models/optimizers/metrics inside this scope allows you to create distributed variables instead of regular variables. Once this is set up, you can fit your model like you would normally. `MirroredStrategy` takes care of replicating the model's training on the available GPUs, aggregating gradients, and more.\n```\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100).batch(10)\nmodel.fit(dataset, epochs=2)\nmodel.evaluate(dataset)\n\n```\n```\n2024-10-25 03:10:12.686928: W tensorflow/core/framework/dataset.cc:993] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\nEpoch 1/2\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 2s 287ms/step - loss: 0.649210/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.5412\nEpoch 2/2\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step - loss: 0.286910/10 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.2392\n 1/10 ━━━━━━━━━━━━━━━━━━━━ 1s 168ms/step - loss: 0.126810/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.1268\n0.1268472820520401\n\n```\n\nHere a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) provides the training and eval input. You can also use NumPy arrays:\n```\nimportnumpyasnp\n\ninputs, targets = np.ones((100, 1)), np.ones((100, 1))\nmodel.fit(inputs, targets, epochs=2, batch_size=10)\n\n```\n```\nEpoch 1/2\n 2/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.124410/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.1058\nEpoch 2/2\n 3/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0539 10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - loss: 0.0468\n<keras.src.callbacks.history.History at 0x7fab904bcfa0>\n\n```\n\nIn both cases—with `Dataset` or NumPy—each batch of the given input is divided equally among the multiple replicas. For instance, if you are using the `MirroredStrategy` with 2 GPUs, each batch of size 10 will be divided among the 2 GPUs, with each receiving 5 input examples in each step. Each epoch will then train faster as you add more GPUs. Typically, you would want to increase your batch size as you add more accelerators, so as to make effective use of the extra computing power. You will also need to re-tune your learning rate, depending on the model. You can use `strategy.num_replicas_in_sync` to get the number of replicas.\n```\nmirrored_strategy.num_replicas_in_sync\n\n```\n```\n1\n\n```\n```\n# Compute a global batch size using a number of replicas.\nBATCH_SIZE_PER_REPLICA = 5\nglobal_batch_size = (BATCH_SIZE_PER_REPLICA *\n                     mirrored_strategy.num_replicas_in_sync)\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(100)\ndataset = dataset.batch(global_batch_size)\n\nLEARNING_RATES_BY_BATCH_SIZE = {5: 0.1, 10: 0.15, 20:0.175}\nlearning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]\n\n```\n\n### What's supported now?\nTraining API | `MirroredStrategy` | `TPUStrategy` | `MultiWorkerMirroredStrategy` | `ParameterServerStrategy` | `CentralStorageStrategy`  \n---|---|---|---|---|---  \nKeras [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) | Supported | Supported | Supported | Experimental support | Experimental support  \n### Examples and tutorials\nHere is a list of tutorials and examples that illustrate the above integration end-to-end with Keras [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit):\n  1. [Tutorial](https://www.tensorflow.org/tutorials/distribute/keras): Training with [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and `MirroredStrategy`.\n  2. [Tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras): Training with [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and `MultiWorkerMirroredStrategy`.\n  3. [Guide](https://www.tensorflow.org/guide/tpu): Contains an example of using [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and `TPUStrategy`.\n  4. [Tutorial](https://www.tensorflow.org/tutorials/distribute/parameter_server_training): Parameter server training with [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and `ParameterServerStrategy`.\n  5. [Tutorial](https://www.tensorflow.org/text/tutorials/bert_glue): Fine-tuning BERT for many tasks from the GLUE benchmark with [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and `TPUStrategy`.\n  6. TensorFlow Model Garden \n\n\n## Use tf.distribute.Strategy with custom training loops\nAs demonstrated above, using [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with Keras [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) requires changing only a couple lines of your code. With a little more effort, you can also use [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) [with custom training loops](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch).\nIf you need more flexibility and control over your training loops than is possible with Estimator or Keras, you can write custom training loops. For instance, when using a GAN, you may want to take a different number of generator or discriminator steps each round. Similarly, the high level frameworks are not very suitable for Reinforcement Learning training.\nThe [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) classes provide a core set of methods to support custom training loops. Using these may require minor restructuring of the code initially, but once that is done, you should be able to switch between GPUs, TPUs, and multiple machines simply by changing the strategy instance.\nBelow is a brief snippet illustrating this use case for a simple training example using the same Keras model as before.\nFirst, create the model and optimizer inside the strategy's scope. This ensures that any variables created with the model and optimizer are mirrored variables.\n```\nwith mirrored_strategy.scope():\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(1, input_shape=(1,),\n                            kernel_regularizer=tf.keras.regularizers.L2(1e-4))])\n  optimizer = tf.keras.optimizers.SGD()\n\n```\n\nNext, create the input dataset and call [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) to distribute the dataset based on the strategy.\n```\ndataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(\n    global_batch_size)\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n\n```\n\nThen, define one step of the training. Use [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) to compute gradients and optimizer to apply those gradients to update your model's variables. To distribute this training step, put it in a function `train_step` and pass it to [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) along with the dataset inputs you got from the `dist_dataset` created before:\n```\n# Sets `reduction=NONE` to leave it to tf.nn.compute_average_loss() below.\nloss_object = tf.keras.losses.BinaryCrossentropy(\n  from_logits=True,\n  reduction=tf.keras.losses.Reduction.NONE)\n\ndeftrain_step(inputs):\n  features, labels = inputs\n\n  with tf.GradientTape() as tape:\n    predictions = model(features, training=True)\n    per_example_loss = loss_object(labels, predictions)\n    loss = tf.nn.compute_average_loss(per_example_loss)\n    model_losses = model.losses\n    if model_losses:\n      loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n  return loss\n\n@tf.function\ndefdistributed_train_step(dist_inputs):\n  per_replica_losses = mirrored_strategy.run(train_step, args=(dist_inputs,))\n  return mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n\n```\n\nA few other things to note in the code above:\n  1. You used [`tf.nn.compute_average_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss) to reduce the per-example prediction losses to a scalar. [`tf.nn.compute_average_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss) sums the per example loss and divides the sum by the global batch size. This is important because later after the gradients are calculated on each replica, they are aggregated across the replicas by **summing** them.\nBy default, the global batch size is taken to be `tf.get_strategy().num_replicas_in_sync * tf.shape(per_example_loss)[0]`. It can also be specified explicitly as a keyword argument `global_batch_size=`. Without short batches, the default is equivalent to `tf.nn.compute_average_loss(..., global_batch_size=global_batch_size)` with the `global_batch_size` defined above. (For more on short batches and how to avoid or handle them, see the [Custom Training tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training).)\n  2. You used [`tf.nn.scale_regularization_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/scale_regularization_loss) to scale regularization losses registered with the `Model` object, if any, by `1/num_replicas_in_sync` as well. For those regularization losses that are input-dependent, it falls on the modeling code, not the custom training loop, to perform the averaging over the per-replica(!) batch size; that way the modeling code can remain agnostic of replication while the training loop remains agnostic of how regularization losses are computed.\n  3. When you call `apply_gradients` within a distribution strategy scope, its behavior is modified. Specifically, before applying gradients on each parallel instance during synchronous training, it performs a sum-over-all-replicas of the gradients.\n  4. You also used the [`tf.distribute.Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce) API to aggregate the results returned by [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) for reporting. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) returns results from each local replica in the strategy, and there are multiple ways to consume this result. You can `reduce` them to get an aggregated value. You can also do [`tf.distribute.Strategy.experimental_local_results`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_local_results) to get the list of values contained in the result, one per local replica.\n\n\nFinally, once you have defined the training step, you can iterate over `dist_dataset` and run the training in a loop:\n```\nfor dist_inputs in dist_dataset:\n  print(distributed_train_step(dist_inputs))\n\n```\n```\ntf.Tensor(0.9024367, shape=(), dtype=float32)\ntf.Tensor(0.8953863, shape=(), dtype=float32)\ntf.Tensor(0.8884038, shape=(), dtype=float32)\ntf.Tensor(0.88148874, shape=(), dtype=float32)\ntf.Tensor(0.87464076, shape=(), dtype=float32)\ntf.Tensor(0.86785895, shape=(), dtype=float32)\ntf.Tensor(0.86114323, shape=(), dtype=float32)\ntf.Tensor(0.8544927, shape=(), dtype=float32)\ntf.Tensor(0.84790725, shape=(), dtype=float32)\ntf.Tensor(0.841386, shape=(), dtype=float32)\ntf.Tensor(0.83492863, shape=(), dtype=float32)\ntf.Tensor(0.8285344, shape=(), dtype=float32)\ntf.Tensor(0.82220304, shape=(), dtype=float32)\ntf.Tensor(0.8159339, shape=(), dtype=float32)\ntf.Tensor(0.8097264, shape=(), dtype=float32)\ntf.Tensor(0.8035801, shape=(), dtype=float32)\ntf.Tensor(0.79749453, shape=(), dtype=float32)\ntf.Tensor(0.79146886, shape=(), dtype=float32)\ntf.Tensor(0.785503, shape=(), dtype=float32)\ntf.Tensor(0.779596, shape=(), dtype=float32)\ntf.Tensor(0.77374756, shape=(), dtype=float32)\ntf.Tensor(0.7679571, shape=(), dtype=float32)\ntf.Tensor(0.7622242, shape=(), dtype=float32)\ntf.Tensor(0.7565481, shape=(), dtype=float32)\ntf.Tensor(0.75092846, shape=(), dtype=float32)\ntf.Tensor(0.7453647, shape=(), dtype=float32)\ntf.Tensor(0.73985624, shape=(), dtype=float32)\ntf.Tensor(0.7344028, shape=(), dtype=float32)\ntf.Tensor(0.7290035, shape=(), dtype=float32)\ntf.Tensor(0.723658, shape=(), dtype=float32)\ntf.Tensor(0.7183659, shape=(), dtype=float32)\ntf.Tensor(0.71312654, shape=(), dtype=float32)\ntf.Tensor(0.7079393, shape=(), dtype=float32)\ntf.Tensor(0.70280397, shape=(), dtype=float32)\ntf.Tensor(0.6977197, shape=(), dtype=float32)\ntf.Tensor(0.69268626, shape=(), dtype=float32)\ntf.Tensor(0.687703, shape=(), dtype=float32)\ntf.Tensor(0.68276954, shape=(), dtype=float32)\ntf.Tensor(0.67788523, shape=(), dtype=float32)\ntf.Tensor(0.6730496, shape=(), dtype=float32)\ntf.Tensor(0.66826224, shape=(), dtype=float32)\ntf.Tensor(0.66352266, shape=(), dtype=float32)\ntf.Tensor(0.6588302, shape=(), dtype=float32)\ntf.Tensor(0.6541846, shape=(), dtype=float32)\ntf.Tensor(0.6495853, shape=(), dtype=float32)\ntf.Tensor(0.64503175, shape=(), dtype=float32)\ntf.Tensor(0.6405235, shape=(), dtype=float32)\ntf.Tensor(0.6360602, shape=(), dtype=float32)\ntf.Tensor(0.6316412, shape=(), dtype=float32)\ntf.Tensor(0.62726617, shape=(), dtype=float32)\ntf.Tensor(0.6229345, shape=(), dtype=float32)\ntf.Tensor(0.61864597, shape=(), dtype=float32)\ntf.Tensor(0.6143999, shape=(), dtype=float32)\ntf.Tensor(0.6101959, shape=(), dtype=float32)\ntf.Tensor(0.60603356, shape=(), dtype=float32)\ntf.Tensor(0.60191244, shape=(), dtype=float32)\ntf.Tensor(0.597832, shape=(), dtype=float32)\ntf.Tensor(0.5937919, shape=(), dtype=float32)\ntf.Tensor(0.5897917, shape=(), dtype=float32)\ntf.Tensor(0.585831, shape=(), dtype=float32)\ntf.Tensor(0.58190924, shape=(), dtype=float32)\ntf.Tensor(0.5780261, shape=(), dtype=float32)\ntf.Tensor(0.57418114, shape=(), dtype=float32)\ntf.Tensor(0.57037395, shape=(), dtype=float32)\ntf.Tensor(0.5666041, shape=(), dtype=float32)\ntf.Tensor(0.56287116, shape=(), dtype=float32)\ntf.Tensor(0.55917484, shape=(), dtype=float32)\ntf.Tensor(0.5555145, shape=(), dtype=float32)\ntf.Tensor(0.55189, shape=(), dtype=float32)\ntf.Tensor(0.54830086, shape=(), dtype=float32)\ntf.Tensor(0.54474664, shape=(), dtype=float32)\ntf.Tensor(0.54122704, shape=(), dtype=float32)\ntf.Tensor(0.5377416, shape=(), dtype=float32)\ntf.Tensor(0.5342899, shape=(), dtype=float32)\ntf.Tensor(0.5308717, shape=(), dtype=float32)\ntf.Tensor(0.5274865, shape=(), dtype=float32)\ntf.Tensor(0.52413404, shape=(), dtype=float32)\ntf.Tensor(0.52081394, shape=(), dtype=float32)\ntf.Tensor(0.51752573, shape=(), dtype=float32)\ntf.Tensor(0.5142692, shape=(), dtype=float32)\ntf.Tensor(0.51104385, shape=(), dtype=float32)\ntf.Tensor(0.50784945, shape=(), dtype=float32)\ntf.Tensor(0.50468564, shape=(), dtype=float32)\ntf.Tensor(0.50155205, shape=(), dtype=float32)\ntf.Tensor(0.49844825, shape=(), dtype=float32)\ntf.Tensor(0.4953741, shape=(), dtype=float32)\ntf.Tensor(0.49232918, shape=(), dtype=float32)\ntf.Tensor(0.4893132, shape=(), dtype=float32)\ntf.Tensor(0.48632562, shape=(), dtype=float32)\ntf.Tensor(0.4833664, shape=(), dtype=float32)\ntf.Tensor(0.4804351, shape=(), dtype=float32)\ntf.Tensor(0.47753143, shape=(), dtype=float32)\ntf.Tensor(0.47465506, shape=(), dtype=float32)\ntf.Tensor(0.47180572, shape=(), dtype=float32)\ntf.Tensor(0.46898302, shape=(), dtype=float32)\ntf.Tensor(0.4661867, shape=(), dtype=float32)\ntf.Tensor(0.46341658, shape=(), dtype=float32)\ntf.Tensor(0.4606722, shape=(), dtype=float32)\ntf.Tensor(0.4579534, shape=(), dtype=float32)\ntf.Tensor(0.4552598, shape=(), dtype=float32)\ntf.Tensor(0.45259115, shape=(), dtype=float32)\ntf.Tensor(0.44994718, shape=(), dtype=float32)\ntf.Tensor(0.44732755, shape=(), dtype=float32)\ntf.Tensor(0.44473216, shape=(), dtype=float32)\ntf.Tensor(0.44216052, shape=(), dtype=float32)\ntf.Tensor(0.4396125, shape=(), dtype=float32)\ntf.Tensor(0.43708783, shape=(), dtype=float32)\ntf.Tensor(0.4345862, shape=(), dtype=float32)\ntf.Tensor(0.4321074, shape=(), dtype=float32)\ntf.Tensor(0.42965108, shape=(), dtype=float32)\ntf.Tensor(0.4272171, shape=(), dtype=float32)\ntf.Tensor(0.42480516, shape=(), dtype=float32)\ntf.Tensor(0.42241505, shape=(), dtype=float32)\ntf.Tensor(0.42004645, shape=(), dtype=float32)\ntf.Tensor(0.41769922, shape=(), dtype=float32)\ntf.Tensor(0.41537297, shape=(), dtype=float32)\ntf.Tensor(0.41306767, shape=(), dtype=float32)\ntf.Tensor(0.41078293, shape=(), dtype=float32)\ntf.Tensor(0.4085186, shape=(), dtype=float32)\ntf.Tensor(0.4062744, shape=(), dtype=float32)\ntf.Tensor(0.4040502, shape=(), dtype=float32)\ntf.Tensor(0.40184572, shape=(), dtype=float32)\ntf.Tensor(0.39966068, shape=(), dtype=float32)\ntf.Tensor(0.3974949, shape=(), dtype=float32)\ntf.Tensor(0.39534825, shape=(), dtype=float32)\ntf.Tensor(0.39322042, shape=(), dtype=float32)\ntf.Tensor(0.39111122, shape=(), dtype=float32)\ntf.Tensor(0.3890205, shape=(), dtype=float32)\ntf.Tensor(0.38694802, shape=(), dtype=float32)\ntf.Tensor(0.38489357, shape=(), dtype=float32)\ntf.Tensor(0.38285697, shape=(), dtype=float32)\ntf.Tensor(0.38083804, shape=(), dtype=float32)\ntf.Tensor(0.3788365, shape=(), dtype=float32)\ntf.Tensor(0.37685227, shape=(), dtype=float32)\ntf.Tensor(0.3748851, shape=(), dtype=float32)\ntf.Tensor(0.37293482, shape=(), dtype=float32)\ntf.Tensor(0.37100127, shape=(), dtype=float32)\ntf.Tensor(0.36908418, shape=(), dtype=float32)\ntf.Tensor(0.36718345, shape=(), dtype=float32)\ntf.Tensor(0.3652989, shape=(), dtype=float32)\ntf.Tensor(0.36343032, shape=(), dtype=float32)\ntf.Tensor(0.36157757, shape=(), dtype=float32)\ntf.Tensor(0.35974047, shape=(), dtype=float32)\ntf.Tensor(0.3579188, shape=(), dtype=float32)\ntf.Tensor(0.35611248, shape=(), dtype=float32)\ntf.Tensor(0.3543213, shape=(), dtype=float32)\ntf.Tensor(0.35254508, shape=(), dtype=float32)\ntf.Tensor(0.3507837, shape=(), dtype=float32)\ntf.Tensor(0.34903696, shape=(), dtype=float32)\ntf.Tensor(0.34730473, shape=(), dtype=float32)\ntf.Tensor(0.3455869, shape=(), dtype=float32)\ntf.Tensor(0.3438832, shape=(), dtype=float32)\ntf.Tensor(0.34219357, shape=(), dtype=float32)\ntf.Tensor(0.3405178, shape=(), dtype=float32)\ntf.Tensor(0.3388558, shape=(), dtype=float32)\ntf.Tensor(0.3372074, shape=(), dtype=float32)\ntf.Tensor(0.33557245, shape=(), dtype=float32)\ntf.Tensor(0.33395082, shape=(), dtype=float32)\ntf.Tensor(0.33234236, shape=(), dtype=float32)\ntf.Tensor(0.33074695, shape=(), dtype=float32)\ntf.Tensor(0.32916442, shape=(), dtype=float32)\ntf.Tensor(0.3275946, shape=(), dtype=float32)\ntf.Tensor(0.3260375, shape=(), dtype=float32)\ntf.Tensor(0.3244928, shape=(), dtype=float32)\ntf.Tensor(0.3229605, shape=(), dtype=float32)\ntf.Tensor(0.32144046, shape=(), dtype=float32)\ntf.Tensor(0.31993246, shape=(), dtype=float32)\ntf.Tensor(0.3184365, shape=(), dtype=float32)\ntf.Tensor(0.31695238, shape=(), dtype=float32)\ntf.Tensor(0.31548, shape=(), dtype=float32)\ntf.Tensor(0.31401917, shape=(), dtype=float32)\ntf.Tensor(0.3125699, shape=(), dtype=float32)\ntf.Tensor(0.31113195, shape=(), dtype=float32)\ntf.Tensor(0.30970532, shape=(), dtype=float32)\ntf.Tensor(0.3082898, shape=(), dtype=float32)\ntf.Tensor(0.30688527, shape=(), dtype=float32)\ntf.Tensor(0.3054917, shape=(), dtype=float32)\ntf.Tensor(0.30410892, shape=(), dtype=float32)\ntf.Tensor(0.3027368, shape=(), dtype=float32)\ntf.Tensor(0.30137527, shape=(), dtype=float32)\ntf.Tensor(0.3000242, shape=(), dtype=float32)\ntf.Tensor(0.29868355, shape=(), dtype=float32)\ntf.Tensor(0.29735315, shape=(), dtype=float32)\ntf.Tensor(0.29603288, shape=(), dtype=float32)\ntf.Tensor(0.29472268, shape=(), dtype=float32)\ntf.Tensor(0.2934224, shape=(), dtype=float32)\ntf.Tensor(0.29213202, shape=(), dtype=float32)\ntf.Tensor(0.29085135, shape=(), dtype=float32)\ntf.Tensor(0.28958035, shape=(), dtype=float32)\ntf.Tensor(0.2883189, shape=(), dtype=float32)\ntf.Tensor(0.28706694, shape=(), dtype=float32)\ntf.Tensor(0.28582436, shape=(), dtype=float32)\ntf.Tensor(0.28459102, shape=(), dtype=float32)\ntf.Tensor(0.28336692, shape=(), dtype=float32)\ntf.Tensor(0.2821518, shape=(), dtype=float32)\ntf.Tensor(0.28094578, shape=(), dtype=float32)\ntf.Tensor(0.27974862, shape=(), dtype=float32)\ntf.Tensor(0.2785603, shape=(), dtype=float32)\ntf.Tensor(0.27738073, shape=(), dtype=float32)\ntf.Tensor(0.2762098, shape=(), dtype=float32)\n\n```\n\nIn the example above, you iterated over the `dist_dataset` to provide input to your training. You are also provided with the `tf.distribute.Strategy.make_experimental_numpy_dataset` to support NumPy inputs. You can use this API to create a dataset before calling [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset).\nAnother way of iterating over your data is to explicitly use iterators. You may want to do this when you want to run for a given number of steps as opposed to iterating over the entire dataset. The above iteration would now be modified to first create an iterator and then explicitly call `next` on it to get the input data.\n```\niterator = iter(dist_dataset)\nfor _ in range(10):\n  print(distributed_train_step(next(iterator)))\n\n```\n```\ntf.Tensor(0.27504745, shape=(), dtype=float32)\ntf.Tensor(0.2738936, shape=(), dtype=float32)\ntf.Tensor(0.2727481, shape=(), dtype=float32)\ntf.Tensor(0.27161098, shape=(), dtype=float32)\ntf.Tensor(0.27048206, shape=(), dtype=float32)\ntf.Tensor(0.26936132, shape=(), dtype=float32)\ntf.Tensor(0.26824862, shape=(), dtype=float32)\ntf.Tensor(0.26714393, shape=(), dtype=float32)\ntf.Tensor(0.26604718, shape=(), dtype=float32)\ntf.Tensor(0.26495826, shape=(), dtype=float32)\n\n```\n\nThis covers the simplest case of using [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) API to distribute custom training loops.\n### What's supported now?\nTraining API | `MirroredStrategy` | `TPUStrategy` | `MultiWorkerMirroredStrategy` | `ParameterServerStrategy` | `CentralStorageStrategy`  \n---|---|---|---|---|---  \nCustom training loop | Supported | Supported | Supported | Experimental support | Experimental support  \n### Examples and tutorials\nHere are some examples for using distribution strategies with custom training loops:\n  1. [Tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training): Training with a custom training loop and `MirroredStrategy`.\n  2. [Tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl): Training with a custom training loop and `MultiWorkerMirroredStrategy`.\n  3. [Guide](https://www.tensorflow.org/guide/tpu): Contains an example of a custom training loop with `TPUStrategy`.\n  4. [Tutorial](https://www.tensorflow.org/tutorials/distribute/parameter_server_training): Parameter server training with a custom training loop and `ParameterServerStrategy`.\n  5. TensorFlow Model Garden \n\n\n## Other topics\nThis section covers some topics that are relevant to multiple use cases.\n### Setting up the TF_CONFIG environment variable\nFor multi-worker training, as mentioned before, you need to set up the `'TF_CONFIG'` environment variable for each binary running in your cluster. The `'TF_CONFIG'` environment variable is a JSON string which specifies what tasks constitute a cluster, their addresses and each task's role in the cluster. The `'TF_CONFIG'` for your training tasks.\nThere are two components of `'TF_CONFIG'`: a cluster and a task.\n  * A cluster provides information about the training cluster, which is a dict consisting of different types of jobs such as workers. In multi-worker training, there is usually one worker that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular worker does. Such worker is referred to as the \"chief\" worker, and it is customary that the worker with index `0` is appointed as the chief worker (in fact this is how [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) is implemented).\n  * A task on the other hand provides information about the current task. The first component cluster is the same for all workers, and the second component task is different on each worker and specifies the type and index of that worker.\n\n\nOne example of `'TF_CONFIG'` is:\n```\nos.environ[\"TF_CONFIG\"] = json.dumps({\n    \"cluster\": {\n        \"worker\": [\"host1:port\", \"host2:port\", \"host3:port\"],\n        \"ps\": [\"host4:port\", \"host5:port\"]\n    },\n   \"task\": {\"type\": \"worker\", \"index\": 1}\n})\n\n```\n\nThis `'TF_CONFIG'` specifies that there are three workers and two `\"ps\"` tasks in the `\"cluster\"` along with their hosts and ports. The `\"task\"` part specifies the role of the current task in the `\"cluster\"`—worker `1` (the second worker). Valid roles in a cluster are `\"chief\"`, `\"worker\"`, `\"ps\"`, and `\"evaluator\"`. There should be no `\"ps\"` job except when using `tf.distribute.experimental.ParameterServerStrategy`.\n## What's next?\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) is actively under development. Try it out and provide your feedback using \n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfdv)  \n---  \nIn this notebook-based tutorial, we will create and run TFX pipelines to validate input data and create an ML model. This notebook is based on the TFX pipeline we built in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple). If you have not read that tutorial yet, you should read it before proceeding with this notebook.\nThe first task in any data science or ML project is to understand and clean the data, which includes:\n  * Understanding the data types, distributions, and other information (e.g., mean value, or number of uniques) about each feature\n  * Generating a preliminary schema that describes the data\n  * Identifying anomalies and missing values in the data with respect to given schema\n\n\nIn this tutorial, we will create two TFX pipelines.\nFirst, we will create a pipeline to analyze the dataset and generate a preliminary schema of the given dataset. This pipeline will include two new components, `StatisticsGen` and `SchemaGen`.\nOnce we have a proper schema of the data, we will create a pipeline to train an ML classification model based on the pipeline from the previous tutorial. In this pipeline, we will use the schema from the first pipeline and a new component, `ExampleValidator`, to validate the input data.\nThe three new components, StatisticsGen, SchemaGen and ExampleValidator, are TFX components for data analysis and validation, and they are implemented using the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) library.\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n## Set Up\nWe first need to install the TFX Python package and download the dataset which we will use for our model.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we are running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TFX\n```\npip\n```\n\n### Did you restart the runtime?\nIf you are using Google Colab, the first time that you run the cell above, you must restart the runtime by clicking above \"RESTART RUNTIME\" button or using \"Runtime > Restart runtime ...\" menu. This is because of the way that Colab loads packages.\nCheck the TensorFlow and TFX versions.\n```\nimporttensorflowastf\nprint('TensorFlow version: {}'.format(tf.__version__))\nfromtfximport v1 as tfx\nprint('TFX version: {}'.format(tfx.__version__))\n\n```\n```\n2024-05-08 09:36:04.670322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 09:36:04.670389: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 09:36:04.671916: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTensorFlow version: 2.15.1\nTFX version: 1.15.0\n\n```\n\n### Set up variables\nThere are some variables used to define a pipeline. You can customize these variables as you want. By default all output from the pipeline will be generated under the current directory.\n```\nimportos\n\n# We will create two pipelines. One for schema generation and one for training.\nSCHEMA_PIPELINE_NAME = \"penguin-tfdv-schema\"\nPIPELINE_NAME = \"penguin-tfdv\"\n\n# Output directory to store artifacts generated from the pipeline.\nSCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME)\nPIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n# Path to a SQLite DB file to use as an MLMD storage.\nSCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME,\n                                    'metadata.db')\nMETADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n\n# Output directory where created models from the pipeline will be exported.\nSERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n\nfromabslimport logging\nlogging.set_verbosity(logging.INFO)  # Set default logging level.\n\n```\n\n### Prepare example data\nWe will download the example dataset for use in our TFX pipeline. The dataset we are using is \nThere are four numeric features in this dataset:\n  * culmen_length_mm\n  * culmen_depth_mm\n  * flipper_length_mm\n  * body_mass_g\n\n\nAll features were already normalized to have range [0,1]. We will build a classification model which predicts the `species` of penguins.\nBecause the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it.\n```\nimporturllib.request\nimporttempfile\n\nDATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\nurllib.request.urlretrieve(_data_url, _data_filepath)\n\n```\n```\n('/tmpfs/tmp/tfx-dataj_6ovg52/data.csv',\n <http.client.HTTPMessage at 0x7ff39cac30a0>)\n\n```\n\nTake a quick look at the CSV file.\n```\nhead{_data_filepath}\n```\n```\nspecies,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\n0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\n0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\n0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\n0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\n0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\n0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\n0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\n0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\n0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\n\n```\n\nYou should be able to see five feature columns. `species` is one of 0, 1 or 2, and all other features should have values between 0 and 1. We will create a TFX pipeline to analyze this dataset.\n## Generate a preliminary schema\nTFX pipelines are defined using Python APIs. We will create a pipeline to generate a schema from the input examples automatically. This schema can be reviewed by a human and adjusted as needed. Once the schema is finalized it can be used for training and example validation in later tasks.\nIn addition to `CsvExampleGen` which is used in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple), we will use `StatisticsGen` and `SchemaGen`:\n  * [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen) calculates statistics for the dataset.\n  * [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) examines the statistics and creates an initial data schema.\n\n\nSee the guides for each component or [TFX components tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras) to learn more on these components.\n### Write a pipeline definition\nWe define a function to create a TFX pipeline. A `Pipeline` object represents a TFX pipeline which can be run using one of pipeline orchestration systems that TFX supports.\n```\ndef_create_schema_pipeline(pipeline_name: str,\n                            pipeline_root: str,\n                            data_root: str,\n                            metadata_path: str) -> tfx.dsl.Pipeline:\n\"\"\"Creates a pipeline for schema generation.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # NEW: Computes statistics over data for visualization and schema generation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # NEW: Generates schema based on the generated statistics.\n  schema_gen = tfx.components.SchemaGen(\n      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n\n  components = [\n      example_gen,\n      statistics_gen,\n      schema_gen,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n\n```\n\n### Run the pipeline\nWe will use `LocalDagRunner` as in the previous tutorial.\n```\ntfx.orchestration.LocalDagRunner().run(\n  _create_schema_pipeline(\n      pipeline_name=SCHEMA_PIPELINE_NAME,\n      pipeline_root=SCHEMA_PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      metadata_path=SCHEMA_METADATA_PATH))\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Using deployment config:\n executor_specs {\n  key: \"CsvExampleGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"SchemaGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.schema_gen.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"StatisticsGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n      }\n    }\n  }\n}\ncustom_driver_specs {\n  key: \"CsvExampleGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n    }\n  }\n}\nmetadata_connection_config {\n  database_connection_config {\n    sqlite {\n      filename_uri: \"metadata/penguin-tfdv-schema/metadata.db\"\n      connection_mode: READWRITE_OPENCREATE\n    }\n  }\n}\n\nINFO:absl:Using connection config:\n sqlite {\n  filename_uri: \"metadata/penguin-tfdv-schema/metadata.db\"\n  connection_mode: READWRITE_OPENCREATE\n}\n\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataj_6ovg52\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:[CsvExampleGen] Resolved inputs: ({},)\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 1\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}), exec_properties={'output_data_format': 6, 'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_base': '/tmpfs/tmp/tfx-dataj_6ovg52', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970'}, execution_output_uri='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/stateful_working_dir/d65151e8-a6c7-4b12-8076-f56938dd89f4', tmp_dir='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataj_6ovg52\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv-schema\"\n, pipeline_run_id='2024-05-08T09:36:10.555564', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating examples.\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-dataj_6ovg52/* to TFExample.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:absl:Examples generated.\nINFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\nINFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 1 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv-schema/CsvExampleGen/.system/stateful_working_dir/d65151e8-a6c7-4b12-8076-f56938dd89f4\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}) for execution 1\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\nINFO:absl:Component StatisticsGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:10.555564\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"SchemaGen\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160971690\nlast_update_time_since_epoch: 1715160971690\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 2\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160971690\nlast_update_time_since_epoch: 1715160971690\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv-schema/StatisticsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/StatisticsGen/.system/stateful_working_dir/3dc1ed50-c155-41f6-8457-b71a0b0ebe51', tmp_dir='pipelines/penguin-tfdv-schema/StatisticsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:10.555564\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"SchemaGen\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv-schema\"\n, pipeline_run_id='2024-05-08T09:36:10.555564', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating statistics for split train.\nINFO:absl:Statistics for split train written to pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2/Split-train.\nINFO:absl:Generating statistics for split eval.\nINFO:absl:Statistics for split eval written to pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2/Split-eval.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 2 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv-schema/StatisticsGen/.system/stateful_working_dir/3dc1ed50-c155-41f6-8457-b71a0b0ebe51\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}) for execution 2\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component StatisticsGen is finished.\nINFO:absl:Component SchemaGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.schema_gen.component.SchemaGen\"\n    base_type: PROCESS\n  }\n  id: \"SchemaGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.SchemaGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:10.555564\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n  parameters {\n    key: \"infer_feature_shape\"\n    value {\n      field_value {\n        int_value: 1\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[SchemaGen] Resolved inputs: ({'statistics': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160975131\nlast_update_time_since_epoch: 1715160975131\n, artifact_type: id: 17\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 3\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'statistics': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160975131\nlast_update_time_since_epoch: 1715160975131\n, artifact_type: id: 17\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}, output_dict=defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/SchemaGen/schema/3\"\n, artifact_type: name: \"Schema\"\n)]}), exec_properties={'infer_feature_shape': 1, 'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv-schema/SchemaGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/SchemaGen/.system/stateful_working_dir/9cb63ad1-17a3-4aaa-a3d3-5059f958bf6f', tmp_dir='pipelines/penguin-tfdv-schema/SchemaGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.schema_gen.component.SchemaGen\"\n    base_type: PROCESS\n  }\n  id: \"SchemaGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:10.555564\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv-schema.SchemaGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:10.555564\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv-schema.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"schema\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n  parameters {\n    key: \"infer_feature_shape\"\n    value {\n      field_value {\n        int_value: 1\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv-schema\"\n, pipeline_run_id='2024-05-08T09:36:10.555564', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Processing schema from statistics for split train.\nINFO:absl:Processing schema from statistics for split eval.\nINFO:absl:Schema written to pipelines/penguin-tfdv-schema/SchemaGen/schema/3/schema.pbtxt.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 3 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv-schema/SchemaGen/.system/stateful_working_dir/9cb63ad1-17a3-4aaa-a3d3-5059f958bf6f\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/SchemaGen/schema/3\"\n, artifact_type: name: \"Schema\"\n)]}) for execution 3\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component SchemaGen is finished.\n\n```\n\nYou should see \"INFO:absl:Component SchemaGen is finished.\" if the pipeline finished successfully.\nWe will examine the output of the pipeline to understand our dataset.\n### Review outputs of the pipeline\nAs explained in the previous tutorial, a TFX pipeline produces two kinds of outputs, artifacts and a [metadata DB(MLMD)](https://www.tensorflow.org/tfx/guide/mlmd) which contains metadata of artifacts and pipeline executions. We defined the location of these outputs in the above cells. By default, artifacts are stored under the `pipelines` directory and metadata is stored as a sqlite database under the `metadata` directory.\nYou can use MLMD APIs to locate these outputs programatically. First, we will define some utility functions to search for the output artifacts that were just produced.\n```\nfromml_metadata.protoimport metadata_store_pb2\n# Non-public APIs, just for showcase.\nfromtfx.orchestration.portable.mlmdimport execution_lib\n\n# TODO(b/171447278): Move these functions into the TFX library.\n\ndefget_latest_artifacts(metadata, pipeline_name, component_id):\n\"\"\"Output artifacts of the latest run of the component.\"\"\"\n  context = metadata.store.get_context_by_type_and_name(\n      'node', f'{pipeline_name}.{component_id}')\n  executions = metadata.store.get_executions_by_context(context.id)\n  latest_execution = max(executions,\n                         key=lambda e:e.last_update_time_since_epoch)\n  return execution_lib.get_output_artifacts(metadata, latest_execution.id)\n\n# Non-public APIs, just for showcase.\nfromtfx.orchestration.experimental.interactiveimport visualizations\n\ndefvisualize_artifacts(artifacts):\n\"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n  for artifact in artifacts:\n    visualization = visualizations.get_registry().get_visualization(\n        artifact.type_name)\n    if visualization:\n      visualization.display(artifact)\n\nfromtfx.orchestration.experimental.interactiveimport standard_visualizations\nstandard_visualizations.register_standard_visualizations()\n\n```\n\nNow we can examine the outputs from the pipeline execution.\n```\n# Non-public APIs, just for showcase.\nfromtfx.orchestration.metadataimport Metadata\nfromtfx.typesimport standard_component_specs\n\nmetadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    SCHEMA_METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  # Find output artifacts from MLMD.\n  stat_gen_output = get_latest_artifacts(metadata_handler, SCHEMA_PIPELINE_NAME,\n                                         'StatisticsGen')\n  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n\n  schema_gen_output = get_latest_artifacts(metadata_handler,\n                                           SCHEMA_PIPELINE_NAME, 'SchemaGen')\n  schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]\n\n```\n```\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nIt is time to examine the outputs from each component. As described above, [Tensorflow Data Validation(TFDV)](https://www.tensorflow.org/tfx/data_validation/get_started) is used in `StatisticsGen` and `SchemaGen`, and TFDV also provides visualization of the outputs from these components.\nIn this tutorial, we will use the visualization helper methods in TFX which use TFDV internally to show the visualization.\n#### Examine the output from StatisticsGen\n```\n# docs-infra: no-execute\nvisualize_artifacts(stats_artifacts)\n\n```\n\nYou can see various stats for the input data. These statistics are supplied to `SchemaGen` to construct an initial schema of data automatically.\n#### Examine the output from SchemaGen\n```\nvisualize_artifacts(schema_artifacts)\n\n```\n\nThis schema is automatically inferred from the output of StatisticsGen. You should be able to see 4 FLOAT features and 1 INT feature.\n### Export the schema for future use\nWe need to review and refine the generated schema. The reviewed schema needs to be persisted to be used in subsequent pipelines for ML model training. In other words, you might want to add the schema file to your version control system for actual use cases. In this tutorial, we will just copy the schema to a predefined filesystem path for simplicity.\n```\nimportshutil\n\n_schema_filename = 'schema.pbtxt'\nSCHEMA_PATH = 'schema'\n\nos.makedirs(SCHEMA_PATH, exist_ok=True)\n_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n\n# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\nshutil.copy(_generated_path, SCHEMA_PATH)\n\n```\n```\n'schema/schema.pbtxt'\n\n```\n\nThe schema file uses \n```\nprint(f'Schema at {SCHEMA_PATH}-----')\n!cat {SCHEMA_PATH}/*\n\n```\n```\nSchema at schema-----\nfeature {\n  name: \"body_mass_g\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"culmen_depth_mm\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"culmen_length_mm\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"flipper_length_mm\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"species\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\n\n```\n\nYou should be sure to review and possibly edit the schema definition as needed. In this tutorial, we will just use the generated schema unchanged.\n## Validate input examples and train an ML model\nWe will go back to the pipeline that we created in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple), to train an ML model and use the generated schema for writing the model training code.\nWe will also add an [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) component which will look for anomalies and missing values in the incoming dataset with respect to the schema.\n### Write model training code\nWe need to write the model code as we did in [Simple TFX Pipeline Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple).\nThe model itself is the same as in the previous tutorial, but this time we will use the schema generated from the previous pipeline instead of specifying features manually. Most of the code was not changed. The only difference is that we do not need to specify the names and types of features in this file. Instead, we read them from the _schema_ file.\n```\n_trainer_module_file = 'penguin_trainer.py'\n\n```\n```\n%%writefile {_trainer_module_file}\n\nfromtypingimport List\nfromabslimport logging\nimporttensorflowastf\nfromtensorflowimport keras\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfximport v1 as tfx\nfromtfx_bsl.publicimport tfxio\nfromtensorflow_metadata.proto.v0import schema_pb2\n\n# We don't need to specify _FEATURE_KEYS and _FEATURE_SPEC any more.\n# Those information can be read from the given schema file.\n\n_LABEL_KEY = 'species'\n\n_TRAIN_BATCH_SIZE = 20\n_EVAL_BATCH_SIZE = 10\n\ndef_input_fn(file_pattern: List[str],\n              data_accessor: tfx.components.DataAccessor,\n              schema: schema_pb2.Schema,\n              batch_size: int = 200) -> tf.data.Dataset:\n\"\"\"Generates features and label for training.\n\n  Args:\n    file_pattern: List of paths or patterns of input tfrecord files.\n    data_accessor: DataAccessor for converting input to RecordBatch.\n    schema: schema of the input data.\n    batch_size: representing the number of consecutive elements of returned\n      dataset to combine in a single batch\n\n  Returns:\n    A dataset that contains (features, indices) tuple where features is a\n      dictionary of Tensors, and indices is a single Tensor of label indices.\n  \"\"\"\n  return data_accessor.tf_dataset_factory(\n      file_pattern,\n      tfxio.TensorFlowDatasetOptions(\n          batch_size=batch_size, label_key=_LABEL_KEY),\n      schema=schema).repeat()\n\n\ndef_build_keras_model(schema: schema_pb2.Schema) -> tf.keras.Model:\n\"\"\"Creates a DNN Keras model for classifying penguin data.\n\n  Returns:\n    A Keras Model.\n  \"\"\"\n  # The model below is built with Functional API, please refer to\n  # https://www.tensorflow.org/guide/keras/overview for all API options.\n\n  # ++ Changed code: Uses all features in the schema except the label.\n  feature_keys = [f.name for f in schema.feature if f.name != _LABEL_KEY]\n  inputs = [keras.layers.Input(shape=(1,), name=f) for f in feature_keys]\n  # ++ End of the changed code.\n\n  d = keras.layers.concatenate(inputs)\n  for _ in range(2):\n    d = keras.layers.Dense(8, activation='relu')(d)\n  outputs = keras.layers.Dense(3)(d)\n\n  model = keras.Model(inputs=inputs, outputs=outputs)\n  model.compile(\n      optimizer=keras.optimizers.Adam(1e-2),\n      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n\n  model.summary(print_fn=logging.info)\n  return model\n\n\n# TFX Trainer will call this function.\ndefrun_fn(fn_args: tfx.components.FnArgs):\n\"\"\"Train the model based on given args.\n\n  Args:\n    fn_args: Holds args used to train the model as name/value pairs.\n  \"\"\"\n\n  # ++ Changed code: Reads in schema file passed to the Trainer component.\n  schema = tfx.utils.parse_pbtxt_file(fn_args.schema_path, schema_pb2.Schema())\n  # ++ End of the changed code.\n\n  train_dataset = _input_fn(\n      fn_args.train_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_TRAIN_BATCH_SIZE)\n  eval_dataset = _input_fn(\n      fn_args.eval_files,\n      fn_args.data_accessor,\n      schema,\n      batch_size=_EVAL_BATCH_SIZE)\n\n  model = _build_keras_model(schema)\n  model.fit(\n      train_dataset,\n      steps_per_epoch=fn_args.train_steps,\n      validation_data=eval_dataset,\n      validation_steps=fn_args.eval_steps)\n\n  # The result of the training should be saved in `fn_args.serving_model_dir`\n  # directory.\n  model.save(fn_args.serving_model_dir, save_format='tf')\n\n```\n```\nWriting penguin_trainer.py\n\n```\n\nNow you have completed all preparation steps to build a TFX pipeline for model training.\n### Write a pipeline definition\nWe will add two new components, `Importer` and `ExampleValidator`. Importer brings an external file into the TFX pipeline. In this case, it is a file containing schema definition. ExampleValidator will examine the input data and validate whether all input data conforms the data schema we provided.\n```\ndef_create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n                     schema_path: str, module_file: str, serving_model_dir: str,\n                     metadata_path: str) -> tfx.dsl.Pipeline:\n\"\"\"Creates a pipeline using predefined schema with TFX.\"\"\"\n  # Brings data into the pipeline.\n  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n\n  # Computes statistics over data for visualization and example validation.\n  statistics_gen = tfx.components.StatisticsGen(\n      examples=example_gen.outputs['examples'])\n\n  # NEW: Import the schema.\n  schema_importer = tfx.dsl.Importer(\n      source_uri=schema_path,\n      artifact_type=tfx.types.standard_artifacts.Schema).with_id(\n          'schema_importer')\n\n  # NEW: Performs anomaly detection based on statistics and data schema.\n  example_validator = tfx.components.ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_importer.outputs['result'])\n\n  # Uses user-provided Python function that trains a model.\n  trainer = tfx.components.Trainer(\n      module_file=module_file,\n      examples=example_gen.outputs['examples'],\n      schema=schema_importer.outputs['result'],  # Pass the imported schema.\n      train_args=tfx.proto.TrainArgs(num_steps=100),\n      eval_args=tfx.proto.EvalArgs(num_steps=5))\n\n  # Pushes the model to a filesystem destination.\n  pusher = tfx.components.Pusher(\n      model=trainer.outputs['model'],\n      push_destination=tfx.proto.PushDestination(\n          filesystem=tfx.proto.PushDestination.Filesystem(\n              base_directory=serving_model_dir)))\n\n  components = [\n      example_gen,\n\n      # NEW: Following three components were added to the pipeline.\n      statistics_gen,\n      schema_importer,\n      example_validator,\n\n      trainer,\n      pusher,\n  ]\n\n  return tfx.dsl.Pipeline(\n      pipeline_name=pipeline_name,\n      pipeline_root=pipeline_root,\n      metadata_connection_config=tfx.orchestration.metadata\n      .sqlite_metadata_connection_config(metadata_path),\n      components=components)\n\n```\n\n### Run the pipeline\n```\ntfx.orchestration.LocalDagRunner().run(\n  _create_pipeline(\n      pipeline_name=PIPELINE_NAME,\n      pipeline_root=PIPELINE_ROOT,\n      data_root=DATA_ROOT,\n      schema_path=SCHEMA_PATH,\n      module_file=_trainer_module_file,\n      serving_model_dir=SERVING_MODEL_DIR,\n      metadata_path=METADATA_PATH))\n\n```\n```\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Excluding no splits because exclude_splits is not set.\nINFO:absl:Generating ephemeral wheel package for '/tmpfs/src/temp/docs/tutorials/tfx/penguin_trainer.py' (including modules: ['penguin_trainer']).\nINFO:absl:User module package has hash fingerprint version 000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '/tmpfs/tmp/tmpw96a2pj7/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmpfs/tmp/tmp42iap5mu', '--dist-dir', '/tmpfs/tmp/tmpx8p04zcg']\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nINFO:absl:Successfully built user code wheel distribution at 'pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl'; target user module is 'penguin_trainer'.\nINFO:absl:Full user module path is 'penguin_trainer@pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl'\nINFO:absl:Using deployment config:\n executor_specs {\n  key: \"CsvExampleGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"ExampleValidator\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_validator.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"Pusher\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.pusher.executor.Executor\"\n    }\n  }\n}\nexecutor_specs {\n  key: \"StatisticsGen\"\n  value {\n    beam_executable_spec {\n      python_executor_spec {\n        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n      }\n    }\n  }\n}\nexecutor_specs {\n  key: \"Trainer\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n    }\n  }\n}\ncustom_driver_specs {\n  key: \"CsvExampleGen\"\n  value {\n    python_class_executable_spec {\n      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n    }\n  }\n}\nmetadata_connection_config {\n  database_connection_config {\n    sqlite {\n      filename_uri: \"metadata/penguin-tfdv/metadata.db\"\n      connection_mode: READWRITE_OPENCREATE\n    }\n  }\n}\n\nINFO:absl:Using connection config:\n sqlite {\n  filename_uri: \"metadata/penguin-tfdv/metadata.db\"\n  connection_mode: READWRITE_OPENCREATE\n}\n\nINFO:absl:Component CsvExampleGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataj_6ovg52\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:[CsvExampleGen] Resolved inputs: ({},)\nrunning bdist_wheel\nrunning build\nrunning build_py\ncreating build\ncreating build/lib\ncopying penguin_trainer.py -> build/lib\ninstalling to /tmpfs/tmp/tmp42iap5mu\nrunning install\nrunning install_lib\ncopying build/lib/penguin_trainer.py -> /tmpfs/tmp/tmp42iap5mu\nrunning install_egg_info\nrunning egg_info\ncreating tfx_user_code_Trainer.egg-info\nwriting tfx_user_code_Trainer.egg-info/PKG-INFO\nwriting dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\nwriting top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nreading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nwriting manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\nCopying tfx_user_code_Trainer.egg-info to /tmpfs/tmp/tmp42iap5mu/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3.9.egg-info\nrunning install_scripts\ncreating /tmpfs/tmp/tmp42iap5mu/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.dist-info/WHEEL\ncreating '/tmpfs/tmp/tmpx8p04zcg/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl' and adding '/tmpfs/tmp/tmp42iap5mu' to it\nadding 'penguin_trainer.py'\nadding 'tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.dist-info/METADATA'\nadding 'tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.dist-info/WHEEL'\nadding 'tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.dist-info/top_level.txt'\nadding 'tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2.dist-info/RECORD'\nremoving /tmpfs/tmp/tmp42iap5mu\nINFO:absl:select span and version = (0, None)\nINFO:absl:latest span and version = (0, None)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 1\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}), exec_properties={'output_file_format': 5, 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_base': '/tmpfs/tmp/tfx-dataj_6ovg52', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970'}, execution_output_uri='pipelines/penguin-tfdv/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv/CsvExampleGen/.system/stateful_working_dir/0858d568-1a97-401a-afc6-a9932ff9a1e3', tmp_dir='pipelines/penguin-tfdv/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n  }\n  id: \"CsvExampleGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.CsvExampleGen\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"examples\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Examples\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          properties {\n            key: \"version\"\n            value: INT\n          }\n          base_type: DATASET\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"input_base\"\n    value {\n      field_value {\n        string_value: \"/tmpfs/tmp/tfx-dataj_6ovg52\"\n      }\n    }\n  }\n  parameters {\n    key: \"input_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_config\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_data_format\"\n    value {\n      field_value {\n        int_value: 6\n      }\n    }\n  }\n  parameters {\n    key: \"output_file_format\"\n    value {\n      field_value {\n        int_value: 5\n      }\n    }\n  }\n}\ndownstream_nodes: \"StatisticsGen\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv\"\n, pipeline_run_id='2024-05-08T09:36:15.816321', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating examples.\nINFO:absl:Processing input csv data /tmpfs/tmp/tfx-dataj_6ovg52/* to TFExample.\nINFO:absl:Examples generated.\nINFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\nINFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 1 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv/CsvExampleGen/.system/stateful_working_dir/0858d568-1a97-401a-afc6-a9932ff9a1e3\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\n, artifact_type: name: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}) for execution 1\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component CsvExampleGen is finished.\nINFO:absl:Component schema_importer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.dsl.components.common.importer.Importer\"\n  }\n  id: \"schema_importer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.schema_importer\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"result\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Schema\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"artifact_uri\"\n    value {\n      field_value {\n        string_value: \"schema\"\n      }\n    }\n  }\n  parameters {\n    key: \"output_key\"\n    value {\n      field_value {\n        string_value: \"result\"\n      }\n    }\n  }\n  parameters {\n    key: \"reimport\"\n    value {\n      field_value {\n        int_value: 0\n      }\n    }\n  }\n}\ndownstream_nodes: \"ExampleValidator\"\ndownstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:Running as an importer node.\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Processing source uri: schema, properties: {}, custom_properties: {}\nINFO:absl:Component schema_importer is finished.\nINFO:absl:Component StatisticsGen is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"ExampleValidator\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160976759\nlast_update_time_since_epoch: 1715160976759\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 3\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160976759\nlast_update_time_since_epoch: 1715160976759\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/StatisticsGen/statistics/3\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv/StatisticsGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv/StatisticsGen/.system/stateful_working_dir/110d150d-d7b8-4a54-9e4b-de96d2f275fe', tmp_dir='pipelines/penguin-tfdv/StatisticsGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n    base_type: PROCESS\n  }\n  id: \"StatisticsGen\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.StatisticsGen\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"statistics\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleStatistics\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n          base_type: STATISTICS\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\ndownstream_nodes: \"ExampleValidator\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv\"\n, pipeline_run_id='2024-05-08T09:36:15.816321', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Generating statistics for split train.\nINFO:absl:Statistics for split train written to pipelines/penguin-tfdv/StatisticsGen/statistics/3/Split-train.\nINFO:absl:Generating statistics for split eval.\nINFO:absl:Statistics for split eval written to pipelines/penguin-tfdv/StatisticsGen/statistics/3/Split-eval.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 3 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv/StatisticsGen/.system/stateful_working_dir/110d150d-d7b8-4a54-9e4b-de96d2f275fe\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/StatisticsGen/statistics/3\"\n, artifact_type: name: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}) for execution 3\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component StatisticsGen is finished.\nINFO:absl:Component Trainer is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"schema_importer\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Trainer] Resolved inputs: ({'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160976782\nlast_update_time_since_epoch: 1715160976782\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160976759\nlast_update_time_since_epoch: 1715160976759\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 4\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160976782\nlast_update_time_since_epoch: 1715160976782\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'examples': [Artifact(artifact: id: 1\ntype_id: 15\nuri: \"pipelines/penguin-tfdv/CsvExampleGen/examples/1\"\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"file_format\"\n  value {\n    string_value: \"tfrecords_gzip\"\n  }\n}\ncustom_properties {\n  key: \"input_fingerprint\"\n  value {\n    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1715160970,sum_checksum:1715160970\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"payload_format\"\n  value {\n    string_value: \"FORMAT_TF_EXAMPLE\"\n  }\n}\ncustom_properties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Examples\"\ncreate_time_since_epoch: 1715160976759\nlast_update_time_since_epoch: 1715160976759\n, artifact_type: id: 15\nname: \"Examples\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nproperties {\n  key: \"version\"\n  value: INT\n}\nbase_type: DATASET\n)]}, output_dict=defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Trainer/model/4\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Trainer/model_run/4\"\n, artifact_type: name: \"ModelRun\"\n)]}), exec_properties={'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl'}, execution_output_uri='pipelines/penguin-tfdv/Trainer/.system/executor_execution/4/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv/Trainer/.system/stateful_working_dir/d3fcfa17-7c4e-4e63-a48c-deb3cc064ab5', tmp_dir='pipelines/penguin-tfdv/Trainer/.system/executor_execution/4/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.trainer.component.Trainer\"\n    base_type: TRAIN\n  }\n  id: \"Trainer\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.Trainer\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"examples\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"CsvExampleGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.CsvExampleGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Examples\"\n            base_type: DATASET\n          }\n        }\n        output_key: \"examples\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"Model\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n  outputs {\n    key: \"model_run\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ModelRun\"\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"eval_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 5\\n}\"\n      }\n    }\n  }\n  parameters {\n    key: \"module_path\"\n    value {\n      field_value {\n        string_value: \"penguin_trainer@pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl\"\n      }\n    }\n  }\n  parameters {\n    key: \"train_args\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"CsvExampleGen\"\nupstream_nodes: \"schema_importer\"\ndownstream_nodes: \"Pusher\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv\"\n, pipeline_run_id='2024-05-08T09:36:15.816321', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Train on the 'train' split when train_args.splits is not set.\nINFO:absl:Evaluate on the 'eval' split when eval_args.splits is not set.\nINFO:absl:udf_utils.get_fn {'custom_config': 'null', 'train_args': '{\\n  \"num_steps\": 100\\n}', 'eval_args': '{\\n  \"num_steps\": 5\\n}', 'module_path': 'penguin_trainer@pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl'} 'run_fn'\nINFO:absl:Installing 'pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl' to a temporary directory.\nINFO:absl:Executing: ['/tmpfs/src/tf_docs_env/bin/python', '-m', 'pip', 'install', '--target', '/tmpfs/tmp/tmp8javf_ly', 'pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl']\nProcessing ./pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl\nINFO:absl:Successfully installed 'pipelines/penguin-tfdv/_wheels/tfx_user_code_Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2-py3-none-any.whl'.\nINFO:absl:Training model.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nInstalling collected packages: tfx-user-code-Trainer\nSuccessfully installed tfx-user-code-Trainer-0.0+000876a22093ec764e3751d5a3ed939f1b107d1d6ade133f954ea2a767b8dfb2\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tfx_bsl/tfxio/tf_example_record.py:343: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature body_mass_g has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_depth_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature culmen_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature flipper_length_mm has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Feature species has a shape dim {\n  size: 1\n}\n. Setting to DenseTensor.\nINFO:absl:Model: \"model\"\nINFO:absl:__________________________________________________________________________________________________\nINFO:absl: Layer (type)                Output Shape                 Param #   Connected to                  \nINFO:absl:==================================================================================================\nINFO:absl: body_mass_g (InputLayer)    [(None, 1)]                  0         []                            \nINFO:absl:                                                                                                  \nINFO:absl: culmen_depth_mm (InputLaye  [(None, 1)]                  0         []                            \nINFO:absl: r)                                                                                               \nINFO:absl:                                                                                                  \nINFO:absl: culmen_length_mm (InputLay  [(None, 1)]                  0         []                            \nINFO:absl: er)                                                                                              \nINFO:absl:                                                                                                  \nINFO:absl: flipper_length_mm (InputLa  [(None, 1)]                  0         []                            \nINFO:absl: yer)                                                                                             \nINFO:absl:                                                                                                  \nINFO:absl: concatenate (Concatenate)   (None, 4)                    0         ['body_mass_g[0][0]',         \nINFO:absl:                                                                     'culmen_depth_mm[0][0]',     \nINFO:absl:                                                                     'culmen_length_mm[0][0]',    \nINFO:absl:                                                                     'flipper_length_mm[0][0]']   \nINFO:absl:                                                                                                  \nINFO:absl: dense (Dense)               (None, 8)                    40        ['concatenate[0][0]']         \nINFO:absl:                                                                                                  \nINFO:absl: dense_1 (Dense)             (None, 8)                    72        ['dense[0][0]']               \nINFO:absl:                                                                                                  \nINFO:absl: dense_2 (Dense)             (None, 3)                    27        ['dense_1[0][0]']             \nINFO:absl:                                                                                                  \nINFO:absl:==================================================================================================\nINFO:absl:Total params: 139 (556.00 Byte)\nINFO:absl:Trainable params: 139 (556.00 Byte)\nINFO:absl:Non-trainable params: 0 (0.00 Byte)\nINFO:absl:__________________________________________________________________________________________________\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1715160986.539652   29188 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n100/100 [==============================] - 2s 5ms/step - loss: 0.5183 - sparse_categorical_accuracy: 0.8225 - val_loss: 0.1883 - val_sparse_categorical_accuracy: 0.9000\nINFO:absl:Function `_wrapped_model` contains input name(s) resource with unsupported characters which will be renamed to model_dense_2_biasadd_readvariableop_resource in the SavedModel.\nINFO:tensorflow:Assets written to: pipelines/penguin-tfdv/Trainer/model/4/Format-Serving/assets\nINFO:tensorflow:Assets written to: pipelines/penguin-tfdv/Trainer/model/4/Format-Serving/assets\nINFO:absl:Writing fingerprint to pipelines/penguin-tfdv/Trainer/model/4/Format-Serving/fingerprint.pb\nINFO:absl:Training complete. Model written to pipelines/penguin-tfdv/Trainer/model/4/Format-Serving. ModelRun written to pipelines/penguin-tfdv/Trainer/model_run/4\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 4 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv/Trainer/.system/stateful_working_dir/d3fcfa17-7c4e-4e63-a48c-deb3cc064ab5\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Trainer/model/4\"\n, artifact_type: name: \"Model\"\nbase_type: MODEL\n)], 'model_run': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Trainer/model_run/4\"\n, artifact_type: name: \"ModelRun\"\n)]}) for execution 4\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Trainer is finished.\nINFO:absl:Component ExampleValidator is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.example_validator.component.ExampleValidator\"\n  }\n  id: \"ExampleValidator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.ExampleValidator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nupstream_nodes: \"schema_importer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[ExampleValidator] Resolved inputs: ({'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160976782\nlast_update_time_since_epoch: 1715160976782\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'statistics': [Artifact(artifact: id: 3\ntype_id: 19\nuri: \"pipelines/penguin-tfdv/StatisticsGen/statistics/3\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160979570\nlast_update_time_since_epoch: 1715160979570\n, artifact_type: id: 19\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 5\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'schema': [Artifact(artifact: id: 2\ntype_id: 17\nuri: \"schema\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 1\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Schema\"\ncreate_time_since_epoch: 1715160976782\nlast_update_time_since_epoch: 1715160976782\n, artifact_type: id: 17\nname: \"Schema\"\n)], 'statistics': [Artifact(artifact: id: 3\ntype_id: 19\nuri: \"pipelines/penguin-tfdv/StatisticsGen/statistics/3\"\nproperties {\n  key: \"span\"\n  value {\n    int_value: 0\n  }\n}\nproperties {\n  key: \"split_names\"\n  value {\n    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n  }\n}\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"stats_dashboard_link\"\n  value {\n    string_value: \"\"\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"ExampleStatistics\"\ncreate_time_since_epoch: 1715160979570\nlast_update_time_since_epoch: 1715160979570\n, artifact_type: id: 19\nname: \"ExampleStatistics\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\nbase_type: STATISTICS\n)]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/ExampleValidator/anomalies/5\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv/ExampleValidator/.system/executor_execution/5/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv/ExampleValidator/.system/stateful_working_dir/37093a55-ba7a-42c7-a4fc-388b5f69b7d8', tmp_dir='pipelines/penguin-tfdv/ExampleValidator/.system/executor_execution/5/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.example_validator.component.ExampleValidator\"\n  }\n  id: \"ExampleValidator\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.ExampleValidator\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"schema\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"schema_importer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.schema_importer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Schema\"\n          }\n        }\n        output_key: \"result\"\n      }\n      min_count: 1\n    }\n  }\n  inputs {\n    key: \"statistics\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"StatisticsGen\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.StatisticsGen\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"ExampleStatistics\"\n            base_type: STATISTICS\n          }\n        }\n        output_key: \"statistics\"\n      }\n      min_count: 1\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"anomalies\"\n    value {\n      artifact_spec {\n        type {\n          name: \"ExampleAnomalies\"\n          properties {\n            key: \"span\"\n            value: INT\n          }\n          properties {\n            key: \"split_names\"\n            value: STRING\n          }\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"exclude_splits\"\n    value {\n      field_value {\n        string_value: \"[]\"\n      }\n    }\n  }\n}\nupstream_nodes: \"StatisticsGen\"\nupstream_nodes: \"schema_importer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv\"\n, pipeline_run_id='2024-05-08T09:36:15.816321', top_level_pipeline_run_id=None, frontend_url=None)\nINFO:absl:Validating schema against the computed statistics for split train.\nINFO:absl:Anomalies alerts created for split train.\nINFO:absl:Validation complete for split train. Anomalies written to pipelines/penguin-tfdv/ExampleValidator/anomalies/5/Split-train.\nINFO:absl:Validating schema against the computed statistics for split eval.\nINFO:absl:Anomalies alerts created for split eval.\nINFO:absl:Validation complete for split eval. Anomalies written to pipelines/penguin-tfdv/ExampleValidator/anomalies/5/Split-eval.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 5 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv/ExampleValidator/.system/stateful_working_dir/37093a55-ba7a-42c7-a4fc-388b5f69b7d8\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/ExampleValidator/anomalies/5\"\n, artifact_type: name: \"ExampleAnomalies\"\nproperties {\n  key: \"span\"\n  value: INT\n}\nproperties {\n  key: \"split_names\"\n  value: STRING\n}\n)]}) for execution 5\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component ExampleValidator is finished.\nINFO:absl:Component Pusher is running.\nINFO:absl:Running launcher for node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-tfdv\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n\nINFO:absl:MetadataStore with DB connection initialized\nWARNING:absl:ArtifactQuery.property_predicate is not supported.\nINFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 4\ntype_id: 21\nuri: \"pipelines/penguin-tfdv/Trainer/model/4\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715160988205\nlast_update_time_since_epoch: 1715160988205\n, artifact_type: id: 21\nname: \"Model\"\nbase_type: MODEL\n)]},)\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Going to run a new execution 6\nINFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'model': [Artifact(artifact: id: 4\ntype_id: 21\nuri: \"pipelines/penguin-tfdv/Trainer/model/4\"\ncustom_properties {\n  key: \"is_external\"\n  value {\n    int_value: 0\n  }\n}\ncustom_properties {\n  key: \"tfx_version\"\n  value {\n    string_value: \"1.15.0\"\n  }\n}\nstate: LIVE\ntype: \"Model\"\ncreate_time_since_epoch: 1715160988205\nlast_update_time_since_epoch: 1715160988205\n, artifact_type: id: 21\nname: \"Model\"\nbase_type: MODEL\n)]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Pusher/pushed_model/6\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"serving_model/penguin-tfdv\"\\n  }\\n}'}, execution_output_uri='pipelines/penguin-tfdv/Pusher/.system/executor_execution/6/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv/Pusher/.system/stateful_working_dir/5146eed9-4c4d-4bef-b849-ce87e44956ad', tmp_dir='pipelines/penguin-tfdv/Pusher/.system/executor_execution/6/.temp/', pipeline_node=node_info {\n  type {\n    name: \"tfx.components.pusher.component.Pusher\"\n    base_type: DEPLOY\n  }\n  id: \"Pusher\"\n}\ncontexts {\n  contexts {\n    type {\n      name: \"pipeline\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"pipeline_run\"\n    }\n    name {\n      field_value {\n        string_value: \"2024-05-08T09:36:15.816321\"\n      }\n    }\n  }\n  contexts {\n    type {\n      name: \"node\"\n    }\n    name {\n      field_value {\n        string_value: \"penguin-tfdv.Pusher\"\n      }\n    }\n  }\n}\ninputs {\n  inputs {\n    key: \"model\"\n    value {\n      channels {\n        producer_node_query {\n          id: \"Trainer\"\n        }\n        context_queries {\n          type {\n            name: \"pipeline\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"pipeline_run\"\n          }\n          name {\n            field_value {\n              string_value: \"2024-05-08T09:36:15.816321\"\n            }\n          }\n        }\n        context_queries {\n          type {\n            name: \"node\"\n          }\n          name {\n            field_value {\n              string_value: \"penguin-tfdv.Trainer\"\n            }\n          }\n        }\n        artifact_query {\n          type {\n            name: \"Model\"\n            base_type: MODEL\n          }\n        }\n        output_key: \"model\"\n      }\n    }\n  }\n}\noutputs {\n  outputs {\n    key: \"pushed_model\"\n    value {\n      artifact_spec {\n        type {\n          name: \"PushedModel\"\n          base_type: MODEL\n        }\n      }\n    }\n  }\n}\nparameters {\n  parameters {\n    key: \"custom_config\"\n    value {\n      field_value {\n        string_value: \"null\"\n      }\n    }\n  }\n  parameters {\n    key: \"push_destination\"\n    value {\n      field_value {\n        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"serving_model/penguin-tfdv\\\"\\n  }\\n}\"\n      }\n    }\n  }\n}\nupstream_nodes: \"Trainer\"\nexecution_options {\n  caching_options {\n  }\n}\n, pipeline_info=id: \"penguin-tfdv\"\n, pipeline_run_id='2024-05-08T09:36:15.816321', top_level_pipeline_run_id=None, frontend_url=None)\nWARNING:absl:Pusher is going to push the model without validation. Consider using Evaluator or InfraValidator in your pipeline.\nINFO:absl:Model version: 1715160988\nINFO:absl:Model written to serving path serving_model/penguin-tfdv/1715160988.\nINFO:absl:Model pushed to pipelines/penguin-tfdv/Pusher/pushed_model/6.\nINFO:absl:Cleaning up stateless execution info.\nINFO:absl:Execution 6 succeeded.\nINFO:absl:Cleaning up stateful execution info.\nINFO:absl:Deleted stateful_working_dir pipelines/penguin-tfdv/Pusher/.system/stateful_working_dir/5146eed9-4c4d-4bef-b849-ce87e44956ad\nINFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"pipelines/penguin-tfdv/Pusher/pushed_model/6\"\n, artifact_type: name: \"PushedModel\"\nbase_type: MODEL\n)]}) for execution 6\nINFO:absl:MetadataStore with DB connection initialized\nINFO:absl:Component Pusher is finished.\n\n```\n\nYou should see \"INFO:absl:Component Pusher is finished.\" if the pipeline finished successfully.\n### Examine outputs of the pipeline\nWe have trained the classification model for penguins, and we also have validated the input examples in the ExampleValidator component. We can analyze the output from ExampleValidator as we did with the previous pipeline.\n```\nmetadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n    METADATA_PATH)\n\nwith Metadata(metadata_connection_config) as metadata_handler:\n  ev_output = get_latest_artifacts(metadata_handler, PIPELINE_NAME,\n                                   'ExampleValidator')\n  anomalies_artifacts = ev_output[standard_component_specs.ANOMALIES_KEY]\n\n```\n```\nINFO:absl:MetadataStore with DB connection initialized\n\n```\n\nExampleAnomalies from the ExampleValidator can be visualized as well.\n```\nvisualize_artifacts(anomalies_artifacts)\n\n```\n\nYou should see \"No anomalies found\" for each split of examples. Because we used the same data which was used for the schema generation in this pipeline, no anomaly is expected here. If you run this pipeline repeatedly with new incoming data, ExampleValidator should be able to find any discrepancies between the new data and the existing schema.\nIf any anomalies were found, you may review your data to check to see if any examples do not follow your assumptions. Outputs from other components like StatisticsGen might be useful. However, any anomalies which are found will NOT block further pipeline executions.\n## Next steps\nYou can find more resources on <https://www.tensorflow.org/tfx/tutorials>\nPlease see [Understanding TFX Pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines) to learn more about various concepts in TFX.\n",
  "https://www.tensorflow.org/api_docs/python/tf/io/parse_example": "Parses `Example` protos into a `dict` of tensors.  \n```\ntf.io.parse_example(\n    serialized, features, example_names=None, name=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n\n| \n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n  * [Introduction to Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Example_Colab)\n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n\n  \nParses a number of serialized [`Example`](https://www.tensorflow.org/code/tensorflow/core/example/example.proto) protos given in `serialized`. We refer to `serialized` as a batch with `batch_size` many entries of individual `Example` protos.\n`example_names` may contain descriptive names for the corresponding serialized protos. These may be useful for debugging purposes, but they have no effect on the output. If not `None`, `example_names` must be the same length as `serialized`.\nThis op parses serialized examples into a dictionary mapping keys to `Tensor` `SparseTensor`, and `RaggedTensor` objects. `features` is a Mapping from keys to `VarLenFeature`, `SparseFeature`, `RaggedFeature`, and `FixedLenFeature` objects. Each `VarLenFeature` and `SparseFeature` is mapped to a `SparseTensor`; each `FixedLenFeature` is mapped to a `Tensor`; and each `RaggedFeature` is mapped to a `RaggedTensor`.\nEach `VarLenFeature` maps to a `SparseTensor` of the specified type representing a ragged matrix. Its indices are `[batch, index]` where `batch` identifies the example in `serialized`, and `index` is the value's index in the list of values associated with that feature and example.\nEach `SparseFeature` maps to a `SparseTensor` of the specified type representing a Tensor of `dense_shape` `[batch_size] + SparseFeature.size`. Its `values` come from the feature in the examples with key `value_key`. A `values[i]` comes from a position `k` in the feature of an example at batch entry `batch`. This positional information is recorded in `indices[i]` as `[batch, index_0, index_1, ...]` where `index_j` is the `k-th` value of the feature in the example at with key [`SparseFeature.index_key[j]`](https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature#index_key). In other words, we split the indices (except the first index indicating the batch entry) of a `SparseTensor` by dimension into different features of the `Example`. Due to its complexity a `VarLenFeature` should be preferred over a `SparseFeature` whenever possible.\nEach `FixedLenFeature` `df` maps to a `Tensor` of the specified type (or [`tf.float32`](https://www.tensorflow.org/api_docs/python/tf#float32) if not specified) and shape `(serialized.size(),) + df.shape`.\n`FixedLenFeature` entries with a `default_value` are optional. With no default value, we will fail if that `Feature` is missing from any example in `serialized`.\nEach `FixedLenSequenceFeature` `df` maps to a `Tensor` of the specified type (or [`tf.float32`](https://www.tensorflow.org/api_docs/python/tf#float32) if not specified) and shape `(serialized.size(), None) + df.shape`. All examples in `serialized` will be padded with `default_value` along the second dimension.\nEach `RaggedFeature` maps to a `RaggedTensor` of the specified type. It is formed by stacking the `RaggedTensor` for each example, where the `RaggedTensor` for each individual example is constructed using the tensors specified by `RaggedTensor.values_key` and [`RaggedTensor.partition`](https://www.tensorflow.org/tfx/tf_metadata/api_docs/python/tfmd/proto/schema_pb2/TensorRepresentation/RaggedTensor#partition). See the [`tf.io.RaggedFeature`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature) documentation for details and examples.\n#### Examples:\nFor example, if one expects a [`tf.float32`](https://www.tensorflow.org/api_docs/python/tf#float32) `VarLenFeature` `ft` and three serialized `Example`s are provided:\n```\nserialized = [\n  features\n    { feature { key: \"ft\" value { float_list { value: [1.0, 2.0] } } } },\n  features\n    { feature []},\n  features\n    { feature { key: \"ft\" value { float_list { value: [3.0] } } }\n]\n\n```\n\nthen the output will look like:\n```\n{\"ft\": SparseTensor(indices=[[0, 0], [0, 1], [2, 0]],\n                    values=[1.0, 2.0, 3.0],\n                    dense_shape=(3, 2)) }\n\n```\n\nIf instead a `FixedLenSequenceFeature` with `default_value = -1.0` and `shape=[]` is used then the output will look like:\n```\n{\"ft\": [[1.0, 2.0], [3.0, -1.0]]}\n\n```\n\nGiven two `Example` input protos in `serialized`:\n```\n[\n  features {\n    feature { key: \"kw\" value { bytes_list { value: [ \"knit\", \"big\" ] } } }\n    feature { key: \"gps\" value { float_list { value: [] } } }\n  },\n  features {\n    feature { key: \"kw\" value { bytes_list { value: [ \"emmy\" ] } } }\n    feature { key: \"dank\" value { int64_list { value: [ 42 ] } } }\n    feature { key: \"gps\" value { } }\n  }\n]\n\n```\n\nAnd arguments\n```\nexample_names: [\"input0\", \"input1\"],\nfeatures: {\n    \"kw\": VarLenFeature(tf.string),\n    \"dank\": VarLenFeature(tf.int64),\n    \"gps\": VarLenFeature(tf.float32),\n}\n\n```\n\nThen the output is a dictionary:\n```\n{\n  \"kw\": SparseTensor(\n      indices=[[0, 0], [0, 1], [1, 0]],\n      values=[\"knit\", \"big\", \"emmy\"]\n      dense_shape=[2, 2]),\n  \"dank\": SparseTensor(\n      indices=[[1, 0]],\n      values=[42],\n      dense_shape=[2, 1]),\n  \"gps\": SparseTensor(\n      indices=[],\n      values=[],\n      dense_shape=[2, 0]),\n}\n\n```\n\nFor dense results in two serialized `Example`s:\n```\n[\n  features {\n    feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n   },\n   features {\n    feature { key: \"age\" value { int64_list { value: [] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n  }\n]\n\n```\n\n#### We can use arguments:\n```\nexample_names: [\"input0\", \"input1\"],\nfeatures: {\n    \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n    \"gender\": FixedLenFeature([], dtype=tf.string),\n}\n\n```\n\nAnd the expected output is:\n```\n{\n  \"age\": [[0], [-1]],\n  \"gender\": [[\"f\"], [\"f\"]],\n}\n\n```\n\nAn alternative to `VarLenFeature` to obtain a `SparseTensor` is `SparseFeature`. For example, given two `Example` input protos in `serialized`:\n```\n[\n  features {\n    feature { key: \"val\" value { float_list { value: [ 0.5, -1.0 ] } } }\n    feature { key: \"ix\" value { int64_list { value: [ 3, 20 ] } } }\n  },\n  features {\n    feature { key: \"val\" value { float_list { value: [ 0.0 ] } } }\n    feature { key: \"ix\" value { int64_list { value: [ 42 ] } } }\n  }\n]\n\n```\n\nAnd arguments\n```\nexample_names: [\"input0\", \"input1\"],\nfeatures: {\n    \"sparse\": SparseFeature(\n        index_key=\"ix\", value_key=\"val\", dtype=tf.float32, size=100),\n}\n\n```\n\nThen the output is a dictionary:\n```\n{\n  \"sparse\": SparseTensor(\n      indices=[[0, 3], [0, 20], [1, 42]],\n      values=[0.5, -1.0, 0.0]\n      dense_shape=[2, 100]),\n}\n\n```\n\nSee the [`tf.io.RaggedFeature`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature) documentation for examples showing how `RaggedFeature` can be used to obtain `RaggedTensor`s.\n## Args  \n---  \n`serialized` |  A vector (1-D Tensor) of strings, a batch of binary serialized `Example` protos.   \n`features` |  A mapping of feature keys to `FixedLenFeature`, `VarLenFeature`, `SparseFeature`, and `RaggedFeature` values.   \n`example_names` |  A vector (1-D Tensor) of strings (optional), the names of the serialized protos in the batch.   \n`name` |  A name for this operation (optional).   \n## Returns  \n---  \nA `dict` mapping feature keys to `Tensor`, `SparseTensor`, and `RaggedTensor` values.   \n## Raises  \n---  \n`ValueError` |  if any feature is invalid. \n",
  "https://www.tensorflow.org/tfx": "#  TFX is an end-to-end platform for deploying production ML pipelines \nWhen you're ready to move your models from research to production, use TFX to create and manage a production pipeline.\nGet started by exploring each built-in component of TFX. \n[ View tutorials ](https://www.tensorflow.org/tfx/tutorials)\nLearn how to use TFX with end-to-end examples. \n[ View the guide ](https://www.tensorflow.org/tfx/guide)\nGuides explain the concepts and components of TFX. \n[ Explore addons ](https://www.tensorflow.org/tfx/addons)\nAdditional TFX components contributed by the community. \n###  How it works \nA TFX pipeline is a sequence of components that implement an ML pipeline which is specifically designed for scalable, high-performance machine learning tasks. Components are built using TFX libraries which can also be used individually.\n##  How companies are using TFX \n[ Airbus  ](https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html)\n[ OpenX  ](https://blog.tensorflow.org/2021/02/how-openx-trains-and-serves-for-million-queries-per-second.html)\n##  Solutions to common problems \nExplore step-by-step tutorials to help you with your projects.\nIntermediate \n[ Train and serve a TensorFlow model with TensorFlow Serving ](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple)\nThis guide trains a neural network model to classify images of clothing, like sneakers and shirts, saves the trained model, and then serves it with TensorFlow Serving. The focus is on TensorFlow Serving, rather than the modeling and training in TensorFlow.\nIntermediate \n[ Create TFX pipelines hosted on Google Cloud ](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines)\nAn introduction to TFX and Cloud AI Platform Pipelines to create your own machine learning pipelines on Google Cloud. Follow a typical ML development process, starting by examining the dataset, and ending up with a complete working pipeline.\nIntermediate \n[ Use TFX with TensorFlow Lite for on-device inference ](https://www.tensorflow.org/tfx/tutorials/tfx/tfx_for_mobile)\nLearn how TFX can create and evaluate machine learning models that will be deployed on-device. TFX now provides native support for TFLite, which makes it possible to perform highly efficient inference on mobile devices.\n##  News & announcements \nCheck out our [blog](https://blog.tensorflow.org/search?label=TFX&max-results=20) and and subscribe to our TensorFlow newsletter to get the latest announcements sent directly to your inbox.\n##  Community participation \nSee more ways to participate in the TensorFlow community.\n[ Ask a question on TensorFlow Forum ](https://discuss.tensorflow.org/tag/tfx)\n##  Get started with TFX \n[ Explore tutorials ](https://www.tensorflow.org/tfx/tutorials)\n",
  "https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/Resolver": "Definition for TFX Resolver.\nInherits From: [`BaseNode`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseNode)\n```\ntfx.v1.dsl.Resolver(\n    strategy_class: Optional[Type[ResolverStrategy]] = None,\n    config: Optional[Dict[str, json_utils.JsonableType]] = None,\n    **channels\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Model analysis using TFX Pipeline and TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma)\n\n  \nResolver is a special TFX node which handles special artifact resolution logics that will be used as inputs for downstream nodes.\nTo use Resolver, pass the followings to the Resolver constructor:\n  * Name of the Resolver instance\n  * A subclass of ResolverStrategy\n  * Configs that will be used to construct an instance of ResolverStrategy\n  * Channels to resolve with their tag, in the form of kwargs\n\n\n#### Here is an example:\n```\nexample_gen = ImportExampleGen(...)\nexamples_resolver = Resolver(\n      strategy_class=tfx.dsl.experimental.SpanRangeStrategy,\n      config={'range_config': range_config},\n      examples=Channel(type=Examples, producer_component_id=example_gen.id)\n      ).with_id('Resolver.span_resolver')\ntrainer = Trainer(\n    examples=examples_resolver.outputs['examples'],\n    ...)\n\n```\n\nYou can find experimental `ResolverStrategy` classes under [`tfx.v1.dsl.experimental`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/experimental) module, including `LatestArtifactStrategy`, `LatestBlessedModelStrategy`, `SpanRangeStrategy`, etc.\n## Args  \n---  \n`strategy_class` |  Optional `ResolverStrategy` which contains the artifact resolution logic.   \n`config` |  Optional dict of key to Jsonable type for constructing resolver_strategy.   \n`**channels` |  Input channels to the Resolver node as keyword arguments.   \n## Attributes  \n---  \n`outputs` |  Output Channel dict that contains resolved artifacts.   \n## Methods\n### `with_node_execution_options`\n```\nwith_node_execution_options(\n    node_execution_options: utils.NodeExecutionOptions\n) -> typing_extensions.Self\n\n```\n\n",
  "https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/BigQueryExampleGen": "Cloud BigQueryExampleGen component.  \nInherits From: [`BaseBeamComponent`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseBeamComponent), [`BaseComponent`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseComponent), [`BaseNode`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseNode)\n```\ntfx.v1.extensions.google_cloud_big_query.BigQueryExampleGen(\n    query: Optional[str] = None,\n    input_config: Optional[Union[[tfx.v1.proto.Input](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/proto/Input), [tfx.v1.dsl.experimental.RuntimeParameter](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/experimental/RuntimeParameter)]] = None,\n    output_config: Optional[Union[[tfx.v1.proto.Output](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/proto/Output), [tfx.v1.dsl.experimental.RuntimeParameter](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/experimental/RuntimeParameter)]] = None,\n    range_config: Optional[Union[range_config_pb2.RangeConfig, data_types.RuntimeParameter,\n        ph.Placeholder]] = None,\n    custom_executor_spec: Optional[executor_spec.ExecutorSpec] = None,\n    custom_config: Optional[Union[[tfx.v1.proto.CustomConfig](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/proto/CustomConfig), [tfx.v1.dsl.experimental.RuntimeParameter](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/experimental/RuntimeParameter)]] = None\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Reading data from BigQuery with TFX and Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_bq)\n\n  \nThe BigQuery examplegen component takes a query, and generates train and eval examples for downstream components.\nComponent `outputs` contains:\n  * `examples`: Channel of type [`standard_artifacts.Examples`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/Examples) for output train and eval examples.\n\n\n## Args  \n---  \n`query` |  BigQuery sql string, query result will be treated as a single split, can be overwritten by input_config.   \n`input_config` |  An example_gen_pb2.Input instance with Split.pattern as BigQuery sql string. If set, it overwrites the 'query' arg, and allows different queries per split. If any field is provided as a RuntimeParameter, input_config should be constructed as a dict with the same field names as Input proto message.   \n`output_config` |  An example_gen_pb2.Output instance, providing output configuration. If unset, default splits will be 'train' and 'eval' with size 2:1. If any field is provided as a RuntimeParameter, input_config should be constructed as a dict with the same field names as Output proto message.   \n`range_config` |  An optional range_config_pb2.RangeConfig instance, specifying the range of span values to consider.   \n`custom_executor_spec` |  Optional custom executor spec overriding the default executor spec specified in the component attribute.   \n`custom_config` |  An   \n## Raises  \n---  \n`RuntimeError` |  Only one of query and input_config should be set.   \n## Attributes  \n---  \n`outputs` |  Component's output channel dict.   \n## Methods\n### `with_beam_pipeline_args`\n```\nwith_beam_pipeline_args(\n    beam_pipeline_args: Iterable[Union[str, placeholder.Placeholder]]\n) -> 'BaseBeamComponent'\n\n```\n\nAdd per component Beam pipeline args.\nArgs  \n---  \n`beam_pipeline_args` |  List of Beam pipeline args to be added to the Beam executor spec.   \nReturns  \n---  \nthe same component itself.   \n### `with_node_execution_options`\n```\nwith_node_execution_options(\n    node_execution_options: utils.NodeExecutionOptions\n) -> typing_extensions.Self\n\n```\n\n## Class Variables  \n---  \nPOST_EXECUTABLE_SPEC  |  `None`  \nPRE_EXECUTABLE_SPEC  |  `None`\n",
  "https://www.tensorflow.org/guide/saved_model": "[View on TensorFlow.org](https://www.tensorflow.org/guide/saved_model)  \n---  \nA SavedModel contains a complete TensorFlow program, including trained parameters (i.e, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s) and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying with [TFLite](https://tensorflow.org/lite), [TensorFlow.js](https://js.tensorflow.org/), [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/tutorials/Serving_REST_simple), or [TensorFlow Hub](https://tensorflow.org/hub).\nYou can save and load a model in the SavedModel format using the following APIs:\n  * Low-level [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model) API. This document describes how to use this API in detail. \n    * Save: [`tf.saved_model.save(model, path_to_dir)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)\n    * Load: `model = tf.saved_model.load(path_to_dir)`\n  * High-level [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) API. Refer to [the keras save and serialize guide](https://www.tensorflow.org/guide/keras/save_and_serialize).\n  * If you just want to save/load weights during training, refer to [the checkpoints guide](https://www.tensorflow.org/guide/checkpoint).\n\n\n## Creating a SavedModel from Keras\nFor a quick introduction, this section exports a pre-trained Keras model and serves image classification requests with it. The rest of the guide will fill in details and discuss other ways to create SavedModels.\n```\nimportos\nimporttempfile\n\nfrommatplotlibimport pyplot as plt\nimportnumpyasnp\nimporttensorflowastf\n\ntmpdir = tempfile.mkdtemp()\n\n```\n```\nphysical_devices = tf.config.list_physical_devices('GPU')\nfor device in physical_devices:\n  tf.config.experimental.set_memory_growth(device, True)\n\n```\n```\nfile=tf.keras.utils.get_file(\n\"grace_hopper.jpg\",\n\"https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg\")\nimg=tf.keras.utils.load_img(file,target_size=[224,224])\nplt.imshow(img)\nplt.axis('off')\nx=tf.keras.utils.img_to_array(img)\nx=tf.keras.applications.mobilenet.preprocess_input(\nx[tf.newaxis,...])\n\n```\n\nYou'll use an image of Grace Hopper as a running example, and a Keras pre-trained image classification model since it's easy to use. Custom models work too, and are covered in detail later.\n```\nlabels_path=tf.keras.utils.get_file(\n'ImageNetLabels.txt',\n'https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')\nimagenet_labels=np.array(open(labels_path).read().splitlines())\n\n```\n```\npretrained_model = tf.keras.applications.MobileNet()\nresult_before_save = pretrained_model(x)\n\ndecoded = imagenet_labels[np.argsort(result_before_save)[0,::-1][:5]+1]\n\nprint(\"Result before saving:\\n\", decoded)\n\n```\n\nThe top prediction for this image is \"military uniform\".\n```\nmobilenet_save_path = os.path.join(tmpdir, \"mobilenet/1/\")\ntf.saved_model.save(pretrained_model, mobilenet_save_path)\n\n```\n\nThe save-path follows a convention used by TensorFlow Serving where the last path component (`1/` here) is a version number for your model - it allows tools like Tensorflow Serving to reason about the relative freshness.\nYou can load the SavedModel back into Python with [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) and see how Admiral Hopper's image is classified.\n```\nloaded=tf.saved_model.load(mobilenet_save_path)\nprint(list(loaded.signatures.keys()))# [\"serving_default\"]\n\n```\n\nImported signatures always return dictionaries. To customize signature names and output dictionary keys, see [Specifying signatures during export](https://www.tensorflow.org/guide/saved_model#specifying_signatures_during_export).\n```\ninfer=loaded.signatures[\"serving_default\"]\nprint(infer.structured_outputs)\n\n```\n\nRunning inference from the SavedModel gives the same result as the original model.\n```\nlabeling=infer(tf.constant(x))[pretrained_model.output_names[0]]\n\ndecoded=imagenet_labels[np.argsort(labeling)[0,::-1][:5]+1]\n\nprint(\"Result after saving and loading:\\n\",decoded)\n\n```\n\n## Running a SavedModel in TensorFlow Serving\nSavedModels are usable from Python (more on that below), but production environments typically use a dedicated service for inference without running Python code. This is easy to set up from a SavedModel using TensorFlow Serving.\nSee the [TensorFlow Serving REST tutorial](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple) for an end-to-end tensorflow-serving example.\n## The SavedModel format on disk\nA SavedModel is a directory containing serialized signatures and the state needed to run them, including variable values and vocabularies.\n```\nls{mobilenet_save_path}\n```\n\nThe `saved_model.pb` file stores the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs.\nSavedModels may contain multiple variants of the model (multiple `v1.MetaGraphDefs`, identified with the `--tag_set` flag to `saved_model_cli`), but this is rare. APIs which create multiple variants of a model include [`tf.Estimator.experimental_export_all_saved_models`](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#experimental_export_all_saved_models) and in TensorFlow 1.x `tf.saved_model.Builder`.\n```\nsaved_model_cli{mobilenet_save_path}\n```\n\nThe `variables` directory contains a standard training checkpoint (see the [guide to training checkpoints](https://www.tensorflow.org/guide/checkpoint)).\n```\nls{mobilenet_save_path}/variables\n```\n\nThe `assets` directory contains files used by the TensorFlow graph, for example text files used to initialize vocabulary tables. It is unused in this example.\nSavedModels may have an `assets.extra` directory for any files not used by the TensorFlow graph, for example information for consumers about what to do with the SavedModel. TensorFlow itself does not use this directory.\nThe `fingerprint.pb` file contains the [`tf.saved_model.experimental.read_fingerprint`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/read_fingerprint) can be used to read the SavedModel fingerprint into a [`tf.saved_model.experimental.Fingerprint`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/Fingerprint) object.\n## Saving a custom model\n[`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) supports saving [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) objects and its subclasses, like [`tf.keras.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer) and [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\nLet's look at an example of saving and restoring a [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module).\n```\nclassCustomModule(tf.Module):\n\ndef__init__(self):\nsuper(CustomModule,self).__init__()\nself.v=tf.Variable(1.)\n\n@tf.function\ndef__call__(self,x):\nprint('Tracing with',x)\nreturnx*self.v\n\n@tf.function(input_signature=[tf.TensorSpec([],tf.float32)])\ndefmutate(self,new_v):\nself.v.assign(new_v)\n\nmodule=CustomModule()\n\n```\n\nWhen you save a [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module), any [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) attributes, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)-decorated methods, and [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module)s found via recursive traversal are saved. (See the [Checkpoint tutorial](https://www.tensorflow.org/guide/checkpoint) for more about this recursive traversal.) However, any Python attributes, functions, and data are lost. This means that when a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is saved, no Python code is saved.\nIf no Python code is saved, how does SavedModel know how to restore the function?\nBriefly, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) works by tracing the Python code to generate a ConcreteFunction (a callable wrapper around [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)). When saving a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), you're really saving the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)'s cache of ConcreteFunctions.\nTo learn more about the relationship between [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) and ConcreteFunctions, refer to the [tf.function guide](https://www.tensorflow.org/guide/function).\n```\nmodule_no_signatures_path=os.path.join(tmpdir,'module_no_signatures')\nmodule(tf.constant(0.))\nprint('Saving model...')\ntf.saved_model.save(module,module_no_signatures_path)\n\n```\n\n## Loading and using a custom model\nWhen you load a SavedModel in Python, all [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) attributes, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)-decorated methods, and [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module)s are restored in the same object structure as the original saved [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module).\n```\nimported=tf.saved_model.load(module_no_signatures_path)\nassertimported(tf.constant(3.)).numpy()==3\nimported.mutate(tf.constant(2.))\nassertimported(tf.constant(3.)).numpy()==6\n\n```\n\nBecause no Python code is saved, calling a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) with a new input signature will fail:\n```\nimported(tf.constant([3.]))\n\n```\n```\nValueError: Could not find matching function to call for canonicalized inputs ((,), {}). Only existing signatures are [((TensorSpec(shape=(), dtype=tf.float32, name=u'x'),), {})].\n\n```\n\n### Basic fine-tuning\nVariable objects are available, and you can backprop through imported functions. That is enough to fine-tune (i.e. retrain) a SavedModel in simple cases.\n```\noptimizer=tf.keras.optimizers.SGD(0.05)\n\ndeftrain_step():\nwithtf.GradientTape()astape:\nloss=(10.-imported(tf.constant(2.)))**2\nvariables=tape.watched_variables()\ngrads=tape.gradient(loss,variables)\noptimizer.apply_gradients(zip(grads,variables))\nreturnloss\n\n```\n```\nfor _ in range(10):\n  # \"v\" approaches 5, \"loss\" approaches 0\n  print(\"loss={:.2f} v={:.2f}\".format(train_step(), imported.v.numpy()))\n\n```\n\n### General fine-tuning\nA SavedModel from Keras provides `__call__` to address more advanced cases of fine-tuning. TensorFlow Hub recommends to provide the following of those, if applicable, in SavedModels shared for the purpose of fine-tuning:\n  * If the model uses dropout or another technique in which the forward pass differs between training and inference (like batch normalization), the `__call__` method takes an optional, Python-valued `training=` argument that defaults to `False` but can be set to `True`.\n  * Next to the `__call__` attribute, there are `.variable` and `.trainable_variable` attributes with the corresponding lists of variables. A variable that was originally trainable but is meant to be frozen during fine-tuning is omitted from `.trainable_variables`.\n  * For the sake of frameworks like Keras that represent weight regularizers as attributes of layers or sub-models, there can also be a `.regularization_losses` attribute. It holds a list of zero-argument functions whose values are meant for addition to the total loss.\n\n\nGoing back to the initial MobileNet example, you can see some of those in action:\n```\nloaded=tf.saved_model.load(mobilenet_save_path)\nprint(\"MobileNet has {} trainable variables: {}, ...\".format(\nlen(loaded.trainable_variables),\n\", \".join([v.nameforvinloaded.trainable_variables[:5]])))\n\n```\n```\ntrainable_variable_ids={id(v)forvinloaded.trainable_variables}\nnon_trainable_variables=[vforvinloaded.variables\nifid(v)notintrainable_variable_ids]\nprint(\"MobileNet also has {} non-trainable variables: {}, ...\".format(\nlen(non_trainable_variables),\n\", \".join([v.nameforvinnon_trainable_variables[:3]])))\n\n```\n\n## Specifying signatures during export\nTools like TensorFlow Serving and `saved_model_cli` can interact with SavedModels. To help these tools determine which ConcreteFunctions to use, you need to specify serving signatures. [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)s automatically specify serving signatures, but you'll have to explicitly declare a serving signature for our custom modules.\nBy default, no signatures are declared in a custom [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module).\n```\nassert len(imported.signatures) == 0\n\n```\n\nTo declare a serving signature, specify a ConcreteFunction using the `signatures` kwarg. When specifying a single signature, its signature key will be `'serving_default'`, which is saved as the constant `tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY`.\n```\nmodule_with_signature_path = os.path.join(tmpdir, 'module_with_signature')\ncall = module.__call__.get_concrete_function(tf.TensorSpec(None, tf.float32))\ntf.saved_model.save(module, module_with_signature_path, signatures=call)\n\n```\n```\nimported_with_signatures=tf.saved_model.load(module_with_signature_path)\nlist(imported_with_signatures.signatures.keys())\n\n```\n\nTo export multiple signatures, pass a dictionary of signature keys to ConcreteFunctions. Each signature key corresponds to one ConcreteFunction.\n```\nmodule_multiple_signatures_path=os.path.join(tmpdir,'module_with_multiple_signatures')\nsignatures={\"serving_default\":call,\n\"array_input\":module.__call__.get_concrete_function(tf.TensorSpec([None],tf.float32))}\n\ntf.saved_model.save(module,module_multiple_signatures_path,signatures=signatures)\n\n```\n```\nimported_with_multiple_signatures=tf.saved_model.load(module_multiple_signatures_path)\nlist(imported_with_multiple_signatures.signatures.keys())\n\n```\n\nBy default, the output tensor names are fairly generic, like `output_0`. To control the names of outputs, modify your [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to return a dictionary that maps output names to outputs. The names of inputs are derived from the Python function arg names.\n```\nclassCustomModuleWithOutputName(tf.Module):\ndef__init__(self):\nsuper(CustomModuleWithOutputName,self).__init__()\nself.v=tf.Variable(1.)\n\n@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\ndef__call__(self,x):\nreturn{'custom_output_name':x*self.v}\n\nmodule_output=CustomModuleWithOutputName()\ncall_output=module_output.__call__.get_concrete_function(tf.TensorSpec(None,tf.float32))\nmodule_output_path=os.path.join(tmpdir,'module_with_output_name')\ntf.saved_model.save(module_output,module_output_path,\nsignatures={'serving_default':call_output})\n\n```\n```\nimported_with_output_name=tf.saved_model.load(module_output_path)\nimported_with_output_name.signatures['serving_default'].structured_outputs\n\n```\n\n## Proto-splitting\nDue to limits of the protobuf implementation, proto sizes cannot exceed 2GB. This can lead to the following errors when attempting to save very large models:\n```\nValueError:Messagetensorflow.SavedModelexceedsmaximumprotobufsizeof2GB:...\n\n```\n```\ngoogle.protobuf.message.DecodeError:Errorparsingmessageasthemessageexceededtheprotobuflimitwithtype'tensorflow.GraphDef'\n\n```\n\nIf you wish to save models that exceed the 2GB limit, then you'll need to save using the new proto-splitting option:\n```\ntf.saved_model.save(\n  ...,\n  options=tf.saved_model.SaveOptions(experimental_image_format=True)\n)\n\n```\n\nMore information can be found in the \n## Load a SavedModel in C++\nThe C++ version of the SavedModel \n```\nconststringexport_dir=...\nSavedModelBundlebundle;\n...\nLoadSavedModel(session_options,run_options,export_dir,{kSavedModelTagTrain},\nbundle);\n\n```\n\nDetails of the SavedModel command line interface You can use the SavedModel Command Line Interface (CLI) to inspect and execute a SavedModel. For example, you can use the CLI to inspect the model's `SignatureDef`s. The CLI enables you to quickly confirm that the input Tensor dtype and shape match the model. Moreover, if you want to test your model, you can use the CLI to do a sanity check by passing in sample inputs in various formats (for example, Python expressions) and then fetching the output. Install the SavedModel CLI Broadly speaking, you can install TensorFlow in either of the following two ways:\n  * By installing a pre-built TensorFlow binary.\n  * By building TensorFlow from source code.\n\nIf you installed TensorFlow through a pre-built TensorFlow binary, then the SavedModel CLI is already installed on your system at pathname `bin/saved_model_cli`. If you built TensorFlow from source code, you must run the following additional command to build `saved_model_cli`: Overview of commands The SavedModel CLI supports the following two commands on a SavedModel:\n  * `show`, which shows the computations available from a SavedModel.\n  * `run`, which runs a computation from a SavedModel.\n\n`show` command\nA SavedModel contains one or more model variants (technically, [`v1.MetaGraphDef`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef)s), identified by their tag-sets. To serve a model, you might wonder what kind of `SignatureDef`s are in each model variant, and what are their inputs and outputs. The `show` command let you examine the contents of the SavedModel in hierarchical order. Here's the syntax:\n```\nusage:saved_model_clishow[-h]--dirDIR[--all]\n[--tag_setTAG_SET][--signature_defSIGNATURE_DEF_KEY]\n\n```\n\nFor example, the following command shows all available tag-sets in the SavedModel:\nThe following command shows all available `SignatureDef` keys for a tag set:\n```\n$saved_model_clishow--dir/tmp/saved_model_dir--tag_setserve\nThegivenSavedModel`MetaGraphDef`contains`SignatureDefs`withthe\nfollowingkeys:\nSignatureDefkey:\"classify_x2_to_y3\"\nSignatureDefkey:\"classify_x_to_y\"\nSignatureDefkey:\"regress_x2_to_y3\"\nSignatureDefkey:\"regress_x_to_y\"\nSignatureDefkey:\"regress_x_to_y2\"\nSignatureDefkey:\"serving_default\"\n\n```\n\nIf there are _multiple_ tags in the tag-set, you must specify all tags, each tag separated by a comma. For example:\n```\n$ saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve,gpu\n\n```\n\nTo show all inputs and outputs TensorInfo for a specific `SignatureDef`, pass in the `SignatureDef` key to `signature_def` option. This is very useful when you want to know the tensor key value, dtype and shape of the input tensors for executing the computation graph later. For example:\n```\n$\\\n/tmp/saved_model_dir(s):\n['x'](-1,1)\n(s):\n['y'](-1,1)\n\n```\n\nTo show all available information in the SavedModel, use the `--all` option. For example:\n```\n$ saved_model_cli show --dir /tmp/saved_model_dir --all\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n\nsignature_def['classify_x2_to_y3']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['inputs'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x2:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['scores'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y3:0\n  Method name is: tensorflow/serving/classify\n\n...\n\nsignature_def['serving_default']:\n  The given SavedModel SignatureDef contains the following input(s):\n    inputs['x'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: x:0\n  The given SavedModel SignatureDef contains the following output(s):\n    outputs['y'] tensor_info:\n        dtype: DT_FLOAT\n        shape: (-1, 1)\n        name: y:0\n  Method name is: tensorflow/serving/predict\n\n```\n\n###  `run` command\nInvoke the `run` command to run a graph computation, passing inputs and then displaying (and optionally saving) the outputs. Here's the syntax:\n```\nusage:saved_model_clirun[-h]--dirDIR--tag_setTAG_SET--signature_def\nSIGNATURE_DEF_KEY[--inputsINPUTS]\n[--input_exprsINPUT_EXPRS]\n[--input_examplesINPUT_EXAMPLES][--outdirOUTDIR]\n[--overwrite][--tf_debug]\n\n```\n\nThe `run` command provides the following three ways to pass inputs to the model:\n  * `--inputs` option enables you to pass numpy ndarray in files.\n  * `--input_exprs` option enables you to pass Python expressions.\n  * `--input_examples` option enables you to pass `tf.train.Example`.\n\n\n#### `--inputs`\nTo pass input data in files, specify the `--inputs` option, which takes the following general format:\n```\n--inputs\n```\n\nwhere _INPUTS_ is either of the following formats:\n  * `<input_key>=<filename>`\n  * `<input_key>=<filename>[<variable_name>]`\n\n\nYou may pass multiple _INPUTS_. If you do pass multiple inputs, use a semicolon to separate each of the _INPUTS_.\n`saved_model_cli` uses `numpy.load` to load the _filename_. The _filename_ may be in any of the following formats:\n  * `.npy`\n  * `.npz`\n  * pickle format\n\n\nA `.npy` file always contains a numpy ndarray. Therefore, when loading from a `.npy` file, the content will be directly assigned to the specified input tensor. If you specify a _variable_name_ with that `.npy` file, the _variable_name_ will be ignored and a warning will be issued.\nWhen loading from a `.npz` (zip) file, you may optionally specify a _variable_name_ to identify the variable within the zip file to load for the input tensor key. If you don't specify a _variable_name_ , the SavedModel CLI will check that only one file is included in the zip file and load it for the specified input tensor key.\nWhen loading from a pickle file, if no `variable_name` is specified in the square brackets, whatever that is inside the pickle file will be passed to the specified input tensor key. Otherwise, the SavedModel CLI will assume a dictionary is stored in the pickle file and the value corresponding to the _variable_name_ will be used.\n#### `--input_exprs`\nTo pass inputs through Python expressions, specify the `--input_exprs` option. This can be useful for when you don't have data files lying around, but still want to sanity check the model with some simple inputs that match the dtype and shape of the model's `SignatureDef`s. For example:\n```\n`<input_key>=[[1],[2],[3]]`\n\n```\n\nIn addition to Python expressions, you may also pass numpy functions. For example:\n```\n`<input_key>=np.ones((32,32,3))`\n\n```\n\n(Note that the `numpy` module is already available to you as `np`.)\n#### `--input_examples`\nTo pass [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) as inputs, specify the `--input_examples` option. For each input key, it takes a list of dictionary, where each dictionary is an instance of [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example). The dictionary keys are the features and the values are the value lists for each feature. For example:\n```\n`<input_key>=[{\"age\":[22,24],\"education\":[\"BS\",\"MS\"]}]`\n\n```\n\n#### Save output\nBy default, the SavedModel CLI writes output to stdout. If a directory is passed to `--outdir` option, the outputs will be saved as `.npy` files named after output tensor keys under the given directory.\nUse `--overwrite` to overwrite existing output files.\n",
  "https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable": "A generic hash table that is immutable once initialized.\nInherits From: [`TrackableResource`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/TrackableResource)\n```\ntf.lookup.StaticHashTable(\n    initializer, default_value, name=None, experimental_is_anonymous=False\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Federated Learning for Text Generation](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation)\n  * [Client-efficient large-model federated learning via `federated_select` and sparse aggregation](https://www.tensorflow.org/federated/tutorials/sparse_federated_learning)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n\n  \n#### Example usage:\n```\nkeys_tensor = tf.constant(['a', 'b', 'c'])\nvals_tensor = tf.constant([7, 8, 9])\ninput_tensor = tf.constant(['a', 'f'])\ntable = tf.lookup.StaticHashTable(\n    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n    default_value=-1)\ntable.lookup(input_tensor).numpy()\narray([ 7, -1], dtype=int32)\n```\n\nOr for more pythonic code:\n```\ntable[input_tensor].numpy()\narray([ 7, -1], dtype=int32)\n```\n\nThe result of a lookup operation has the same shape as the argument:\n```\ninput_tensor = tf.constant([['a', 'b'], ['c', 'd']])\ntable[input_tensor].numpy()\narray([[ 7,  8],\n       [ 9, -1]], dtype=int32)\n```\n\n## Args  \n---  \n`initializer` |  The table initializer to use. See `HashTable` kernel for supported key and value types.   \n`default_value` |  The value to use if a key is missing in the table.   \n`name` |  A name for the operation (optional).   \n`experimental_is_anonymous` |  Whether to use anonymous mode for the table (default is False). In anonymous mode, the table resource can only be accessed via a resource handle. It can't be looked up by a name. When all resource handles pointing to that resource are gone, the resource will be deleted automatically.   \n## Attributes  \n---  \n`default_value` |  The default value of the table.   \n`key_dtype` |  The table key dtype.   \n`name` |  The name of the table.   \n`resource_handle` |  Returns the resource handle associated with this Resource.   \n`value_dtype` |  The table value dtype.   \n## Methods\n### `export`\n```\nexport(\n    name=None\n)\n\n```\n\nReturns tensors of all keys and values in the table.\nArgs  \n---  \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA pair of tensors with the first tensor containing all keys and the second tensors containing all values in the table.   \n### `lookup`\n```\nlookup(\n    keys, name=None\n)\n\n```\n\nLooks up `keys` in a table, outputs the corresponding values.\nThe `default_value` is used for keys not present in the table.\nArgs  \n---  \n`keys` |  Keys to look up. May be either a `SparseTensor` or dense `Tensor`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `SparseTensor` if keys are sparse, a `RaggedTensor` if keys are ragged, otherwise a dense `Tensor`.   \nRaises  \n---  \n`TypeError` |  when `keys` or `default_value` doesn't match the table data types.   \n### `size`\n```\nsize(\n    name=None\n)\n\n```\n\nCompute the number of elements in this table.\nArgs  \n---  \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA scalar tensor containing the number of elements in this table.   \n### `__getitem__`\n```\n__getitem__(\n    keys\n)\n\n```\n\nLooks up `keys` in a table, outputs the corresponding values.\n",
  "https://www.tensorflow.org/tensorboard/get_started": "[View on TensorFlow.org](https://www.tensorflow.org/tensorboard/get_started)  \n---  \nIn machine learning, to improve something you often need to be able to measure it. TensorBoard is a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\nThis quickstart will show how to quickly get started with TensorBoard. The remaining guides in this website provide more details on specific capabilities, many of which are not included here. \n```\n# Load the TensorBoard notebook extension\n%load_exttensorboard\n\n```\n```\nimporttensorflowastf\nimportdatetime\n\n```\n```\n# Clear any logs from previous runs\nrm\n```\n\nUsing the \n```\nmnist=tf.keras.datasets.mnist\n\n(x_train,y_train),(x_test,y_test)=mnist.load_data()\nx_train,x_test=x_train/255.0,x_test/255.0\n\ndefcreate_model():\nreturntf.keras.models.Sequential([\ntf.keras.layers.Input(shape=(28,28),name='layers_input'),\ntf.keras.layers.Flatten(name='layers_flatten'),\ntf.keras.layers.Dense(512,activation='relu',name='layers_dense'),\ntf.keras.layers.Dropout(0.2,name='layers_dropout'),\ntf.keras.layers.Dense(10,activation='softmax',name='layers_dense_2')\n])\n\n```\n```\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n11493376/11490434 [==============================] - 0s 0us/step\n\n```\n\n## Using TensorBoard with Keras Model.fit()\nWhen training with Keras's [Model.fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit), adding the [`tf.keras.callbacks.TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard) callback ensures that logs are created and stored. Additionally, enable histogram computation every epoch with `histogram_freq=1` (this is off by default)\nPlace the logs in a timestamped subdirectory to allow easy selection of different training runs.\n```\nmodel=create_model()\nmodel.compile(optimizer='adam',\nloss='sparse_categorical_crossentropy',\nmetrics=['accuracy'])\n\nlog_dir=\"logs/fit/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback=tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n\nmodel.fit(x=x_train,y=y_train,epochs=5,validation_data=(x_test,y_test),callbacks=[tensorboard_callback])\n\n```\n```\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/5\n60000/60000 [==============================] - 15s 246us/sample - loss: 0.2217 - accuracy: 0.9343 - val_loss: 0.1019 - val_accuracy: 0.9685\nEpoch 2/5\n60000/60000 [==============================] - 14s 229us/sample - loss: 0.0975 - accuracy: 0.9698 - val_loss: 0.0787 - val_accuracy: 0.9758\nEpoch 3/5\n60000/60000 [==============================] - 14s 231us/sample - loss: 0.0718 - accuracy: 0.9771 - val_loss: 0.0698 - val_accuracy: 0.9781\nEpoch 4/5\n60000/60000 [==============================] - 14s 227us/sample - loss: 0.0540 - accuracy: 0.9820 - val_loss: 0.0685 - val_accuracy: 0.9795\nEpoch 5/5\n60000/60000 [==============================] - 14s 228us/sample - loss: 0.0433 - accuracy: 0.9862 - val_loss: 0.0623 - val_accuracy: 0.9823\n<tensorflow.python.keras.callbacks.History at 0x7fc8a5ee02e8>\n\n```\n\nStart TensorBoard through the command line or within a notebook experience. The two interfaces are generally the same. In notebooks, use the `%tensorboard` line magic. On the command line, run the same command without \"%\".\n```\n%tensorboard--logdirlogs/fit\n\n```\n\nA brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n  * **Scalars** show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the **Time Series** or **Scalars** dashboards.\n  * **Graphs** help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the **Graphs** dashboard.\n  * **Histograms** and **Distributions** show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the **Time Series** or **Histograms** dashboards. Distributions can be found in the **Distributions** dashboard.\n\n\nAdditional TensorBoard dashboards are automatically enabled when you log other types of data. For example, the Keras TensorBoard callback lets you log images and embeddings as well. You can see what other dashboards are available in TensorBoard by clicking on the \"inactive\" dropdown towards the top right.\n## Using TensorBoard with other methods\nWhen training with methods such as [`tf.GradientTape()`](https://www.tensorflow.org/api_docs/python/tf/GradientTape), use [`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) to log the required information.\nUse the same dataset as above, but convert it to [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to take advantage of batching capabilities:\n```\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\ntrain_dataset = train_dataset.shuffle(60000).batch(64)\ntest_dataset = test_dataset.batch(64)\n\n```\n\nThe training code follows the [advanced quickstart](https://www.tensorflow.org/tutorials/quickstart/advanced) tutorial, but shows how to log metrics to TensorBoard. Choose loss and optimizer:\n```\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam()\n\n```\n\nCreate stateful metrics that can be used to accumulate values during training and logged at any point:\n```\n#Defineourmetrics\ntrain_loss=tf.keras.metrics.Mean('train_loss',dtype=tf.float32)\ntrain_accuracy=tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\ntest_loss=tf.keras.metrics.Mean('test_loss',dtype=tf.float32)\ntest_accuracy=tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')\n\n```\n\nDefine the training and test functions:\n```\ndeftrain_step(model,optimizer,x_train,y_train):\nwithtf.GradientTape()astape:\npredictions=model(x_train,training=True)\nloss=loss_object(y_train,predictions)\ngrads=tape.gradient(loss,model.trainable_variables)\noptimizer.apply_gradients(zip(grads,model.trainable_variables))\n\ntrain_loss(loss)\ntrain_accuracy(y_train,predictions)\n\ndeftest_step(model,x_test,y_test):\npredictions=model(x_test)\nloss=loss_object(y_test,predictions)\n\ntest_loss(loss)\ntest_accuracy(y_test,predictions)\n\n```\n\nSet up summary writers to write the summaries to disk in a different logs directory:\n```\ncurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = 'logs/gradient_tape/' + current_time + '/train'\ntest_log_dir = 'logs/gradient_tape/' + current_time + '/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\ntest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n\n```\n\nStart training. Use [`tf.summary.scalar()`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar) to log metrics (loss and accuracy) during training/testing within the scope of the summary writers to write the summaries to disk. You have control over which metrics to log and how often to do it. Other [`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) functions enable logging other types of data.\n```\nmodel = create_model() # reset our model\n\nEPOCHS = 5\n\nfor epoch in range(EPOCHS):\n  for (x_train, y_train) in train_dataset:\n    train_step(model, optimizer, x_train, y_train)\n  with train_summary_writer.as_default():\n    tf.summary.scalar('loss', train_loss.result(), step=epoch)\n    tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n\n  for (x_test, y_test) in test_dataset:\n    test_step(model, x_test, y_test)\n  with test_summary_writer.as_default():\n    tf.summary.scalar('loss', test_loss.result(), step=epoch)\n    tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n  print (template.format(epoch+1,\n                         train_loss.result(), \n                         train_accuracy.result()*100,\n                         test_loss.result(), \n                         test_accuracy.result()*100))\n\n  # Reset metrics every epoch\n  train_loss.reset_state()\n  test_loss.reset_state()\n  train_accuracy.reset_state()\n  test_accuracy.reset_state()\n\n```\n```\nEpoch 1, Loss: 0.24321186542510986, Accuracy: 92.84333801269531, Test Loss: 0.13006582856178284, Test Accuracy: 95.9000015258789\nEpoch 2, Loss: 0.10446818172931671, Accuracy: 96.84833526611328, Test Loss: 0.08867532759904861, Test Accuracy: 97.1199951171875\nEpoch 3, Loss: 0.07096975296735764, Accuracy: 97.80166625976562, Test Loss: 0.07875105738639832, Test Accuracy: 97.48999786376953\nEpoch 4, Loss: 0.05380449816584587, Accuracy: 98.34166717529297, Test Loss: 0.07712937891483307, Test Accuracy: 97.56999969482422\nEpoch 5, Loss: 0.041443776339292526, Accuracy: 98.71833038330078, Test Loss: 0.07514958828687668, Test Accuracy: 97.5\n\n```\n\nOpen TensorBoard again, this time pointing it at the new log directory. We could have also started TensorBoard to monitor training while it progresses.\n```\n%tensorboard--logdirlogs/gradient_tape\n\n```\n\nThat's it! You have now seen how to use TensorBoard both through the Keras callback and through [`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary) for more custom scenarios. \n",
  "https://www.tensorflow.org/lite": "LiteRT (short for Lite Runtime), formerly known as TensorFlow Lite, is Google's high-performance runtime for on-device AI. You can find ready-to-run LiteRT models for a wide range of ML/AI tasks, or convert and run TensorFlow, PyTorch, and JAX models to the TFLite format using the AI Edge conversion and optimization tools.\n## Key features\n  * **Optimized for on-device machine learning** : LiteRT addresses five key ODML constraints: latency (there's no round-trip to a server), privacy (no personal data leaves the device), connectivity (internet connectivity is not required), size (reduced model and binary size) and power consumption (efficient inference and a lack of network connections).\n  * **Multi-platform support** : Compatible with [Android](https://ai.google.dev/edge/litert/android) and [iOS](https://ai.google.dev/edge/litert/ios/quickstart) devices, [embedded Linux](https://ai.google.dev/edge/litert/microcontrollers/python), and [microcontrollers](https://ai.google.dev/edge/litert/microcontrollers/overview).\n  * **Multi-framework model options** : AI Edge provides tools to convert models from TensorFlow, PyTorch, and JAX models into the FlatBuffers format (`.tflite`), enabling you to use a wide range of state-of-the-art models on LiteRT. You also have access to model optimization tools that can handle quantization and metadata.\n  * **Diverse language support** : Includes SDKs for Java/Kotlin, Swift, Objective-C, C++, and Python.\n  * **High performance** : [Hardware acceleration](https://ai.google.dev/edge/litert/performance/delegates) through specialized delegates like GPU and iOS Core ML.\n\n\n## Development workflow\nThe LiteRT development workflow involves identifying an ML/AI problem, choosing a model that solves that problem, and implementing the model on-device. The following steps walk you through the workflow and provides links to further instructions.\n### 1. Identify the most suitable solution to the ML problem\nLiteRT offers users a high level of flexibility and customizability when it comes to solving machine learning problems, making it a good fit for users who require a specific model or a specialized implementation. Users looking for plug-and-play solutions may prefer [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks), which provides ready-made solutions for common machine learning tasks like object detection, text classification, and LLM inference.\nChoose one of the following AI Edge frameworks:\n  * **LiteRT** : Flexible and customizable runtime that can run a wide range of models. Choose a model for your use case, convert it to the LiteRT format (if necessary), and run it on-device. If you intend to use LiteRT, keep reading.\n  * **MediaPipe Tasks** : Plug-and-play solutions with default models that allow for customization. Choose the task that solves your AI/ML problem, and implement it on multiple platforms. If you intend to use MediaPipe Tasks, refer to the [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks) documentation.\n\n\n### 2. Choose a model\nA LiteRT model is represented in an efficient portable format known as `.tflite` file extension.\nYou can use a LiteRT model in the following ways:\n  * **Use an existing LiteRT model:** The simplest approach is to use a LiteRT model already in the `.tflite` format. These models do not require any added conversion steps. You can find LiteRT models on \n  * **Convert a model into a LiteRT model:** You can use the [TensorFlow Converter](https://ai.google.dev/edge/litert/models/convert_tf), [PyTorch Converter](https://ai.google.dev/edge/litert/models/convert_pytorch), or [JAX converter](https://ai.google.dev/edge/litert/models/convert_jax) to convert models to the FlatBuffers format (`.tflite`) and run them in LiteRT. To get started, you can find models on the following sites:\n    * **TensorFlow models** on \n    * **PyTorch models** on \n    * **JAX models** on \n\n\nA LiteRT model can optionally include _metadata_ that contains human-readable model descriptions and machine-readable data for automatic generation of pre- and post-processing pipelines during on-device inference. Refer to [Add metadata](https://ai.google.dev/edge/litert/models/metadata) for more details.\n### 3. Integrate the model into your app\nYou can implement your LiteRT models to run inferences completely on-device on web, embedded, and mobile devices. LiteRT contains APIs for [Python](https://ai.google.dev/edge/api/tflite/python/tf/lite), [Java and Kotlin](https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/package-summary) for Android, [Swift](https://ai.google.dev/edge/api/tflite/swift/Classes) for iOS, and [C++](https://ai.google.dev/edge/api/tflite/cc) for micro-devices.\nUse the following guides to implement a LiteRT model on your preferred platform:\n  * [Run on Android](https://ai.google.dev/edge/litert/android/index): Run models on Android devices using the Java/Kotlin APIs.\n  * [Run on iOS](https://ai.google.dev/edge/litert/ios/quickstart): Run models on iOS devices using the Swift APIs.\n  * [Run on Micro](https://ai.google.dev/edge/litert/microcontrollers/overview): Run models on embedded devices using the C++ APIs.\n\n\nOn Android and iOS devices, you can improve performance using hardware acceleration. On either platform you can use a [GPU Delegate](https://ai.google.dev/edge/litert/performance/gpu), and on iOS you can use the [Core ML Delegate](https://ai.google.dev/edge/litert/ios/coreml). To add support for new hardware accelerators, you can [define your own delegate](https://ai.google.dev/edge/litert/performance/implementing_delegate).\nYou can run inference in the following ways based on the model type:\n  * **Models without metadata** : Use the [LiteRT Interpreter](https://ai.google.dev/edge/litert/inference) API. Supported on multiple platforms and languages such as Java, Swift, C++, Objective-C and Python.\n  * **Models with metadata** : You can build custom inference pipelines with the [LiteRT Support Library](https://ai.google.dev/edge/litert/android/metadata/lite_support).\n\n\n## Migrate from TF Lite\nApplications that use TF Lite libraries will continue to function, but all new active development and updates will only be included in LiteRT packages. The LiteRT APIs contain the same method names as the TF Lite APIs, so migrating to LiteRT does not require detailed code changes.\nFor more information, refer to the [migration guide](https://ai.google.dev/edge/litert/migration).\n## Next steps\nNew users should get started with the [LiteRT quickstart](https://ai.google.dev/edge/litert/inference). For specific information, see the following sections:\n**Model conversion**\n  * [Convert TensorFlow models](https://ai.google.dev/edge/litert/models/convert_tf)\n  * [Convert PyTorch models](https://ai.google.dev/edge/litert/models/convert_pytorch)\n  * [Convert PyTorch Generative AI models](https://ai.google.dev/edge/litert/models/edge_generative)\n\n\n**Platform guides**\n\n\n",
  "https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/extensions/google_cloud_big_query/Pusher": "Cloud Big Query Pusher component.  \nInherits From: [`Pusher`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Pusher), [`BaseComponent`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseComponent), [`BaseNode`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/BaseNode)\n```\ntfx.v1.extensions.google_cloud_big_query.Pusher(\n    model: Optional[[tfx.v1.dsl.Channel](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/Channel)] = None,\n    model_blessing: Optional[[tfx.v1.dsl.Channel](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/Channel)] = None,\n    infra_blessing: Optional[[tfx.v1.dsl.Channel](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/dsl/Channel)] = None,\n    custom_config: Optional[Dict[str, Any]] = None\n)\n\n```\n\nComponent `outputs` contains:\n  * `pushed_model`: Channel of type [`standard_artifacts.PushedModel`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/PushedModel) with result of push.\n\n\n## Args  \n---  \n`model` |  An optional Channel of type [`standard_artifacts.Model`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/Model), usually produced by a Trainer component.   \n`model_blessing` |  An optional Channel of type [`standard_artifacts.ModelBlessing`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/ModelBlessing), usually produced from an Evaluator component.   \n`infra_blessing` |  An optional Channel of type [`standard_artifacts.InfraBlessing`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/InfraBlessing), usually produced from an InfraValidator component.   \n`custom_config` |  A dict which contains the deployment job parameters to be passed to Cloud platforms.   \n## Attributes  \n---  \n`outputs` |  Component's output channel dict.   \n## Methods\n### `with_node_execution_options`\n```\nwith_node_execution_options(\n    node_execution_options: utils.NodeExecutionOptions\n) -> typing_extensions.Self\n\n```\n\n## Class Variables  \n---  \nPOST_EXECUTABLE_SPEC  |  `None`  \nPRE_EXECUTABLE_SPEC  |  `None`\n",
  "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics": "Visualize the input statistics using Facets.  \n```\ntfdv.visualize_statistics(\n    lhs_statistics: statistics_pb2.DatasetFeatureStatisticsList,\n    rhs_statistics: Optional[statistics_pb2.DatasetFeatureStatisticsList] = None,\n    lhs_name: Text = 'lhs_statistics',\n    rhs_name: Text = 'rhs_statistics',\n    allowlist_features: Optional[List[types.FeaturePath]] = None,\n    denylist_features: Optional[List[types.FeaturePath]] = None\n) -> None\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)\n  * [FaceSSD Fairness Indicators Example Colab](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Facessd_Fairness_Indicators_Example_Colab)\n  * [Introduction to Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Example_Colab)\n\n  \n## Args  \n---  \n`lhs_statistics` |  A DatasetFeatureStatisticsList protocol buffer.   \n`rhs_statistics` |  An optional DatasetFeatureStatisticsList protocol buffer to compare with lhs_statistics.   \n`lhs_name` |  Name to use for the lhs_statistics dataset if a name is not already provided within the protocol buffer.   \n`rhs_name` |  Name to use for the rhs_statistics dataset if a name is not already provided within the protocol buffer.   \n`allowlist_features` |  Set of features to be visualized.   \n`denylist_features` |  Set of features to ignore for visualization.   \n## Raises  \n---  \n`TypeError` |  If the input argument is not of the expected type.   \n`ValueError` |  If the input statistics protos does not have only one dataset. \n",
  "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv": "Compute data statistics from CSV files.  \n```\ntfdv.generate_statistics_from_csv(\n    data_location: Text,\n    column_names: Optional[List[types.FeatureName]] = None,\n    delimiter: Text = ',',\n    output_path: Optional[bytes] = None,\n    stats_options: [tfdv.StatsOptions](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions) = [options.StatsOptions()](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions),\n    pipeline_options: Optional[PipelineOptions] = None,\n    compression_type: Text = CompressionTypes.AUTO\n) -> statistics_pb2.DatasetFeatureStatisticsList\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)\n\n  \nRuns a Beam pipeline to compute the data statistics and return the result data statistics proto.\nThis is a convenience method for users with data in CSV format. Users with data in unsupported file/data formats, or users who wish to create their own Beam pipelines need to use the 'GenerateStatistics' PTransform API directly instead.\n## Args  \n---  \n`data_location` |  The location of the input data files.   \n`column_names` |  A list of column names to be treated as the CSV header. Order must match the order in the input CSV files. If this argument is not specified, we assume the first line in the input CSV files as the header. Note that this option is valid only for 'csv' input file format.   \n`delimiter` |  A one-character string used to separate fields in a CSV file.   \n`output_path` |  The file path to output data statistics result to. If None, we use a temporary directory. It will be a TFRecord file containing a single data statistics proto, and can be read with the 'load_statistics' API. If you run this function on Google Cloud, you must specify an output_path. Specifying None may cause an error.   \n`stats_options` |  [`tfdv.StatsOptions`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions) for generating data statistics.   \n`pipeline_options` |  Optional beam pipeline options. This allows users to specify various beam pipeline execution parameters like pipeline runner (DirectRunner or DataflowRunner), cloud dataflow service project id, etc. See https://cloud.google.com/dataflow/pipelines/specifying-exec-params for more details.   \n`compression_type` |  Used to handle compressed input files. Default value is CompressionTypes.AUTO, in which case the file_path's extension will be used to detect the compression.   \n## Returns  \n---  \nA DatasetFeatureStatisticsList proto. \n",
  "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema": "Displays the input schema (for use in a Jupyter notebook).  \n```\ntfdv.display_schema(\n    schema: schema_pb2.Schema\n) -> None\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n\n  \n## Args  \n---  \n`schema` |  A Schema protocol buffer. \n",
  "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/load_statistics": "Loads data statistics proto from file.  \n```\ntfdv.load_statistics(\n    input_path: Text\n) -> statistics_pb2.DatasetFeatureStatisticsList\n\n```\n\n## Args  \n---  \n`input_path` |  Data statistics file path. The file should be a one-record TFRecord file or a plain file containing the statistics proto in Proto Text Format.   \n## Returns  \n---  \nA DatasetFeatureStatisticsList proto.   \n## Raises  \n---  \n`IOError` |  If the input path does not exist. \n",
  "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema": "Infers schema from the input statistics.  \n```\ntfdv.infer_schema(\n    statistics: statistics_pb2.DatasetFeatureStatisticsList,\n    infer_feature_shape: bool = True,\n    max_string_domain_size: int = 100,\n    schema_transformations: Optional[List[Callable[[schema_pb2.Schema, statistics_pb2.\n        DatasetFeatureStatistics], schema_pb2.Schema]]] = None\n) -> schema_pb2.Schema\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)\n\n  \n## Args  \n---  \n`statistics` |  A DatasetFeatureStatisticsList protocol buffer. Schema inference is currently supported only for lists with a single DatasetFeatureStatistics proto or lists with multiple DatasetFeatureStatistics protos corresponding to data slices that include the default slice (i.e., the slice with all examples). If a list with multiple DatasetFeatureStatistics protos is used, this function will infer the schema from the statistics corresponding to the default slice.   \n`infer_feature_shape` |  A boolean to indicate if shape of the features need to be inferred from the statistics.   \n`max_string_domain_size` |  Maximum size of the domain of a string feature in order to be interpreted as a categorical feature.   \n`schema_transformations` |  List of transformation functions to apply to the auto-inferred schema. Each transformation function should take the schema and statistics as input and should return the transformed schema. The transformations are applied in the order provided in the list.   \n## Returns  \n---  \nA Schema protocol buffer.   \n## Raises  \n---  \n`TypeError` |  If the input argument is not of the expected type.   \n`ValueError` |  If the input statistics proto contains multiple datasets, none of which corresponds to the default slice. \n",
  "https://www.tensorflow.org/tfx/data_validation": "Once your data is in a TFX pipeline, you can use TFX components to analyze and transform it. You can use these tools even before you train a model.\nThere are many reasons to analyze and transform your data:\n  * To find problems in your data. Common problems include: \n    * Missing data, such as features with empty values.\n    * Labels treated as features, so that your model gets to peek at the right answer during training.\n    * Features with values outside the range you expect.\n    * Data anomalies.\n    * Transfer learned model has preprocessing that does not match the training data.\n  * To engineer more effective feature sets. For example, you can identify: \n    * Especially informative features.\n    * Redundant features.\n    * Features that vary so widely in scale that they may slow learning.\n    * Features with little or no unique predictive information.\n\n\nTFX tools can both help find data bugs, and help with feature engineering.\n## TensorFlow Data Validation\n  * [Schema Based Example Validation](https://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation)\n  * [Training-Serving Skew Detection](https://www.tensorflow.org/tfx/guide/tfdv#skewdetect)\n  * [Drift Detection](https://www.tensorflow.org/tfx/guide/tfdv#drift_detection)\n\n\n### Overview\nTensorFlow Data Validation identifies anomalies in training and serving data, and can automatically create a schema by examining the data. The component can be configured to detect different classes of anomalies in the data. It can\n  1. Perform validity checks by comparing data statistics against a schema that codifies expectations of the user.\n  2. Detect training-serving skew by comparing examples in training and serving data.\n  3. Detect data drift by looking at a series of data.\n\n\nWe document each of these functionalities independently:\n  * [Schema Based Example Validation](https://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation)\n  * [Training-Serving Skew Detection](https://www.tensorflow.org/tfx/guide/tfdv#skewdetect)\n  * [Drift Detection](https://www.tensorflow.org/tfx/guide/tfdv#drift_detection)\n\n\n### Schema Based Example Validation\nTensorFlow Data Validation identifies any anomalies in the input data by comparing data statistics against a schema. The schema codifies properties which the input data is expected to satisfy, such as data types or categorical values, and can be modified or replaced by the user.\nTensorflow Data Validation is typically invoked multiple times within the context of the TFX pipeline: (i) for every split obtained from ExampleGen, (ii) for all pre-transformed data used by Transform and (iii) for all post-transform data generated by Transform. When invoked in the context of Transform (ii-iii), statistics options and schema-based constraints can be set by defining the [`stats_options_updater_fn`](https://www.tensorflow.org/tfx/guide/tft). This is particilarly useful when validating unstructured data (e.g. text features). See the \n#### Advanced Schema Features\nThis section covers more advanced schema configuration that can help with special setups.\n##### Sparse Features\nEncoding sparse features in Examples usually introduces multiple Features that are expected to have the same valency for all Examples. For example the sparse feature: \n```\n\nWeightedCategories = [('CategoryA', 0.3), ('CategoryX', 0.7)]\n\n```\nwould be encoded using separate Features for index and value: ```\n\nWeightedCategoriesIndex = ['CategoryA', 'CategoryX']\nWeightedCategoriesValue = [0.3, 0.7]\n\n```\nwith the restriction that the valency of the index and value feature should match for all Examples. This restriction can be made explicit in the schema by defining a sparse_feature: ```\n\nsparse_feature {\n  name: 'WeightedCategories'\n  index_feature { name: 'WeightedCategoriesIndex' }\n  value_feature { name: 'WeightedCategoriesValue' }\n}\n\n```\n\nThe sparse feature definition requires one or more index and one value feature which refer to features that exist in the schema. Explicitly defining sparse features enables TFDV to check that the valencies of all referred features match.\nSome use cases introduce similar valency restrictions between Features, but do not necessarily encode a sparse feature. Using sparse feature should unblock you, but is not ideal.\n##### Schema Environments\nBy default validations assume that all Examples in a pipeline adhere to a single schema. In some cases introducing slight schema variations is necessary, for instance features used as labels are required during training (and should be validated), but are missing during serving. Environments can be used to express such requirements, in particular `default_environment()`, `in_environment()`, `not_in_environment()`.\nFor example, assume a feature named 'LABEL' is required for training, but is expected to be missing from serving. This can be expressed by:\n  * Define two distinct environments in the schema: [\"SERVING\", \"TRAINING\"] and associate 'LABEL' only with environment \"TRAINING\".\n  * Associate the training data with environment \"TRAINING\" and the serving data with environment \"SERVING\".\n\n\n##### Schema Generation\nThe input data schema is specified as an instance of the TensorFlow \nInstead of constructing a schema manually from scratch, a developer can rely on TensorFlow Data Validation's automatic schema construction. Specifically, TensorFlow Data Validation automatically constructs an initial schema based on statistics computed over training data available in the pipeline. Users can simply review this autogenerated schema, modify it as needed, check it into a version control system, and push it explicitly into the pipeline for further validation.\nTFDV includes `infer_schema()` to generate a schema automatically. For example:\n```\nschema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n\n```\n\nThis triggers an automatic schema generation based on the following rules:\n  * If a schema has already been auto-generated then it is used as is.\n  * Otherwise, TensorFlow Data Validation examines the available data statistics and computes a suitable schema for the data.\n\n\n_Note: The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that users review and modify it as needed._\n### Training-Serving Skew Detection \n#### Overview\nTensorFlow Data Validation can detect distribution skew between training and serving data. Distribution skew occurs when the distribution of feature values for training data is significantly different from serving data. One of the key causes for distribution skew is using either a completely different corpus for training data generation to overcome lack of initial data in the desired corpus. Another reason is a faulty sampling mechanism that only chooses a subsample of the serving data to train on.\n##### Example Scenario\nSee the [TensorFlow Data Validation Get Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for information about configuring training-serving skew detection.\n### Drift Detection\nDrift detection is supported between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of \nSee the [TensorFlow Data Validation Get Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for information about configuring drift detection.\n## Using Visualizations to Check Your Data\nTensorFlow Data Validation provides tools for visualizing the distribution of feature values. By examining these distributions in a Jupyter notebook using \n### Identifying Suspicious Distributions\nYou can identify common bugs in your data by using a Facets Overview display to look for suspicious distributions of feature values.\n#### Unbalanced Data\nAn unbalanced feature is a feature for which one value predominates. Unbalanced features can occur naturally, but if a feature always has the same value you may have a data bug. To detect unbalanced features in a Facets Overview, choose \"Non-uniformity\" from the \"Sort by\" dropdown.\nThe most unbalanced features will be listed at the top of each feature-type list. For example, the following screenshot shows one feature that is all zeros, and a second that is highly unbalanced, at the top of the \"Numeric Features\" list:\n#### Uniformly Distributed Data\nA uniformly distributed feature is one for which all possible values appear with close to the same frequency. As with unbalanced data, this distribution can occur naturally, but can also be produced by data bugs.\nTo detect uniformly distributed features in a Facets Overview, choose \"Non- uniformity\" from the \"Sort by\" dropdown and check the \"Reverse order\" checkbox:\nString data is represented using bar charts if there are 20 or fewer unique values, and as a cumulative distribution graph if there are more than 20 unique values. So for string data, uniform distributions can appear as either flat bar graphs like the one above or straight lines like the one below:\n##### Bugs That Can Produce Uniformly Distributed Data\nHere are some common bugs that can produce uniformly distributed data:\n  * Using strings to represent non-string data types such as dates. For example, you will have many unique values for a datetime feature with representations like \"2017-03-01-11-45-03\". Unique values will be distributed uniformly.\n  * Including indices like \"row number\" as features. Here again you have many unique values.\n\n\n#### Missing Data\nTo check whether a feature is missing values entirely:\n  1. Choose \"Amount missing/zero\" from the \"Sort by\" drop-down.\n  2. Check the \"Reverse order\" checkbox.\n  3. Look at the \"missing\" column to see the percentage of instances with missing values for a feature.\n\n\nA data bug can also cause incomplete feature values. For example you may expect a feature's value list to always have three elements and discover that sometimes it only has one. To check for incomplete values or other cases where feature value lists don't have the expected number of elements:\n  1. Choose \"Value list length\" from the \"Chart to show\" drop-down menu on the right.\n  2. Look at the chart to the right of each feature row. The chart shows the range of value list lengths for the feature. For example, the highlighted row in the screenshot below shows a feature that has some zero-length value lists:\n\n\n#### Large Differences in Scale Between Features\nIf your features vary widely in scale, then the model may have difficulties learning. For example, if some features vary from 0 to 1 and others vary from 0 to 1,000,000,000, you have a big difference in scale. Compare the \"max\" and \"min\" columns across features to find widely varying scales.\nConsider normalizing feature values to reduce these wide variations.\n#### Labels with Invalid Labels\nTensorFlow's Estimators have restrictions on the type of data they accept as labels. For example, binary classifiers typically only work with {0, 1} labels.\nReview the label values in the Facets Overview and make sure they conform to the \n",
  "https://www.tensorflow.org/tfx/data_validation/get_started": "TensorFlow Data Validation (TFDV) can analyze training and serving data to:\n  * compute descriptive \n  * infer a \n  * detect \n\n\nThe core API supports each piece of functionality, with convenience methods that build on top and can be called in the context of notebooks.\n## Computing descriptive data statistics\nTFDV can compute descriptive \nFor example, suppose that `path` points to a file in the `TFRecord` format (which holds records of type `tensorflow.Example`). The following snippet illustrates the computation of statistics using TFDV:\n```\n    stats = tfdv.generate_statistics_from_tfrecord(data_location=path)\n\n```\n\nThe returned value is a \n```\n    tfdv.visualize_statistics(stats)\n\n```\n\nThe previous example assumes that the data is stored in a `TFRecord` file. TFDV also supports CSV input format, with extensibility for other common formats. You can find the available data decoders [`tfdv.generate_statistics_from_dataframe`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe) utility function for users with in-memory data represented as a pandas DataFrame.\nIn addition to computing a default set of data statistics, TFDV can also compute statistics for semantic domains (e.g., images, text). To enable computation of semantic domain statistics, pass a `enable_semantic_domain_stats` set to True to [`tfdv.generate_statistics_from_tfrecord`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord).\n### Running on Google Cloud\nInternally, TFDV uses [generate statistics for data in custom format](https://www.tensorflow.org/tfx/data_validation/get_started#writing_custom_data_connector)), the API also exposes a Beam PTransform for statistics generation.\nTo run TFDV on Google Cloud, the TFDV wheel file must be downloaded and provided to the Dataflow workers. Download the wheel file to the current directory as follows:\n```\npip\\\n\\\n\\\n=:all:\n\n\n```\n\nThe following snippet shows an example usage of TFDV on Google Cloud:\n```\n\nimporttensorflow_data_validationastfdv\nfromapache_beam.options.pipeline_optionsimport PipelineOptions, GoogleCloudOptions, StandardOptions, SetupOptions\n\nPROJECT_ID = ''\nJOB_NAME = ''\nGCS_STAGING_LOCATION = ''\nGCS_TMP_LOCATION = ''\nGCS_DATA_LOCATION = ''\n# GCS_STATS_OUTPUT_PATH is the file path to which to output the data statistics\n# result.\nGCS_STATS_OUTPUT_PATH = ''\n\nPATH_TO_WHL_FILE = ''\n\n\n# Create and set your PipelineOptions.\noptions = PipelineOptions()\n\n# For Cloud execution, set the Cloud Platform project, job_name,\n# staging location, temp_location and specify DataflowRunner.\ngoogle_cloud_options = options.view_as(GoogleCloudOptions)\ngoogle_cloud_options.project = PROJECT_ID\ngoogle_cloud_options.job_name = JOB_NAME\ngoogle_cloud_options.staging_location = GCS_STAGING_LOCATION\ngoogle_cloud_options.temp_location = GCS_TMP_LOCATION\noptions.view_as(StandardOptions).runner = 'DataflowRunner'\n\nsetup_options = options.view_as(SetupOptions)\n# PATH_TO_WHL_FILE should point to the downloaded tfdv wheel file.\nsetup_options.extra_packages = [PATH_TO_WHL_FILE]\n\ntfdv.generate_statistics_from_tfrecord(GCS_DATA_LOCATION,\n                                       output_path=GCS_STATS_OUTPUT_PATH,\n                                       pipeline_options=options)\n\n\n```\n\nIn this case, the generated statistics proto is stored in a TFRecord file written to `GCS_STATS_OUTPUT_PATH`.\nNOTE When calling any of the `tfdv.generate_statistics_...` functions (e.g., [`tfdv.generate_statistics_from_tfrecord`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord)) on Google Cloud, you must provide an `output_path`. Specifying None may cause an error.\n## Inferring a schema over the data\nThe \n  * which features are expected to be present\n  * their type\n  * the number of values for a feature in each example\n  * the presence of each feature across all examples\n  * the expected domains of features.\n\n\nIn short, the schema describes the expectations for \"correct\" data and can thus be used to detect errors in the data (described below). Moreover, the same schema can be used to set up \nSince writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics:\n```\n    schema = tfdv.infer_schema(stats)\n\n```\n\nIn general, TFDV uses conservative heuristics to infer stable data properties from the statistics in order to avoid overfitting the schema to the specific dataset. It is strongly advised to **review the inferred schema and refine it as needed** , to capture any domain knowledge about the data that TFDV's heuristics might have missed.\nBy default, [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) infers the shape of each required feature, if `value_count.min` equals `value_count.max` for the feature. Set the `infer_feature_shape` argument to False to disable shape inference.\nThe schema itself is stored as a `payment_type` that takes a single value:\n```\nfeature{\nname:\"payment_type\"\nvalue_count{\nmin:1\nmax:1\n}\ntype:BYTES\ndomain:\"payment_type\"\npresence{\nmin_fraction:1.0\nmin_count:1\n}\n}\n\n```\n\nTo mark that the feature should be populated in at least 50% of the examples:\n```\n    tfdv.get_feature(schema, 'payment_type').presence.min_fraction = 0.5\n\n```\n\nThe \n## Checking the data for errors\nGiven a schema, it is possible to check whether a dataset conforms to the expectations set in the schema or whether there exist any \n### Matching the statistics of the dataset against a schema\nTo check for errors in the aggregate, TFDV matches the statistics of the dataset against the schema and marks any discrepancies. For example:\n```\n    # Assume that other_path points to another TFRecord file\n    other_stats = tfdv.generate_statistics_from_tfrecord(data_location=other_path)\n    anomalies = tfdv.validate_statistics(statistics=other_stats, schema=schema)\n\n```\n\nThe result is an instance of the `other_path` contains examples with values for the feature `payment_type` outside the domain specified in the schema.\nThis produces an anomaly\n```\n   payment_type  Unexpected string values  Examples contain values missing fromthe schema: Prcard (<1%).\n\n```\n\nindicating that an out of domain value was found in the stats in < 1% of the feature values.\nIf this was expected, then the schema can be updated as follows:\n```\n   tfdv.get_domain(schema, 'payment_type').value.append('Prcard')\n\n```\n\nIf the anomaly truly indicates a data error, then the underlying data should be fixed before using it for training.\nThe various anomaly types that can be detected by this module are listed \nThe \n### Checking for errors on a per-example basis\nTFDV also provides the option to validate data on a per-example basis, instead of comparing dataset-wide statistics against the schema. TFDV provides functions for validating data on a per-example basis and then generating summary statistics for the anomalous examples found. For example:\n```\n   options = tfdv.StatsOptions(schema=schema)\n   anomalous_example_stats = tfdv.validate_examples_in_tfrecord(\n       data_location=input, stats_options=options)\n\n```\n\nThe `anomalous_example_stats` that `validate_examples_in_tfrecord` returns is a \n## Schema Environments\nBy default, validations assume that all datasets in a pipeline adhere to a single schema. In some cases introducing slight schema variations is necessary, for instance features used as labels are required during training (and should be validated), but are missing during serving.\n**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using default_environment, in_environment and not_in_environment.\nFor example, if the **tips** feature is being used as the label in training, but missing in the serving data. Without environment specified, it will show up as an anomaly.\n```\n    serving_stats = tfdv.generate_statistics_from_tfrecord(data_location=serving_data_path)\n    serving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\n```\n\nTo fix this, we need to set the default environment for all features to be both 'TRAINING' and 'SERVING', and exclude the 'tips' feature from SERVING environment.\n```\n    # All features are by default in both TRAINING and SERVING environments.\n    schema.default_environment.append('TRAINING')\n    schema.default_environment.append('SERVING')\n\n    # Specify that 'tips' feature is not in SERVING environment.\n    tfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n\n    serving_anomalies_with_env = tfdv.validate_statistics(\n        serving_stats, schema, environment='SERVING')\n\n```\n\n## Checking data skew and drift\nIn addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect:\n  * skew between training and serving data\n  * drift between different days of training data\n\n\nTFDV performs this check by comparing the statistics of different datasets based on the drift/skew comparators specified in the schema. For example, to check if there is any skew between 'payment_type' feature within training and serving dataset:\n```\n    # Assume we have already generated the statistics of training dataset, and\n    # inferred a schema from it.\n    serving_stats = tfdv.generate_statistics_from_tfrecord(data_location=serving_data_path)\n    # Add a skew comparator to schema for 'payment_type' and set the threshold\n    # of L-infinity norm for triggering skew anomaly to be 0.01.\n    tfdv.get_feature(schema, 'payment_type').skew_comparator.infinity_norm.threshold = 0.01\n    skew_anomalies = tfdv.validate_statistics(\n        statistics=train_stats, schema=schema, serving_statistics=serving_stats)\n\n```\n\nNOTE L-infinity norm will only detect skew for the categorical features. Instead of specifying an `infinity_norm` threshold, specifying a `jensen_shannon_divergence` threshold in the `skew_comparator` would detect skew for both numeric and categorical features.\nSame with checking whether a dataset conform to the expectations set in the schema, the result is also an instance of the `payement_type` having value `Cash`, this produces a skew anomaly\n```\n   payment_type  High L-infinity distance between serving and training  The L-infinity distance between serving and training is 0.0435984 (up to six significant digits), above the threshold 0.01. The feature value with maximum difference is: Cash\n\n```\n\nIf the anomaly truly indicates a skew between training and serving data, then further investigation is necessary as this could have a direct impact on model performance.\nThe \nDetecting drift between different days of training data can be done in a similar way\n```\n    # Assume we have already generated the statistics of training dataset for\n    # day 2, and inferred a schema from it.\n    train_day1_stats = tfdv.generate_statistics_from_tfrecord(data_location=train_day1_data_path)\n    # Add a drift comparator to schema for 'payment_type' and set the threshold\n    # of L-infinity norm for triggering drift anomaly to be 0.01.\n    tfdv.get_feature(schema, 'payment_type').drift_comparator.infinity_norm.threshold = 0.01\n    drift_anomalies = tfdv.validate_statistics(\n        statistics=train_day2_stats, schema=schema, previous_statistics=train_day1_stats)\n\n```\n\nNOTE L-infinity norm will only detect skew for the categorical features. Instead of specifying an `infinity_norm` threshold, specifying a `jensen_shannon_divergence` threshold in the `skew_comparator` would detect skew for both numeric and categorical features.\n## Writing custom data connector\nTo compute data statistics, TFDV provides several `TFRecord` of [tf.train.Example](https://www.tensorflow.org/api_docs/python/tf/train/Example), CSV, etc). If your data format is not in this list, you need to write a custom data connector for reading input data, and connect it with the TFDV core API for computing data statistics.\nThe TFDV `DatasetFeatureStatisticsList` protocol buffer.\nOnce you have implemented the custom data connector that batches your input examples in an Arrow RecordBatch, you need to connect it with the [`tfdv.GenerateStatistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/GenerateStatistics) API for computing the data statistics. Take `TFRecord` of [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)'s for example. `tfx_bsl` provides the [TFExampleRecord](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TFExampleRecord) data connector, and below is an example of how to connect it with the [`tfdv.GenerateStatistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/GenerateStatistics) API.\n```\nimporttensorflow_data_validationastfdv\nfromtfx_bsl.publicimport tfxio\nimportapache_beamasbeam\nfromtensorflow_metadata.proto.v0import statistics_pb2\n\nDATA_LOCATION = ''\nOUTPUT_LOCATION = ''\n\nwith beam.Pipeline() as p:\n    _ = (\n    p\n    # 1. Read and decode the data with tfx_bsl.\n    | 'TFXIORead' >> (\n          tfxio.TFExampleRecord(\n              file_pattern=[DATA_LOCATION],\n              telemetry_descriptors=['my', 'tfdv']).BeamSource())\n    # 2. Invoke TFDV `GenerateStatistics` API to compute the data statistics.\n    | 'GenerateStatistics' >> tfdv.GenerateStatistics()\n    # 3. Materialize the generated data statistics.\n    | 'WriteStatsOutput' >> WriteStatisticsToTFRecord(OUTPUT_LOCATION))\n\n```\n\n## Computing statistics over slices of data\nTFDV can be configured to compute statistics over slices of data. Slicing can be enabled by providing slicing functions which take in an Arrow `RecordBatch` and output a sequence of tuples of form `(slice key, record batch)`. TFDV provides an easy way to [`tfdv.StatsOptions`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/StatsOptions) when computing statistics.\nWhen slicing is enabled, the output \n```\nimporttensorflow_data_validationastfdv\nfromtensorflow_data_validation.utilsimport slicing_util\n\n# Slice on country feature (i.e., every unique value of the feature).\nslice_fn1 = slicing_util.get_feature_value_slicer(features={'country': None})\n\n# Slice on the cross of country and state feature (i.e., every unique pair of\n# values of the cross).\nslice_fn2 = slicing_util.get_feature_value_slicer(\n    features={'country': None, 'state': None})\n\n# Slice on specific values of a feature.\nslice_fn3 = slicing_util.get_feature_value_slicer(\n    features={'age': [10, 50, 70]})\n\nstats_options = tfdv.StatsOptions(\n    slice_functions=[slice_fn1, slice_fn2, slice_fn3])\n\n\n```\n\n",
  "https://www.tensorflow.org/tfx/guide/a%20href%3D%22https%3A/badge.fury.io/py/tfx%22%3Ehttps%3A/badge.fury.io/py/tfx%3C/a": "\n",
  "https://www.tensorflow.org/tfx/guide/a%20href%3D%22https%3A/github.com/tensorflow/tfx%22%3Ehttps%3A/github.com/tensorflow/tfx%3C/a": "\n",
  "https://www.tensorflow.org/tfx/guide/bulkinferrer": "The BulkInferrer TFX component performs batch inference on unlabeled data. The generated InferenceResult(\nBulkInferrer consumes:\n  * A trained model in [SavedModel](https://www.tensorflow.org/guide/saved_model.md) format.\n  * Unlabelled tf.Examples that contain features.\n  * (Optional) Validation result from [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator.md) component.\n\n\nBulkInferrer emits:\n## Using the BulkInferrer Component\nA BulkInferrer TFX component is used to perform batch inference on unlabeled tf.Examples. It is typically deployed after an [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator.md) component to perform inference with a validated model, or after a [Trainer](https://www.tensorflow.org/tfx/guide/trainer.md) component to directly perform inference on exported model.\nIt currently performs in-memory model inference and remote inference. Remote inference requires the model to be hosted on Cloud AI Platform.\nTypical code looks like this:\n```\nbulk_inferrer = BulkInferrer(\n    examples=examples_gen.outputs['examples'],\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    data_spec=bulk_inferrer_pb2.DataSpec(),\n    model_spec=bulk_inferrer_pb2.ModelSpec()\n)\n\n```\n\nMore details are available in the [BulkInferrer API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/BulkInferrer).\n",
  "https://www.tensorflow.org/tfx/guide/evaluator": "The Evaluator TFX pipeline component performs deep analysis on the training results for your models, to help you understand how your model performs on subsets of your data. The Evaluator also helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.\nWhen validation is enabled, the Evaluator compares new models against a baseline (such as the currently serving model) to determine if they're \"good enough\" relative to the baseline. It does so by evaluating both models on an eval dataset and computing their performance on metrics (e.g. AUC, loss). If the new model's metrics meet developer-specified criteria relative to the baseline model (e.g. AUC is not lower), the model is \"blessed\" (marked as good), indicating to the [Pusher](https://www.tensorflow.org/tfx/guide/pusher) that it is ok to push the model to production.\n  * Consumes: \n    * An eval split from [Examples](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/types/standard_artifacts/Examples)\n    * A trained model from [Trainer](https://www.tensorflow.org/tfx/guide/trainer)\n    * A previously blessed model (if validation to be performed)\n  * Emits: \n    * Analysis results to [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd)\n    * Validation results to [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) (if validation to be performed)\n\n\n## Evaluator and TensorFlow Model Analysis\nEvaluator leverages the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/guide/tfma) library to perform the analysis, which in turn use [Apache Beam](https://www.tensorflow.org/tfx/guide/beam) for scalable processing.\n## Using the Evaluator Component\nAn Evaluator pipeline component is typically very easy to deploy and requires little customization, since most of the work is done by the Evaluator TFX component.\nTo setup the evaluator the following information is needed:\n  * Metrics to configure (only required if additional metrics are being added outside of those saved with the model). See \n  * Slices to configure (if no slices are given then an \"overall\" slice will be added by default). See \n\n\nIf validation is to be included, the following additional information is needed:\n  * Which model to compare against (latest blessed, etc).\n  * Model validations (thresholds) to verify. See \n\n\nWhen enabled, validation will be performed against all of the metrics and slices that were defined.\nTypical code looks like this:\n```\nimporttensorflow_model_analysisastfma\n...\n\n# For TFMA evaluation\n\neval_config = tfma.EvalConfig(\n    model_specs=[\n        # This assumes a serving model with signature 'serving_default'. If\n        # using estimator based EvalSavedModel, add signature_name='eval' and\n        # remove the label_key. Note, if using a TFLite model, then you must set\n        # model_type='tf_lite'.\n        tfma.ModelSpec(label_key='<label_key>')\n    ],\n    metrics_specs=[\n        tfma.MetricsSpec(\n            # The metrics added here are in addition to those saved with the\n            # model (assuming either a keras model or EvalSavedModel is used).\n            # Any metrics added into the saved model (for example using\n            # model.compile(..., metrics=[...]), etc) will be computed\n            # automatically.\n            metrics=[\n                tfma.MetricConfig(class_name='ExampleCount'),\n                tfma.MetricConfig(\n                    class_name='BinaryAccuracy',\n                    threshold=tfma.MetricThreshold(\n                        value_threshold=tfma.GenericValueThreshold(\n                            lower_bound={'value': 0.5}),\n                        change_threshold=tfma.GenericChangeThreshold(\n                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n                            absolute={'value': -1e-10})))\n            ]\n        )\n    ],\n    slicing_specs=[\n        # An empty slice spec means the overall slice, i.e. the whole dataset.\n        tfma.SlicingSpec(),\n        # Data can be sliced along a feature column. In this case, data is\n        # sliced along feature column trip_start_hour.\n        tfma.SlicingSpec(feature_keys=['trip_start_hour'])\n    ])\n\n# The following component is experimental and may change in the future. This is\n# required to specify the latest blessed model will be used as the baseline.\nmodel_resolver = Resolver(\n      strategy_class=dsl.experimental.LatestBlessedModelStrategy,\n      model=Channel(type=Model),\n      model_blessing=Channel(type=ModelBlessing)\n).with_id('latest_blessed_model_resolver')\n\nmodel_analyzer = Evaluator(\n      examples=examples_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      baseline_model=model_resolver.outputs['model'],\n      # Change threshold will be ignored if there is no baseline (first run).\n      eval_config=eval_config)\n\n```\n\nThe evaluator produces an [EvalResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult) (and optionally a [ValidationResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ValidationResult) if validation was used) that can be loaded using [TFMA](https://www.tensorflow.org/tfx/guide/tfma). The following is an example of how to load the results into a Jupyter notebook:\n```\nimporttensorflow_model_analysisastfma\n\noutput_path = evaluator.outputs['evaluation'].get()[0].uri\n\n# Load the evaluation results.\neval_result = tfma.load_eval_result(output_path)\n\n# Visualize the metrics and plots using tfma.view.render_slicing_metrics,\n# tfma.view.render_plot, etc.\ntfma.view.render_slicing_metrics(tfma_result)\n...\n\n# Load the validation results\nvalidation_result = tfma.load_validation_result(output_path)\nif not validation_result.validation_ok:\n  ...\n\n```\n\nMore details are available in the [Evaluator API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Evaluator).\n",
  "https://www.tensorflow.org/tfx/guide/airflow": "## Apache Airflow\nSee the \n",
  "https://www.tensorflow.org/tfx/guide/custom_function_component": "Python function-based component definition makes it easier for you to create TFX custom components, by saving you the effort of defining a component specification class, executor class, and component interface class. In this component definition style, you write a function that is annotated with type hints. The type hints describe the input artifacts, output artifacts, and parameters of your component.\nWriting your custom component in this style is very straightforward, as in the following example.\n```\nclassMyOutput(TypedDict):\n  accuracy: float\n\n@component\ndefMyValidationComponent(\n    model: InputArtifact[Model],\n    blessing: OutputArtifact[Model],\n    accuracy_threshold: Parameter[int] = 10,\n) -> MyOutput:\n'''My simple custom model validation component.'''\n\n  accuracy = evaluate_model(model)\n  if accuracy >= accuracy_threshold:\n    write_output_blessing(blessing)\n\n  return {\n    'accuracy': accuracy\n  }\n\n```\n\nUnder the hood, this defines a custom component that is a subclass of \nIf you want to define a subclass of `beam_pipeline_args` when compiling the pipeline (`use_beam=True` in the decorator and add another `BeamComponentParameter` with default value `None` in your function as the following example:\n```\n@component(use_beam=True)\ndefMyDataProcessor(\n    examples: InputArtifact[Example],\n    processed_examples: OutputArtifact[Example],\n    beam_pipeline: BeamComponentParameter[beam.Pipeline] = None,\n    ) -> None:\n'''My simple custom model validation component.'''\n\n  with beam_pipeline as p:\n    # data pipeline definition with beam_pipeline begins\n    ...\n    # data pipeline definition with beam_pipeline ends\n\n```\n\nIf you are new to TFX pipelines, [learn more about the core concepts of TFX pipelines](https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines).\n## Inputs, outputs, and parameters\nIn TFX, inputs and outputs are tracked as Artifact objects which describe the location of and metadata properties associated with the underlying data; this information is stored in ML Metadata. Artifacts can describe complex data types or simple data types, such as: int, float, bytes, or unicode strings.\nA parameter is an argument (int, float, bytes, or unicode string) to a component known at pipeline construction time. Parameters are useful for specifying arguments and hyperparameters like training iteration count, dropout rate, and other configuration to your component. Parameters are stored as properties of component executions when tracked in ML Metadata.\n## Definition\nTo create a custom component, write a function that implements your custom logic and decorate it with the `tfx.dsl.component.experimental.decorators` module. To define your component’s input and output schema, annotate your function’s arguments and return value using annotations from the `tfx.dsl.component.experimental.annotations` module:\n  * For each **artifact input** , apply the `InputArtifact[ArtifactType]` type hint annotation. Replace `ArtifactType` with the artifact’s type, which is a subclass of `tfx.types.Artifact`. These inputs can be optional arguments.\n  * For each **output artifact** , apply the `OutputArtifact[ArtifactType]` type hint annotation. Replace `ArtifactType` with the artifact’s type, which is a subclass of `tfx.types.Artifact`. Component output artifacts should be passed as input arguments of the function, so that your component can write outputs to a system-managed location and set appropriate artifact metadata properties. This argument can be optional or this argument can be defined with a default value.\n  * For each **parameter** , use the type hint annotation `Parameter[T]`. Replace `T` with the type of the parameter. We currently only support primitive python types: `bool`, `int`, `float`, `str`, or `bytes`.\n  * For **beam pipeline** , use the type hint annotation `BeamComponentParameter[beam.Pipeline]`. Set the default value to be `None`. The value `None` will be replaced by an instantiated beam pipeline created by `_make_beam_pipeline()` of \n  * For each **simple data type input** (`int`, `float`, `str` or `bytes`) not known at pipeline construction time, use the type hint `T`. Note that in the TFX 0.22 release, concrete values cannot be passed at pipeline construction time for this type of input (use the `Parameter` annotation instead, as described in the previous section). This argument can be optional or this argument can be defined with a default value. If your component has simple data type outputs (`int`, `float`, `str` or `bytes`), you can return these outputs by using a `TypedDict` as a return type annotation, and returning an appropriate dict object.\n\n\nIn the body of your function, input and output artifacts are passed as `tfx.types.Artifact` objects; you can inspect its `.uri` to get its system-managed location and read/set any properties. Input parameters and simple data type inputs are passed as objects of the specified type. Simple data type outputs should be returned as a dictionary, where the keys are the appropriate output names and the values are the desired return values.\nThe completed function component can look like this:\n```\nfromtypingimport TypedDict\nimporttfx.v1astfx\nfromtfx.dsl.component.experimental.decoratorsimport component\n\nclassMyOutput(TypedDict):\n  loss: float\n  accuracy: float\n\n@component\ndefMyTrainerComponent(\n    training_data: tfx.dsl.components.InputArtifact[tfx.types.standard_artifacts.Examples],\n    model: tfx.dsl.components.OutputArtifact[tfx.types.standard_artifacts.Model],\n    dropout_hyperparameter: float,\n    num_iterations: tfx.dsl.components.Parameter[int] = 10\n) -> MyOutput:\n'''My simple trainer component.'''\n\n  records = read_examples(training_data.uri)\n  model_obj = train_model(records, num_iterations, dropout_hyperparameter)\n  model_obj.write_to(model.uri)\n\n  return {\n    'loss': model_obj.loss,\n    'accuracy': model_obj.accuracy\n  }\n\n# Example usage in a pipeline graph definition:\n# ...\ntrainer = MyTrainerComponent(\n    examples=example_gen.outputs['examples'],\n    dropout_hyperparameter=other_component.outputs['dropout'],\n    num_iterations=1000)\npusher = Pusher(model=trainer.outputs['model'])\n# ...\n\n```\n\nThe preceding example defines `MyTrainerComponent` as a Python function-based custom component. This component consumes an `examples` artifact as its input, and produces a `model` artifact as its output. The component uses the `artifact_instance.uri` to read or write the artifact at its system-managed location. The component takes a `num_iterations` input parameter and a `dropout_hyperparameter` simple data type value, and the component outputs `loss` and `accuracy` metrics as simple data type output values. The output `model` artifact is then used by the `Pusher` component.\n",
  "https://www.tensorflow.org/tfx/guide/beam_orchestrator": "\n",
  "https://www.tensorflow.org/tfx/guide/beam": "## Deployment and Scalability\nAs workload requirements increase Beam can scale to very large deployments across large compute clusters. This is limited only by the scalability of the underlying runner. Runners in large deployments will typically be deployed to a container orchestration system such as Kubernetes or Apache Mesos for automating application deployment, scaling, and management.\nSee the \nFor Google Cloud users, \n## Custom Python Code and Dependencies\nOne notable complexity of using Beam in a TFX pipeline is handling custom code and/or the dependencies needed from additional Python modules. Here are some examples of when this might be an issue:\n  * preprocessing_fn needs to refer to the user's own Python module\n  * a custom extractor for the Evaluator component\n  * custom modules which are sub-classed from a TFX component\n\n\nTFX relies on Beam's support for \n  1. Providing Python Code and Dependencies as Source Package\n  2. [Dataflow only] Using a Container Image as Worker\n\n\nThese are discussed next.\n### Providing Python Code and Dependencies as a Source Package\nThis is recommended for users who:\n  1. Are familiar with Python packaging and\n  2. Only use Python source code (i.e., no C modules or shared libraries).\n\n\nPlease follow one of the paths in \n  * --setup_file\n  * --extra_package\n  * --requirements_file\n\n\nNotice: In any of above cases, please make sure that the same version of `tfx` is listed as a dependency.\n### [Dataflow only] Using a Container Image for a Worker\nTFX 0.26.0 and above has experimental support for using \nIn order to use this, you have to:\n  * Build a Docker image which has both `tfx` and the users' custom code and dependencies pre-installed. \n    * For users who (1) use `tfx>=0.26` and (2) uses python 3.7 to develop their pipelines, the easiest way to do this is extending the corresponding version of the official `tensorflow/tfx` image:\n\n```\n# You can use a build-arg to dynamically pass in the\n# version of TFX being used to your Dockerfile.\n\nARGFROMtensorflow/tfx:${TFX_VERSION}\n# COPY your code and dependencies in\n\n```\n\n  * Push the image built to a container image registry which is accessible by the project used by Dataflow. \n    * Google Cloud users can consider using \n  * Provide following `beam_pipeline_args`:\n\n```\nbeam_pipeline_args.extend([\n    '--runner=DataflowRunner',\n    '--project={project-id}',\n    '--worker_harness_container_image={image-ref}',\n    '--experiments=use_runner_v2',\n])\n\n```\n\n**TODO(b/171733562): Remove use_runner_v2 once it is default for Dataflow.**\n**TODO(b/179738639): Create documentation for how to test custom container locally after**\n## Beam Pipeline Arguments\nSeveral TFX components rely on Beam for distributed data processing. They are configured with `beam_pipeline_args`, which is specified during during pipeline creation:\n```\nmy_pipeline = Pipeline(\n    ...,\n    beam_pipeline_args=[...])\n\n```\n\nTFX 0.30 and above adds an interface, `with_beam_pipeline_args`, for extending the pipeline level beam args per component:\n```\nexample_gen = CsvExampleGen(input_base=data_root).with_beam_pipeline_args([...])\n\n```\n\n",
  "https://www.tensorflow.org/tfx/guide/examplegen": "The ExampleGen TFX Pipeline component ingests data into TFX pipelines. It consumes external files/services to generate Examples which will be read by other TFX components. It also provides consistent and configurable partition, and shuffles the dataset for ML best practice.\n  * Consumes: Data from external data sources such as CSV, `TFRecord`, Avro, Parquet and BigQuery.\n  * Emits: `tf.Example` records, `tf.SequenceExample` records, or proto format, depending on the payload format.\n\n\n## ExampleGen and Other Components\nExampleGen provides data to components that make use of the [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) library, such as [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen), [StatisticsGen](https://www.tensorflow.org/tfx/guide/statsgen), and [Example Validator](https://www.tensorflow.org/tfx/guide/exampleval). It also provides data to [Transform](https://www.tensorflow.org/tfx/guide/transform), which makes use of the [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) library, and ultimately to deployment targets during inference.\n## Data Sources and Formats\nCurrently a standard installation of TFX includes full ExampleGen components for these data sources and formats:\nCustom executors are also available which enable the development of ExampleGen components for these data sources and formats:\nSee the usage examples in the source code and [this discussion](https://www.tensorflow.org/tfx/guide/examplegen#custom_examplegen) for more information on how to use and develop custom executors.\nIn addition, these data sources and formats are available as [custom component](https://www.tensorflow.org/tfx/guide/understanding_custom_components) examples:\n### Ingesting data formats which are supported by Apache Beam\nApache Beam supports ingesting data from a [see below](https://www.tensorflow.org/tfx/guide/examplegen#additional_data_formats)). These capabilities can be used to create custom ExampleGen components for TFX, which is demonstrated by some existing ExampleGen components ([see below](https://www.tensorflow.org/tfx/guide/examplegen#additional_data_formats)).\n## How to use an ExampleGen Component\nFor supported data sources (currently, CSV files, TFRecord files with `tf.Example`, `tf.SequenceExample` and proto format, and results of BigQuery queries) the ExampleGen pipeline component can be used directly in deploy and requires little customization. For example:\n```\nexample_gen = CsvExampleGen(input_base='data_root')\n\n```\n\nor like below for importing external TFRecord with `tf.Example` directly:\n```\nexample_gen = ImportExampleGen(input_base=path_to_tfrecord_dir)\n\n```\n\n## Span, Version and Split\nA Span is a grouping of training examples. If your data is persisted on a filesystem, each Span may be stored in a separate directory. The semantics of a Span are not hardcoded into TFX; a Span may correspond to a day of data, an hour of data, or any other grouping that is meaningful to your task.\nEach Span can hold multiple Versions of data. To give an example, if you remove some examples from a Span to clean up poor quality data, this could result in a new Version of that Span. By default, TFX components operate on the latest Version within a Span.\nEach Version within a Span can further be subdivided into multiple Splits. The most common use-case for splitting a Span is to split it into training and eval data.\n### Custom input/output split\nTo customize the train/eval split ratio which ExampleGen will output, set the `output_config` for ExampleGen component. For example:\n```\n# Input has a single split 'input_dir/*'.\n# Output 2 splits: train:eval=3:1.\noutput = proto.Output(\n             split_config=example_gen_pb2.SplitConfig(splits=[\n                 proto.SplitConfig.Split(name='train', hash_buckets=3),\n                 proto.SplitConfig.Split(name='eval', hash_buckets=1)\n             ]))\nexample_gen = CsvExampleGen(input_base=input_dir, output_config=output)\n\n```\n\nNotice how the `hash_buckets` were set in this example.\nFor an input source which has already been split, set the `input_config` for ExampleGen component:\n```\n\n# Input train split is 'input_dir/train/*', eval split is 'input_dir/eval/*'.\n# Output splits are generated one-to-one mapping from input splits.\ninput = proto.Input(splits=[\n                example_gen_pb2.Input.Split(name='train', pattern='train/*'),\n                example_gen_pb2.Input.Split(name='eval', pattern='eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input)\n\n```\n\nFor file based example gen (e.g. CsvExampleGen and ImportExampleGen), `pattern` is a glob relative file pattern that maps to input files with root directory given by input base path. For query-based example gen (e.g. BigQueryExampleGen, PrestoExampleGen), `pattern` is a SQL query.\nBy default, the entire input base dir is treated as a single input split, and the train and eval output split is generated with a 2:1 ratio.\nPlease refer to [downstream components guide](https://www.tensorflow.org/tfx/guide/examplegen#examplegen_downstream_components) for utilizing the custom splits downstream.\n#### Splitting Method\nWhen using `hash_buckets` splitting method, instead of the entire record, one can use a feature for partitioning the examples. If a feature is present, ExampleGen will use a fingerprint of that feature as the partition key.\nThis feature can be used to maintain a stable split w.r.t. certain properties of examples: for example, a user will always be put in the same split if \"user_id\" were selected as the partition feature name.\nThe interpretation of what a \"feature\" means and how to match a \"feature\" with the specified name depends on the ExampleGen implementation and the type of the examples.\nFor ready-made ExampleGen implementations:\n  * If it generates tf.Example, then a \"feature\" means an entry in tf.Example.features.feature.\n  * If it generates tf.SequenceExample, then a \"feature\" means an entry in tf.SequenceExample.context.feature.\n  * Only int64 and bytes features are supported.\n\n\nIn the following cases, ExampleGen throws runtime errors:\n  * Specified feature name does not exist in the example.\n  * Empty feature: [`tf.train.Feature()`](https://www.tensorflow.org/api_docs/python/tf/train/Feature).\n  * Non supported feature types, e.g., float features.\n\n\nTo output the train/eval split based on a feature in the examples, set the `output_config` for ExampleGen component. For example:\n```\n# Input has a single split 'input_dir/*'.\n# Output 2 splits based on 'user_id' features: train:eval=3:1.\noutput = proto.Output(\n             split_config=proto.SplitConfig(splits=[\n                 proto.SplitConfig.Split(name='train', hash_buckets=3),\n                 proto.SplitConfig.Split(name='eval', hash_buckets=1)\n             ],\n             partition_feature_name='user_id'))\nexample_gen = CsvExampleGen(input_base=input_dir, output_config=output)\n\n```\n\nNotice how the `partition_feature_name` was set in this example.\n### Span\nSpan can be retrieved by using '{SPAN}' spec in the \n  * This spec matches digits and maps the data into the relevant SPAN numbers. For example, 'data_{SPAN}-*.tfrecord' will collect files like 'data_12-a.tfrecord', 'data_12-b.tfrecord'.\n  * Optionally, this spec can be specified with the width of the integers when mapped. For example, 'data_{SPAN:2}.file' maps to files like 'data_02.file' and 'data_27.file' (as inputs for Span-2 and Span-27 respectively), but does not map to 'data_1.file' nor 'data_123.file'.\n  * When SPAN spec is missing, it's assumed to be always Span '0'.\n  * If SPAN is specified, pipeline will process the latest span, and store the span number in metadata.\n\n\nFor example, let's assume there are input data:\n  * '/tmp/span-1/train/data'\n  * '/tmp/span-1/eval/data'\n  * '/tmp/span-2/train/data'\n  * '/tmp/span-2/eval/data'\n\n\nand the input config is shown as below:\n```\nsplits {\n  name: 'train'\n  pattern: 'span-{SPAN}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: 'span-{SPAN}/eval/*'\n}\n\n```\n\nwhen triggering the pipeline, it will process:\n  * '/tmp/span-2/train/data' as train split\n  * '/tmp/span-2/eval/data' as eval split\n\n\nwith span number as '2'. If later on '/tmp/span-3/...' are ready, simply trigger the pipeline again and it will pick up span '3' for processing. Below shows the code example for using span spec:\n```\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n\n```\n\nRetrieving a certain span can be done with RangeConfig, which is detailed below.\n### Date\nIf your data source is organized on filesystem by date, TFX supports mapping dates directly to span numbers. There are three specs to represent mapping from dates to spans: {YYYY}, {MM} and {DD}:\n  * The three specs should be altogether present in the \n  * Either {SPAN} spec or this set of date specs can be specified exclusively.\n  * A calendar date with the year from YYYY, the month from MM, and the day of the month from DD is calculated, then the span number is calculated as as the number of days since unix epoch (i.e. 1970-01-01). For example, 'log-{YYYY}{MM}{DD}.data' matches to a file 'log-19700101.data' and consumes it as input for Span-0, and 'log-20170101.data' as input for Span-17167.\n  * If this set of date specs is specified, pipeline will process the latest latest date, and store the corresponding span number in metadata.\n\n\nFor example, let's assume there are input data organized by calendar date:\n  * '/tmp/1970-01-02/train/data'\n  * '/tmp/1970-01-02/eval/data'\n  * '/tmp/1970-01-03/train/data'\n  * '/tmp/1970-01-03/eval/data'\n\n\nand the input config is shown as below:\n```\nsplits {\n  name: 'train'\n  pattern: '{YYYY}-{MM}-{DD}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: '{YYYY}-{MM}-{DD}/eval/*'\n}\n\n```\n\nwhen triggering the pipeline, it will process:\n  * '/tmp/1970-01-03/train/data' as train split\n  * '/tmp/1970-01-03/eval/data' as eval split\n\n\nwith span number as '2'. If later on '/tmp/1970-01-04/...' are ready, simply trigger the pipeline again and it will pick up span '3' for processing. Below shows the code example for using date spec:\n```\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='{YYYY}-{MM}-{DD}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='{YYYY}-{MM}-{DD}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n\n```\n\n### Version\nVersion can be retrieved by using '{VERSION}' spec in the \n  * This spec matches digits and maps the data to the relevant VERSION numbers under the SPAN. Note that the Version spec can be used combination with either Span or Date spec.\n  * This spec can also be optionally specified with the width in the same way as SPAN spec. e.g. 'span-{SPAN}/version-{VERSION:4}/data-*'.\n  * When VERSION spec is missing, version is set to be None.\n  * If SPAN and VERSION are both specified, pipeline will process the latest version for the latest span, and store the version number in metadata.\n  * If VERSION is specified, but not SPAN (or date spec), an error will be thrown.\n\n\nFor example, let's assume there are input data:\n  * '/tmp/span-1/ver-1/train/data'\n  * '/tmp/span-1/ver-1/eval/data'\n  * '/tmp/span-2/ver-1/train/data'\n  * '/tmp/span-2/ver-1/eval/data'\n  * '/tmp/span-2/ver-2/train/data'\n  * '/tmp/span-2/ver-2/eval/data'\n\n\nand the input config is shown as below:\n```\nsplits {\n  name: 'train'\n  pattern: 'span-{SPAN}/ver-{VERSION}/train/*'\n}\nsplits {\n  name: 'eval'\n  pattern: 'span-{SPAN}/ver-{VERSION}/eval/*'\n}\n\n```\n\nwhen triggering the pipeline, it will process:\n  * '/tmp/span-2/ver-2/train/data' as train split\n  * '/tmp/span-2/ver-2/eval/data' as eval split\n\n\nwith span number as '2' and version number as '2'. If later on '/tmp/span-2/ver-3/...' are ready, simply trigger the pipeline again and it will pick up span '2' and version '3' for processing. Below shows the code example for using version spec:\n```\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN}/ver-{VERSION}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN}/ver-{VERSION}/eval/*')\n            ])\nexample_gen = CsvExampleGen(input_base='/tmp', input_config=input)\n\n```\n\n### Range Config\nTFX supports retrieval and processing of a specific span in file-based ExampleGen using range config, an abstract config used to describe ranges for different TFX entities. To retrieve a specific span, set the `range_config` for a file-based ExampleGen component. For example, let's assume there are input data:\n  * '/tmp/span-01/train/data'\n  * '/tmp/span-01/eval/data'\n  * '/tmp/span-02/train/data'\n  * '/tmp/span-02/eval/data'\n\n\nTo specifically retrieve and process data with span '1', we specify a range config in addition to the input config. Note that ExampleGen only supports single-span static ranges (to specify processing of specific individual spans). Thus, for StaticRange, start_span_number must equal end_span_number. Using the provided span, and the span width information (if provided) for zero-padding, ExampleGen will replace the SPAN spec in the provided split patterns with the desired span number. An example of usage is shown below:\n```\n# In cases where files have zero-padding, the width modifier in SPAN spec is\n# required so TFX can correctly substitute spec with zero-padded span number.\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='span-{SPAN:2}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='span-{SPAN:2}/eval/*')\n            ])\n# Specify the span number to be processed here using StaticRange.\nrange = proto.RangeConfig(\n                static_range=proto.StaticRange(\n                        start_span_number=1, end_span_number=1)\n            )\n\n# After substitution, the train and eval split patterns will be\n# 'input_dir/span-01/train/*' and 'input_dir/span-01/eval/*', respectively.\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input,\n                            range_config=range)\n\n```\n\nRange config can also be used to process specific dates, if the date spec is used instead of SPAN spec. For example, let's assume there are input data organized by calendar date:\n  * '/tmp/1970-01-02/train/data'\n  * '/tmp/1970-01-02/eval/data'\n  * '/tmp/1970-01-03/train/data'\n  * '/tmp/1970-01-03/eval/data'\n\n\nTo specifically retrieve and process data on January 2nd, 1970, we do the following:\n```\nfromtfx.components.example_genimport utils\n\ninput = proto.Input(splits=[\n                proto.Input.Split(name='train',\n                                            pattern='{YYYY}-{MM}-{DD}/train/*'),\n                proto.Input.Split(name='eval',\n                                            pattern='{YYYY}-{MM}-{DD}/eval/*')\n            ])\n# Specify date to be converted to span number to be processed using StaticRange.\nspan = utils.date_to_span_number(1970, 1, 2)\nrange = proto.RangeConfig(\n                static_range=range_config_pb2.StaticRange(\n                        start_span_number=span, end_span_number=span)\n            )\n\n# After substitution, the train and eval split patterns will be\n# 'input_dir/1970-01-02/train/*' and 'input_dir/1970-01-02/eval/*',\n# respectively.\nexample_gen = CsvExampleGen(input_base=input_dir, input_config=input,\n                            range_config=range)\n\n```\n\n## Custom ExampleGen\nIf the currently available ExampleGen components don't fit your needs, you can create a custom ExampleGen, which will enable you to read from different data sources or in different data formats.\n### File-Based ExampleGen Customization (Experimental)\nFirst, extend BaseExampleGenExecutor with a custom Beam PTransform, which provides the conversion from your train/eval input split to TF examples. For example, the \nThen, create a component with above executor, as done in \n```\nfromtfx.components.baseimport executor_spec\nfromtfx.components.example_gen.csv_example_genimport executor\n\nexample_gen = FileBasedExampleGen(\n    input_base=os.path.join(base_dir, 'data/simple'),\n    custom_executor_spec=executor_spec.ExecutorClassSpec(executor.Executor))\n\n```\n\nNow, we also support reading Avro and Parquet files using this \n### Additional Data Formats\nApache Beam supports reading a number of \n```\n  return (pipeline\n          | 'ReadFromAvro' >> beam.io.ReadFromAvro(avro_pattern)\n          | 'ToTFExample' >> beam.Map(utils.dict_to_example))\n\n```\n\nAs of this writing the currently supported formats and data sources for the Beam Python SDK include:\n  * Amazon S3\n  * Apache Avro\n  * Apache Hadoop\n  * Apache Kafka\n  * Apache Parquet\n  * Google Cloud BigQuery\n  * Google Cloud BigTable\n  * Google Cloud Datastore\n  * Google Cloud Pub/Sub\n  * Google Cloud Storage (GCS)\n  * MongoDB\n\n\nCheck the \n### Query-Based ExampleGen Customization (Experimental)\nFirst, extend BaseExampleGenExecutor with a custom Beam PTransform, which reads from the external data source. Then, create a simple component by extending QueryBasedExampleGen.\nThis may or may not require additional connection configurations. For example, the \nIf a connection configuration is required for a custom ExampleGen component, create a new protobuf and pass it in through custom_config, which is now an optional execution parameter. Below is an example of how to use a configured component.\n```\nfromtfx.examples.custom_components.presto_example_gen.protoimport presto_config_pb2\nfromtfx.examples.custom_components.presto_example_gen.presto_component.componentimport PrestoExampleGen\n\npresto_config = presto_config_pb2.PrestoConnConfig(host='localhost', port=8080)\nexample_gen = PrestoExampleGen(presto_config, query='SELECT * FROM chicago_taxi_trips')\n\n```\n\n## ExampleGen Downstream Components\nCustom split configuration is supported for downstream components.\n### StatisticsGen\nDefault behavior is to perform stats generation for all splits.\nTo exclude any splits, set the `exclude_splits` for StatisticsGen component. For example:\n```\n# Exclude the 'eval' split.\nstatistics_gen = StatisticsGen(\n             examples=example_gen.outputs['examples'],\n             exclude_splits=['eval'])\n\n```\n\n### SchemaGen\nDefault behavior is to generate a schema based on all splits.\nTo exclude any splits, set the `exclude_splits` for SchemaGen component. For example:\n```\n# Exclude the 'eval' split.\nschema_gen = SchemaGen(\n             statistics=statistics_gen.outputs['statistics'],\n             exclude_splits=['eval'])\n\n```\n\n### ExampleValidator\nDefault behavior is to validate the statistics of all splits on input examples against a schema.\nTo exclude any splits, set the `exclude_splits` for ExampleValidator component. For example:\n```\n# Exclude the 'eval' split.\nexample_validator = ExampleValidator(\n             statistics=statistics_gen.outputs['statistics'],\n             schema=schema_gen.outputs['schema'],\n             exclude_splits=['eval'])\n\n```\n\n### Transform\nDefault behavior is analyze and produce the metadata from the 'train' split and transform all splits.\nTo specify the analyze splits and transform splits, set the `splits_config` for Transform component. For example:\n```\n# Analyze the 'train' split and transform all splits.\ntransform = Transform(\n      examples=example_gen.outputs['examples'],\n      schema=schema_gen.outputs['schema'],\n      module_file=_taxi_module_file,\n      splits_config=proto.SplitsConfig(analyze=['train'],\n                                               transform=['train', 'eval']))\n\n```\n\n### Trainer and Tuner\nDefault behavior is train on the 'train' split and evaluate on the 'eval' split.\nTo specify the train splits and evaluate splits, set the `train_args` and `eval_args` for Trainer component. For example:\n```\n# Train on the 'train' split and evaluate on the 'eval' split.\nTrainer = Trainer(\n      module_file=_taxi_module_file,\n      examples=transform.outputs['transformed_examples'],\n      schema=schema_gen.outputs['schema'],\n      transform_graph=transform.outputs['transform_graph'],\n      train_args=proto.TrainArgs(splits=['train'], num_steps=10000),\n      eval_args=proto.EvalArgs(splits=['eval'], num_steps=5000))\n\n```\n\n### Evaluator\nDefault behavior is provide metrics computed on the 'eval' split.\nTo compute evaluation statistics on custom splits, set the `example_splits` for Evaluator component. For example:\n```\n# Compute metrics on the 'eval1' split and the 'eval2' split.\nevaluator = Evaluator(\n      examples=example_gen.outputs['examples'],\n      model=trainer.outputs['model'],\n      example_splits=['eval1', 'eval2'])\n\n```\n\nMore details are available in the [CsvExampleGen API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/CsvExampleGen), [ImportExampleGen API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ImportExampleGen).\n",
  "https://www.tensorflow.org/tfx/guide/kubeflow": "## Kubeflow Pipelines\nSee the [TFX example on Kubeflow Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines) for details on running TFX at scale on Google cloud.\n",
  "https://www.tensorflow.org/tfx/guide/exampleval": "The ExampleValidator pipeline component identifies anomalies in training and serving data. It can detect different classes of anomalies in the data. For example it can:\n  1. perform validity checks by comparing data statistics against a schema that codifies expectations of the user.\n  2. detect training-serving skew by comparing training and serving data.\n  3. detect data drift by looking at a series of data.\n  4. perform \n\n\nThe ExampleValidator pipeline component identifies any anomalies in the example data by comparing data statistics computed by the StatisticsGen pipeline component against a schema. The inferred schema codifies properties which the input data is expected to satisfy, and can be modified by the developer.\n  * Consumes: A schema from a SchemaGen component, and statistics from a StatisticsGen component.\n  * Emits: Validation results\n\n\n## ExampleValidator and TensorFlow Data Validation\nExampleValidator makes extensive use of [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) for validating your input data.\n## Using the ExampleValidator Component\nAn ExampleValidator pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:\n```\nvalidate_stats = ExampleValidator(\n      statistics=statistics_gen.outputs['statistics'],\n      schema=schema_gen.outputs['schema']\n      )\n\n```\n\nMore details are available in the [ExampleValidator API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ExampleValidator).\n",
  "https://www.tensorflow.org/tfx/guide/infra_validator": "InfraValidator is a TFX component that is used as an early warning layer before pushing a model into production. The name \"infra\" validator came from the fact that it is validating the model in the actual model serving \"infrastructure\". If [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) is to guarantee the performance of the model, InfraValidator is to guarantee the model is mechanically fine and prevents bad models from being pushed.\n## How does it work?\nInfraValidator takes the model, launches a sand-boxed model server with the model, and sees if it can be successfully loaded and optionally queried. The infra validation result will be generated in the `blessing` output in the same way as [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) does.\nInfraValidator focuses on the compatibility between the model server binary (e.g. [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)) and the model to deploy. Despite the name \"infra\" validator, it is the **user's responsibility** to configure the environment correctly, and infra validator only interacts with the model server in the user-configured environment to see if it works fine. Configuring this environment correctly will ensure that infra validation passing or failing will be indicative of whether the model would be servable in the production serving environment. This implies some of, but is not limited to, the following:\n  1. InfraValidator is using the same model server binary as will be used in production. This is the minimal level to which the infra validation environment must converge.\n  2. InfraValidator is using the same resources (e.g. allocation quantity and type of CPU, memory, and accelerators) as will be used in production.\n  3. InfraValidator is using the same model server configuration as will be used in production.\n\n\nDepending on the situation, users can choose to what degree InfraValidator should be identical to the production environment. Technically, a model can be infra validated in a local Docker environment and then served in a completely different environment (e.g. Kubernetes cluster) without a problem. However, InfraValidator will not have checked for this divergence.\n### Operation mode\nDepending on the configuration, infra validation is done in one of the following modes:\n  * `LOAD_ONLY` mode: checking whether the model was successfully loaded in the serving infrastructure or not. **OR**\n  * `LOAD_AND_QUERY` mode: `LOAD_ONLY` mode plus sending some sample requests to check if model is capable of serving inferences. InfraValidator does not care the prediction was correct or not. Only whether the request was successful or not matters.\n\n\n## How do I use it?\nUsually InfraValidator is defined next to an Evaluator component, and its output is fed to a Pusher. If InfraValidator fails, the model will not be pushed.\n```\nevaluator = Evaluator(\n    model=trainer.outputs['model'],\n    examples=example_gen.outputs['examples'],\n    baseline_model=model_resolver.outputs['model'],\n    eval_config=tfx.proto.EvalConfig(...)\n)\n\ninfra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(...)\n)\n\npusher = Pusher(\n    model=trainer.outputs['model'],\n    model_blessing=evaluator.outputs['blessing'],\n    infra_blessing=infra_validator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(...)\n)\n\n```\n\n### Configuring an InfraValidator component.\nThere are three kinds of protos to configure InfraValidator.\n#### `ServingSpec`\n`ServingSpec` is the most crucial configuration for the InfraValidator. It defines:\n  * _what_ type of model server to run\n  * _where_ to run it\n\n\nFor model server types (called serving binary) we support\n  * [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving)\n\n\nFollowing serving platforms are currently supported:\n  * Local Docker (Docker should be installed in advance)\n  * Kubernetes (limited support for KubeflowDagRunner only)\n\n\nThe choice for serving binary and serving platform are made by specifying a `ServingSpec`. For example to use TensorFlow Serving binary running on the Kubernetes cluster, `tensorflow_serving` and `kubernetes` field should be set.\n```\ninfra_validator=InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(\n        tensorflow_serving=tfx.proto.TensorFlowServing(\n            tags=['latest']\n        ),\n        kubernetes=tfx.proto.KubernetesConfig()\n    )\n)\n\n```\n\nTo further configure `ServingSpec`, please check out the \n#### `ValidationSpec`\nOptional configuration to adjust the infra validation criteria or workflow.\n```\ninfra_validator=InfraValidator(\n    model=trainer.outputs['model'],\n    serving_spec=tfx.proto.ServingSpec(...),\n    validation_spec=tfx.proto.ValidationSpec(\n        # How much time to wait for model to load before automatically making\n        # validation fail.\n        max_loading_time_seconds=60,\n        # How many times to retry if infra validation fails.\n        num_tries=3\n    )\n)\n\n```\n\nAll ValidationSpec fields have a sound default value. Check more detail from the \n#### `RequestSpec`\nOptional configuration to specify how to build sample requests when running infra validation in `LOAD_AND_QUERY` mode. In order to use `LOAD_AND_QUERY` mode, it is required to specify both `request_spec` execution properties as well as `examples` input channel in the component definition.\n```\ninfra_validator = InfraValidator(\n    model=trainer.outputs['model'],\n    # This is the source for the data that will be used to build a request.\n    examples=example_gen.outputs['examples'],\n    serving_spec=tfx.proto.ServingSpec(\n        # Depending on what kind of model server you're using, RequestSpec\n        # should specify the compatible one.\n        tensorflow_serving=tfx.proto.TensorFlowServing(tags=['latest']),\n        local_docker=tfx.proto.LocalDockerConfig(),\n    ),\n    request_spec=tfx.proto.RequestSpec(\n        # InfraValidator will look at how \"classification\" signature is defined\n        # in the model, and automatically convert some samples from `examples`\n        # artifact to prediction RPC requests.\n        tensorflow_serving=tfx.proto.TensorFlowServingRequestSpec(\n            signature_names=['classification']\n        ),\n        num_examples=10  # How many requests to make.\n    )\n)\n\n```\n\n### Producing a SavedModel with warmup\n(From version 0.30.0)\nSince InfraValidator validates model with real requests, it can easily reuse these validation requests as [warmup requests](https://www.tensorflow.org/tfx/serving/saved_model_warmup) of a SavedModel. InfraValidator provides an option ([`RequestSpec.make_warmup`](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/proto/RequestSpec#make_warmup)) to export a SavedModel with warmup.\n```\ninfra_validator = InfraValidator(\n    ...,\n    request_spec=tfx.proto.RequestSpec(..., make_warmup=True)\n)\n\n```\n\nThen the output `InfraBlessing` artifact will contain a SavedModel with warmup, and can also be pushed by the [Pusher](https://www.tensorflow.org/tfx/guide/pusher), just like `Model` artifact.\n## Limitations\nCurrent InfraValidator is not complete yet, and has some limitations.\n  * Only TensorFlow [SavedModel](https://www.tensorflow.org/guide/saved_model) model format can be validated.\n  * When running TFX on Kubernetes, the pipeline should be executed by `KubeflowDagRunner` inside Kubeflow Pipelines. The model server will be launched in the same Kubernetes cluster and the namespace that Kubeflow is using.\n  * InfraValidator is primarily focused on deployments to [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving), and while still useful it is less accurate for deployments to [TensorFlow Lite](https://www.tensorflow.org/lite) and [TensorFlow.js](https://www.tensorflow.org/js), or other inference frameworks.\n  * There's a limited support on `LOAD_AND_QUERY` mode for the [Predict](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/predict_signature_def) method signature (which is the only exportable method in TensorFlow 2). InfraValidator requires the Predict signature to consume a serialized [`tf.Example`](https://www.tensorflow.org/tutorials/load_data/tfrecord#tfexample) as the only input.\n```\n@tf.function\ndefparse_and_run(serialized_example):\n  features = tf.io.parse_example(serialized_example, FEATURES)\n  return model(features)\n\nmodel.save('path/to/save', signatures={\n  # This exports \"Predict\" method signature under name \"serving_default\".\n  'serving_default': parse_and_run.get_concrete_function(\n      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples'))\n})\n\n```\n\n    * Check out an \n\n\n",
  "https://www.tensorflow.org/tfx/guide/mlmd": "[TensorFlow Extended (TFX)](https://www.tensorflow.org/tfx), but is designed so that it can be used independently.\nEvery run of a production ML pipeline generates metadata containing information about the various pipeline components, their executions (e.g. training runs), and resulting artifacts (e.g. trained models). In the event of unexpected pipeline behavior or errors, this metadata can be leveraged to analyze the lineage of pipeline components and debug issues. Think of this metadata as the equivalent of logging in software development.\nMLMD helps you understand and analyze all the interconnected parts of your ML pipeline instead of analyzing them in isolation and can help you answer questions about your ML pipeline such as:\n  * Which dataset did the model train on?\n  * What were the hyperparameters used to train the model?\n  * Which pipeline run created the model?\n  * Which training run led to this model?\n  * Which version of TensorFlow created this model?\n  * When was the failed model pushed?\n\n\n## Metadata store\nMLMD registers the following types of metadata in a database called the **Metadata Store**.\n  1. Metadata about the artifacts generated through the components/steps of your ML pipelines\n  2. Metadata about the executions of these components/steps\n  3. Metadata about pipelines and associated lineage information\n\n\nThe Metadata Store provides APIs to record and retrieve metadata to and from the storage backend. The storage backend is pluggable and can be extended. MLMD provides reference implementations for SQLite (which supports in-memory and disk) and MySQL out of the box.\nThis graphic shows a high-level overview of the various components that are part of MLMD.\n### Metadata storage backends and store connection configuration\nThe `MetadataStore` object receives a connection configuration that corresponds to the storage backend used.\n  * **Fake Database** provides an in-memory DB (using SQLite) for fast experimentation and local runs. The database is deleted when the store object is destroyed.\n\n```\nimportml_metadataasmlmd\nfromml_metadata.metadata_storeimport metadata_store\nfromml_metadata.protoimport metadata_store_pb2\n\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.fake_database.SetInParent() # Sets an empty fake database proto.\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\n  * **SQLite** reads and writes files from disk.\n\n```\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.sqlite.filename_uri = '...'\nconnection_config.sqlite.connection_mode = 3 # READWRITE_OPENCREATE\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\n  * **MySQL** connects to a MySQL server.\n\n```\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.mysql.host = '...'\nconnection_config.mysql.port = '...'\nconnection_config.mysql.database = '...'\nconnection_config.mysql.user = '...'\nconnection_config.mysql.password = '...'\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\nSimilarly, when using a MySQL instance with Google CloudSQL (\n```\nconnection_config.mysql.ssl_options.key = '...'\nconnection_config.mysql.ssl_options.cert = '...'\nconnection_config.mysql.ssl_options.ca = '...'\nconnection_config.mysql.ssl_options.capath = '...'\nconnection_config.mysql.ssl_options.cipher = '...'\nconnection_config.mysql.ssl_options.verify_server_cert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\n  * **PostgreSQL** connects to a PostgreSQL server.\n\n```\nconnection_config = metadata_store_pb2.ConnectionConfig()\nconnection_config.postgresql.host = '...'\nconnection_config.postgresql.port = '...'\nconnection_config.postgresql.user = '...'\nconnection_config.postgresql.password = '...'\nconnection_config.postgresql.dbname = '...'\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\nSimilarly, when using a PostgreSQL instance with Google CloudSQL (\n```\nconnection_config.postgresql.ssloption.sslmode = '...' # disable, allow, verify-ca, verify-full, etc.\nconnection_config.postgresql.ssloption.sslcert = '...'\nconnection_config.postgresql.ssloption.sslkey = '...'\nconnection_config.postgresql.ssloption.sslpassword = '...'\nconnection_config.postgresql.ssloption.sslrootcert = '...'\nstore = metadata_store.MetadataStore(connection_config)\n\n```\n\n## Data model\nThe Metadata Store uses the following data model to record and retrieve metadata from the storage backend.\n  * `ArtifactType` describes an artifact's type and its properties that are stored in the metadata store. You can register these types on-the-fly with the metadata store in code, or you can load them in the store from a serialized format. Once you register a type, its definition is available throughout the lifetime of the store.\n  * An `Artifact` describes a specific instance of an `ArtifactType`, and its properties that are written to the metadata store.\n  * An `ExecutionType` describes a type of component or step in a workflow, and its runtime parameters.\n  * An `Execution` is a record of a component run or a step in an ML workflow and the runtime parameters. An execution can be thought of as an instance of an `ExecutionType`. Executions are recorded when you run an ML pipeline or step.\n  * An `Event` is a record of the relationship between artifacts and executions. When an execution happens, events record every artifact that was used by the execution, and every artifact that was produced. These records allow for lineage tracking throughout a workflow. By looking at all events, MLMD knows what executions happened and what artifacts were created as a result. MLMD can then recurse back from any artifact to all of its upstream inputs.\n  * A `ContextType` describes a type of conceptual group of artifacts and executions in a workflow, and its structural properties. For example: projects, pipeline runs, experiments, owners etc.\n  * A `Context` is an instance of a `ContextType`. It captures the shared information within the group. For example: project name, changelist commit id, experiment annotations etc. It has a user-defined unique name within its `ContextType`.\n  * An `Attribution` is a record of the relationship between artifacts and contexts.\n  * An `Association` is a record of the relationship between executions and contexts.\n\n\n## MLMD Functionality\nTracking the inputs and outputs of all components/steps in an ML workflow and their lineage allows ML platforms to enable several important features. The following list provides a non-exhaustive overview of some of the major benefits.\n  * **List all Artifacts of a specific type.** Example: all Models that have been trained.\n  * **Load two Artifacts of the same type for comparison.** Example: compare results from two experiments.\n  * **Show a DAG of all related executions and their input and output artifacts of a context.** Example: visualize the workflow of an experiment for debugging and discovery.\n  * **Recurse back through all events to see how an artifact was created.** Examples: see what data went into a model; enforce data retention plans.\n  * **Identify all artifacts that were created using a given artifact.** Examples: see all Models trained from a specific dataset; mark models based upon bad data.\n  * **Determine if an execution has been run on the same inputs before.** Example: determine whether a component/step has already completed the same work and the previous output can just be reused.\n  * **Record and query context of workflow runs.** Examples: track the owner and changelist used for a workflow run; group the lineage by experiments; manage artifacts by projects.\n  * **Declarative nodes filtering capabilities on properties and 1-hop neighborhood nodes.** Examples: look for artifacts of a type and under some pipeline context; return typed artifacts where a given property’s value is within a range; find previous executions in a context with the same inputs.\n\n\nSee the [MLMD tutorial](https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial) for an example that shows you how to use the MLMD API and the metadata store to retrieve lineage information.\n### Integrate ML Metadata into your ML Workflows\nIf you are a platform developer interested in integrating MLMD into your system, use the example workflow below to use the low-level MLMD APIs to track the execution of a training task. You can also use higher-level Python APIs in notebook environments to record experiment metadata.\n1) Register artifact types\n```\n# Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\ndata_type_id = store.put_artifact_type(data_type)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\nmodel_type_id = store.put_artifact_type(model_type)\n\n# Query all registered Artifact types.\nartifact_types = store.get_artifact_types()\n\n```\n\n2) Register execution types for all steps in the ML workflow\n```\n# Create an ExecutionType, e.g., Trainer\ntrainer_type = metadata_store_pb2.ExecutionType()\ntrainer_type.name = \"Trainer\"\ntrainer_type.properties[\"state\"] = metadata_store_pb2.STRING\ntrainer_type_id = store.put_execution_type(trainer_type)\n\n# Query a registered Execution type with the returned id\n[registered_type] = store.get_execution_types_by_id([trainer_type_id])\n\n```\n\n3) Create an artifact of DataSet ArtifactType\n```\n# Create an input artifact of type DataSet\ndata_artifact = metadata_store_pb2.Artifact()\ndata_artifact.uri = 'path/to/data'\ndata_artifact.properties[\"day\"].int_value = 1\ndata_artifact.properties[\"split\"].string_value = 'train'\ndata_artifact.type_id = data_type_id\n[data_artifact_id] = store.put_artifacts([data_artifact])\n\n# Query all registered Artifacts\nartifacts = store.get_artifacts()\n\n# Plus, there are many ways to query the same Artifact\n[stored_data_artifact] = store.get_artifacts_by_id([data_artifact_id])\nartifacts_with_uri = store.get_artifacts_by_uri(data_artifact.uri)\nartifacts_with_conditions = store.get_artifacts(\n      list_options=mlmd.ListOptions(\n          filter_query='uri LIKE \"%/data\" AND properties.day.int_value > 0'))\n\n```\n\n4) Create an execution of the Trainer run\n```\n# Register the Execution of a Trainer run\ntrainer_run = metadata_store_pb2.Execution()\ntrainer_run.type_id = trainer_type_id\ntrainer_run.properties[\"state\"].string_value = \"RUNNING\"\n[run_id] = store.put_executions([trainer_run])\n\n# Query all registered Execution\nexecutions = store.get_executions_by_id([run_id])\n# Similarly, the same execution can be queried with conditions.\nexecutions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query='type = \"Trainer\" AND properties.state.string_value IS NOT NULL'))\n\n```\n\n5) Define the input event and read data\n```\n# Define the input event\ninput_event = metadata_store_pb2.Event()\ninput_event.artifact_id = data_artifact_id\ninput_event.execution_id = run_id\ninput_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n\n# Record the input event in the metadata store\nstore.put_events([input_event])\n\n```\n\n6) Declare the output artifact\n```\n# Declare the output artifact of type SavedModel\nmodel_artifact = metadata_store_pb2.Artifact()\nmodel_artifact.uri = 'path/to/model/file'\nmodel_artifact.properties[\"version\"].int_value = 1\nmodel_artifact.properties[\"name\"].string_value = 'MNIST-v1'\nmodel_artifact.type_id = model_type_id\n[model_artifact_id] = store.put_artifacts([model_artifact])\n\n```\n\n7) Record the output event\n```\n# Declare the output event\noutput_event = metadata_store_pb2.Event()\noutput_event.artifact_id = model_artifact_id\noutput_event.execution_id = run_id\noutput_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n\n# Submit output event to the Metadata Store\nstore.put_events([output_event])\n\n```\n\n8) Mark the execution as completed\n```\ntrainer_run.id = run_id\ntrainer_run.properties[\"state\"].string_value = \"COMPLETED\"\nstore.put_executions([trainer_run])\n\n```\n\n9) Group artifacts and executions under a context using attributions and assertions artifacts\n```\n# Create a ContextType, e.g., Experiment with a note property\nexperiment_type = metadata_store_pb2.ContextType()\nexperiment_type.name = \"Experiment\"\nexperiment_type.properties[\"note\"] = metadata_store_pb2.STRING\nexperiment_type_id = store.put_context_type(experiment_type)\n\n# Group the model and the trainer run to an experiment.\nmy_experiment = metadata_store_pb2.Context()\nmy_experiment.type_id = experiment_type_id\n# Give the experiment a name\nmy_experiment.name = \"exp1\"\nmy_experiment.properties[\"note\"].string_value = \"My first experiment.\"\n[experiment_id] = store.put_contexts([my_experiment])\n\nattribution = metadata_store_pb2.Attribution()\nattribution.artifact_id = model_artifact_id\nattribution.context_id = experiment_id\n\nassociation = metadata_store_pb2.Association()\nassociation.execution_id = run_id\nassociation.context_id = experiment_id\n\nstore.put_attributions_and_associations([attribution], [association])\n\n# Query the Artifacts and Executions that are linked to the Context.\nexperiment_artifacts = store.get_artifacts_by_context(experiment_id)\nexperiment_executions = store.get_executions_by_context(experiment_id)\n\n# You can also use neighborhood queries to fetch these artifacts and executions\n# with conditions.\nexperiment_artifacts_with_conditions = store.get_artifacts(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.type = \"Experiment\" AND contexts_a.name = \"exp1\"')))\nexperiment_executions_with_conditions = store.get_executions(\n    list_options = mlmd.ListOptions(\n        filter_query=('contexts_a.id = {}'.format(experiment_id))))\n\n```\n\n## Use MLMD with a remote gRPC server\nYou can use MLMD with remote gRPC servers as shown below:\n  * Start a server\n\n```\nbazelgrpc_no_ares=true\n```\n\nBy default, the server uses a fake in-memory db per request and does not persist the metadata across calls. It can also be configured with a MLMD `MetadataStoreServerConfig` to use SQLite files or MySQL instances. The config can be stored in a text protobuf file and passed to the binary with `--metadata_store_server_config_file=path_to_the_config_file`.\nAn example `MetadataStoreServerConfig` file in text protobuf format:\n```\nconnection_config {\n  sqlite {\n    filename_uri: '/tmp/test_db'\n    connection_mode: READWRITE_OPENCREATE\n  }\n}\n\n```\n\n  * Create the client stub and use it in Python\n\n```\nfromgrpcimport insecure_channel\nfromml_metadata.protoimport metadata_store_pb2\nfromml_metadata.protoimport metadata_store_service_pb2\nfromml_metadata.protoimport metadata_store_service_pb2_grpc\n\nchannel = insecure_channel('localhost:8080')\nstub = metadata_store_service_pb2_grpc.MetadataStoreServiceStub(channel)\n\n```\n\n  * Use MLMD with RPC calls\n\n```\n# Create ArtifactTypes, e.g., Data and Model\ndata_type = metadata_store_pb2.ArtifactType()\ndata_type.name = \"DataSet\"\ndata_type.properties[\"day\"] = metadata_store_pb2.INT\ndata_type.properties[\"split\"] = metadata_store_pb2.STRING\n\nrequest = metadata_store_service_pb2.PutArtifactTypeRequest()\nrequest.all_fields_match = True\nrequest.artifact_type.CopyFrom(data_type)\nstub.PutArtifactType(request)\n\nmodel_type = metadata_store_pb2.ArtifactType()\nmodel_type.name = \"SavedModel\"\nmodel_type.properties[\"version\"] = metadata_store_pb2.INT\nmodel_type.properties[\"name\"] = metadata_store_pb2.STRING\n\nrequest.artifact_type.CopyFrom(model_type)\nstub.PutArtifactType(request)\n\n```\n\n## Resources\nThe MLMD library has a high-level API that you can readily use with your ML pipelines. See the [MLMD API documentation](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd) for more details.\nCheck out \nAlso check out the [MLMD tutorial](https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial) to learn how to use MLMD to trace the lineage of your pipeline components.\nMLMD provides utilities to handle schema and data migrations across releases. See the MLMD \n",
  "https://www.tensorflow.org/tfx/guide/schemagen": "Some TFX components use a description of your input data called a _schema_. The schema is an instance of \n  * Consumes: statistics from a StatisticsGen component\n  * Emits: Data schema proto\n\n\nHere's an excerpt from a schema proto:\n```\n...\nfeature{\nname:\"age\"\nvalue_count{\nmin:1\nmax:1\n}\ntype:FLOAT\npresence{\nmin_fraction:1\nmin_count:1\n}\n}\nfeature{\nname:\"capital-gain\"\nvalue_count{\nmin:1\nmax:1\n}\ntype:FLOAT\npresence{\nmin_fraction:1\nmin_count:1\n}\n}\n...\n\n```\n\nThe following TFX libraries use the schema:\n  * TensorFlow Data Validation\n  * TensorFlow Transform\n  * TensorFlow Model Analysis\n\n\nIn a typical TFX pipeline SchemaGen generates a schema, which is consumed by the other pipeline components. However, the auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that developers review and modify it as needed.\nThe modified schema can be brought back into the pipeline using ImportSchemaGen component. The SchemaGen component for the initial schema generation can be removed and all downstream components can use the output of ImportSchemaGen. It is also recommended to add [ExampleValidator](https://www.tensorflow.org/tfx/guide/exampleval) using the imported schema to examine the training data continuously.\n## SchemaGen and TensorFlow Data Validation\nSchemaGen makes extensive use of [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) for inferring a schema.\n## Using the SchemaGen Component\n### For the initial schema generation\nA SchemaGen pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:\n```\nschema_gen = tfx.components.SchemaGen(\n    statistics=stats_gen.outputs['statistics'])\n\n```\n\nMore details are available in the [SchemaGen API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/SchemaGen).\n### For the reviewed schema import\nAdd ImportSchemaGen component to the pipeline to bring the reviewed schema definition into the pipeline.\n```\nschema_gen = tfx.components.ImportSchemaGen(\n    schema_file='/some/path/schema.pbtxt')\n\n```\n\nThe `schema_file` should be a full path to the text protobuf file.\nMore details are available in the [ImportSchemaGen API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/ImportSchemaGen).\n",
  "https://www.tensorflow.org/tfx/guide/pusher": "The Pusher component is used to push a validated model to a [deployment target](https://www.tensorflow.org/tfx/guide#deployment_targets) during model training or re-training. Before the deployment, Pusher relies on one or more blessings from other validation components to decide whether to push the model or not.\n  * [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) blesses the model if the new trained model is \"good enough\" to be pushed to production.\n  * (Optional but recommended) [InfraValidator](https://www.tensorflow.org/tfx/guide/infra_validator) blesses the model if the model is mechanically servable in a production environment.\n\n\nA Pusher component consumes a trained model in [SavedModel](https://www.tensorflow.org/guide/saved_model) format, and produces the same SavedModel, along with versioning metadata.\n## Using the Pusher Component\nA Pusher pipeline component is typically very easy to deploy and requires little customization, since all of the work is done by the Pusher TFX component. Typical code looks like this:\n```\npusher = Pusher(\n  model=trainer.outputs['model'],\n  model_blessing=evaluator.outputs['blessing'],\n  infra_blessing=infra_validator.outputs['blessing'],\n  push_destination=tfx.proto.PushDestination(\n    filesystem=tfx.proto.PushDestination.Filesystem(\n        base_directory=serving_model_dir)\n  )\n)\n\n```\n\n### Pushing a model produced from InfraValidator.\n(From version 0.30.0)\nInfraValidator can also produce `InfraBlessing` artifact containing a [model with warmup](https://www.tensorflow.org/tfx/guide/infra_validator#producing_a_savedmodel_with_warmup), and Pusher can push it just like a `Model` artifact.\n```\ninfra_validator = InfraValidator(\n    ...,\n    # make_warmup=True will produce a model with warmup requests in its\n    # 'blessing' output.\n    request_spec=tfx.proto.RequestSpec(..., make_warmup=True)\n)\n\npusher = Pusher(\n    # Push model from 'infra_blessing' input.\n    infra_blessing=infra_validator.outputs['blessing'],\n    push_destination=tfx.proto.PushDestination(...)\n)\n\n```\n\nMore details are available in the [Pusher API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Pusher).\n",
  "https://www.tensorflow.org/tfx/guide/serving": "## Introduction\nTensorFlow Serving is a flexible, high-performance serving system for machine learning models, designed for production environments. TensorFlow Serving makes it easy to deploy new algorithms and experiments, while keeping the same server architecture and APIs. TensorFlow Serving provides out-of-the-box integration with TensorFlow models, but can be easily extended to serve other types of models and data.\nDetailed developer documentation on TensorFlow Serving is available:\n  * [Architecture Overview](https://www.tensorflow.org/tfx/serving/architecture)\n\n\n",
  "https://www.tensorflow.org/tfx/guide/tfma": "## Introduction\nAs you tweak your model during development, you need to check whether your changes are improving your model. Just checking accuracy may not be enough. For example, if you have a classifier for a problem in which 95% of your instances are positive, you may be able to improve accuracy by simply always predicting positive, but you won't have a very robust classifier.\n## Overview\nThe goal of TensorFlow Model Analysis is to provide a mechanism for model evaluation in TFX. TensorFlow Model Analysis allows you to perform model evaluations in the TFX pipeline, and view resultant metrics and plots in a Jupyter notebook. Specifically, it can provide:\n  * [Metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) computed on entire training and holdout dataset, as well as next-day evaluations\n  * Tracking metrics over time\n  * Model quality performance on different feature slices\n  * [Model validation](https://www.tensorflow.org/tfx/model_analysis/model_validations) for ensuring that model's maintain consistent performance\n\n\n## Next Steps\nTry our [TFMA tutorial](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic).\nCheck out our [metrics and plots](https://www.tensorflow.org/tfx/model_analysis/metrics) and associated notebook [visualizations](https://www.tensorflow.org/tfx/model_analysis/visualizations).\nSee the [installation](https://www.tensorflow.org/tfx/model_analysis/install) and [getting started](https://www.tensorflow.org/tfx/model_analysis/get_started) guides for information and examples on how to get [set up](https://www.tensorflow.org/tfx/model_analysis/setup) in a standalone pipeline. Recall that TFMA is also used within the [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) component in TFX, so these resources will be useful for getting started in TFX as well.\n",
  "https://www.tensorflow.org/tfx/guide/tfdv": "Once your data is in a TFX pipeline, you can use TFX components to analyze and transform it. You can use these tools even before you train a model.\nThere are many reasons to analyze and transform your data:\n  * To find problems in your data. Common problems include: \n    * Missing data, such as features with empty values.\n    * Labels treated as features, so that your model gets to peek at the right answer during training.\n    * Features with values outside the range you expect.\n    * Data anomalies.\n    * Transfer learned model has preprocessing that does not match the training data.\n  * To engineer more effective feature sets. For example, you can identify: \n    * Especially informative features.\n    * Redundant features.\n    * Features that vary so widely in scale that they may slow learning.\n    * Features with little or no unique predictive information.\n\n\nTFX tools can both help find data bugs, and help with feature engineering.\n## TensorFlow Data Validation\n  * [Schema Based Example Validation](https://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation)\n  * [Training-Serving Skew Detection](https://www.tensorflow.org/tfx/guide/tfdv#skewdetect)\n  * [Drift Detection](https://www.tensorflow.org/tfx/guide/tfdv#drift_detection)\n\n\n### Overview\nTensorFlow Data Validation identifies anomalies in training and serving data, and can automatically create a schema by examining the data. The component can be configured to detect different classes of anomalies in the data. It can\n  1. Perform validity checks by comparing data statistics against a schema that codifies expectations of the user.\n  2. Detect training-serving skew by comparing examples in training and serving data.\n  3. Detect data drift by looking at a series of data.\n\n\nWe document each of these functionalities independently:\n  * [Schema Based Example Validation](https://www.tensorflow.org/tfx/guide/tfdv#schema_based_example_validation)\n  * [Training-Serving Skew Detection](https://www.tensorflow.org/tfx/guide/tfdv#skewdetect)\n  * [Drift Detection](https://www.tensorflow.org/tfx/guide/tfdv#drift_detection)\n\n\n### Schema Based Example Validation\nTensorFlow Data Validation identifies any anomalies in the input data by comparing data statistics against a schema. The schema codifies properties which the input data is expected to satisfy, such as data types or categorical values, and can be modified or replaced by the user.\nTensorflow Data Validation is typically invoked multiple times within the context of the TFX pipeline: (i) for every split obtained from ExampleGen, (ii) for all pre-transformed data used by Transform and (iii) for all post-transform data generated by Transform. When invoked in the context of Transform (ii-iii), statistics options and schema-based constraints can be set by defining the [`stats_options_updater_fn`](https://www.tensorflow.org/tfx/guide/tft). This is particilarly useful when validating unstructured data (e.g. text features). See the \n#### Advanced Schema Features\nThis section covers more advanced schema configuration that can help with special setups.\n##### Sparse Features\nEncoding sparse features in Examples usually introduces multiple Features that are expected to have the same valency for all Examples. For example the sparse feature: \n```\n\nWeightedCategories = [('CategoryA', 0.3), ('CategoryX', 0.7)]\n\n```\nwould be encoded using separate Features for index and value: ```\n\nWeightedCategoriesIndex = ['CategoryA', 'CategoryX']\nWeightedCategoriesValue = [0.3, 0.7]\n\n```\nwith the restriction that the valency of the index and value feature should match for all Examples. This restriction can be made explicit in the schema by defining a sparse_feature: ```\n\nsparse_feature {\n  name: 'WeightedCategories'\n  index_feature { name: 'WeightedCategoriesIndex' }\n  value_feature { name: 'WeightedCategoriesValue' }\n}\n\n```\n\nThe sparse feature definition requires one or more index and one value feature which refer to features that exist in the schema. Explicitly defining sparse features enables TFDV to check that the valencies of all referred features match.\nSome use cases introduce similar valency restrictions between Features, but do not necessarily encode a sparse feature. Using sparse feature should unblock you, but is not ideal.\n##### Schema Environments\nBy default validations assume that all Examples in a pipeline adhere to a single schema. In some cases introducing slight schema variations is necessary, for instance features used as labels are required during training (and should be validated), but are missing during serving. Environments can be used to express such requirements, in particular `default_environment()`, `in_environment()`, `not_in_environment()`.\nFor example, assume a feature named 'LABEL' is required for training, but is expected to be missing from serving. This can be expressed by:\n  * Define two distinct environments in the schema: [\"SERVING\", \"TRAINING\"] and associate 'LABEL' only with environment \"TRAINING\".\n  * Associate the training data with environment \"TRAINING\" and the serving data with environment \"SERVING\".\n\n\n##### Schema Generation\nThe input data schema is specified as an instance of the TensorFlow \nInstead of constructing a schema manually from scratch, a developer can rely on TensorFlow Data Validation's automatic schema construction. Specifically, TensorFlow Data Validation automatically constructs an initial schema based on statistics computed over training data available in the pipeline. Users can simply review this autogenerated schema, modify it as needed, check it into a version control system, and push it explicitly into the pipeline for further validation.\nTFDV includes `infer_schema()` to generate a schema automatically. For example:\n```\nschema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n\n```\n\nThis triggers an automatic schema generation based on the following rules:\n  * If a schema has already been auto-generated then it is used as is.\n  * Otherwise, TensorFlow Data Validation examines the available data statistics and computes a suitable schema for the data.\n\n\n_Note: The auto-generated schema is best-effort and only tries to infer basic properties of the data. It is expected that users review and modify it as needed._\n### Training-Serving Skew Detection \n#### Overview\nTensorFlow Data Validation can detect distribution skew between training and serving data. Distribution skew occurs when the distribution of feature values for training data is significantly different from serving data. One of the key causes for distribution skew is using either a completely different corpus for training data generation to overcome lack of initial data in the desired corpus. Another reason is a faulty sampling mechanism that only chooses a subsample of the serving data to train on.\n##### Example Scenario\nSee the [TensorFlow Data Validation Get Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for information about configuring training-serving skew detection.\n### Drift Detection\nDrift detection is supported between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of \nSee the [TensorFlow Data Validation Get Started Guide](https://www.tensorflow.org/tfx/data_validation/get_started#checking_data_skew_and_drift) for information about configuring drift detection.\n## Using Visualizations to Check Your Data\nTensorFlow Data Validation provides tools for visualizing the distribution of feature values. By examining these distributions in a Jupyter notebook using \n### Identifying Suspicious Distributions\nYou can identify common bugs in your data by using a Facets Overview display to look for suspicious distributions of feature values.\n#### Unbalanced Data\nAn unbalanced feature is a feature for which one value predominates. Unbalanced features can occur naturally, but if a feature always has the same value you may have a data bug. To detect unbalanced features in a Facets Overview, choose \"Non-uniformity\" from the \"Sort by\" dropdown.\nThe most unbalanced features will be listed at the top of each feature-type list. For example, the following screenshot shows one feature that is all zeros, and a second that is highly unbalanced, at the top of the \"Numeric Features\" list:\n#### Uniformly Distributed Data\nA uniformly distributed feature is one for which all possible values appear with close to the same frequency. As with unbalanced data, this distribution can occur naturally, but can also be produced by data bugs.\nTo detect uniformly distributed features in a Facets Overview, choose \"Non- uniformity\" from the \"Sort by\" dropdown and check the \"Reverse order\" checkbox:\nString data is represented using bar charts if there are 20 or fewer unique values, and as a cumulative distribution graph if there are more than 20 unique values. So for string data, uniform distributions can appear as either flat bar graphs like the one above or straight lines like the one below:\n##### Bugs That Can Produce Uniformly Distributed Data\nHere are some common bugs that can produce uniformly distributed data:\n  * Using strings to represent non-string data types such as dates. For example, you will have many unique values for a datetime feature with representations like \"2017-03-01-11-45-03\". Unique values will be distributed uniformly.\n  * Including indices like \"row number\" as features. Here again you have many unique values.\n\n\n#### Missing Data\nTo check whether a feature is missing values entirely:\n  1. Choose \"Amount missing/zero\" from the \"Sort by\" drop-down.\n  2. Check the \"Reverse order\" checkbox.\n  3. Look at the \"missing\" column to see the percentage of instances with missing values for a feature.\n\n\nA data bug can also cause incomplete feature values. For example you may expect a feature's value list to always have three elements and discover that sometimes it only has one. To check for incomplete values or other cases where feature value lists don't have the expected number of elements:\n  1. Choose \"Value list length\" from the \"Chart to show\" drop-down menu on the right.\n  2. Look at the chart to the right of each feature row. The chart shows the range of value list lengths for the feature. For example, the highlighted row in the screenshot below shows a feature that has some zero-length value lists:\n\n\n#### Large Differences in Scale Between Features\nIf your features vary widely in scale, then the model may have difficulties learning. For example, if some features vary from 0 to 1 and others vary from 0 to 1,000,000,000, you have a big difference in scale. Compare the \"max\" and \"min\" columns across features to find widely varying scales.\nConsider normalizing feature values to reduce these wide variations.\n#### Labels with Invalid Labels\nTensorFlow's Estimators have restrictions on the type of data they accept as labels. For example, binary classifiers typically only work with {0, 1} labels.\nReview the label values in the Facets Overview and make sure they conform to the \n",
  "https://www.tensorflow.org/tfx/guide/statsgen": "The StatisticsGen TFX pipeline component generates features statistics over both training and serving data, which can be used by other pipeline components. StatisticsGen uses Beam to scale to large datasets.\n  * Consumes: datasets created by an ExampleGen pipeline component.\n  * Emits: Dataset statistics.\n\n\n## StatisticsGen and TensorFlow Data Validation\nStatisticsGen makes extensive use of [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv) for generating statistics from your dataset.\n## Using the StatsGen Component\nA StatisticsGen pipeline component is typically very easy to deploy and requires little customization. Typical code looks like this:\n```\ncompute_eval_stats = StatisticsGen(\n      examples=example_gen.outputs['examples'],\n      name='compute-eval-stats'\n      )\n\n```\n\n## Using the StatsGen Component With a Schema\nFor the first run of a pipeline, the output of StatisticsGen will be used to infer a schema. However, on subsequent runs you may have a manually curated schema that contains additional information about your data set. By providing this schema to StatisticsGen, TFDV can provide more useful statistics based on declared properties of your data set.\nIn this setting, you will invoke StatisticsGen with a curated schema that has been imported by an ImporterNode like this:\n```\nuser_schema_importer = Importer(\n    source_uri=user_schema_dir, # directory containing only schema text proto\n    artifact_type=standard_artifacts.Schema).with_id('schema_importer')\n\ncompute_eval_stats = StatisticsGen(\n      examples=example_gen.outputs['examples'],\n      schema=user_schema_importer.outputs['result'],\n      name='compute-eval-stats'\n      )\n\n```\n\n### Creating a Curated Schema\n`Schema` in TFX is an instance of the TensorFlow Metadata `SchemaGen` as a starting point. Once the `SchemaGen` component has executed, the schema will be located under the pipeline root in the following path:\n```\n<pipeline_root>/SchemaGen/schema/<artifact_id>/schema.pbtxt\n\n```\n\nWhere `<artifact_id>` represents a unique ID for this version of the schema in MLMD. This schema proto can then be modified to communicate information about the dataset which cannot be reliably inferred, which will make the output of `StatisticsGen` more useful and the validation performed in the [`ExampleValidator`](https://www.tensorflow.org/tfx/guide/exampleval) component more stringent.\nMore details are available in the [StatisticsGen API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/StatisticsGen).\n",
  "https://www.tensorflow.org/tfx/guide/tft": "Transform is available as a standalone library.\n  * [Getting Started with TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n  * [TensorFlow Transform API Reference](https://www.tensorflow.org/tfx/transform/api_docs/python/tft)\n\n\nThe [`tft`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) module documentation is the only module that is relevant to TFX users. The [`tft_beam`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam) module is relevant only when using Transform as a standalone library. Typically, a TFX user constructs a `preprocessing_fn`, and the rest of the Transform library calls are made by the Transform component.\nYou can also use the Apache Beam `MLTransform` class to preprocess data for training and inference. The `MLTransform` class wraps multiple TFX data processing transforms in one class. For more information, see \n",
  "https://www.tensorflow.org/tfx/guide/transform": "The Transform TFX pipeline component performs feature engineering on tf.Examples emitted from an [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen) component, using a data schema created by a [SchemaGen](https://www.tensorflow.org/tfx/guide/schemagen) component, and emits both a SavedModel as well as statistics on both pre-transform and post-transform data. When executed, the SavedModel will accept tf.Examples emitted from an ExampleGen component and emit the transformed feature data.\n  * Consumes: tf.Examples from an ExampleGen component, and a data schema from a SchemaGen component.\n  * Emits: A SavedModel to a Trainer component, pre-transform and post-transform statistics.\n\n\n## Configuring a Transform Component\nOnce your `preprocessing_fn` is written, it needs to be defined in a python module that is then provided to the Transform component as an input. This module will be loaded by transform and the function named `preprocessing_fn` will be found and used by Transform to construct the preprocessing pipeline.\n```\ntransform = Transform(\n    examples=example_gen.outputs['examples'],\n    schema=schema_gen.outputs['schema'],\n    module_file=os.path.abspath(_taxi_transform_module_file))\n\n```\n\nAdditionally, you may wish to provide options to the [TFDV](https://www.tensorflow.org/tfx/guide/tfdv)-based pre-transform or post-transform statistics computation. To do so, define a `stats_options_updater_fn` within the same module.\n## Transform and TensorFlow Transform\nTransform makes extensive use of [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/tft) for performing feature engineering on your dataset. TensorFlow Transform is a great tool for transforming feature data before it goes to your model and as a part of the training process. Common feature transformations include:\n  * **Embedding** : converting sparse features (like the integer IDs produced by a vocabulary) into dense features by finding a meaningful mapping from high- dimensional space to low dimensional space. See the \n  * **Vocabulary generation** : converting strings or other non-numeric features into integers by creating a vocabulary that maps each unique value to an ID number.\n  * **Normalizing values** : transforming numeric features so that they all fall within a similar range.\n  * **Bucketization** : converting continuous-valued features into categorical features by assigning values to discrete buckets.\n  * **Enriching text features** : producing features from raw data like tokens, n-grams, entities, sentiment, etc., to enrich the feature set.\n\n\nTensorFlow Transform provides support for these and many other kinds of transformations:\n  * Automatically generate a vocabulary from your latest data.\n  * Perform arbitrary transformations on your data before sending it to your model. TensorFlow Transform builds transformations into the TensorFlow graph for your model so the same transformations are performed at training and inference time. You can define transformations that refer to global properties of the data, like the max value of a feature across all training instances.\n\n\nYou can transform your data however you like prior to running TFX. But if you do it within TensorFlow Transform, transforms become part of the TensorFlow graph. This approach helps avoid training/serving skew.\nTransformations inside your modeling code use FeatureColumns. Using FeatureColumns, you can define bucketizations, integerizations that use predefined vocabularies, or any other transformations that can be defined without looking at the data.\nBy contrast, TensorFlow Transform is designed for transformations that require a full pass over the data to compute values that are not known in advance. For example, vocabulary generation requires a full pass over the data.\nIn addition to computing values using Apache Beam, TensorFlow Transform allows users to embed these values into a TensorFlow graph, which can then be loaded into the training graph. For example when normalizing features, the [`tft.scale_to_z_score`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score) function will compute the mean and standard deviation of a feature, and also a representation, in a TensorFlow graph, of the function that subtracts the mean and divides by the standard deviation. By emitting a TensorFlow graph, not just statistics, TensorFlow Transform simplifies the process of authoring your preprocessing pipeline.\nSince the preprocessing is expressed as a graph, it can happen on the server, and it's guaranteed to be consistent between training and serving. This consistency eliminates one source of training/serving skew.\nTensorFlow Transform allows users to specify their preprocessing pipeline using TensorFlow code. This means that a pipeline is constructed in the same manner as a TensorFlow graph. If only TensorFlow ops were used in this graph, the pipeline would be a pure map that accepts batches of input and returns batches of output. Such a pipeline would be equivalent to placing this graph inside your `input_fn` when using the `tf.Estimator` API. In order to specify full-pass operations such as computing quantiles, TensorFlow Transform provides special functions called `analyzers` that appear like TensorFlow ops, but in fact specify a deferred computation that will be done by Apache Beam, and the output inserted into the graph as a constant. While an ordinary TensorFlow op will take a single batch as its input, perform some computation on just that batch and emit a batch, an `analyzer` will perform a global reduction (implemented in Apache Beam) over all batches and return the result.\nBy combining ordinary TensorFlow ops and TensorFlow Transform analyzers, users can create complex pipelines to preprocess their data. For example the [`tft.scale_to_z_score`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score) function takes an input tensor and returns that tensor normalized to have mean `0` and variance `1`. It does this by calling the `mean` and `var` analyzers under the hood, which will effectively generate constants in the graph equal to the mean and variance of the input tensor. It will then use TensorFlow ops to subtract the mean and divide by the standard deviation.\n## The TensorFlow Transform `preprocessing_fn`\nThe TFX Transform component simplifies the use of Transform by handling the API calls related to reading and writing data, and writing the output SavedModel to disk. As a TFX user, you only have to define a single function called the `preprocessing_fn`. In `preprocessing_fn` you define a series of functions that manipulate the input dict of tensors to produce the output dict of tensors. You can find helper functions like scale_to_0_1 and compute_and_apply_vocabulary the [TensorFlow Transform API](https://www.tensorflow.org/tfx/transform/api_docs/python/tft) or use regular TensorFlow functions as shown below.\n```\ndefpreprocessing_fn(inputs):\n\"\"\"tf.transform's callback function for preprocessing inputs.\n\n  Args:\n    inputs: map from feature keys to raw not-yet-transformed features.\n\n  Returns:\n    Map from string feature key to transformed feature operations.\n  \"\"\"\n  outputs = {}\n  for key in _DENSE_FLOAT_FEATURE_KEYS:\n    # If sparse make it dense, setting nan's to 0 or '', and apply zscore.\n    outputs[_transformed_name(key)] = transform.scale_to_z_score(\n        _fill_in_missing(inputs[key]))\n\n  for key in _VOCAB_FEATURE_KEYS:\n    # Build a vocabulary for this feature.\n    outputs[_transformed_name(\n        key)] = transform.compute_and_apply_vocabulary(\n            _fill_in_missing(inputs[key]),\n            top_k=_VOCAB_SIZE,\n            num_oov_buckets=_OOV_SIZE)\n\n  for key in _BUCKET_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = transform.bucketize(\n        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n\n  for key in _CATEGORICAL_FEATURE_KEYS:\n    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\n\n  # Was this passenger a big tipper?\n  taxi_fare = _fill_in_missing(inputs[_FARE_KEY])\n  tips = _fill_in_missing(inputs[_LABEL_KEY])\n  outputs[_transformed_name(_LABEL_KEY)] = tf.where(\n      tf.is_nan(taxi_fare),\n      tf.cast(tf.zeros_like(taxi_fare), tf.int64),\n      # Test if the tip was > 20% of the fare.\n      tf.cast(\n          tf.greater(tips, tf.multiply(taxi_fare, tf.constant(0.2))), tf.int64))\n\n  return outputs\n\n```\n\n### Understanding the inputs to the preprocessing_fn\nThe `preprocessing_fn` describes a series of operations on tensors (that is, `Tensor`s, `SparseTensor`s, or `RaggedTensor`s). In order to define the `preprocessing_fn` correctly it is necessary to understand how the data is represented as tensors. The input to the `preprocessing_fn` is determined by the schema. A \n## Using TensorFlow Transform to handle string labels\nUsually one wants to use TensorFlow Transform to both generate a vocabulary and apply that vocabulary to convert strings to integers. When following this workflow, the `input_fn` constructed in the model will output the integerized string. However labels are an exception, because in order for the model to be able to map the output (integer) labels back to strings, the model needs the `input_fn` to output a string label, together with a list of possible values of the label. E.g. if the labels are `cat` and `dog` then the output of the `input_fn` should be these raw strings, and the keys `[\"cat\", \"dog\"]` need to be passed into the estimator as a parameter (see details below).\nIn order to handle the mapping of string labels to integers, you should use TensorFlow Transform to generate a vocabulary. We demonstrate this in the code snippet below:\n```\ndef_preprocessing_fn(inputs):\n\"\"\"Preprocess input features into transformed features.\"\"\"\n\n  ...\n\n\n  education = inputs[features.RAW_LABEL_KEY]\n  _ = tft.vocabulary(education, vocab_filename=features.RAW_LABEL_KEY)\n\n  ...\n\n```\n\nThe preprocessing function above takes the raw input feature (which will also be returned as part of the output of the preprocessing function) and calls [`tft.vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/vocabulary) on it. This results in a vocabulary being generated for `education` that can be accessed in the model.\nThe example also shows how to transform a label and then generate a vocabulary for the transformed label. In particular it takes the raw label `education` and converts all but the top 5 labels (by frequency) to `UNKNOWN`, without converting the label to an integer.\nIn the model code, the classifier must be given the vocabulary generated by [`tft.vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/vocabulary) as the `label_vocabulary` argument. This is done by first reading this vocabulary as a list with a helper function. This is shown in the snippet below. Note the example code uses the transformed label discussed above but here we show code for using the raw label.\n```\ndefcreate_estimator(pipeline_inputs, hparams):\n\n  ...\n\n  tf_transform_output = trainer_util.TFTransformOutput(\n      pipeline_inputs.transform_dir)\n\n  # vocabulary_by_name() returns a Python list.\n  label_vocabulary = tf_transform_output.vocabulary_by_name(\n      features.RAW_LABEL_KEY)\n\n  return tf.contrib.learn.DNNLinearCombinedClassifier(\n      ...\n      n_classes=len(label_vocab),\n      label_vocabulary=label_vocab,\n      ...)\n\n```\n\n## Configuring pre-transform and post-transform statistics\nAs mentioned above, the Transform component invokes TFDV to compute both pre-transform and post-transform statistics. TFDV takes as input an optional `stats_options_updater_fn` in the module file.\n```\ndefstats_options_updater_fn(stats_type, stats_options):\n  ...\n  if stats_type == stats_options_util.StatsType.PRE_TRANSFORM:\n    # Update stats_options to modify pre-transform statistics computation.\n    # Most constraints are specified in the schema which can be accessed\n    # via stats_options.schema.\n  if stats_type == stats_options_util.StatsType.POST_TRANSFORM\n    # Update stats_options to modify post-transform statistics computation.\n    # Most constraints are specified in the schema which can be accessed\n    # via stats_options.schema.\n  return stats_options\n\n```\n\nPost-transform statistics often benefit from knowledge of the vocabulary being used for preprocessing a feature. The vocabulary name to path mapping is provided to StatsOptions (and hence TFDV) for every TFT-generated vocabulary. Additionally, mappings for externally-created vocabularies can be added by either (i) directly modifying the `vocab_paths` dictionary within StatsOptions or by (ii) using [`tft.annotate_asset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/annotate_asset).\n",
  "https://www.tensorflow.org/tfx/guide/trainer": "The Trainer TFX pipeline component trains a TensorFlow model.\n## Trainer and TensorFlow\nTrainer makes extensive use of the Python [TensorFlow](https://www.tensorflow.org) API for training models.\n## Component\nTrainer takes:\n  * tf.Examples used for training and eval.\n  * A user provided module file that defines the trainer logic.\n  * (Optional) A data schema created by a SchemaGen pipeline component and optionally altered by the developer.\n  * (Optional) transform graph produced by an upstream Transform component.\n  * (Optional) pre-trained models used for scenarios such as warmstart.\n  * (Optional) hyperparameters, which will be passed to user module function. Details of the integration with Tuner can be found [here](https://www.tensorflow.org/tfx/guide/tuner).\n\n\nTrainer emits: At least one model for inference/serving (typically in SavedModelFormat) and optionally another model for eval (typically an EvalSavedModel).\nWe provide support for alternate model formats such as [TFLite](https://www.tensorflow.org/lite) through the \n## Generic Trainer\nGeneric trainer enables developers to use any TensorFlow model API with the Trainer component. In addition to TensorFlow Estimators, developers can use Keras models or custom training loops. For details, please see the \n### Configuring the Trainer Component\nTypical pipeline DSL code for the generic Trainer would look like this:\n```\nfromtfx.componentsimport Trainer\n\n...\n\ntrainer = Trainer(\n    module_file=module_file,\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n\n```\n\nTrainer invokes a training module, which is specified in the `module_file` parameter. Instead of `trainer_fn`, a `run_fn` is required in the module file if the `GenericExecutor` is specified in the `custom_executor_spec`. The `trainer_fn` was responsible for creating the model. In addition to that, `run_fn` also needs to handle the training part and output the trained model to a the desired location given by \n```\nfromtfx.components.trainer.fn_args_utilsimport FnArgs\n\ndefrun_fn(fn_args: FnArgs) -> None:\n\"\"\"Build the TF model and train it.\"\"\"\n  model = _build_keras_model()\n  model.fit(...)\n  # Save model to fn_args.serving_model_dir.\n  model.save(fn_args.serving_model_dir, ...)\n\n```\n\nHere is an `run_fn`.\nNote that if the Transform component is not used in the pipeline, then the Trainer would take the examples from ExampleGen directly:\n```\ntrainer = Trainer(\n    module_file=module_file,\n    examples=example_gen.outputs['examples'],\n    schema=infer_schema.outputs['schema'],\n    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n\n```\n\nMore details are available in the [Trainer API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Trainer).\n",
  "https://www.tensorflow.org/tfx/guide/train": "When designing your TensorFlow modeling code for TFX there are a few items to be aware of, including the choice of a modeling API.\n  * Consumes: SavedModel from [Transform](https://www.tensorflow.org/tfx/guide/transform), and data from [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)\n  * Emits: Trained model in SavedModel format\n\n\nYour model's input layer should consume from the SavedModel that was created by a [Transform](https://www.tensorflow.org/tfx/guide/transform) component, and the layers of the Transform model should be included with your model so that when you export your SavedModel and EvalSavedModel they will include the transformations that were created by the [Transform](https://www.tensorflow.org/tfx/guide/transform) component.\nA typical TensorFlow model design for TFX looks like this:\n```\ndef_build_estimator(tf_transform_dir,\n                     config,\n                     hidden_units=None,\n                     warm_start_from=None):\n\"\"\"Build an estimator for predicting the tipping behavior of taxi riders.\n\n  Args:\n    tf_transform_dir: directory in which the tf-transform model was written\n      during the preprocessing step.\n    config: tf.contrib.learn.RunConfig defining the runtime environment for the\n      estimator (including model_dir).\n    hidden_units: [int], the layer sizes of the DNN (input layer first)\n    warm_start_from: Optional directory to warm start from.\n\n  Returns:\n    Resulting DNNLinearCombinedClassifier.\n  \"\"\"\n  metadata_dir = os.path.join(tf_transform_dir,\n                              transform_fn_io.TRANSFORMED_METADATA_DIR)\n  transformed_metadata = metadata_io.read_metadata(metadata_dir)\n  transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n\n  transformed_feature_spec.pop(_transformed_name(_LABEL_KEY))\n\n  real_valued_columns = [\n      tf.feature_column.numeric_column(key, shape=())\n      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n  ]\n  categorical_columns = [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n  ]\n  categorical_columns += [\n      tf.feature_column.categorical_column_with_identity(\n          key, num_buckets=num_buckets, default_value=0)\n      for key, num_buckets in zip(\n          _transformed_names(_CATEGORICAL_FEATURE_KEYS),  #\n          _MAX_CATEGORICAL_FEATURE_VALUES)\n  ]\n  return tf.estimator.DNNLinearCombinedClassifier(\n      config=config,\n      linear_feature_columns=categorical_columns,\n      dnn_feature_columns=real_valued_columns,\n      dnn_hidden_units=hidden_units or [100, 70, 50, 25],\n      warm_start_from=warm_start_from)\n\n```\n\n",
  "https://www.tensorflow.org/tfx/guide/tuner": "The Tuner component tunes the hyperparameters for the model.\n## Tuner Component and KerasTuner Library\nThe Tuner component makes extensive use of the Python [KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) API for tuning hyperparameters.\n## Component\nTuner takes:\n  * tf.Examples used for training and eval.\n  * A user provided module file (or module fn) that defines the tuning logic, including model definition, hyperparameter search space, objective etc.\n  * (Optional) \n  * (Optional) transform graph produced by an upstream Transform component.\n  * (Optional) A data schema created by a SchemaGen pipeline component and optionally altered by the developer.\n\n\nWith the given data, model, and objective, Tuner tunes the hyperparameters and emits the best result.\n## Instructions\nA user module function `tuner_fn` with the following signature is required for Tuner:\n```\n...\nfromkeras_tuner.engineimport base_tuner\n\nTunerFnResult = NamedTuple('TunerFnResult', [('tuner', base_tuner.BaseTuner),\n                                             ('fit_kwargs', Dict[Text, Any])])\n\ndeftuner_fn(fn_args: FnArgs) -> TunerFnResult:\n\"\"\"Build the tuner using the KerasTuner API.\n  Args:\n    fn_args: Holds args as name/value pairs.\n      - working_dir: working dir for tuning.\n      - train_files: List of file paths containing training tf.Example data.\n      - eval_files: List of file paths containing eval tf.Example data.\n      - train_steps: number of train steps.\n      - eval_steps: number of eval steps.\n      - schema_path: optional schema of the input data.\n      - transform_graph_path: optional transform graph produced by TFT.\n  Returns:\n    A namedtuple contains the following:\n      - tuner: A BaseTuner that will be used for tuning.\n      - fit_kwargs: Args to pass to tuner's run_trial function for fitting the\n                    model , e.g., the training and validation dataset. Required\n                    args depend on the above tuner's implementation.\n  \"\"\"\n  ...\n\n```\n\nIn this function, you define both the model and hyperparameter search spaces, and choose the objective and algorithm for tuning. The Tuner component takes this module code as input, tunes the hyperparameters, and emits the best result.\nTrainer can take Tuner's output hyperparameters as input and utilize them in its user module code. The pipeline definition looks like this:\n```\n...\ntuner = Tuner(\n    module_file=module_file,  # Contains `tuner_fn`.\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    train_args=trainer_pb2.TrainArgs(num_steps=20),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n\ntrainer = Trainer(\n    module_file=module_file,  # Contains `run_fn`.\n    examples=transform.outputs['transformed_examples'],\n    transform_graph=transform.outputs['transform_graph'],\n    schema=schema_gen.outputs['schema'],\n    # This will be passed to `run_fn`.\n    hyperparameters=tuner.outputs['best_hyperparameters'],\n    train_args=trainer_pb2.TrainArgs(num_steps=100),\n    eval_args=trainer_pb2.EvalArgs(num_steps=5))\n...\n\n```\n\nYou might not want to tune the hyperparameters every time you retrain your model. Once you have used Tuner to determine a good set of hyperparameters, you can remove Tuner from your pipeline and use `ImporterNode` to import the Tuner artifact from a previous training run to feed to Trainer.\n```\nhparams_importer = Importer(\n    # This can be Tuner's output file or manually edited file. The file contains\n    # text format of hyperparameters (keras_tuner.HyperParameters.get_config())\n    source_uri='path/to/best_hyperparameters.txt',\n    artifact_type=HyperParameters,\n).with_id('import_hparams')\n\ntrainer = Trainer(\n    ...\n    # An alternative is directly use the tuned hyperparameters in Trainer's user\n    # module code and set hyperparameters to None here.\n    hyperparameters = hparams_importer.outputs['result'])\n\n```\n\n## Tuning on Google Cloud Platform (GCP)\nWhen running on the Google Cloud Platform (GCP), the Tuner component can take advantage of two services:\n### AI Platform Vizier as the backend of hyperparameter tuning\n[KerasTuner](https://www.tensorflow.org/tutorials/keras/keras_tuner) which talks to the AI Platform Vizier service as the study backend. Since CloudTuner is a subclass of `keras_tuner.Tuner`, it can be used as a drop-in replacement in the `tuner_fn` module, and execute as a part of the TFX Tuner component.\nBelow is a code snippet which shows how to use `CloudTuner`. Notice that configuration to `CloudTuner` requires items which are specific to GCP, such as the `project_id` and `region`.\n```\n...\nfromtensorflow_cloudimport CloudTuner\n\n...\ndeftuner_fn(fn_args: FnArgs) -> TunerFnResult:\n\"\"\"An implementation of tuner_fn that instantiates CloudTuner.\"\"\"\n\n  ...\n  tuner = CloudTuner(\n      _build_model,\n      hyperparameters=...,\n      ...\n      project_id=...,       # GCP Project ID\n      region=...,           # GCP Region where Vizier service is run.\n  )\n\n  ...\n  return TuneFnResult(\n      tuner=tuner,\n      fit_kwargs={...}\n  )\n\n\n```\n\n### Parallel tuning on Cloud AI Platform Training distributed worker flock\nThe KerasTuner framework as the underlying implementation of the Tuner component has ability to conduct hyperparameter search in parallel. While the stock Tuner component does not have ability to execute more than one search worker in parallel, by using the \n```\ntuner = google_cloud_ai_platform.Tuner(\n    ...   # Same kwargs as the above stock Tuner component.\n    tune_args=proto.TuneArgs(num_parallel_trials=3),  # 3-worker parallel\n    custom_config={\n        # Configures Cloud AI Platform-specific configs . For for details, see\n        # https://cloud.google.com/ai-platform/training/docs/reference/rest/v1/projects.jobs#traininginput.\n        TUNING_ARGS_KEY:\n            {\n                'project': ...,\n                'region': ...,\n                # Configuration of machines for each master/worker in the flock.\n                'masterConfig': ...,\n                'workerConfig': ...,\n                ...\n            }\n    })\n...\n\n\n```\n\nThe behavior and the output of the extension Tuner component is the same as the stock Tuner component, except that multiple hyperparameter searches are executed in parallel on different worker machines, and as a result, the `num_trials` will be completed faster. This is particularly effective when the search algorithm is embarrassingly parallelizable, such as `RandomSearch`. However, if the search algorithm uses information from results of prior trials, such as Google Vizier algorithm implemented in the AI Platform Vizier does, an excessively parallel search would negatively affect the efficacy of the search.\n## Links\n[KerasTuner tutorial](https://www.tensorflow.org/tutorials/keras/keras_tuner)\nMore details are available in the [Tuner API reference](https://www.tensorflow.org/tfx/api_docs/python/tfx/v1/components/Tuner).\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult": "The result of a single model analysis run.  \n```\ntfma.EvalResult(\n    slicing_metrics,\n    plots,\n    attributions,\n    config,\n    data_location,\n    file_format,\n    model_location\n)\n\n```\n\n## Attributes  \n---  \n`slicing_metrics` |  a list of `tfma.SlicedMetrics`, containing metric values for each slice.   \n`plots` |  List of slice-plot pairs.   \n`attributions` |  List of SlicedAttributions containing attribution values for each slice.   \n`config` |  The config containing slicing and metrics specification.   \n`data_location` |  Optional location for data used with config.   \n`file_format` |  Optional format for data used with config.   \n`model_location` |  Optional location(s) for model(s) used with config.   \n## Methods\n### `get_attributions_for_all_slices`\n```\nget_attributions_for_all_slices(\n    metric_name: str = '',\n    output_name: str = '',\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None\n) -> Dict[str, AttributionsByFeatureKey]\n\n```\n\nGet attribution feature keys and values for every slice.\nArgs  \n---  \n`metric_name` |  Name of metric to get attributions for. Optional if only one metric used.   \n`output_name` |  The name of the output (optional, only used for multi-output models).   \n`class_id` |  Used with multi-class metrics to identify a specific class ID.   \nUsed with multi-class metrics to identify the kth predicted value.   \n`top_k` |  Used with multi-class and ranking metrics to identify top-k predicted values.   \nReturns  \n---  \nDictionary mapping slices to attribution feature keys and values.   \n### `get_attributions_for_slice`\n```\nget_attributions_for_slice(\n    slice_name: slicer.SliceKeyType = (),\n    metric_name: str = '',\n    output_name: str = '',\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None\n) -> Union[AttributionsByFeatureKey, None]\n\n```\n\nGet attribution features names and values for a slice.\nArgs  \n---  \n`slice_name` |  A tuple of the form (column, value), indicating which slice to get attributions from. Optional; if excluded, use overall slice.   \n`metric_name` |  Name of metric to get attributions for. Optional if only one metric used.   \n`output_name` |  The name of the output. Optional, only used for multi-output models.   \n`class_id` |  Used with multi-class models to identify a specific class ID.   \nUsed with multi-class models to identify the kth predicted value.   \n`top_k` |  Used with multi-class models to identify top-k attribution values.   \nReturns  \n---  \nDictionary containing feature keys and values for the specified slice.   \nRaises  \n---  \n`ValueError` |  If metric_name is required.   \n### `get_metric_names`\n```\nget_metric_names() -> Sequence[str]\n\n```\n\nGet names of metrics.\nReturns  \n---  \nList of metric names.   \n### `get_metrics_for_all_slices`\n```\nget_metrics_for_all_slices(\n    output_name: str = '',\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None\n) -> Dict[str, MetricsByTextKey]\n\n```\n\nGet metric names and values for every slice.\nArgs  \n---  \n`output_name` |  The name of the output (optional, only used for multi-output models).   \n`class_id` |  Used with multi-class metrics to identify a specific class ID.   \nUsed with multi-class metrics to identify the kth predicted value.   \n`top_k` |  Used with multi-class and ranking metrics to identify top-k predicted values.   \nReturns  \n---  \nDictionary mapping slices to metric names and values.   \n### `get_metrics_for_slice`\n```\nget_metrics_for_slice(\n    slice_name: slicer.SliceKeyType = (),\n    output_name: str = '',\n    class_id: Optional[int] = None,\n    k: Optional[int] = None,\n    top_k: Optional[int] = None\n) -> Union[MetricsByTextKey, None]\n\n```\n\nGet metric names and values for a slice.\nArgs  \n---  \n`slice_name` |  A tuple of the form (column, value), indicating which slice to get metrics from. Optional; if excluded, return overall metrics.   \n`output_name` |  The name of the output. Optional, only used for multi-output models.   \n`class_id` |  Used with multi-class metrics to identify a specific class ID.   \nUsed with multi-class metrics to identify the kth predicted value.   \n`top_k` |  Used with multi-class and ranking metrics to identify top-k predicted values.   \nReturns  \n---  \nDictionary containing metric names and values for the specified slice.   \n### `get_slice_names`\n```\nget_slice_names() -> Sequence[str]\n\n```\n\nGet names of slices.\nReturns  \n---  \nList of slice names. \n",
  "https://www.tensorflow.org/tfx/guide/understanding_tfx_pipelines": "MLOps is the practice of applying DevOps practices to help automate, manage, and audit machine learning (ML) workflows. ML workflows include steps to:\n  * Prepare, analyze, and transform data.\n  * Train and evaluate a model.\n  * Deploy trained models to production.\n  * Track ML artifacts and understand their dependencies.\n\n\nManaging these steps in an ad-hoc manner can be difficult and time-consuming.\nTFX makes it easier to implement MLOps by providing a toolkit that helps you orchestrate your ML process on various orchestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. By implementing your workflow as a TFX pipeline, you can:\n  * Automate your ML process, which lets you regularly retrain, evaluate, and deploy your model.\n  * Utilize distributed compute resources for processing large datasets and workloads.\n  * Increase the velocity of experimentation by running a pipeline with different sets of hyperparameters.\n\n\nThis guide describes the core concepts required to understand TFX pipelines.\n## Artifact\nThe outputs of steps in a TFX pipeline are called **artifacts**. Subsequent steps in your workflow may use these artifacts as inputs. In this way, TFX lets you transfer data between workflow steps.\nFor instance, the `ExampleGen` standard component emits serialized examples, which components such as the `StatisticsGen` standard component use as inputs.\nArtifacts must be strongly typed with an **artifact type** registered in the [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) store. Learn more about the [concepts used in ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd#concepts).\nArtifact types have a name and define a schema of its properties. Artifact type names must be unique in your ML Metadata store. TFX provides several \n## Parameter\nParameters are inputs to pipelines that are known before your pipeline is executed. Parameters let you change the behavior of a pipeline, or a part of a pipeline, through configuration instead of code.\nFor example, you can use parameters to run a pipeline with different sets of hyperparameters without changing the pipeline's code.\nUsing parameters lets you increase the velocity of experimentation by making it easier to run your pipeline with different sets of parameters.\nLearn more about the \n## Component\nA **component** is an implementation of an ML task that you can use as a step in your TFX pipeline. Components are composed of:\n  * A component specification, which defines the component's input and output artifacts, and the component's required parameters.\n  * An executor, which implements the code to perform a step in your ML workflow, such as ingesting and transforming data or training and evaluating a model.\n  * A component interface, which packages the component specification and executor for use in a pipeline.\n\n\nTFX provides several [standard components](https://www.tensorflow.org/tfx/guide#tfx_standard_components) that you can use in your pipelines. If these components do not meet your needs, you can build custom components. [Learn more about custom components](https://www.tensorflow.org/tfx/guide/understanding_custom_components).\n## Pipeline\nA TFX pipeline is a portable implementation of an ML workflow that can be run on various orchestrators, such as: Apache Airflow, Apache Beam, and Kubeflow Pipelines. A pipeline is composed of component instances and input parameters.\nComponent instances produce artifacts as outputs and typically depend on artifacts produced by upstream component instances as inputs. The execution sequence for component instances is determined by creating a directed acyclic graph of the artifact dependencies.\nFor example, consider a pipeline that does the following:\n  * Ingests data directly from a proprietary system using a custom component.\n  * Calculates statistics for the training data using the StatisticsGen standard component.\n  * Creates a data schema using the SchemaGen standard component.\n  * Checks the training data for anomalies using the ExampleValidator standard component.\n  * Performs feature engineering on the dataset using the Transform standard component.\n  * Trains a model using the Trainer standard component.\n  * Evaluates the trained model using the Evaluator component.\n  * If the model passes its evaluation, the pipeline enqueues the trained model to a proprietary deployment system using a custom component.\n\n\nTo determine the execution sequence for the component instances, TFX analyzes the artifact dependencies.\n  * The data ingestion component does not have any artifact dependencies, so it can be the first node in the graph.\n  * StatisticsGen depends on the _examples_ produced by data ingestion, so it must be executed after data ingestion.\n  * SchemaGen depends on the _statistics_ created by StatisticsGen, so it must be executed after StatisticsGen.\n  * ExampleValidator depends on the _statistics_ created by StatisticsGen and the _schema_ created by SchemaGen, so it must be executed after StatisticsGen and SchemaGen.\n  * Transform depends on the _examples_ produced by data ingestion and the _schema_ created by SchemaGen, so it must be executed after data ingestion and SchemaGen.\n  * Trainer depends on the _examples_ produced by data ingestion, the _schema_ created by SchemaGen, and the _saved model_ produced by Transform. The Trainer can be executed only after data ingestion, SchemaGen, and Transform.\n  * Evaluator depends on the _examples_ produced by data ingestion and the _saved model_ produced by the Trainer, so it must be executed after data ingestion and the Trainer.\n  * The custom deployer depends on the _saved model_ produced by the Trainer and the _analysis results_ created by the Evaluator, so the deployer must be executed after the Trainer and the Evaluator.\n\n\nBased on this analysis, an orchestrator runs:\n  * The data ingestion, StatisticsGen, SchemaGen component instances sequentially.\n  * The ExampleValidator and Transform components can run in parallel since they share input artifact dependencies and do not depend on each other's output.\n  * After the Transform component is complete, the Trainer, Evaluator, and custom deployer component instances run sequentially.\n\n\nLearn more about [building a TFX pipeline](https://www.tensorflow.org/tfx/guide/build_tfx_pipeline).\n## TFX Pipeline Template\nTFX Pipeline Templates make it easier to get started with pipeline development by providing a prebuilt pipeline that you can customize for your use case.\nLearn more about [customizing a TFX pipeline template](https://www.tensorflow.org/tfx/guide/build_tfx_pipeline#pipeline_templates).\n## Pipeline Run\nA run is a single execution of a pipeline.\n## Orchestrator\nAn Orchestrator is a system where you can execute pipeline runs. TFX supports orchestrators such as: [Apache Airflow](https://www.tensorflow.org/tfx/guide/airflow), [Apache Beam](https://www.tensorflow.org/tfx/guide/beam), and [Kubeflow Pipelines](https://www.tensorflow.org/tfx/guide/kubeflow). TFX also uses the term _DagRunner_ to refer to an implementation that supports an orchestrator.\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig": "A ProtocolMessage  \n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Using Counterfactual Logit Pairing with Keras](https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras)\n\n| \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n  * [Introduction to Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Example_Colab)\n  * [Pandas DataFrame to Fairness Indicators Case Study](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Pandas_Case_Study)\n  * [TensorFlow Constrained Optimization Example Using CelebA Dataset](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_TFCO_CelebA_Case_Study)\n  * [Better ML Engineering with ML Metadata](https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial)\n\n  \n## Attributes  \n---  \n`cross_slicing_specs` |  `repeated CrossSlicingSpec cross_slicing_specs`  \n`metrics_specs` |  `repeated MetricsSpec metrics_specs`  \n`model_specs` |  `repeated ModelSpec model_specs`  \n`options` |  `Options options`  \n`slicing_specs` |  `repeated SlicingSpec slicing_specs`\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalSharedModel": "Shared model used during extraction and evaluation.  \nView aliases\n**Main aliases**\n[`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/types/EvalSharedModel)\n```\ntfma.types.EvalSharedModel(\n    model_path: Optional[str] = None,\n    add_metrics_callbacks: Optional[List[AddMetricsCallbackType]] = None,\n    include_default_metrics: Optional[bool] = True,\n    example_weight_key: Optional[Union[str, Dict[str, str]]] = None,\n    additional_fetches: Optional[List[str]] = None,\n    model_loader: Optional[[tfma.types.ModelLoader](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/types/ModelLoader)] = None,\n    model_name: str = '',\n    model_type: str = '',\n    rubber_stamp: bool = False,\n    is_baseline: bool = False,\n    resource_hints: Optional[Dict[str, Any]] = None,\n    backend_config: Optional[Any] = None,\n    construct_fn: Optional[Callable[[], Any]] = None\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [FaceSSD Fairness Indicators Example Colab](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Facessd_Fairness_Indicators_Example_Colab)\n  * [Wiki Talk Comments Toxicity Prediction](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_TFCO_Wiki_Case_Study)\n\n  \nMore details on add_metrics_callbacks:\nEach add_metrics_callback should have the following prototype: def add_metrics_callback(features_dict, predictions_dict, labels_dict):\nNote that features_dict, predictions_dict and labels_dict are not necessarily dictionaries - they might also be Tensors, depending on what the model's eval_input_receiver_fn returns.\nIt should create and return a metric_ops dictionary, such that metric_ops['metric_name'] = (value_op, update_op), just as in the Trainer.\nShort example:\ndef add_metrics_callback(features_dict, predictions_dict, labels): metrics_ops = {} metric_ops['mean_label'] = tf.metrics.mean(labels) metric_ops['mean_probability'] = tf.metrics.mean(tf.slice( predictions_dict['probabilities'], [0, 1], [2, 1])) return metric_ops\n## Attributes  \n---  \n`model_path` |  Path to EvalSavedModel (containing the saved_model.pb file).   \n`add_metrics_callbacks` |  Optional list of callbacks for adding additional metrics to the graph. The names of the metrics added by the callbacks should not conflict with existing metrics. See below for more details about what each callback should do. The callbacks are only used during evaluation.   \n`include_default_metrics` |  True to include the default metrics that are part of the saved model graph during evaluation.   \n`example_weight_key` |  Example weight key (single-output model) or dict of example weight keys (multi-output model) keyed by output_name.   \n`additional_fetches` |  Prefixes of additional tensors stored in signature_def.inputs that should be fetched at prediction time. The \"features\" and \"labels\" tensors are handled automatically and should not be included in this list.   \n`model_loader` |  Model loader.   \n`model_name` |  Model name (should align with ModelSpecs.name).   \n`model_type` |  Model type (tfma.TF_KERAS, tfma.TF_LITE, tfma.TF_ESTIMATOR, ..).   \n`rubber_stamp` |  True if this model is being rubber stamped. When a model is rubber stamped diff thresholds will be ignored if an associated baseline model is not passed.   \n`is_baseline` |  The model is the baseline for comparison or not.   \n`resource_hints` |  The beam resource hints to apply to the PTransform which runs inference for this model.   \n`backend_config` |  The backend config for running model inference. \n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricThreshold": "A ProtocolMessage  \n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Better ML Engineering with ML Metadata](https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial)\n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Model analysis using TFX Pipeline and TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma)\n\n  \n## Attributes  \n---  \n`change_threshold` |  `GenericChangeThreshold change_threshold`  \n`value_threshold` |  `GenericValueThreshold value_threshold`\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ValidationResult": "A ProtocolMessage  \n## Attributes  \n---  \n`metric_validations_per_slice` |  `repeated MetricsValidationForSlice metric_validations_per_slice`  \n`missing_cross_slices` |  `repeated CrossSlicingSpec missing_cross_slices`  \n`missing_slices` |  `repeated SlicingSpec missing_slices`  \n`missing_thresholds` |  `bool missing_thresholds`  \n`rubber_stamp` |  `bool rubber_stamp`  \n`validation_details` |  `ValidationDetails validation_details`  \n`validation_ok` |  `bool validation_ok`\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_validation_result": "Read and deserialize the ValidationResult.  \n```\ntfma.load_validation_result(\n    output_path: str, output_file_format: str = ''\n) -> [tfma.ValidationResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ValidationResult)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n\n\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_eval_results": "Loads results for multiple models or multiple data sets.  \n```\ntfma.load_eval_results(\n    output_paths: Union[str, List[str]],\n    output_file_format: Optional[str] = 'tfrecord',\n    mode: str = constants.MODEL_CENTRIC_MODE,\n    model_name: Optional[str] = None\n) -> view_types.EvalResults\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n\n  \n## Args  \n---  \n`output_paths` |  A single path or list of output paths of completed tfma runs.   \n`output_file_format` |  Optional file extension to filter files by.   \n`mode` |  The mode of the evaluation. Currently, tfma.DATA_CENTRIC_MODE and tfma.MODEL_CENTRIC_MODE are supported.   \n`model_name` |  Filters to only return results for given model. If unset all models are returned.   \n## Returns  \n---  \nAn EvalResults containing the evaluation results serialized at output_paths. This can be used to construct a time series view. \n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec": "A ProtocolMessage\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n  * [Model analysis using TFX Pipeline and TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma)\n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Better ML Engineering with ML Metadata](https://www.tensorflow.org/tfx/tutorials/mlmd/mlmd_tutorial)\n\n  \n## Attributes  \n---  \n`feature_keys` |  `repeated string feature_keys`  \n`feature_values` |  `repeated FeatureValuesEntry feature_values`  \n`slice_keys_sql` |  `string slice_keys_sql`  \n## Child Classes\n[`class FeatureValuesEntry`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec/FeatureValuesEntry)\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis": "Runs TensorFlow model analysis.  \n```\ntfma.run_model_analysis(\n    eval_shared_model: Optional[[tfma.types.EvalSharedModel](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/types/EvalSharedModel)] = None,\n    eval_config: Optional[[tfma.EvalConfig](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig)] = None,\n    data_location: str = '',\n    file_format: str = 'tfrecords',\n    output_path: Optional[str] = None,\n    extractors: Optional[List[extractor.Extractor]] = None,\n    evaluators: Optional[List[evaluator.Evaluator]] = None,\n    writers: Optional[List[writer.Writer]] = None,\n    pipeline_options: Optional[Any] = None,\n    slice_spec: Optional[List[slicer.SingleSliceSpec]] = None,\n    write_config: Optional[bool] = True,\n    compute_confidence_intervals: Optional[bool] = False,\n    min_slice_size: int = 1,\n    random_seed_for_testing: Optional[int] = None,\n    schema: Optional[schema_pb2.Schema] = None\n) -> Union[[tfma.EvalResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult), view_types.EvalResults]\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Using Counterfactual Logit Pairing with Keras](https://www.tensorflow.org/responsible_ai/model_remediation/counterfactual/guide/counterfactual_keras)\n\n| \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n\n  \nIt runs a Beam pipeline to compute the slicing metrics exported in TensorFlow Eval SavedModel and returns the results.\nThis is a simplified API for users who want to quickly get something running locally. Users who wish to create their own Beam pipelines can use the Evaluate PTransform instead.\n## Args  \n---  \n`eval_shared_model` |  Optional shared model (single-model evaluation) or list of shared models (multi-model evaluation). Only required if needed by default extractors, evaluators, or writers.   \n`eval_config` |  Eval config.   \n`data_location` |  The location of the data files.   \n`file_format` |  The file format of the data, can be either 'text' or 'tfrecords' for now. By default, 'tfrecords' will be used.   \n`output_path` |  The directory to output metrics and results to. If None, we use a temporary directory.   \n`extractors` |  Optional list of Extractors to apply to Extracts. Typically these will be added by calling the default_extractors function. If no extractors are provided, default_extractors (non-materialized) will be used.   \n`evaluators` |  Optional list of Evaluators for evaluating Extracts. Typically these will be added by calling the default_evaluators function. If no evaluators are provided, default_evaluators will be used.   \n`writers` |  Optional list of Writers for writing Evaluation output. Typically these will be added by calling the default_writers function. If no writers are provided, default_writers will be used.   \n`pipeline_options` |  Optional arguments to run the Pipeline, for instance whether to run directly.   \n`slice_spec` |  Deprecated (use EvalConfig).   \n`write_config` |  Deprecated (use EvalConfig).   \n`compute_confidence_intervals` |  Deprecated (use EvalConfig).   \n`min_slice_size` |  Deprecated (use EvalConfig).   \n`random_seed_for_testing` |  Provide for deterministic tests only.   \n`schema` |  Optional tf.Metadata schema of the input data.   \n## Returns  \n---  \nAn EvalResult that can be used with the TFMA visualization functions.   \n## Raises  \n---  \n`ValueError` |  If the file_format is unknown to us. \n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_plot": "Renders the plot view as widget.  \n```\ntfma.view.render_plot(\n    result: [tfma.EvalResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult),\n    slicing_spec: Optional[Union[slicer.SingleSliceSpec, [tfma.SlicingSpec](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec)]] = None,\n    output_name: Optional[str] = None,\n    class_id: Optional[int] = None,\n    top_k: Optional[int] = None,\n    k: Optional[int] = None,\n    label: Optional[str] = None\n) -> Optional[visualization.PlotViewer]\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n\n  \n## Args  \n---  \n`result` |  An tfma.EvalResult.   \n`slicing_spec` |  The tfma.SlicingSpec to identify the slice. Show overall if unset.   \n`output_name` |  A string representing the output name.   \n`class_id` |  A number representing the class id if multi class.   \n`top_k` |  The k used to compute prediction in the top k position.   \nThe k used to compute prediciton at the kth position.   \n`label` |  A partial label used to match a set of plots in the results.   \n## Returns  \n---  \nA PlotViewer object if in Jupyter notebook; None if in Colab. \n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_slicing_metrics": "Renders the slicing metrics view as widget.  \n```\ntfma.view.render_slicing_metrics(\n    result: [tfma.EvalResult](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult),\n    slicing_column: Optional[str] = None,\n    slicing_spec: Optional[Union[slicer.SingleSliceSpec, [tfma.SlicingSpec](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec)]] = None,\n    weighted_example_column: Optional[str] = None,\n    event_handlers: Optional[Callable[[Dict[str, Union[str, float]]], None]] = None\n) -> Optional[visualization.SlicingMetricsViewer]\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Create a TFX pipeline for your data with Penguin template](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_template)\n  * [Model analysis using TFX Pipeline and TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tfma)\n\n  \n## Args  \n---  \n`result` |  An tfma.EvalResult.   \n`slicing_column` |  The column to slice on.   \n`slicing_spec` |  The tfma.SlicingSpec to filter results. If neither column nor spec is set, show overall.   \n`weighted_example_column` |  Override for the weighted example column. This can be used when different weights are applied in different aprts of the model (eg: multi-head).   \n`event_handlers` |  The event handlers   \n## Returns  \n---  \nA SlicingMetricsViewer object if in Jupyter notebook; None if in Colab. \n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/experimental": "\n",
  "https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/view/render_time_series": "Renders the time series view as widget.  \n```\ntfma.view.render_time_series(\n    results: view_types.EvalResults,\n    slicing_spec: Optional[Union[slicer.SingleSliceSpec, [tfma.SlicingSpec](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec)]] = None,\n    display_full_path: bool = False\n) -> Optional[visualization.TimeSeriesViewer]\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic)\n\n  \n## Args  \n---  \n`results` |  An tfma.EvalResults.   \n`slicing_spec` |  A tfma.SlicingSpec determining the slice to show time series on. Show overall if not set.   \n`display_full_path` |  Whether to display the full path to model / data in the visualization or just show file name.   \n## Returns  \n---  \nA TimeSeriesViewer object if in Jupyter notebook; None if in Colab. \n",
  "https://www.tensorflow.org/tfx/serving/signature_defs": "## Objective\nThis document provides examples for the intended usage of SignatureDefs in SavedModel that map to TensorFlow Serving's APIs.\n## Overview\nA \n## Background\n`Signatures` in TF-Exporter will be replaced by `SignatureDefs` in SavedModel.\n## SignatureDef Structure\nA SignatureDef requires specification of:\n  * `inputs` as a map of string to TensorInfo.\n  * `outputs` as a map of string to TensorInfo.\n  * `method_name` (which corresponds to a supported method name in the loading tool/system).\n\n\nNote that \n## Related constants and utils\nFor ease of reuse and sharing across tools and systems, commonly used constants related to SignatureDefs that will be supported in TensorFlow Serving are defined as constants. Specifically:\nIn addition, SavedModel provides a \n## Sample structures\nTensorFlow Serving provides high level APIs for performing inference. To enable these APIs, models must include one or more SignatureDefs that define the exact TensorFlow nodes to use for input and output. See below for examples of the specific SignatureDefs that TensorFlow Serving supports for each API.\nNote that TensorFlow Serving depends on the keys of each TensorInfo (in the inputs and outputs of the SignatureDef), as well as the method_name of the SignatureDef. The actual contents of the TensorInfo are specific to your graph.\n### Classification SignatureDef\nClassification SignatureDefs support structured calls to TensorFlow Serving's Classification API. These prescribe that there must be an `inputs` Tensor, and that there are two optional output Tensors: `classes` and `scores`, at least one of which must be present.\n```\nsignature_def:{\nkey:\"my_classification_signature\"\nvalue:{\ninputs:{\nkey:\"inputs\"\nvalue:{\nname:\"tf_example:0\"\ndtype:DT_STRING\ntensor_shape:...\n}\n}\noutputs:{\nkey:\"classes\"\nvalue:{\nname:\"index_to_string:0\"\ndtype:DT_STRING\ntensor_shape:...\n}\n}\noutputs:{\nkey:\"scores\"\nvalue:{\nname:\"TopKV2:0\"\ndtype:DT_FLOAT\ntensor_shape:...\n}\n}\nmethod_name:\"tensorflow/serving/classify\"\n}\n}\n\n```\n\n### Predict SignatureDef\nPredict SignatureDefs support calls to TensorFlow Serving's Predict API. These signatures allow you to flexibly support arbitrarily many input and output Tensors. For the example below, the signature `my_prediction_signature` has a single logical input Tensor `images` that are mapped to the actual Tensor in your graph `x:0`.\nPredict SignatureDefs enable portability across models. This means that you can swap in different SavedModels, possibly with different underlying Tensor names (e.g. instead of `x:0` perhaps you have a new alternate model with a Tensor `z:0`), while your clients can stay online continuously querying the old and new versions of this model without client-side changes.\nPredict SignatureDefs also allow you to add optional additional Tensors to the outputs, that you can explicitly query. Let's say that in addition to the output key below of `scores`, you also wanted to fetch a pooling layer for debugging or other purposes. In that case, you would simply add an additional Tensor with a key like `pool` and appropriate value.\n```\nsignature_def:{\nkey:\"my_prediction_signature\"\nvalue:{\ninputs:{\nkey:\"images\"\nvalue:{\nname:\"x:0\"\ndtype:...\ntensor_shape:...\n}\n}\noutputs:{\nkey:\"scores\"\nvalue:{\nname:\"y:0\"\ndtype:...\ntensor_shape:...\n}\n}\nmethod_name:\"tensorflow/serving/predict\"\n}\n}\n\n```\n\n### Regression SignatureDef\nRegression SignatureDefs support structured calls to TensorFlow Serving's Regression API. These prescribe that there must be exactly one `inputs` Tensor, and one `outputs` Tensor.\n```\nsignature_def:{\nkey:\"my_regression_signature\"\nvalue:{\ninputs:{\nkey:\"inputs\"\nvalue:{\nname:\"x_input_examples_tensor_0\"\ndtype:...\ntensor_shape:...\n}\n}\noutputs:{\nkey:\"outputs\"\nvalue:{\nname:\"y_outputs_0\"\ndtype:DT_FLOAT\ntensor_shape:...\n}\n}\nmethod_name:\"tensorflow/serving/regress\"\n}\n}\n\n```\n\n",
  "https://www.tensorflow.org/tfx/model_analysis/get_started": "## Overview\nTensorFlow Model Analysis (TFMA) is a library for performing model evaluation.\n  * **For** : Machine Learning Engineers or Data Scientists\n  * **who** : want to analyze and understand their TensorFlow models\n  * **it is** : a standalone library or component of a TFX pipeline\n  * **that** : evaluates models on large amounts of data in a distributed manner on the same metrics defined in training. These metrics are compared over slices of data, and visualized in Jupyter or Colab notebooks.\n  * **unlike** : some model introspection tools like tensorboard that offer model introspection\n\n\nTFMA performs its computations in a distributed manner over large amounts of data using [architecture](https://www.tensorflow.org/tfx/model_analysis/architecture) more details on the underlying implementation.\nIf you just want to jump in and get started, check out our \nThis page can also be viewed from [tensorflow.org](https://www.tensorflow.org/tfx/model_analysis/get_started).\n## Model Types Supported\nTFMA is designed to support tensorflow based models, but can be easily extended to support other frameworks as well. Historically, TFMA required an `EvalSavedModel` be created to use TFMA, but the latest version of TFMA supports multiple types of models depending on the user's needs. [Setting up an EvalSavedModel](https://www.tensorflow.org/tfx/model_analysis/eval_saved_model) should only be required if a `tf.estimator` based model is used and custom training time metrics are required.\nNote that because TFMA now runs based on the serving model, TFMA will no longer automatically evaluate metrics added at training time. The exception to this case is if a keras model is used since keras saves the metrics used alongside the saved model. However, if this is a hard requirement, the latest TFMA is backwards compatible such that an `EvalSavedModel` can still be run in a TFMA pipeline.\nThe following table summarizes the models supported by default:\nModel Type | Training Time Metrics | Post Training Metrics  \n---|---|---  \nTF2 (keras) | Y* | Y  \nTF2 (generic) | N/A | Y  \nEvalSavedModel (estimator) | Y | Y  \nNone (pd.DataFrame, etc) | N/A | Y  \n  * Training Time metrics refers to metrics defined at training time and saved with the model (either TFMA EvalSavedModel or keras saved model). Post training metrics refers to metrics added via [`tfma.MetricConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricConfig).\n  * Generic TF2 models are custom models that export signatures that can be used for inference and are not based on either keras or estimator.\n\n\nSee [FAQ](https://www.tensorflow.org/tfx/model_analysis/faq) for more information on how to setup and configure these different model types.\n## Setup\nBefore running an evaluation, a small amount of setup is required. First, a [`tfma.EvalConfig`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig) object must be defined that provides specifications for the model, metrics, and slices that are to be evaluated. Second a [`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/default_eval_shared_model) needs to be created that points to the actual model (or models) to be used during the evaluation. Once these have been defined, evaluation is performed by calling [`tfma.run_model_analysis`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis) with an appropriate dataset. For more details, see the [setup](https://www.tensorflow.org/tfx/model_analysis/setup) guide.\nIf running within a TFX pipeline, see the TFX [guide](https://www.tensorflow.org/tfx/guide) for how to configure TFMA to run as a TFX [Evaluator](https://www.tensorflow.org/tfx/guide/evaluator) component.\n## Examples\n### Single Model Evaluation\nThe following uses [`tfma.run_model_analysis`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis) to perform evaluation on a serving model. For an explanation of the different settings needed see the [setup](https://www.tensorflow.org/tfx/model_analysis/setup) guide.\n```\n# Run in a Jupyter Notebook.\nfromgoogle.protobufimport text_format\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics { class_name: \"AUC\" }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path='/path/to/saved/model', eval_config=eval_config)\n\neval_result = tfma.run_model_analysis(\n    eval_shared_model=eval_shared_model,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location='/path/to/file/containing/tfrecords',\n    output_path='/path/for/output')\n\ntfma.view.render_slicing_metrics(eval_result)\n\n```\n\nFor distributed evaluation, construct an [`tfma.ExtractEvaluateAndWriteResults`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ExtractEvaluateAndWriteResults) for evaluation and to write out the results. The results can be loaded for visualization using [`tfma.load_eval_result`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/load_eval_result).\nFor example:\n```\n# To run the pipeline.\nfromgoogle.protobufimport text_format\nfromtfx_bsl.tfxioimport tf_example_record\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC and as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics { class_name: \"AUC\" }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_model = tfma.default_eval_shared_model(\n    eval_saved_model_path='/path/to/saved/model', eval_config=eval_config)\n\noutput_path = '/path/for/output'\n\ntfx_io = tf_example_record.TFExampleRecord(\n    file_pattern=data_location, raw_record_column_name=tfma.ARROW_INPUT_COLUMN)\n\nwith beam.Pipeline(runner=...) as p:\n  _ = (p\n       # You can change the source as appropriate, e.g. read from BigQuery.\n       # This assumes your data is a TFRecords file containing records in the\n       # tf.train.Example format. If using EvalSavedModel then use the following\n       # instead: 'ReadData' >> beam.io.ReadFromTFRecord(file_pattern=...)\n       | 'ReadData' >> tfx_io.BeamSource()\n       | 'ExtractEvaluateAndWriteResults' >>\n       tfma.ExtractEvaluateAndWriteResults(\n            eval_shared_model=eval_shared_model,\n            eval_config=eval_config,\n            output_path=output_path))\n\n# To load and visualize results.\n# Note that this code should be run in a Jupyter Notebook.\nresult = tfma.load_eval_result(output_path)\ntfma.view.render_slicing_metrics(result)\n\n```\n\n### Model Validation\nTo perform model validation against a candiate and baseline, update the config to include a threshold setting and pass two models to [`tfma.run_model_analysis`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/run_model_analysis).\nFor example:\n```\n# Run in a Jupyter Notebook.\nfromgoogle.protobufimport text_format\n\neval_config = text_format.Parse(\"\"\"\n  ## Model information\n  model_specs {\n    # This assumes a serving model with a \"serving_default\" signature.\n    label_key: \"label\"\n    example_weight_key: \"weight\"\n  }\n  ## Post export metric information\n  metrics_specs {\n    # This adds AUC and as a post training metric. If the model has built in\n    # training metrics which also contains AUC, this metric will replace it.\n    metrics {\n      class_name: \"AUC\"\n      threshold {\n        value_threshold {\n          lower_bound { value: 0.9 }\n\n        change_threshold {\n          direction: HIGHER_IS_BETTER\n          absolute { value: -1e-10 }\n\n\n    }\n    # ... other post training metrics ...\n\n    # Plots are also configured here...\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  ## Slicing information\n  slicing_specs {}  # overall slice\n  slicing_specs {\n    feature_keys: [\"age\"]\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path='/path/to/saved/candiate/model',\n      eval_config=eval_config),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path='/path/to/saved/baseline/model',\n      eval_config=eval_config),\n]\n\noutput_path = '/path/for/output'\n\neval_result = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location='/path/to/file/containing/tfrecords',\n    output_path=output_path)\n\ntfma.view.render_slicing_metrics(eval_result)\ntfma.load_validation_result(output_path)\n\n```\n\n## Visualization\nTFMA evaluation results can be visualized in a Jupyter notebook using the frontend components included in TFMA. For example:\n.\n## More Information\n  * [Metrics and Plots](https://www.tensorflow.org/tfx/model_analysis/metrics)\n  * [Model Validations](https://www.tensorflow.org/tfx/model_analysis/model_validations)\n\n\n",
  "https://www.tensorflow.org/tfx/model_analysis/setup": "## Configuration\nTFMA stores its configuration in a \nAll TFMA pipelines are associated with a baseline (primary) model and zero or more candidate (secondary) models. The baseline and candidate model are defined by the user at the start of the pipeline and each require a unique name. The following are examples of typical configuration setups a user may use:\n  * Single model evaluation: \n    * N/A (i.e. no name)\n  * Validation-based evaluation: \n    * `baseline`\n    * `candidate`\n  * Model comparison evaluation: \n    * `my_model_a`\n    * `my_model_b`\n\n\n### Model Specs\nModel specs are of type [`tfma.ModelSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ModelSpec) and are used to define the location of a model as well as other model specific parameters. For example the following are typical settings that would need to be configured prior to running an evaluation:\n  * `name` - name of model (if multiple models used)\n  * `signature_name` - name of signature used for predictions (default is `serving_default`). Use `eval` if using an EvalSavedModel.\n  * `label_key` - name of the feature associated with the label.\n  * `example_weight_key` - name of the feature assocated with the example weight.\n\n\n### Metrics Specs\nMetrics specs are of type [`tfma.MetricsSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricsSpec) and are used to configure the metrics that will be calculated as part of the evaluation. Different machine learning problems use different types of metrics and TFMA offers a lot of options for configuring and customizing the metrics that are computed. Since metrics are a very large part of TFMA, they are discussed in detail separately in [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics).\n### Slicing Specs\nSlicing specs are of type [`tfma.SlicingSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec) and are used to configure the slices criteria that will be used during the evaluation. Slicing can be done either by `feature_keys`, `feature_values`, or both. Some examples of slicing specs are as follows:\n  * `{}`\n    * Slice consisting of overall data.\n  * `{ feature_keys: [\"country\"] }`\n    * Slices for all values in feature \"country\". For example, we might get slices \"country:us\", \"country:jp\", etc.\n  * `{ feature_values: [{key: \"country\", value: \"us\"}] }`\n    * Slice consisting of \"country:us\".\n  * `{ feature_keys: [\"country\", \"city\"] }`\n    * Slices for all values in feature \"country\" crossed with all values in feature \"city\" (note this may be expensive).\n  * `{ feature_keys: [\"country\"] feature_values: [{key: \"age\", value: \"20\"}] }`\n    * Slices for all values in feature \"country\" crossed with value \"age:20\"\n\n\nNote that feature keys may be either transformed features or raw input features. See [`tfma.SlicingSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec) for more information.\n## EvalSharedModel\nIn addition to the configuration settings, TFMA also requires that an instance of a [`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/types/EvalSharedModel) be created for sharing a model between multiple threads in the same process. The shared model instance includes information about the type of model (keras, etc) and how to load and configure the model from its saved location on disk (e.g. tags, etc). The [`tfma.default_eval_shared_model`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/default_eval_shared_model) API can be used to create a default instance given a path and set of tags.\n",
  "https://www.tensorflow.org/tfx/model_analysis/metrics": "## Overview\nTFMA supports the following metrics and plots:\n  * Standard keras metrics ([`tf.keras.metrics.*`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)) \n    * Note that you do not need a keras model to use keras metrics. Metrics are computed outside of the graph in beam using the metrics classes directly.\n  * Standard TFMA metrics and plots ([`tfma.metrics.*`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics))\n  * Custom keras metrics (metrics derived from [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric))\n  * Custom TFMA metrics (metrics derived from [`tfma.metrics.Metric`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/Metric)) using custom beam combiners or metrics derived from other metrics).\n\n\nTFMA also provides built-in support for converting binary classification metrics for use with multi-class/multi-label problems:\n  * Binarization based on class ID, top K, etc.\n  * Aggregated metrics based on micro averaging, macro averaging, etc.\n\n\nTFMA also provides built-in support for query/ranking based metrics where the examples are grouped by a query key automatically in the pipeline.\nCombined there are over 50+ standard metrics and plots available for a variety of problems including regression, binary classification, multi-class/multi-label classification, ranking, etc.\n## Configuration\nThere are two ways to configure metrics in TFMA: (1) using the [`tfma.MetricsSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/api/MetricsSpec) or (2) by creating instances of `tf.keras.metrics.*` and/or `tfma.metrics.*` classes in python and using [`tfma.metrics.specs_from_metrics`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/api/metrics/specs_from_metrics) to convert them to a list of [`tfma.MetricsSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricsSpec).\nThe following sections describe example configurations for different types of machine learning problems.\n### Regression Metrics\nThe following is an example configuration setup for a regression problem. Consult the `tf.keras.metrics.*` and `tfma.metrics.*` modules for possible additional metrics supported.\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"MeanSquaredError\" }\n    metrics { class_name: \"Accuracy\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics {\n      class_name: \"CalibrationPlot\"\n      config: '\"min_value\": 0, \"max_value\": 10'\n    }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.MeanSquaredError(name='mse'),\n    tf.keras.metrics.Accuracy(name='accuracy'),\n    tfma.metrics.MeanLabel(name='mean_label'),\n    tfma.metrics.MeanPrediction(name='mean_prediction'),\n    tfma.metrics.Calibration(name='calibration'),\n    tfma.metrics.CalibrationPlot(\n        name='calibration', min_value=0, max_value=10)\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n\n```\n\nNote that this setup is also avaliable by calling [`tfma.metrics.default_regression_specs`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/default_regression_specs).\n### Binary Classification Metrics\nThe following is an example configuration setup for a binary classification problem. Consult the `tf.keras.metrics.*` and `tfma.metrics.*` modules for possible additional metrics supported.\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"BinaryCrossentropy\" }\n    metrics { class_name: \"BinaryAccuracy\" }\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"AUCPrecisionRecall\" }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    metrics { class_name: \"Calibration\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n    metrics { class_name: \"CalibrationPlot\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.BinaryCrossentropy(name='binary_crossentropy'),\n    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    tf.keras.metrics.AUC(\n        name='auc_precision_recall', curve='PR', num_thresholds=10000),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n    tfma.metrics.MeanLabel(name='mean_label'),\n    tfma.metrics.MeanPrediction(name='mean_prediction'),\n    tfma.metrics.Calibration(name='calibration'),\n    tfma.metrics.ConfusionMatrixPlot(name='confusion_matrix_plot'),\n    tfma.metrics.CalibrationPlot(name='calibration_plot')\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n\n```\n\nNote that this setup is also avaliable by calling [`tfma.metrics.default_binary_classification_specs`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/default_binary_classification_specs).\n### Multi-class/Multi-label Classification Metrics\nThe following is an example configuration setup for a multi-class classification problem. Consult the `tf.keras.metrics.*` and `tfma.metrics.*` modules for possible additional metrics supported.\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"ExampleCount\" }\n    metrics { class_name: \"SparseCategoricalCrossentropy\" }\n    metrics { class_name: \"SparseCategoricalAccuracy\" }\n    metrics { class_name: \"Precision\" config: '\"top_k\": 1' }\n    metrics { class_name: \"Precision\" config: '\"top_k\": 3' }\n    metrics { class_name: \"Recall\" config: '\"top_k\": 1' }\n    metrics { class_name: \"Recall\" config: '\"top_k\": 3' }\n    metrics { class_name: \"MultiClassConfusionMatrixPlot\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    tfma.metrics.ExampleCount(name='example_count'),\n    tf.keras.metrics.SparseCategoricalCrossentropy(\n        name='sparse_categorical_crossentropy'),\n    tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n    tf.keras.metrics.Precision(name='precision', top_k=1),\n    tf.keras.metrics.Precision(name='precision', top_k=3),\n    tf.keras.metrics.Recall(name='recall', top_k=1),\n    tf.keras.metrics.Recall(name='recall', top_k=3),\n    tfma.metrics.MultiClassConfusionMatrixPlot(\n        name='multi_class_confusion_matrix_plot'),\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n\n```\n\nNote that this setup is also avaliable by calling [`tfma.metrics.default_multi_class_classification_specs`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/default_multi_class_classification_specs).\n### Multi-class/Multi-label Binarized Metrics\nMulti-class/multi-label metrics can be binarized to produce metrics per class, per top_k, etc using the [`tfma.BinarizationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/BinarizationOptions). For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    binarize: { class_ids: { values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] } }\n    // Metrics to binarize\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    // Metrics to binarize\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, binarize=tfma.BinarizationOptions(\n        class_ids={'values': [0,1,2,3,4,5,6,7,8,9]}))\n\n```\n\n### Multi-class/Multi-label Aggregate Metrics\nMulti-class/multi-label metrics can be aggregated to produce a single aggregated value for a binary classification metric by using [`tfma.AggregationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/AggregationOptions).\nNote that aggregation settings are independent of binarization settings so you can use both [`tfma.AggregationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/AggregationOptions) and [`tfma.BinarizationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/BinarizationOptions) at the same time.\n#### Micro Average\nMicro averaging can be performed by using the `micro_average` option within [`tfma.AggregationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/AggregationOptions). For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: { micro_average: true }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, aggregate=tfma.AggregationOptions(micro_average=True))\n\n```\n\nMicro averaging also supports setting `top_k` where only the top k values are used in the computation. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      micro_average: true\n      top_k_list: { values: [1, 3] }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(micro_average=True,\n                                      top_k_list={'values': [1, 3]}))\n\n```\n\n#### Macro / Weighted Macro Average\nMacro averaging can be performed by using the `macro_average` or `weighted_macro_average` options within [`tfma.AggregationOptions`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/AggregationOptions). Unless `top_k` settings are used, macro requires setting the `class_weights` in order to know which classes to compute the average for. If a `class_weight` is not provided then 0.0 is assumed. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      macro_average: true\n      class_weights: { key: 0 value: 1.0 }\n      class_weights: { key: 1 value: 1.0 }\n      class_weights: { key: 2 value: 1.0 }\n      class_weights: { key: 3 value: 1.0 }\n      class_weights: { key: 4 value: 1.0 }\n      class_weights: { key: 5 value: 1.0 }\n      class_weights: { key: 6 value: 1.0 }\n      class_weights: { key: 7 value: 1.0 }\n      class_weights: { key: 8 value: 1.0 }\n      class_weights: { key: 9 value: 1.0 }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(\n        macro_average=True, class_weights={i: 1.0 for i in range(10)}))\n\n```\n\nLike micro averaging, macro averaging also supports setting `top_k` where only the top k values are used in the computation. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    aggregate: {\n      macro_average: true\n      top_k_list: { values: [1, 3] }\n    }\n    // Metrics to aggregate\n    metrics { class_name: \"AUC\" }\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    // Metrics to aggregate\n    tf.keras.metrics.AUC(name='auc', num_thresholds=10000),\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics,\n    aggregate=tfma.AggregationOptions(macro_average=True,\n                                      top_k_list={'values': [1, 3]}))\n\n```\n\n### Query / Ranking Based Metrics\nQuery/ranking based metrics are enabled by specifying the `query_key` option in the metrics specs. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    query_key: \"doc_id\"\n    metrics {\n      class_name: \"NDCG\"\n      config: '\"gain_key\": \"gain\", \"top_k_list\": [1, 2]'\n    }\n    metrics { class_name: \"MinLabelPosition\" }\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThis same setup can be created using the following python code:\n```\nmetrics = [\n    tfma.metrics.NDCG(name='ndcg', gain_key='gain', top_k_list=[1, 2]),\n    tfma.metrics.MinLabelPosition(name='min_label_position')\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics, query_key='doc_id')\n\n```\n\n### Multi-model Evaluation Metrics\nTFMA supports evaluating multiple models at the same time. When multi-model evaluation is performed, metrics will be calculated for each model. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    # no model_names means all models\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nIf metrics need to be computed for a subset of models, set `model_names` in the `metric_specs`. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    model_names: [\"my-model1\"]\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThe `specs_from_metrics` API also supports passing model names:\n```\nmetrics = [\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, model_names=['my-model1'])\n\n```\n\n### Model Comparison Metrics\nTFMA supports evaluating comparison metrics for a candidate model against a baseline model. A simple way to setup the candidate and baseline model pair is to pass along a eval_shared_model with the proper model names (tfma.BASELINE_KEY and tfma.CANDIDATE_KEY):\n```\n\neval_config = text_format.Parse(\"\"\"\n  model_specs {\n    # ... model_spec without names ...\n  }\n  metrics_spec {\n    # ... metrics ...\n  }\n\"\"\", tfma.EvalConfig())\n\neval_shared_models = [\n  tfma.default_eval_shared_model(\n      model_name=tfma.CANDIDATE_KEY,\n      eval_saved_model_path='/path/to/saved/candidate/model',\n      eval_config=eval_config),\n  tfma.default_eval_shared_model(\n      model_name=tfma.BASELINE_KEY,\n      eval_saved_model_path='/path/to/saved/baseline/model',\n      eval_config=eval_config),\n]\n\neval_result = tfma.run_model_analysis(\n    eval_shared_models,\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location=\"/path/to/file/containing/tfrecords\",\n    output_path=\"/path/for/output\")\n\n```\n\nComparison metrics are computed automatically for all of the diff-able metrics (currently only scalar value metrics such as accuracy and AUC).\n### Multi-output Model Metrics\nTFMA supports evaluating metrics on models that have different outputs. Multi-output models store their output predictions in the form of a dict keyed by output name. When multi-output model's are used, the names of the outputs associated with a set of metrics must be specified in the `output_names` section of the MetricsSpec. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    output_names: [\"my-output\"]\n    ...\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\nThe `specs_from_metrics` API also supports passing output names:\n```\nmetrics = [\n    ...\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(\n    metrics, output_names=['my-output'])\n\n```\n\n### Customizing Metric Settings\nTFMA allows customizing of the settings that are used with different metrics. For example you might want to change the name, set thresholds, etc. This is done by adding a `config` section to the metric config. The config is specified using the JSON string version of the parameters that would be passed to the metrics `__init__` method (for ease of use the leading and trailing '{' and '}' brackets may be omitted). For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics {\n      class_name: \"ConfusionMatrixAtThresholds\"\n      config: '\"thresholds\": [0.3, 0.5, 0.8]'\n    }\n  }\n\"\"\", tfma.MetricsSpec()).metrics_specs\n\n```\n\nThis customization is of course also supported directly:\n```\nmetrics = [\n   tfma.metrics.ConfusionMatrixAtThresholds(thresholds=[0.3, 0.5, 0.8]),\n]\nmetrics_specs = tfma.metrics.specs_from_metrics(metrics)\n\n```\n\n## Outputs\nThe output of a metric evaluation is a series of metric keys/values and/or plot keys/values based on the configuration used.\n### Metric Keys\n  * Metric name (`auc`, `mean_label`, etc)\n  * Model name (only used if multi-model evaluation)\n  * Output name (only used if multi-output models are evaluated)\n  * Sub key (e.g. class ID if multi-class model is binarized)\n\n\n### Metric Value\n`double`, `ConfusionMatrixAtThresholds`, etc).\nBelow are the supported metric value types:\n  * `bounded_value` - Represents a real value which could be a pointwise estimate, optionally with approximate bounds of some sort. Has properties `value`, `lower_bound`, and `upper_bound`.\n  * `value_at_cutoffs` - Value at cutoffs (e.g. precision@K, recall@K). Has property `values`, each of which has properties `cutoff` and `value`.\n  * `confusion_matrix_at_thresholds` - Confusion matrix at thresholds. Has property `matrices`, each of which has properties for `threshold`, `precision`, `recall`, and confusion matrix values such as `false_negatives`.\n  * `array_value` - For metrics which return an array of values.\n\n\n### Plot Keys\n### Plot Values\nAll the supported plots are stored in a single proto called \n### EvalResult\nThe return from an evaluation run is an [`tfma.EvalResult`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalResult). This record contains `slicing_metrics` that encode the metric key as a multi-level dict where the levels correspond to output name, class ID, metric name, and metric value respectively. This is intended to be used for UI display in a Jupiter notebook. If access to the underlying data is needed the `metrics` result file should be used instead (see \n## Customization\nIn addition to custom metrics that are added as part of a saved keras (or legacy EvalSavedModel). There are two ways to customize metrics in TFMA post saving: (1) by defining a custom keras metric class and (2) by defining a custom TFMA metrics class backed by a beam combiner.\nIn both cases, the metrics are configured by specifying the name of the metric class and associated module. For example:\n```\nfromgoogle.protobufimport text_format\n\nmetrics_specs = text_format.Parse(\"\"\"\n  metrics_specs {\n    metrics { class_name: \"MyMetric\" module: \"my.module\"}\n  }\n\"\"\", tfma.EvalConfig()).metrics_specs\n\n```\n\n### Custom Keras Metrics\nTo create a custom keras metric, users need to extend [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric) with their implementation and then make sure the metric's module is available at evaluation time.\nNote that for metrics added post model save, TFMA only supports metrics that take label (i.e. y_true), prediction (y_pred), and example weight (sample_weight) as parameters to the `update_state` method.\n#### Keras Metric Example\nThe following is an example of a custom keras metric:\n```\nclassMyMetric(tf.keras.metrics.Mean):\n\n  def__init__(self, name='my_metric', dtype=None):\n    super(MyMetric, self).__init__(name=name, dtype=dtype)\n\n  defupdate_state(self, y_true, y_pred, sample_weight=None):\n    return super(MyMetric, self).update_state(\n        y_pred, sample_weight=sample_weight)\n\n```\n\n### Custom TFMA Metrics\nTo create a custom TFMA metric, users need to extend [`tfma.metrics.Metric`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/Metric) with their implementation and then make sure the metric's module is available at evaluation time.\n#### Metric\nA [`tfma.metrics.Metric`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/Metric) implementation is made up of a set of kwargs that define the metrics configuration along with a function for creating the computations (possibly multiple) needed to calcuate the metrics value. There are two main computation types that can be used: [`tfma.metrics.MetricComputation`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/MetricComputation) and [`tfma.metrics.DerivedMetricComputation`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/DerivedMetricComputation) that are described in the sections below. The function that creates these computations will be passed the following parameters as input:\n  * `eval_config: tfam.EvalConfig`\n    * The eval config passed to the evaluator (useful for looking up model spec settings such as prediction key to use, etc).\n  * `model_names: List[Text]`\n    * List of model names to compute metrics for (None if single-model)\n  * `output_names: List[Text]`. \n    * List of output names to compute metrics for (None if single-model)\n  * `sub_keys: List[tfma.SubKey]`. \n    * List of sub keys (class ID, top K, etc) to compute metrics for (or None)\n  * `aggregation_type: tfma.AggregationType`\n    * Type of aggregation if computing an aggregation metric.\n  * `class_weights: Dict[int, float]`. \n    * Class weights to use if computing an aggregation metric.\n  * `query_key: Text`\n    * Query key used if computing a query/ranking based metric.\n\n\nIf a metric is not associated with one or more of these settings then it may leave those parameters out of its signature definition.\nIf a metric is computed the same way for each model, output, and sub key, then the utility [`tfma.metrics.merge_per_key_computations`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/merge_per_key_computations) can be used to perform the same computations for each of these inputs separately.\n#### MetricComputation\nA `MetricComputation` is made up of a combination of `preprocessors` and a `combiner`. The `preprocessors` is a list of `preprocessor`, which is a `beam.DoFn` that takes extracts as its input and outputs the initial state that will be used by the combiner (see [architecture](https://www.tensorflow.org/tfx/model_analysis/architecture) for more info on what are extracts). All preprocessors will be executed sequentially in the order of the list. If the `preprocessors` is empty, then the combiner will be passed [StandardMetricInputs](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/StandardMetricInputs) (standard metric inputs contains labels, predictions, and example_weights). The `combiner` is a `beam.CombineFn` that takes a tuple of (slice key, preprocessor output) as its input and outputs a tuple of (slice_key, metric results dict) as its result.\nNote that slicing happens between the `preprocessors` and `combiner`.\nNote that if a metric computation wants to make use of both the standard metric inputs, but augment it with a few of the features from the `features` extracts, then the special [FeaturePreprocessor](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/FeaturePreprocessor) can be used which will merge the requested features from multiple combiners into a single shared StandardMetricsInputs value that is passed to all the combiners (the combiners are responsible for reading the features they are interested in and ignoring the rest).\n##### Example\nThe following is a very simple example of TFMA metric definition for computing the ExampleCount:\n```\nclassExampleCount(tfma.metrics.Metric):\n\n  def__init__(self, name: Text = 'example_count'):\n    super(ExampleCount, self).__init__(_example_count, name=name)\n\n\ndef_example_count(\n    name: Text = 'example_count') -> tfma.metrics.MetricComputations:\n  key = tfma.metrics.MetricKey(name=name)\n  return [\n      tfma.metrics.MetricComputation(\n          keys=[key],\n          preprocessors=[_ExampleCountPreprocessor()],\n          combiner=_ExampleCountCombiner(key))\n  ]\n\n\nclassExampleCountTest(tfma.test.testutil.TensorflowModelAnalysisTest):\n\n  deftestExampleCount(self):\n    metric = ExampleCount()\n    computations = metric.computations(example_weighted=False)\n    computation = computations[0]\n\n    with beam.Pipeline() as pipeline:\n      result = (\n          pipeline\n          | 'Create' >> beam.Create([...])  # Add inputs\n          | 'PreProcess' >> beam.ParDo(computation.preprocessors[0])\n          | 'Process' >> beam.Map(tfma.metrics.to_standard_metric_inputs)\n          | 'AddSlice' >> beam.Map(lambda x: ((), x))\n          | 'ComputeMetric' >> beam.CombinePerKey(computation.combiner)\n      )\n\n      defcheck_result(got):\n        try:\n          self.assertLen(got, 1)\n          got_slice_key, got_metrics = got[0]\n          self.assertEqual(got_slice_key, ())\n          key = computation.keys[0]\n          self.assertIn(key, got_metrics)\n          self.assertAlmostEqual(got_metrics[key], expected_value, places=5)\n        except AssertionError as err:\n          raise util.BeamAssertException(err)\n\n      util.assert_that(result, check_result, label='result')\n\nclass_ExampleCountPreprocessor(beam.DoFn):\n\n  defprocess(self, extracts: tfma.Extracts) -> Iterable[int]:\n    yield 1\n\n\nclass_ExampleCountPreprocessorTest(unittest.TestCase):\n\n  deftestExampleCountPreprocessor(self):\n    ...  # Init the test case here\n    with beam.Pipeline() as pipeline:\n      updated_pcoll = (\n          pipeline\n          | 'Create' >> beam.Create([...])  # Add inputs\n          | 'Preprocess'\n          >> beam.ParDo(\n              _ExampleCountPreprocessor()\n          )\n      )\n\n      beam_testing_util.assert_that(\n          updated_pcoll,\n          lambda result: ...,  # Assert the test case\n      )\n\n\nclass_ExampleCountCombiner(beam.CombineFn):\n\n  def__init__(self, metric_key: tfma.metrics.MetricKey):\n    self._metric_key = metric_key\n\n  defcreate_accumulator(self) -> int:\n    return 0\n\n  defadd_input(self, accumulator: int, state: int) -> int:\n    return accumulator + state\n\n  defmerge_accumulators(self, accumulators: Iterable[int]) -> int:\n    accumulators = iter(accumulators)\n    result = next(accumulator)\n    for accumulator in accumulators:\n      result += accumulator\n    return result\n\n  defextract_output(self,\n                     accumulator: int) -> Dict[tfma.metrics.MetricKey, int]:\n    return {self._metric_key: accumulator}\n\n```\n\n#### DerivedMetricComputation\nA `DerivedMetricComputation` is made up of a result function that is used to calculate metric values based on the output of other metric computations. The result function takes a dict of computed values as its input and outputs a dict of additional metric results.\nNote that it is acceptable (recommended) to include the computations that a derived computation depends on in the list of computations created by a metric. This avoid having to pre-create and pass computations that are shared between multiple metrics. The evaluator will automatically de-dup computations that have the same definition so ony one computation is actually run.\n##### Example\nThe \n",
  "https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_z_score": "Returns a standardized column with mean 0 and variance 1.  \n```\ntft.scale_to_z_score(\n    x: common_types.ConsistentTensorType,\n    elementwise: bool = False,\n    name: Optional[str] = None,\n    output_dtype: Optional[tf.DType] = None\n) -> common_types.ConsistentTensorType\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n\n  \nScaling to z-score subtracts out the mean and divides by standard deviation. Note that the standard deviation computed here is based on the biased variance (0 delta degrees of freedom), as computed by analyzers.var.\n## Args  \n---  \nA numeric `Tensor`, `SparseTensor`, or `RaggedTensor`.   \n`elementwise` |  If true, scales each element of the tensor independently; otherwise uses the mean and variance of the whole tensor.   \n`name` |  (Optional) A name for this operation.   \n`output_dtype` |  (Optional) If not None, casts the output tensor to this type.   \n## Returns  \n---  \nA `Tensor`, `SparseTensor`, or `RaggedTensor` containing the input column scaled to mean 0 and variance 1 (standard deviation 1), given by: (x - mean(x)) / std_dev(x). If `x` is floating point, the mean will have the same type as `x`. If `x` is integral, the output is cast to tf.float32. If the analysis dataset is empty or contains a single distinct value, then the input is returned without scaling.Note that TFLearn generally permits only tf.int64 and tf.float32, so casting this scaler's output may be necessary. \n",
  "https://www.tensorflow.org/tfx/model_analysis/faq": "## General\n### Is an EvalSavedModel still required?\nPreviously TFMA required all metrics to be stored within a tensorflow graph using a special `EvalSavedModel`. Now, metrics can be computed outside of the TF graph using `beam.CombineFn` implementations.\nSome of the main differences are:\n  * An `EvalSavedModel` requires a special export from the trainer whereas a serving model can be used without any changes required to the training code.\n  * When an `EvalSavedModel` is used, any metrics added at training time are automatically available at evaluation time. Without an `EvalSavedModel` these metrics must be re-added. \n    * The exception to this rule is if a keras model is used the metrics can also be added automatically because keras saves the metric information along side of the saved model.\n\n\n### Can TFMA work with both in-graph metrics and external metrics?\nTFMA allows a hybrid approach to be used where some metrics can be computed in-graph where as others can be computed outside. If you currently have an `EvalSavedModel` then you can continue to use it.\nThere are two cases:\n  1. Use TFMA `EvalSavedModel` for both feature extraction and metric computations but also add additional combiner-based metrics. In this case you would get all the in-graph metrics from the `EvalSavedModel` along with any additional metrics from the combiner-based that might not have been previously supported.\n  2. Use TFMA `EvalSavedModel` for feature/prediction extraction but use combiner-based metrics for all metrics computations. This mode is useful if there are feature transformations present in the `EvalSavedModel` that you would like to use for slicing, but prefer to perform all metric computations outside the graph.\n\n\n## Setup\n### What model types are supported?\nTFMA supports keras models, models based on generic TF2 signature APIs, as well TF estimator based models (although depending on the use case the estimator based models may require an `EvalSavedModel` to be used).\nSee [get_started](https://www.tensorflow.org/tfx/model_analysis/get_started) guide for the full list of model types supported and any restrictions.\n### How do I setup TFMA to work with a native keras based model?\nThe following is an example config for a keras model based on the following assumptions:\n  * Saved model is for serving and uses the signature name `serving_default` (this can be changed using `model_specs[0].signature_name`).\n  * Built in metrics from `model.compile(...)` should be evaluated (this can be disabled via `options.include_default_metric` within the [tfma.EvalConfig](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/EvalConfig)).\n\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: \"<label-key>\"\n    example_weight_key: \"<example-weight-key>\"\n  }\n  metrics_specs {\n    # Add metrics here. For example:\n    #  metrics { class_name: \"ConfusionMatrixPlot\" }\n    #  metrics { class_name: \"CalibrationPlot\" }\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about other types of metrics that can be configured.\n### How do I setup TFMA to work with a generic TF2 signatures based model?\nThe following is an example config for a generic TF2 model. Below, `signature_name` is the name of the specific signature that should be used for evaluation.\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"<signature-name>\"\n    label_key: \"<label-key>\"\n    example_weight_key: \"<example-weight-key>\"\n  }\n  metrics_specs {\n    # Add metrics here. For example:\n    #  metrics { class_name: \"BinaryCrossentropy\" }\n    #  metrics { class_name: \"ConfusionMatrixPlot\" }\n    #  metrics { class_name: \"CalibrationPlot\" }\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about other types of metrics that can be configured.\n### How do I setup TFMA to work with an estimator based model?\nIn this case there are three choices.\n**Option1: Use Serving Model**\nIf this option is used then any metrics added during training will NOT be included in the evaluation.\nThe following is an example config assuming `serving_default` is the signature name used:\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: \"<label-key>\"\n    example_weight_key: \"<example-weight-key>\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about other types of metrics that can be configured.\n**Option2: Use EvalSavedModel along with additional combiner-based metrics**\nIn this case, use `EvalSavedModel` for both feature / prediction extraction and evaluation and also add additional combiner based metrics.\nThe following is an example config:\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"eval\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about other types of metrics that can be configured and [EvalSavedModel](https://www.tensorflow.org/tfx/model_analysis/eval_saved_model) for more information about setting up the EvalSavedModel.\n**Option3: Use EvalSavedModel Model only for Feature / Prediction Extraction**\nSimilar to option(2), but only use `EvalSavedModel` for feature / prediction extraction. This option is useful if only external metrics are desired, but there are feature transformations that you would like to slice on. Similar to option (1) any metrics added during training will NOT be included in the evaluation.\nIn this case the config is the same as above only `include_default_metrics` is disabled.\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    signature_name: \"eval\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n  options {\n    include_default_metrics { value: false }\n  }\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about other types of metrics that can be configured and [EvalSavedModel](https://www.tensorflow.org/tfx/model_analysis/eval_saved_model) for more information about setting up the EvalSavedModel.\n### How do I setup TFMA to work with a keras model-to-estimator based model?\nThe keras `model_to_estimator` setup is similar to the estimator confiugration. However there are a few differences specific to how model to estimator works. In particular, the model-to-esimtator returns its outputs in the form of a dict where the dict key is the name of the last output layer in the associated keras model (if no name is provided, keras will choose a default name for you such as `dense_1` or `output_1`). From a TFMA perspective, this behavior is similar to what would be output for a multi-output model even though the model to estimator may only be for a single model. To account for this difference, an additional step is required to setup the output name. However, the same three options apply as estimator.\nThe following is an example of the changes required to an estimator based config:\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  ... as for estimator ...\n  metrics_specs {\n    output_names: [\"<keras-output-layer>\"]\n    # Add metrics here.\n  }\n  ... as for estimator ...\n\"\"\", tfma.EvalConfig())\n\n```\n\n### How do I setup TFMA to work with pre-calculated (i.e. model-agnostic) predictions? (`TFRecord` and `tf.Example`)\nIn order to configure TFMA to work with pre-calculated predictions, the default `tfma.PredictExtractor` must be disabled and the `tfma.InputExtractor` must be configured to parse the predictions along with the other input features. This is accomplished by configuring a [`tfma.ModelSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ModelSpec) with the name of the feature key used for the predictions alongside of the labels and weights.\nThe following is an example setup:\n```\nfromgoogle.protobufimport text_format\n\nconfig = text_format.Parse(\"\"\"\n  model_specs {\n    prediction_key: \"<prediction-key>\"\n    label_key: \"<label-key>\"\n    example_weight_key: \"<example-weight-key>\"\n  }\n  metrics_specs {\n    # Add metrics here.\n  }\n  slicing_specs {}\n\"\"\", tfma.EvalConfig())\n\n```\n\nSee [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about metrics that can be configured.\nNote that altough a [`tfma.ModelSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ModelSpec) is being configured a model is not actually being used (i.e. there is no [`tfma.EvalSharedModel`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/types/EvalSharedModel)). The call to run model analysis might look as follows:\n```\neval_result = tfma.run_model_analysis(\n    eval_config=eval_config,\n    # This assumes your data is a TFRecords file containing records in the\n    # tf.train.Example format.\n    data_location=\"/path/to/file/containing/tfrecords\",\n    output_path=\"/path/for/metrics_for_slice_proto\")\n\n```\n\n### How do I setup TFMA to work with pre-calculated (i.e. model-agnostic) predictions? (`pd.DataFrame`)\nFor small datasets that can fit in memory, an alternative to a `TFRecord` is a `pandas.DataFrame`s using the [`tfma.analyze_raw_data`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/analyze_raw_data) API. For an explanation of [`tfma.MetricsSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/MetricsSpec) and [`tfma.SlicingSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/SlicingSpec), see the [setup](https://www.tensorflow.org/tfx/model_analysis/setup) guide. See [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics) for more information about metrics that can be configured.\nThe following is an example setup:\n```\n# Run in a Jupyter Notebook.\n\ndf_data = ...  # your pd.DataFrame\n\neval_config = text_format.Parse(\"\"\"\n  model_specs {\n    label_key: 'label'\n    prediction_key: 'prediction'\n  }\n  metrics_specs {\n    metrics { class_name: \"AUC\" }\n    metrics { class_name: \"ConfusionMatrixPlot\" }\n  }\n  slicing_specs {}\n  slicing_specs {\n    feature_keys: 'language'\n  }\n\"\"\", config.EvalConfig())\n\neval_result = tfma.analyze_raw_data(df_data, eval_config)\n\ntfma.view.render_slicing_metrics(eval_result)\n\n```\n\n## Metrics\n### What types of metrics are supported?\nTFMA supports a wide variety of metrics including:\n  * [regression metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#regression_metrics)\n  * [binary classification metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#binary_classification_metrics)\n  * [multi-class/multi-label classification metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#multi_classmulti_label_classification_metrics)\n  * [micro average / macro average metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#multi_classmulti_label_aggregate_metrics)\n  * [query / ranking based metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#query_ranking_based_metrics)\n\n\n### Are metrics from multi-output models supported?\nYes. See [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#multi_output_model_metrics) guide for more details.\n### Are metrics from multiple-models supported?\nYes. See [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#multi_model_evaluation_metrics) guide for more details.\n### Can the metric settings (name, etc) be customized?\nYes. Metrics settings can be customized (e.g. setting specific thresholds, etc) by adding `config` settings to the metric configuration. See [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#customizing_metric_settings) guide has more details.\n### Are custom metrics supported?\nYes. Either by writing a custom [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric) implementation or by writing a custom `beam.CombineFn` implementation. The [metrics](https://www.tensorflow.org/tfx/model_analysis/metrics#customization) guide has more details.\n### What types of metrics are not supported?\nAs long as your metric can be calculated using a `beam.CombineFn`, there are no restrictions on the types of metrics that can be computed based on [`tfma.metrics.Metric`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/metrics/Metric). If working with a metric derived from [`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric) then the following criteria must be satisfied:\n  * It should be possible to compute sufficient statistics for the metric on each example independently, then combine these sufficient statistics by adding them across all the examples, and determine the metric value solely from these sufficient statistics.\n  * For example, for accuracy the sufficient statistics are \"total correct\" and \"total examples\". It’s possible to compute these two numbers for individual examples, and add them up for a group of examples to get the right values for those examples. The final accuracy can be computed used \"total correct / total examples\".\n\n\n## Add-ons\n### Can I use TFMA to evaluate fairness or bias in my model?\nTFMA includes a \n## Customization\n### What if I need more customization?\nTFMA is very flexibile and allows you to customize almost all parts of the pipeline using custom `Extractors`, `Evaluators`, and/or `Writers`. These abstrations are discusssed in more detail in the [architecture](https://www.tensorflow.org/tfx/model_analysis/architecture) document.\n## Troubleshooting, debugging, and getting help\n### Why don't MultiClassConfusionMatrix metrics match binarized ConfusionMatrix metrics\nThese are actually different calculations. Binarization performs a comparison for each class ID independently (i.e. the prediction for each class is compared separately against the thresholds provided). In this case it is possible for two or more classes to all indicate that they matched the prediction because their predicted value was greater than the threshold (this will be even more apparant at lower thresholds). In the case of the multiclass confusion matrix, there is still only one true predicted value and it either matches the actual value or it doesn't. The threshold is only used to force a prediction to match no class if it is less than the threshold. The higher the threshold the harder for a binarized class's prediction to match. Likewise the lower the threshold the easier it is for a binarized class's predictions to match. The means that at thresholds > 0.5 the binarized values and the multiclass matrix values will be closer aligned and at thresholds < 0.5 they will be farther apart.\nFor example, let's say we have 10 classes where class 2 was predicted with a probability of 0.8, but the actual class was class 1 which had a probability of 0.15. If you binarize on class 1 and use a threshold of 0.1, then class 1 will be considered correct (0.15 > 0.1) so it will be counted as a TP, However, for the multiclass case, class 2 will be considered correct (0.8 > 0.1) and since class 1 was the actual, this will be counted as a FN. Because at lower thresholds more values will be considered positives, in general there will be higher TP and FP counts for binarized confusion matrix than for the multiclass confusion matrix, and similarly lower TN and FN.\nThe following is an example of observed differences between MultiClassConfusionMatrixAtThresholds and the corresponding counts from binarization of one of the classes.\n### Why do my precision@1 and recall@1 metrics have the same value?\nAt a top k value of 1 precision and recall are the same thing. Precision is equal to `TP / (TP + FP)` and recall is equal to `TP / (TP + FN)`. The top prediction is always positive and will either match or not match the label. In other words, with `N` examples, `TP + FP = N`. However, if the label doesn't match the top prediction, then this also implies a non-top k prediction was matched and with top k set to 1, all non-top 1 predictions will be 0. This implies FN must be `(N - TP)` or `N = TP + FN`. The end result is `precision@1 = TP / N = recall@1`. Note that this only applies when there is a single label per example, not for multi-label.\n### Why are my mean_label and mean_prediction metrics always 0.5?\nThis is most likely caused because the metrics are configured for a binary classification problem, but the model is outputing probabilities for both of the classes instead of just one. This is common when [tensorflow's classification API](https://www.tensorflow.org/tfx/serving/signature_defs#classification_signaturedef) is used. The solution is to choose the class that you would like the predictions to be based on and then binarize on that class. For example:\n```\neval_config = text_format.Parse(\"\"\"\n  ...\n  metrics_specs {\n    binarize { class_ids: { values: [0] } }\n    metrics { class_name: \"MeanLabel\" }\n    metrics { class_name: \"MeanPrediction\" }\n    ...\n  }\n  ...\n\"\"\", config.EvalConfig())\n\n```\n\n### How to interpret the MultiLabelConfusionMatrixPlot?\nGiven a particular label, the `MultiLabelConfusionMatrixPlot` (and associated `MultiLabelConfusionMatrix`) can be used to compare the outcomes of other labels and their predictions when the chosen label was actually true. For example, let's say that we have three classes `bird`, `plane`, and `superman` and we are classifying pictures to indicate if they contain one or more of any of these classes. The `MultiLabelConfusionMatrix` will compute the cartesian product of each actual class against each other class (called the predicted class). Note that while the pairing is `(actual, predicted)`, the `predicted` class does not necessarily imply a positive prediction, it merely represents the predicted column in the actual vs predicted matrix. For example, let's say we have computed the following matrices:\n```\n   (bird, bird)         ->    { tp: 6, fp: 0, fn: 2, tn: 0}\n   (bird, plane)        ->    { tp: 2, fp: 2, fn: 2, tn: 2}\n   (bird, superman)     ->    { tp: 1, fp: 1, fn: 4, tn: 2}\n   (plane, bird)        ->    { tp: 3, fp: 1, fn: 1, tn: 3}\n   (plane, plane)       ->    { tp: 4, fp: 0, fn: 4, tn: 0}\n   (plane, superman)    ->    { tp: 1, fp: 3, fn: 3, tn: 1}\n   (superman, bird)     ->    { tp: 3, fp: 2, fn: 2, tn: 2}\n   (superman, plane)    ->    { tp: 2, fp: 3, fn: 2, tn: 2}\n   (superman, superman) ->    { tp: 4, fp: 0, fn: 5, tn: 0}\n\n   num_examples: 20\n\n```\n\nThe `MultiLabelConfusionMatrixPlot` has three ways to display this data. In all cases the way to read the table is row by row from the perspective of the actual class.\n1) Total Prediction Count\nIn this case, for a given row (i.e. actual class) what were the `TP + FP` counts for the other classes. For the counts above, our display would be as follows:\nPredicted bird | Predicted plane | Predicted superman  \n---|---|---  \nActual bird | 6 | 4 | 2  \nActual plane | 4 | 4 | 4  \nActual superman | 5 | 5 | 4  \nWhen the pictures actually contained a `bird` we correctly predicted 6 of them. At the same time we also predicted `plane` (either correctly or wrongly) 4 times and `superman` (either correctly or wrongly) 2 times.\n2) Incorrect Prediction Count\nIn this case, for a given row (i.e. actual class) what were the `FP` counts for the other classes. For the counts above, our display would be as follows:\nPredicted bird | Predicted plane | Predicted superman  \n---|---|---  \nActual bird | 0 | 2 | 1  \nActual plane | 1 | 0 | 3  \nActual superman | 2 | 3 | 0  \nWhen the pictures actually contained a `bird` we incorrectly predicted `plane` 2 times and `superman` 1 times.\n3) False Negative Count\nIn this case, for a given row (i.e. actual class) what were the `FN` counts for the other classes. For the counts above, our display would be as follows:\nPredicted bird | Predicted plane | Predicted superman  \n---|---|---  \nActual bird | 2 | 2 | 4  \nActual plane | 1 | 4 | 3  \nActual superman | 2 | 2 | 5  \nWhen the pictures actually contained a `bird` we failed to predict it 2 times. At the same time, we failed to predict `plane` 2 times and `superman` 4 times.\n### Why do I get an error about prediction key not found?\nSome model's output their prediction in the form of a dictionary. For example, a TF estimator for binary classification problem outputs a dictionary containing `probabilities`, `class_ids`, etc. In most cases TFMA has defaults for finding commomly used key names such as `predictions`, `probabilities`, etc. However, if your model is very customized it may output keys under names not known by TFMA. In theses cases a `prediciton_key` setting must be added to the [`tfma.ModelSpec`](https://www.tensorflow.org/tfx/model_analysis/api_docs/python/tfma/ModelSpec) to identify the name of the key the output is stored under.\n",
  "https://www.tensorflow.org/tfx/serving/serving_basic": "This tutorial shows you how to use TensorFlow Serving components to export a trained TensorFlow model and use the standard tensorflow_model_server to serve it. If you are already familiar with TensorFlow Serving, and you want to know more about how the server internals work, see the [TensorFlow Serving advanced tutorial](https://www.tensorflow.org/tfx/serving/serving_advanced).\nThis tutorial uses a simple Softmax Regression model that classifies handwritten digits. It is very similar to the one introduced in the [TensorFlow tutorial on image classification using the Fashion MNIST dataset](https://www.tensorflow.org/tutorials/keras/classification).\nThe code for this tutorial consists of two parts:\n  * A Python file, \n  * A ModelServer binary which can be either installed using Apt, or compiled from a C++ file (\n\n\nBefore getting started, first [install Docker](https://www.tensorflow.org/tfx/serving/docker#installing_docker).\n## Train and export TensorFlow model\nFor the training phase, the TensorFlow graph is launched in TensorFlow session `sess`, with the input tensor (image) as `x` and output tensor (Softmax score) as `y`.\nThen we use TensorFlow's `SavedModelBuilder` saves a \"snapshot\" of the trained model to reliable storage so that it can be loaded later for inference.\nFor details on the SavedModel format, please see the documentation at \nFrom \n```\nexport_path_base = sys.argv[-1]\nexport_path = os.path.join(\n    tf.compat.as_bytes(export_path_base),\n    tf.compat.as_bytes(str(FLAGS.model_version)))\nprint('Exporting trained model to', export_path)\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\nbuilder.add_meta_graph_and_variables(\n    sess, [tf.compat.v1.saved_model.tag_constants.SERVING],\n    signature_def_map={\n        'predict_images':\n            prediction_signature,\n        tf.compat.v1.saved_model.signature_constants\n            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n            classification_signature,\n    },\n    main_op=tf.compat.v1.tables_initializer(),\n    strip_default_attrs=True)\nbuilder.save()\n\n```\n\n`SavedModelBuilder.__init__` takes the following argument:\n  * `export_path` is the path of the export directory.\n\n\n`SavedModelBuilder` will create the directory if it does not exist. In the example, we concatenate the command line argument and `FLAGS.model_version` to obtain the export directory. `FLAGS.model_version` specifies the **version** of the model. You should specify a larger integer value when exporting a newer version of the same model. Each version will be exported to a different sub-directory under the given path.\nYou can add meta graph and variables to the builder using `SavedModelBuilder.add_meta_graph_and_variables()` with the following arguments:\n  * `sess` is the TensorFlow session that holds the trained model you are exporting.\n  * `tags` is the set of tags with which to save the meta graph. In this case, since we intend to use the graph in serving, we use the `serve` tag from predefined SavedModel tag constants. For more details, see [related TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/tag_constants).\n  * `signature_def_map` specifies the map of user-supplied key for a **signature** to a tensorflow::SignatureDef to add to the meta graph. Signature specifies what type of model is being exported, and the input/output tensors to bind to when running inference.\nThe special signature key `serving_default` specifies the default serving signature. The default serving signature def key, along with other constants related to signatures, are defined as part of SavedModel signature constants. For more details, see [related TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_constants).\nFurther, to help build signature defs easily, the SavedModel API provides [signature def utils](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_def_utils).. Specifically, in the original `signature_def_utils.build_signature_def()` to build `predict_signature` and `classification_signature`.\nAs an example for how `predict_signature` is defined, the util takes the following arguments:\n    * `inputs={'images': tensor_info_x}` specifies the input tensor info.\n    * `outputs={'scores': tensor_info_y}` specifies the scores tensor info.\n    * `method_name` is the method used for the inference. For Prediction requests, it should be set to `tensorflow/serving/predict`. For other method names, see [related TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_constants).\n\n\nNote that `tensor_info_x` and `tensor_info_y` have the structure of `tensorflow::TensorInfo` protocol buffer defined [related TensorFlow API documentation](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/utils).\nAlso, note that `images` and `scores` are tensor alias names. They can be whatever unique strings you want, and they will become the logical names of tensor `x` and `y` that you refer to for tensor binding when sending prediction requests later.\nFor instance, if `x` refers to the tensor with name 'long_tensor_name_foo' and `y` refers to the tensor with name 'generated_tensor_name_bar', `builder` will store tensor logical name to real name mapping ('images' -> 'long_tensor_name_foo') and ('scores' -> 'generated_tensor_name_bar'). This allows the user to refer to these tensors with their logical names when running inference.\nLet's run it!\nFirst, if you haven't done so yet, clone this repository to your local machine:\n```\ngitcd\n```\n\nClear the export directory if it already exists:\n```\nrm\n```\n\nNow let's train the model:\n```\ntools/run_in_docker.sh\\\n\n```\n\nThis should result in output that looks like:\n```\nTraining model...\n\n...\n\nDone training!\nExporting trained model to models/mnist\nDone exporting!\n\n```\n\nNow let's take a look at the export directory.\n```\n$ ls1\n\n```\n\nAs mentioned above, a sub-directory will be created for exporting each version of the model. `FLAGS.model_version` has the default value of 1, therefore the corresponding sub-directory `1` is created.\n```\n$ lssaved_model.pb variables\n\n```\n\nEach version sub-directory contains the following files:\n  * `saved_model.pb` is the serialized tensorflow::SavedModel. It includes one or more graph definitions of the model, as well as metadata of the model such as signatures.\n  * `variables` are files that hold the serialized variables of the graphs.\n\n\nWith that, your TensorFlow model is exported and ready to be loaded!\n## Load exported model with standard TensorFlow ModelServer\nUse a Docker serving image to easily load the model for serving:\n```\ndocker8500:8500\\\n--mounttype=bind,source=/tmp/mnist,target=/models/mnist\\\n-eMODEL_NAME=mnist\n```\n\n## Test the server\nWe can use the provided \n```\ntools/run_in_docker.sh\\\n=1000=127.0.0.1:8500\n\n```\n\nThis should output something like\n```\n    ...\n    Inference error rate: 11.13%\n\n```\n\nWe expect around 90% accuracy for the trained Softmax model and we get 11% inference error rate for the first 1000 test images. This confirms that the server loads and runs the trained model successfully!\n",
  "https://www.tensorflow.org/tfx/model_analysis/eval_saved_model": "TensorFlow Model Analysis (TFMA) can export a model's _evaluation graph_ to a special `SavedModel` called `EvalSavedModel`. (Note that the evaluation graph is used and not the graph for training or inference.) The `EvalSavedModel` contains additional information that allows TFMA to compute the same evaluation metrics defined in the model in a distributed manner over a large amount of data and user-defined slices.\n## Modify an existing model\nTo use an existing model with TFMA, first modify the model to export the `EvalSavedModel`. This is done by adding a call to `tfma.export.export_eval_savedmodel` and is similar to `estimator.export_savedmodel`. For example:\n```\n# Define, train and export your estimator as usual\nestimator = tf.estimator.DNNClassifier(...)\nestimator.train(...)\nestimator.export_savedmodel(...)\n\n# Also export the EvalSavedModel\ntfma.export.export_eval_savedmodel(\n  estimator=estimator, export_dir_base=export_dir,\n  eval_input_receiver_fn=eval_input_receiver_fn)\n\n```\n\n`eval_input_receiver_fn` must be defined and is similar to the `serving_input_receiver_fn` for `estimator.export_savedmodel`. Like `serving_input_receiver_fn`, the `eval_input_receiver_fn` function defines an input placeholder example, parses the features from the example, and returns the parsed features. It parses and returns the label.\nThe following snippet defines an example `eval_input_receiver_fn`:\n```\ncountry = tf.feature_column.categorical_column_with_hash('country', 100)\nlanguage = tf.feature_column.categorical_column_with_hash('language', 100)\nage = tf.feature_column.numeric_column('age')\nlabel = tf.feature_column.numeric_column('label')\n\ndefeval_input_receiver_fn():\n  serialized_tf_example = tf.compat.v1.placeholder(\n      dtype=tf.string, shape=[None], name='input_example_placeholder')\n\n  # This *must* be a dictionary containing a single key 'examples', which\n  # points to the input placeholder.\n  receiver_tensors = {'examples': serialized_tf_example}\n\n  feature_spec =  tf.feature_column.make_parse_example_spec(\n      [country, language, age, label])\n  features = tf.io.parse_example(serialized_tf_example, feature_spec)\n\n  return tfma.export.EvalInputReceiver(\n    features=features,\n    receiver_tensors=receiver_tensors,\n    labels=features['label'])\n\n```\n\nIn this example you can see that:\n  * `labels` can also be a dictionary. Useful for a multi-headed model.\n  * The `eval_input_receiver_fn` function will, most likely, be the same as your `serving_input_receiver_fn` function. But, in some cases, you may want to define additional features for slicing. For example, you introduce an `age_category` feature which divides the `age` feature into multiple buckets. You can then slice on this feature in TFMA to help understand how your model's performance differs across different age categories.\n\n\n## Adding Post Export Metrics\nAdditional metrics that are not included in the model can be aded using `add_metrics_callbacks`. For more details, see the Python help for `run_model_analysis`.\n## End-to-end examples\nTry the extensive [TensorFlow Estimators](https://www.tensorflow.org/guide/estimators) for training, \n## Adding a Custom Post Export Metric\nIf you want to add your own custom post export metric in TFMA, please checkout the documentation \n",
  "https://www.tensorflow.org/tfx/transform/get_started": "This guide introduces the basic concepts of `tf.Transform` and how to use them. It will:\n  * Define a _preprocessing function_ , a logical description of the pipeline that transforms the raw data into the data used to train a machine learning model.\n  * Show the _preprocessing function_ into a _Beam pipeline_.\n  * Show additional usage examples.\n\n\n## Setup\n```\npip\n```\n```\npip\n```\n```\nimportpkg_resources\nimportimportlib\nimportlib.reload(pkg_resources)\n\n```\n```\n<module 'pkg_resources' from '/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/pkg_resources/__init__.py'>\n\n```\n```\nimportos\nimporttempfile\n\nimporttensorflowastf\nimporttensorflow_transformastft\nimporttensorflow_transform.beamastft_beam\n\nfromtensorflow_transform.tf_metadataimport dataset_metadata\nfromtensorflow_transform.tf_metadataimport schema_utils\n\nfromtfx_bsl.publicimport tfxio\n\n```\n```\n2023-04-13 09:15:54.685940: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n2023-04-13 09:15:54.686060: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n2023-04-13 09:15:54.686073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n\n```\n\n## Define a preprocessing function\nThe _preprocessing function_ is the most important concept of `tf.Transform`. The preprocessing function is a logical description of a transformation of the dataset. The preprocessing function accepts and returns a dictionary of tensors, where a _tensor_ means `Tensor` or `SparseTensor`. There are two kinds of functions used to define the preprocessing function:\n  1. Any function that accepts and returns tensors. These add TensorFlow operations to the graph that transform raw data into transformed data.\n  2. Any of the _analyzers_ provided by `tf.Transform`. Analyzers also accept and return tensors, but unlike TensorFlow functions, they _do not_ add operations to the graph. Instead, analyzers cause `tf.Transform` to compute a full-pass operation outside of TensorFlow. They use the input tensor values over the entire dataset to generate a constant tensor that is returned as the output. For example, [`tft.min`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/min) computes the minimum of a tensor over the dataset. `tf.Transform` provides a fixed set of analyzers, but this will be extended in future versions.\n\n\n### Preprocessing function example\nBy combining analyzers and regular TensorFlow functions, users can create flexible pipelines for transforming data. The following preprocessing function transforms each of the three features in different ways, and combines two of the features:\n```\ndefpreprocessing_fn(inputs):\n  x = inputs['x']\n  y = inputs['y']\n  s = inputs['s']\n  x_centered = x - tft.mean(x)\n  y_normalized = tft.scale_to_0_1(y)\n  s_integerized = tft.compute_and_apply_vocabulary(s)\n  x_centered_times_y_normalized = x_centered * y_normalized\n  return {\n      'x_centered': x_centered,\n      'y_normalized': y_normalized,\n      'x_centered_times_y_normalized': x_centered_times_y_normalized,\n      's_integerized': s_integerized\n  }\n\n```\n\nHere, `x`, `y` and `s` are `Tensor`s that represent input features. The first new tensor that is created, `x_centered`, is built by applying [`tft.mean`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/mean) to `x` and subtracting this from `x`. [`tft.mean(x)`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/mean) returns a tensor representing the mean of the tensor `x`. `x_centered` is the tensor `x` with the mean subtracted.\nThe second new tensor, `y_normalized`, is created in a similar manner but using the convenience method [`tft.scale_to_0_1`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_0_1). This method does something similar to computing `x_centered`, namely computing a maximum and minimum and using these to scale `y`.\nThe tensor `s_integerized` shows an example of string manipulation. In this case, we take a string and map it to an integer. This uses the convenience function [`tft.compute_and_apply_vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/compute_and_apply_vocabulary). This function uses an analyzer to compute the unique values taken by the input strings, and then uses TensorFlow operations to convert the input strings to indices in the table of unique values.\nThe final column shows that it is possible to use TensorFlow operations to create new features by combining tensors.\nThe preprocessing function defines a pipeline of operations on a dataset. In order to apply the pipeline, we rely on a concrete implementation of the `tf.Transform` API. The Apache Beam implementation provides `PTransform` which applies a user's preprocessing function to data. The typical workflow of a `tf.Transform` user will construct a preprocessing function, then incorporate this into a larger Beam pipeline, creating the data for training.\n### Batching\nBatching is an important part of TensorFlow. Since one of the goals of `tf.Transform` is to provide a TensorFlow graph for preprocessing that can be incorporated into the serving graph (and, optionally, the training graph), batching is also an important concept in `tf.Transform`.\nWhile not obvious in the example above, the user defined preprocessing function is passed tensors representing _batches_ and not individual instances, as happens during training and serving with TensorFlow. On the other hand, analyzers perform a computation over the entire dataset that returns a single value and not a batch of values. `x` is a `Tensor` with a shape of `(batch_size,)`, while [`tft.mean(x)`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/mean) is a `Tensor` with a shape of `()`. The subtraction `x - tft.mean(x)` broadcasts where the value of `tft.mean(x)` is subtracted from every element of the batch represented by `x`.\n## Apache Beam Implementation\nWhile the _preprocessing function_ is intended as a logical description of a _preprocessing pipeline_ implemented on multiple data processing frameworks, `tf.Transform` provides a canonical implementation used on Apache Beam. This implementation demonstrates the functionality required from an implementation. There is no formal API for this functionality, so each implementation can use an API that is idiomatic for its particular data processing framework.\nThe Apache Beam implementation provides two `PTransform`s used to process data for a preprocessing function. The following shows the usage for the composite `PTransform` - [`tft_beam.AnalyzeAndTransformDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset):\n```\nraw_data = [\n    {'x': 1, 'y': 1, 's': 'hello'},\n    {'x': 2, 'y': 2, 's': 'world'},\n    {'x': 3, 'y': 3, 's': 'hello'}\n]\n\nraw_data_metadata = dataset_metadata.DatasetMetadata(\n    schema_utils.schema_from_feature_spec({\n        'y': tf.io.FixedLenFeature([], tf.float32),\n        'x': tf.io.FixedLenFeature([], tf.float32),\n        's': tf.io.FixedLenFeature([], tf.string),\n    }))\n\nwith tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n  transformed_dataset, transform_fn = (\n      (raw_data, raw_data_metadata) |\n      tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse ref() instead.\n2023-04-13 09:15:56.867283: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_transform/tf_utils.py:324: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse ref() instead.\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpzu0d2pwa.json', '--HistoryManager.hist_file=:memory:']\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpdhm3m_yu/tftransform_tmp/88750e1500194862a87b2f23e04367bc/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpdhm3m_yu/tftransform_tmp/88750e1500194862a87b2f23e04367bc/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpdhm3m_yu/tftransform_tmp/8fad0af5a26242cc9733a752a7652277/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpdhm3m_yu/tftransform_tmp/8fad0af5a26242cc9733a752a7652277/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n\n```\n```\ntransformed_data, transformed_metadata = transformed_dataset\n\n```\n\nThe `transformed_data` content is shown below and contains the transformed columns in the same format as the raw data. In particular, the values of `s_integerized` are `[0, 1, 0]`—these values depend on how the words `hello` and `world` were mapped to integers, which is deterministic. For the column `x_centered`, we subtracted the mean so the values of the column `x`, which were `[1.0, 2.0, 3.0]`, became `[-1.0, 0.0, 1.0]`. Similarly, the rest of the columns match their expected values.\n```\ntransformed_data\n\n```\n```\n[{'s_integerized': 0,\n  'x_centered': -1.0,\n  'x_centered_times_y_normalized': -0.0,\n  'y_normalized': 0.0},\n {'s_integerized': 1,\n  'x_centered': 0.0,\n  'x_centered_times_y_normalized': 0.0,\n  'y_normalized': 0.5},\n {'s_integerized': 0,\n  'x_centered': 1.0,\n  'x_centered_times_y_normalized': 1.0,\n  'y_normalized': 1.0}]\n\n```\n\nBoth `raw_data` and `transformed_data` are datasets. The next two sections show how the Beam implementation represents datasets and how to read and write data to disk. The other return value, `transform_fn`, represents the transformation applied to the data, covered in detail below.\nThe [`tft_beam.AnalyzeAndTransformDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) class is the composition of the two fundamental transforms provided by the implementation [`tft_beam.AnalyzeDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeDataset) and [`tft_beam.TransformDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/TransformDataset). So the following two code snippets are equivalent:\n```\nmy_data = (raw_data, raw_data_metadata)\n\n```\n```\nwith tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n  transformed_data, transform_fn = (\n      my_data | tft_beam.AnalyzeAndTransformDataset(preprocessing_fn))\n\n```\n```\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpzu0d2pwa.json', '--HistoryManager.hist_file=:memory:']\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8afa0l99/tftransform_tmp/8dc250e431e848a386d53f050ae886df/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8afa0l99/tftransform_tmp/8dc250e431e848a386d53f050ae886df/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8afa0l99/tftransform_tmp/46d2e23e8b9745219e9812f9b7f5aee1/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8afa0l99/tftransform_tmp/46d2e23e8b9745219e9812f9b7f5aee1/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n\n```\n```\nwith tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n  transform_fn = my_data | tft_beam.AnalyzeDataset(preprocessing_fn)\n  transformed_data = (my_data, transform_fn) | tft_beam.TransformDataset()\n\n```\n```\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpzu0d2pwa.json', '--HistoryManager.hist_file=:memory:']\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpoezjiky4/tftransform_tmp/2f6feb69b15d4a429fa4f56dd7fb02a3/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpoezjiky4/tftransform_tmp/2f6feb69b15d4a429fa4f56dd7fb02a3/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpoezjiky4/tftransform_tmp/26cbcc6000e947c798b5af9ad57c0b42/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpoezjiky4/tftransform_tmp/26cbcc6000e947c798b5af9ad57c0b42/assets\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:tensorflow:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpzu0d2pwa.json', '--HistoryManager.hist_file=:memory:']\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n\n```\n\n`transform_fn` is a pure function that represents an operation that is applied to each row of the dataset. In particular, the analyzer values are already computed and treated as constants. In the example, the `transform_fn` contains as constants the mean of column `x`, the min and max of column `y`, and the vocabulary used to map the strings to integers.\nAn important feature of `tf.Transform` is that `transform_fn` represents a map _over rows_ —it is a pure function applied to each row separately. All of the computation for aggregating rows is done in `AnalyzeDataset`. Furthermore, the `transform_fn` is represented as a TensorFlow `Graph` which can be embedded into the serving graph.\n`AnalyzeAndTransformDataset` is provided for optimizations in this special case. This is the same pattern used in `fit`, `transform`, and `fit_transform` methods.\n## Data Formats and Schema\nTFT Beam implementation accepts two different input data formats. The \"instance dict\" format (as seen in the example above and [simple.ipynb](https://www.tensorflow.org/tfx/tutorials/transform/simple) &\nThe \"metadata\" accompanying the `PCollection` tells the Beam implementation the format of the `PCollection`.\n```\n(raw_data, raw_data_metadata) | tft.AnalyzeDataset(...)\n\n```\n\n  * If `raw_data_metadata` is a `dataset_metadata.DatasetMetadata` (see below, \"The 'instance dict' format\" section), then `raw_data` is expected to be in the \"instance dict\" format.\n  * If `raw_data_metadata` is a [`tfxio.TensorAdapterConfig`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorAdapterConfig) (see below, \"The TFXIO format\" section), then `raw_data` is expected to be in the TFXIO format.\n\n\n### The \"instance dict\" format\nThe previous code examples used this format. The metadata contains the schema that defines the layout of the data and how it is read from and written to various formats. Even this in-memory format is not self-describing and requires the schema in order to be interpreted as tensors.\nAgain, here is the definition of the schema for the example data:\n```\nimporttensorflow_transformastft\n\nraw_data_metadata = tft.DatasetMetadata.from_feature_spec({\n        's': tf.io.FixedLenFeature([], tf.string),\n        'y': tf.io.FixedLenFeature([], tf.float32),\n        'x': tf.io.FixedLenFeature([], tf.float32),\n    })\n\n```\n\nThe `Schema` proto contains the information needed to parse the data from its on-disk or in-memory format, into tensors. It is typically constructed by calling `schema_utils.schema_from_feature_spec` with a dict mapping feature keys to [`tf.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature), [`tf.io.VarLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature), and [`tf.io.SparseFeature`](https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature) values. See the documentation for [`tf.parse_example`](https://www.tensorflow.org/api_docs/python/tf/parse_example) for more details.\nAbove we use [`tf.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature) to indicate that each feature contains a fixed number of values, in this case a single scalar value. Because `tf.Transform` batches instances, the actual `Tensor` representing the feature will have shape `(None,)` where the unknown dimension is the batch dimension.\n### The TFXIO format\nWith this format, the data is expected to be contained in a `RecordBatch`es that consist of columns of the following types:\n  * `pa.list_(<primitive>)`, where `<primitive>` is `pa.int64()`, `pa.float32()` `pa.binary()` or `pa.large_binary()`.\n  * `pa.large_list(<primitive>)`\n\n\nThe toy input dataset we used above, when represented as a `RecordBatch`, looks like the following:\n```\nimportpyarrowaspa\n\nraw_data = [\n    pa.record_batch(\n    data=[\n        pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n        pa.array([[1], [2], [3]], pa.list_(pa.float32())),\n        pa.array([['hello'], ['world'], ['hello']], pa.list_(pa.binary())),\n    ],\n    names=['x', 'y', 's'])\n]\n\n```\n\nSimilar to the `dataset_metadata.DatasetMetadata` instance that accompanies the \"instance dict\" format, a [`tfxio.TensorAdapterConfig`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorAdapterConfig) is must accompany the `RecordBatch`es. It consists of the Arrow schema of the `RecordBatch`es, and [`tfxio.TensorRepresentations`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorRepresentations) to uniquely determine how columns in `RecordBatch`es can be interpreted as TensorFlow Tensors (including but not limited to [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), [`tf.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)).\n[`tfxio.TensorRepresentations`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorRepresentations) is type alias for a `Dict[str, tensorflow_metadata.proto.v0.schema_pb2.TensorRepresentation]` which establishes the relationship between a Tensor that a `preprocessing_fn` accepts and columns in the `RecordBatch`es. For example:\n```\nfromgoogle.protobufimport text_format\nfromtensorflow_metadata.proto.v0import schema_pb2\n\ntensor_representation = {\n    'x': text_format.Parse(\n\"\"\"dense_tensor { column_name: \"col1\" shape { dim { size: 2 } } }\"\"\",\n        schema_pb2.TensorRepresentation())\n}\n\n```\n\nMeans that `inputs['x']` in `preprocessing_fn` should be a dense `tf.Tensor`, whose values come from a column of name `'col1'` in the input `RecordBatch`es, and its (batched) shape should be `[batch_size, 2]`.\nA [`schema_pb2.TensorRepresentation`](https://www.tensorflow.org/tfx/tf_metadata/api_docs/python/tfmd/proto/schema_pb2/TensorRepresentation) is a Protobuf defined in \n## Compatibility with TensorFlow\n`tf.Transform` provides support for exporting the `transform_fn` as a SavedModel, see the [simple tutorial](https://www.tensorflow.org/tfx/tutorials/transform/simple) for an example. The default behavior before the `0.30` release exported a TF 1.x SavedModel. Starting with the `0.30` release, the default behavior is to export a TF 2.x SavedModel unless TF 2.x behaviors are explicitly disabled (by calling [`tf.compat.v1.disable_v2_behavior()`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_v2_behavior)).\nIf using TF 1.x concepts such as `tf.estimator` and `tf.Sessions`, you can retain the previous behavior by passing `force_tf_compat_v1=True` to [`tft_beam.Context`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/Context) if using `tf.Transform` as a standalone library or to the [Transform](https://www.tensorflow.org/tfx/api_docs/python/tfx/components/Transform) component in TFX.\nWhen exporting the `transform_fn` as a TF 2.x SavedModel, the `preprocessing_fn` is expected to be traceable using [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Additionally, if running your pipeline remotely (for example with the `DataflowRunner`), ensure that the `preprocessing_fn` and any dependencies are packaged properly as described \nKnown issues with using `tf.Transform` to export a TF 2.x SavedModel are documented [here](https://www.tensorflow.org/tfx/transform/tf2_support).\n## Input and output with Apache Beam\nSo far, we've seen input and output data in python lists (of `RecordBatch`es or instance dictionaries). This is a simplification that relies on Apache Beam's ability to work with lists as well as its main representation of data, the `PCollection`.\nA `PCollection` is a data representation that forms a part of a Beam pipeline. A Beam pipeline is formed by applying various `PTransform`s, including `AnalyzeDataset` and `TransformDataset`, and running the pipeline. A `PCollection` is not created in the memory of the main binary, but instead is distributed among the workers (although this section uses the in-memory execution mode).\n### Pre-canned `PCollection` Sources (`TFXIO`)\nThe `RecordBatch` format that our implementation accepts is a common format that other TFX libraries accept. Therefore TFX offers convenient \"sources\" (a.k.a `TFXIO`) that read files of various formats on disk and produce `RecordBatch`es and can also give [`tfxio.TensorAdapterConfig`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorAdapterConfig), including inferred [`tfxio.TensorRepresentations`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/TensorRepresentations).\nThose `TFXIO`s can be found in package `tfx_bsl` ([`tfx_bsl.public.tfxio`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio)).\n## Example: \"Census Income\" dataset\nThe following example requires both reading and writing data on disk and representing data as a `PCollection` (not a list), see: \nHere is some code to download and preview this data:\n```\nwget\n```\n```\n--2023-04-13 09:16:10--  https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.203.128, 74.125.141.128, 142.250.98.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.203.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3974305 (3.8M) [application/octet-stream]\nSaving to: ‘adult.data’\n\nadult.data          100%[===================>]   3.79M  --.-KB/s    in 0.02s   \n\n2023-04-13 09:16:10 (153 MB/s) - ‘adult.data’ saved [3974305/3974305]\n\n```\n```\nimportpandasaspd\n\ntrain_data_file = \"adult.data\"\n\n```\n\nThere's some configuration code hidden in the cell below.\nToggle code```\nORDERED_CSV_COLUMNS = [\n    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n]\n\nCATEGORICAL_FEATURE_KEYS = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n]\n\nNUMERIC_FEATURE_KEYS = [\n    'age',\n    'capital-gain',\n    'capital-loss',\n    'hours-per-week',\n    'education-num',\n]\n\nLABEL_KEY = 'label'\n\nRAW_DATA_FEATURE_SPEC = dict(\n    [(name, tf.io.FixedLenFeature([], tf.string))\n     for name in CATEGORICAL_FEATURE_KEYS] +\n    [(name, tf.io.FixedLenFeature([], tf.float32))\n     for name in NUMERIC_FEATURE_KEYS] +\n    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n)\n\nSCHEMA = tft.tf_metadata.dataset_metadata.DatasetMetadata(\n    tft.tf_metadata.schema_utils.schema_from_feature_spec(RAW_DATA_FEATURE_SPEC)).schema\n\n```\n```\npd.read_csv(train_data_file, names = ORDERED_CSV_COLUMNS).head()\n\n```\n\nThe columns of the dataset are either categorical or numeric. This dataset describes a classification problem: predicting the last column where the individual earns more or less than 50K per year. However, from the perspective of `tf.Transform`, this label is just another categorical column.\nWe use a Pre-canned [`tfxio.BeamRecordCsvTFXIO`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/BeamRecordCsvTFXIO) to translate the CSV lines into `RecordBatches`. `TFXIO` requires two important piece of information:\n  * a TensorFlow Metadata Schema,`tfmd.proto.v0.shema_pb2`, that contains type and shape information about each CSV column. [`schema_pb2.TensorRepresentation`](https://www.tensorflow.org/tfx/tf_metadata/api_docs/python/tfmd/proto/schema_pb2/TensorRepresentation)s are an optional part of the Schema; if not provided (which is the case in this example), they will be inferred from the type and shape information. One can get the Schema either by using a helper function we provide to translate from TF parsing specs (shown in this example), or by running [TensorFlow Data Validation](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic).\n  * a list of column names, in the order they appear in the CSV file. Note that those names must match the feature names in the Schema.\n\n```\npip\n```\n```\nfromtfx_bsl.publicimport tfxio\nfromtfx_bsl.coders.example_coderimport RecordBatchToExamples\n\nimportapache_beamasbeam\n\n```\n```\npipeline = beam.Pipeline()\n\ncsv_tfxio = tfxio.BeamRecordCsvTFXIO(\n    physical_format='text', column_names=ORDERED_CSV_COLUMNS, schema=SCHEMA)\n\nraw_data = (\n    pipeline\n    | 'ReadTrainData' >> beam.io.ReadFromText(\n        train_data_file, coder=beam.coders.BytesCoder())\n    | 'FixCommasTrainData' >> beam.Map(\n        lambda line: line.replace(b', ', b','))\n    | 'DecodeTrainData' >> csv_tfxio.BeamSource())\n\n```\n```\nraw_data\n\n```\n```\n<PCollection[[21]: DecodeTrainData/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches.None] at 0x7feeaa6fd5b0>\n\n```\n\nNote that we had to do some additional fix-ups after the CSV lines are read in. Otherwise, we could rely on the [`tfxio.CsvTFXIO`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/CsvTFXIO) to handle both reading the files and translating to `RecordBatch`es:\n```\ncsv_tfxio = tfxio.CsvTFXIO(train_data_file,\n                           telemetry_descriptors=[], #???\n                           column_names=ORDERED_CSV_COLUMNS,\n                           schema=SCHEMA)\n\np2 = beam.Pipeline()\nraw_data_2 = p2 | 'TFXIORead' >> csv_tfxio.BeamSource()\n\n```\n\nPreprocessing for this dataset is similar to the previous example, except the preprocessing function is programmatically generated instead of manually specifying each column. In the preprocessing function below, `NUMERICAL_COLUMNS` and `CATEGORICAL_COLUMNS` are lists that contain the names of the numeric and categorical columns:\n```\nNUM_OOV_BUCKETS = 1\n\ndefpreprocessing_fn(inputs):\n\"\"\"Preprocess input columns into transformed columns.\"\"\"\n  # Since we are modifying some features and leaving others unchanged, we\n  # start by setting `outputs` to a copy of `inputs.\n  outputs = inputs.copy()\n\n  # Scale numeric columns to have range [0, 1].\n  for key in NUMERIC_FEATURE_KEYS:\n    outputs[key] = tft.scale_to_0_1(outputs[key])\n\n  # For all categorical columns except the label column, we generate a\n  # vocabulary but do not modify the feature.  This vocabulary is instead\n  # used in the trainer, by means of a feature column, to convert the feature\n  # from a string to an integer id.\n  for key in CATEGORICAL_FEATURE_KEYS:\n    outputs[key] = tft.compute_and_apply_vocabulary(\n        tf.strings.strip(inputs[key]),\n        num_oov_buckets=NUM_OOV_BUCKETS,\n        vocab_filename=key)\n\n  # For the label column we provide the mapping from string to index.\n  with tf.init_scope():\n    # `init_scope` - Only initialize the table once.\n    initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=['>50K', '<=50K'],\n        values=tf.cast(tf.range(2), tf.int64),\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n\n  outputs[LABEL_KEY] = table.lookup(outputs[LABEL_KEY])\n\n  return outputs\n\n```\n\nOne difference from the previous example is the label column manually specifies the mapping from the string to an index. So `'>50'` is mapped to `0` and `'<=50K'` is mapped to `1` because it's useful to know which index in the trained model corresponds to which label.\nThe `record_batches` variable represents a `PCollection` of `pyarrow.RecordBatch`es. The `tensor_adapter_config` is given by `csv_tfxio`, which is inferred from `SCHEMA` (and ultimately, in this example, from the TF parsing specs).\nThe final stage is to write the transformed data to disk and has a similar form to reading the raw data. The schema used to do this is part of the output of [`tft_beam.AnalyzeAndTransformDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) which infers a schema for the output data. The code to write to disk is shown below. The schema is a part of the metadata but uses the two interchangeably in the `tf.Transform` API (i.e. pass the metadata to the [`tft.coders.ExampleProtoCoder`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/coders/ExampleProtoCoder)). Be aware that this writes to a different format. Instead of `textio.WriteToText`, use Beam's built-in support for the `TFRecord` format and use a coder to encode the data as `Example` protos. This is a better format to use for training, as shown in the next section. `transformed_eval_data_base` provides the base filename for the individual shards that are written.\n```\nraw_dataset = (raw_data, csv_tfxio.TensorAdapterConfig())\n\n```\n```\nworking_dir = tempfile.mkdtemp()\nwith tft_beam.Context(temp_dir=working_dir):\n  transformed_dataset, transform_fn = (\n      raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n          preprocessing_fn, output_record_batches=True))\n\n```\n```\noutput_dir = tempfile.mkdtemp()\n\n```\n```\ntransformed_data, _ = transformed_dataset\n\n_ = (\n    transformed_data\n    | 'EncodeTrainData' >>\n    beam.FlatMapTuple(lambda batch, _: RecordBatchToExamples(batch))\n    | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n        os.path.join(output_dir , 'transformed.tfrecord')))\n\n```\n\nIn addition to the training data, `transform_fn` is also written out with the metadata:\n```\n_ = (\n    transform_fn\n    | 'WriteTransformFn' >> tft_beam.WriteTransformFn(output_dir))\n\n```\n\nRun the entire Beam pipeline with `pipeline.run().wait_until_finish()`. Up until this point, the Beam pipeline represents a deferred, distributed computation. It provides instructions for what will be done, but the instructions have not been executed. This final call executes the specified pipeline.\n```\nresult = pipeline.run().wait_until_finish()\n\n```\n```\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmphiyrst4f/tftransform_tmp/c633cd0eb0c14a2bba2bc6f7ba556ce3/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmphiyrst4f/tftransform_tmp/c633cd0eb0c14a2bba2bc6f7ba556ce3/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmphiyrst4f/tftransform_tmp/9080e8c73e2443fea34d6505feed4129/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmphiyrst4f/tftransform_tmp/9080e8c73e2443fea34d6505feed4129/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n\n```\n\nAfter running the pipeline the output directory contains two artifacts.\n  * The transformed data, and the metadata describing it.\n  * The [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model) containing the resulting `preprocessing_fn`\n\n```\nls{output_dir}\n```\n```\ntransform_fn  transformed.tfrecord-00000-of-00001  transformed_metadata\n\n```\n\nTo see how to use these artifacts refer to the [Advanced preprocessing tutorial](https://www.tensorflow.org/tfx/tutorials/transform/census).\n",
  "https://www.tensorflow.org/tutorials/keras/keras_tuner": "[View on TensorFlow.org](https://www.tensorflow.org/tutorials/keras/keras_tuner)  \n---  \n## Overview\nThe Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called _hyperparameter tuning_ or _hypertuning_.\nHyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n  1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n  2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n\n\nIn this tutorial, you will use the Keras Tuner to perform hypertuning for an image classification application.\n## Setup\n```\nimporttensorflowastf\nfromtensorflowimport keras\n\n```\n```\n2024-08-16 01:25:04.811063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-16 01:25:04.832191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-16 01:25:04.838460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\nInstall and import the Keras Tuner.\n```\npip\n```\n```\nimportkeras_tuneraskt\n\n```\n\n## Download and prepare the dataset\nIn this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the \nLoad the data.\n```\n(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()\n\n```\n```\n# Normalize pixel values between 0 and 1\nimg_train = img_train.astype('float32') / 255.0\nimg_test = img_test.astype('float32') / 255.0\n\n```\n\n## Define the model\nWhen you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a _hypermodel_.\nYou can define a hypermodel through two approaches:\n  * By using a model builder function\n  * By subclassing the `HyperModel` class of the Keras Tuner API\n\n\nYou can also use two pre-defined \nIn this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model.\n```\ndefmodel_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n\n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(10))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  return model\n\n```\n\n## Instantiate the tuner and perform hypertuning\nInstantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the \nTo instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`).\n```\ntuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=10,\n                     factor=3,\n                     directory='my_dir',\n                     project_name='intro_to_kt')\n\n```\n```\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723771509.637777   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.641612   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.644868   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.648549   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.660168   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.663655   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.666633   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.670142   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.673591   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.677149   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.680118   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771509.683613   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.907409   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.909510   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.911592   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.913615   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.915637   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.917579   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.919546   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.921484   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.923384   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.925354   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.927319   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.929258   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.967243   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.969279   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.971299   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.973289   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.975342   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.977295   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.979282   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.981238   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.983175   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.985662   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.988059   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723771510.990406   14090 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n\n```\n\nThe Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log`factor`(`max_epochs`) and rounding it up to the nearest integer.\nCreate a callback to stop training early after reaching a certain value for the validation loss.\n```\nstop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n\n```\n\nRun the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above.\n```\ntuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")\n\n```\n```\nTrial 30 Complete [00h 00m 25s]\nval_accuracy: 0.8913333415985107\n\nBest val_accuracy So Far: 0.8913333415985107\nTotal elapsed time: 00h 05m 37s\n\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is 416 and the optimal learning rate for the optimizer\nis 0.001.\n\n```\n\n## Train the model\nFind the optimal number of epochs to train the model with the hyperparameters obtained from the search.\n```\n# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n\nval_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))\n\n```\n```\nEpoch 1/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 4s 2ms/step - accuracy: 0.7774 - loss: 0.6344 - val_accuracy: 0.8590 - val_loss: 0.4024\nEpoch 2/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8643 - loss: 0.3766 - val_accuracy: 0.8632 - val_loss: 0.3783\nEpoch 3/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8790 - loss: 0.3291 - val_accuracy: 0.8803 - val_loss: 0.3296\nEpoch 4/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8884 - loss: 0.3036 - val_accuracy: 0.8708 - val_loss: 0.3529\nEpoch 5/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8955 - loss: 0.2840 - val_accuracy: 0.8817 - val_loss: 0.3297\nEpoch 6/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8998 - loss: 0.2655 - val_accuracy: 0.8581 - val_loss: 0.4232\nEpoch 7/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9040 - loss: 0.2564 - val_accuracy: 0.8808 - val_loss: 0.3396\nEpoch 8/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9101 - loss: 0.2421 - val_accuracy: 0.8718 - val_loss: 0.3550\nEpoch 9/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9089 - loss: 0.2353 - val_accuracy: 0.8920 - val_loss: 0.3055\nEpoch 10/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9160 - loss: 0.2239 - val_accuracy: 0.8918 - val_loss: 0.3077\nEpoch 11/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9211 - loss: 0.2111 - val_accuracy: 0.8913 - val_loss: 0.3258\nEpoch 12/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9223 - loss: 0.2076 - val_accuracy: 0.8936 - val_loss: 0.3115\nEpoch 13/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9260 - loss: 0.1968 - val_accuracy: 0.8892 - val_loss: 0.3134\nEpoch 14/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9273 - loss: 0.1914 - val_accuracy: 0.8890 - val_loss: 0.3284\nEpoch 15/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9321 - loss: 0.1836 - val_accuracy: 0.8911 - val_loss: 0.3366\nEpoch 16/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9363 - loss: 0.1710 - val_accuracy: 0.8952 - val_loss: 0.3252\nEpoch 17/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9374 - loss: 0.1710 - val_accuracy: 0.8898 - val_loss: 0.3381\nEpoch 18/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9383 - loss: 0.1629 - val_accuracy: 0.8913 - val_loss: 0.3500\nEpoch 19/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9414 - loss: 0.1552 - val_accuracy: 0.8954 - val_loss: 0.3418\nEpoch 20/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9435 - loss: 0.1495 - val_accuracy: 0.8926 - val_loss: 0.3455\nEpoch 21/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9431 - loss: 0.1509 - val_accuracy: 0.8903 - val_loss: 0.3748\nEpoch 22/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9454 - loss: 0.1431 - val_accuracy: 0.8960 - val_loss: 0.3444\nEpoch 23/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9490 - loss: 0.1361 - val_accuracy: 0.8948 - val_loss: 0.3433\nEpoch 24/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9492 - loss: 0.1342 - val_accuracy: 0.8918 - val_loss: 0.3569\nEpoch 25/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9488 - loss: 0.1342 - val_accuracy: 0.8910 - val_loss: 0.3757\nEpoch 26/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9528 - loss: 0.1263 - val_accuracy: 0.8914 - val_loss: 0.3831\nEpoch 27/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9518 - loss: 0.1261 - val_accuracy: 0.8935 - val_loss: 0.3801\nEpoch 28/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9545 - loss: 0.1193 - val_accuracy: 0.8914 - val_loss: 0.4115\nEpoch 29/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9551 - loss: 0.1182 - val_accuracy: 0.8816 - val_loss: 0.4434\nEpoch 30/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9577 - loss: 0.1131 - val_accuracy: 0.8971 - val_loss: 0.3876\nEpoch 31/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9558 - loss: 0.1169 - val_accuracy: 0.8903 - val_loss: 0.4025\nEpoch 32/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9574 - loss: 0.1101 - val_accuracy: 0.8984 - val_loss: 0.4147\nEpoch 33/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9583 - loss: 0.1114 - val_accuracy: 0.8970 - val_loss: 0.4005\nEpoch 34/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9623 - loss: 0.1007 - val_accuracy: 0.8935 - val_loss: 0.4260\nEpoch 35/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9614 - loss: 0.1021 - val_accuracy: 0.8926 - val_loss: 0.4296\nEpoch 36/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9642 - loss: 0.0961 - val_accuracy: 0.8928 - val_loss: 0.4305\nEpoch 37/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9633 - loss: 0.0964 - val_accuracy: 0.8891 - val_loss: 0.4603\nEpoch 38/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9634 - loss: 0.0962 - val_accuracy: 0.8977 - val_loss: 0.4350\nEpoch 39/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9645 - loss: 0.0953 - val_accuracy: 0.8938 - val_loss: 0.4520\nEpoch 40/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9661 - loss: 0.0875 - val_accuracy: 0.8923 - val_loss: 0.4823\nEpoch 41/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9652 - loss: 0.0903 - val_accuracy: 0.8904 - val_loss: 0.4852\nEpoch 42/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9686 - loss: 0.0844 - val_accuracy: 0.8822 - val_loss: 0.5031\nEpoch 43/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9672 - loss: 0.0860 - val_accuracy: 0.8942 - val_loss: 0.4723\nEpoch 44/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9691 - loss: 0.0818 - val_accuracy: 0.8944 - val_loss: 0.4678\nEpoch 45/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9726 - loss: 0.0719 - val_accuracy: 0.8940 - val_loss: 0.4623\nEpoch 46/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9697 - loss: 0.0792 - val_accuracy: 0.8934 - val_loss: 0.4757\nEpoch 47/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9720 - loss: 0.0728 - val_accuracy: 0.8953 - val_loss: 0.5138\nEpoch 48/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9717 - loss: 0.0752 - val_accuracy: 0.8931 - val_loss: 0.5226\nEpoch 49/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9728 - loss: 0.0746 - val_accuracy: 0.8975 - val_loss: 0.5169\nEpoch 50/50\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9723 - loss: 0.0755 - val_accuracy: 0.8923 - val_loss: 0.5257\nBest epoch: 32\n\n```\n\nRe-instantiate the hypermodel and train it with the optimal number of epochs from above.\n```\nhypermodel = tuner.hypermodel.build(best_hps)\n\n# Retrain the model\nhypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)\n\n```\n```\nEpoch 1/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 3s 2ms/step - accuracy: 0.7789 - loss: 0.6225 - val_accuracy: 0.8573 - val_loss: 0.4007\nEpoch 2/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8678 - loss: 0.3664 - val_accuracy: 0.8658 - val_loss: 0.3632\nEpoch 3/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.8781 - loss: 0.3343 - val_accuracy: 0.8702 - val_loss: 0.3546\nEpoch 4/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.8879 - loss: 0.3070 - val_accuracy: 0.8768 - val_loss: 0.3472\nEpoch 5/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.8908 - loss: 0.2902 - val_accuracy: 0.8777 - val_loss: 0.3441\nEpoch 6/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9002 - loss: 0.2703 - val_accuracy: 0.8832 - val_loss: 0.3262\nEpoch 7/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9053 - loss: 0.2552 - val_accuracy: 0.8917 - val_loss: 0.3017\nEpoch 8/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9085 - loss: 0.2475 - val_accuracy: 0.8852 - val_loss: 0.3255\nEpoch 9/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9112 - loss: 0.2351 - val_accuracy: 0.8930 - val_loss: 0.3077\nEpoch 10/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9157 - loss: 0.2237 - val_accuracy: 0.8913 - val_loss: 0.3110\nEpoch 11/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9194 - loss: 0.2138 - val_accuracy: 0.8927 - val_loss: 0.3143\nEpoch 12/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9213 - loss: 0.2086 - val_accuracy: 0.8829 - val_loss: 0.3420\nEpoch 13/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9240 - loss: 0.2002 - val_accuracy: 0.8898 - val_loss: 0.3196\nEpoch 14/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9279 - loss: 0.1937 - val_accuracy: 0.8892 - val_loss: 0.3296\nEpoch 15/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9314 - loss: 0.1839 - val_accuracy: 0.8842 - val_loss: 0.3548\nEpoch 16/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9333 - loss: 0.1788 - val_accuracy: 0.8895 - val_loss: 0.3340\nEpoch 17/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9348 - loss: 0.1738 - val_accuracy: 0.8977 - val_loss: 0.3317\nEpoch 18/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9369 - loss: 0.1676 - val_accuracy: 0.8918 - val_loss: 0.3366\nEpoch 19/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9379 - loss: 0.1621 - val_accuracy: 0.8974 - val_loss: 0.3227\nEpoch 20/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9410 - loss: 0.1548 - val_accuracy: 0.8919 - val_loss: 0.3713\nEpoch 21/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9417 - loss: 0.1532 - val_accuracy: 0.8823 - val_loss: 0.4058\nEpoch 22/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9467 - loss: 0.1451 - val_accuracy: 0.8979 - val_loss: 0.3486\nEpoch 23/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9464 - loss: 0.1425 - val_accuracy: 0.8975 - val_loss: 0.3381\nEpoch 24/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9485 - loss: 0.1373 - val_accuracy: 0.8963 - val_loss: 0.3478\nEpoch 25/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9500 - loss: 0.1341 - val_accuracy: 0.8926 - val_loss: 0.3846\nEpoch 26/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9525 - loss: 0.1279 - val_accuracy: 0.8879 - val_loss: 0.3929\nEpoch 27/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9518 - loss: 0.1268 - val_accuracy: 0.8972 - val_loss: 0.3604\nEpoch 28/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 2ms/step - accuracy: 0.9546 - loss: 0.1209 - val_accuracy: 0.8967 - val_loss: 0.3876\nEpoch 29/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9537 - loss: 0.1233 - val_accuracy: 0.8942 - val_loss: 0.3985\nEpoch 30/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9564 - loss: 0.1163 - val_accuracy: 0.8955 - val_loss: 0.4011\nEpoch 31/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9559 - loss: 0.1171 - val_accuracy: 0.8975 - val_loss: 0.3997\nEpoch 32/32\n1500/1500 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.9596 - loss: 0.1086 - val_accuracy: 0.8938 - val_loss: 0.4147\n<keras.src.callbacks.history.History at 0x7fa8b80d4970>\n\n```\n\nTo finish this tutorial, evaluate the hypermodel on the test data.\n```\neval_result = hypermodel.evaluate(img_test, label_test)\nprint(\"[test loss, test accuracy]:\", eval_result)\n\n```\n```\n313/313 ━━━━━━━━━━━━━━━━━━━━ 1s 2ms/step - accuracy: 0.8873 - loss: 0.4595\n[test loss, test accuracy]: [0.4649185538291931, 0.8881000280380249]\n\n```\n\nThe `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner.\n## Summary\nIn this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n  * [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n\n\nAlso check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters.\n",
  "https://www.tensorflow.org/tfx/tutorials/tfx/airflow_workshop": "## Overview\n## Overview\nThis tutorial is designed to help you learn to create your own machine learning pipelines using TensorFlow Extended (TFX) and Apache Airflow as the orchestrator. It runs on on Vertex AI Workbench, and shows integration with TFX and TensorBoard as well as interaction with TFX in a Jupyter Lab environment.\n### What you'll be doing?\nYou’ll learn how to create an ML pipeline using TFX\n  * A TFX pipeline is a Directed Acyclic Graph, or \"DAG\". We will often refer to pipelines as DAGs.\n  * TFX pipelines are appropriate when you will be deploying a production ML application\n  * TFX pipelines are appropriate when datasets are large, or may grow to be large\n  * TFX pipelines are appropriate when training/serving consistency is important\n  * TFX pipelines are appropriate when version management for inference is important\n  * Google uses TFX pipelines for production ML\n\n\nPlease see the [TFX User Guide](https://www.tensorflow.org/tfx/guide) to learn more.\nYou'll follow a typical ML development process:\n  * Ingesting, understanding, and cleaning our data\n  * Feature engineering\n  * Training\n  * Analyzing model performance\n  * Lather, rinse, repeat\n  * Ready for production\n\n\n## **Apache Airflow for Pipeline Orchestration**\nTFX orchestrators are responsible for scheduling components of the TFX pipeline based on the dependencies defined by the pipeline. TFX is designed to be portable to multiple environments and orchestration frameworks. One of the default orchestrators supported by TFX is [Apache Airflow](https://www.tensorflow.org/tfx/guide/airflow). This lab illustrates the use of Apache Airflow for TFX pipeline orchestration. Apache Airflow is a platform to programmatically author, schedule and monitor workflows. TFX uses Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Apache Airflow workflows are defined as code. This makes them more maintainable, versionable, testable, and collaborative. Apache Airflow is suited for batch processing pipelines. It is lightweight and easy to learn.\nIn this example, we are going to run a TFX pipeline on an instance by manually setting up Airflow.\nThe other default orchestrators supported by TFX are Apache Beam and Kubeflow. [Apache Beam](https://www.tensorflow.org/tfx/guide/beam_orchestrator) can run on multiple data processing backends (Beam Ruunners). Cloud Dataflow is one such beam runner which can be used for running TFX pipelines. Apache Beam can be used for both streaming and batch processing pipelines. [Kubeflow](https://www.tensorflow.org/tfx/guide/kubeflow) is an open source ML platform dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Kubeflow can be used as an orchestrator for TFFX pipelines when they need to be deployed on Kubernetes clusters. In addition, you can also use your own [custom orchestrator](https://www.tensorflow.org/tfx/guide/custom_orchestrator) to run a TFX pipeline.\nRead more about Airflow \n## **Chicago Taxi Dataset**\nYou'll be using the \n### Model Goal - Binary classification\nWill the customer tip more or less than 20%?\n## Setup the Google Cloud Project\n**Before you click the Start Lab button** Read these instructions. Labs are timed and you cannot pause them. The timer, which starts when you click **Start Lab** , shows how long Google Cloud resources will be made available to you.\nThis hands-on lab lets you do the lab activities yourself in a real cloud environment, not in a simulation or demo environment. It does so by giving you new, temporary credentials that you use to sign in and access Google Cloud for the duration of the lab.\n**What you need** To complete this lab, you need:\n  * Access to a standard internet browser (Chrome browser recommended).\n  * Time to complete the lab.\n\n\n**How to start your lab and sign in to the Google Cloud Console** 1. Click the **Start Lab** button. If you need to pay for the lab, a pop-up opens for you to select your payment method. On the left is a panel populated with the temporary credentials that you must use for this lab.\n  1. Copy the username, and then click **Open Google Console**. The lab spins up resources, and then opens another tab that shows the **Sign in** page.\n\n\n**_Tip:_** Open the tabs in separate windows, side-by-side.\n  1. In the **Sign in** page, paste the username that you copied from the left panel. Then copy and paste the password.\n\n\n  1. Click through the subsequent pages:\n  2. Accept the terms and conditions.\n\n\n  * Do not add recovery options or two-factor authentication (because this is a temporary account).\n  * Do not sign up for free trials.\n\n\nAfter a few moments, the Cloud Console opens in this tab.\n### Activate Cloud Shell\nCloud Shell is a virtual machine that is loaded with development tools. It offers a persistent 5GB home directory and runs on the Google Cloud. Cloud Shell provides command-line access to your Google Cloud resources.\nIn the Cloud Console, in the top right toolbar, click the **Activate Cloud Shell** button.\nClick **Continue**.\nIt takes a few moments to provision and connect to the environment. When you are connected, you are already authenticated, and the project is set to your _PROJECT _ID_. For example:\n`gcloud` is the command-line tool for Google Cloud. It comes pre-installed on Cloud Shell and supports tab-completion.\nYou can list the active account name with this command:\n```\ngcloud auth list\n\n```\n\n(Output)\n> ACTIVE: * ACCOUNT: student-01-xxxxxxxxxxxx@qwiklabs.net To set the active account, run: $ gcloud config set account `ACCOUNT`\nYou can list the project ID with this command: `gcloud config list project` (Output)\n> [core] project = \n(Example output)\n> [core] project = qwiklabs-gcp-44776a13dea667a6\nFor full documentation of gcloud see the \n## Enable Google Cloud services\n  1. In Cloud Shell, use gcloud to enable the services used in the lab. `gcloud services enable notebooks.googleapis.com`\n\n\n## Deploy Vertex Notebook instance\n  1. Click on the **Navigation Menu** and navigate to **Vertex AI** , then to **Workbench**.\n\n\n  1. On the Notebook instances page, click **New Notebook**.\n  2. In the Customize instance menu, select **TensorFlow Enterprise** and choose the version of **TensorFlow Enterprise 2.x (with LTS)** > **Without GPUs**.\n\n\n  1. In the **New notebook instance** dialog, click the pencil icon to **Edit** instance properties.\n  2. For **Instance name** , enter a name for your instance.\n  3. For **Region** , select `us-east1` and for **Zone** , select a zone within the selected region.\n  4. Scroll down to Machine configuration and select **e2-standard-2** for Machine type.\n  5. Leave the remaining fields with their default and click **Create**.\n\n\nAfter a few minutes, the Vertex AI console will display your instance name, followed by **Open Jupyterlab**.\n  1. Click **Open JupyterLab**. A JupyterLab window will open in a new tab.\n\n\n## Setup the environment\n### Clone the lab repository\nNext you'll clone the `tfx` repository in your JupyterLab instance. 1. In JupyterLab, click the **Terminal** icon to open a new terminal.\n**Note:** If prompted, click `Cancel` for Build Recommended.\n  1. To clone the `tfx` Github repository, type in the following command, and press **Enter**.\n\n```\ngit clone https://github.com/tensorflow/tfx.git\n\n```\n\n  1. To confirm that you have cloned the repository, double-click the `tfx` directory and confirm that you can see its contents.\n\n\n### Install lab dependencies\n  1. Run the following to go to the `tfx/tfx/examples/airflow_workshop/taxi/setup/` folder, then run `./setup_demo.sh` to install lab dependencies:\n\n```\ncd\n```\n\nThe above code will\n  * Install the required packages.\n  * Create an `airflow` folder in the home folder.\n  * Copy the `dags` folder from `tfx/tfx/examples/airflow_workshop/taxi/setup/` folder to `~/airflow/` folder.\n  * Copy the csv file from `tfx/tfx/examples/airflow_workshop/taxi/setup/data` to `~/airflow/data`.\n\n\n## Configuring Airflow server\n### Create firewall rule to access to airflow server in the browser\n  1. Go to `<a href=\"https://console.cloud.google.com/networking/firewalls/list\">https://console.cloud.google.com/networking/firewalls/list</a>` and make sure the project name is selected appropriately\n  2. Click on `CREATE FIREWALL RULE` option on top\n\n\nIn the **Create a firewall dialog** , follow the steps listed below.\n  1. For **Name** , put `airflow-tfx`.\n  2. For **Priority** , select `1`.\n  3. For **Targets** , select `All instances in the network`.\n  4. For **Source IPv4 ranges** , select `0.0.0.0/0`\n  5. For **Protocols and ports** , click on `tcp` and enter `7000` in the box next to `tcp`\n  6. Click `Create`.\n\n\n### Run airflow server from your shell\nIn the Jupyter Lab Terminal window, change to home directory, run the `airflow users create` command to create an admin user for Airflow:\n```\ncd\nairflow\n```\n\nThen run the `airflow webserver` and `airflow scheduler` command to run the server. Choose port `7000` since it is allowed through firewall.\n```\nnohup7000 &> webserver.out &> scheduler.out\n```\n\n### Get your external ip\n  1. In Cloud Shell, use `gcloud` to get the External IP.\n\n```\ngcloud compute instances list\n\n```\n\n## Running a DAG/Pipeline\n### In a browser\nOpen a browser and go to http://:7000\n  * In the login page, enter the username(`admin`) and password(`admin`) you chose when running the `airflow users create` command.\n\n\nAirflow loads DAGs from Python source files. It takes each file and executes it. Then it loads any DAG objects from that file. All `.py` files which define DAG objects will be listed as pipelines in the airflow homepage.\nIn this tutorial, Airflow scans the `~/airflow/dags/` folder for DAG objects.\nIf you open `~/airflow/dags/taxi_pipeline.py` and scroll to the bottom, you can see that it creates and stores a DAG object in a variable named `DAG`. Hence it will be listed as a pipeline in the airflow homepage as shown below:\nIf you click on taxi, you will be redirected to the grid view of the DAG. You can click the `Graph` option on top to get the graph view of the DAG.\n### Trigger the taxi pipeline\nOn the homepage you can see the buttons that can be used to interact with the DAG.\nUnder the **actions** header, click on the **trigger** button to trigger the pipeline.\nIn the taxi **DAG** page, use the button on the right to refresh the state of the graph view of the DAG as the pipeline runs. Additionally, you can enable **Auto Refresh** to instruct Airflow to automatically refresh the graph view as and when the state changes.\nYou can also use the \n```\n# enable/disable\nairflow# trigger\nairflow\n```\n\n#### Waiting for the pipeline to complete\nAfter you've triggered your pipeline, in the DAGs view, you can watch the progress of your pipeline while it is running. As each component runs, the outline color of the component in the DAG graph will change to show its state. When a component has finished processing the outline will turn dark green to show that it's done.\n## Understanding the components\nNow we will look at the components of this pipeline in detail, and individually look at the outputs produced by each step in the pipeline.\n  1. In JupyterLab go to `~/tfx/tfx/examples/airflow_workshop/taxi/notebooks/`\n  2. Open **notebook.ipynb.**\n  3. Continue the lab in the notebook, and run each cell by clicking the **Run** ( ) icon at the top of the screen. Alternatively, you can execute the code in a cell with **SHIFT + ENTER**.\n\n\nRead the narrative and make sure you understand what's happening in each cell.\n",
  "https://www.tensorflow.org/tfx/tutorials/transform/census": "[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/transform/census)  \n---  \n**_The Feature Engineering Component of TensorFlow Extended (TFX)_**\nThis example colab notebook provides a somewhat more advanced example of how [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started) (`tf.Transform`) can be used to preprocess data using exactly the same code for both training a model and serving inferences in production.\nTensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset. For example, using TensorFlow Transform you could:\n  * Normalize an input value by using the mean and standard deviation\n  * Convert strings to integers by generating a vocabulary over all of the input values\n  * Convert floats to integers by assigning them to buckets, based on the observed data distribution\n\n\nTensorFlow has built-in support for manipulations on a single example or a batch of examples. `tf.Transform` extends these capabilities to support full passes over the entire training dataset.\nThe output of `tf.Transform` is exported as a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.\n## What we're doing in this example\nIn this example we'll be processing a `tf.Transform`.\n### Install TensorFlow Transform\n```\npip\n```\n```\n# This cell is only necessary because packages were installed while python was\n# running. It avoids the need to restart the runtime when running in Colab.\nimportpkg_resources\nimportimportlib\n\nimportlib.reload(pkg_resources)\n\n```\n```\n/tmpfs/tmp/ipykernel_186972/639106435.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n<module 'pkg_resources' from '/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/pkg_resources/__init__.py'>\n\n```\n\n## Imports and globals\nFirst import the stuff we need.\n```\nimportmath\nimportos\nimportpprint\n\nimportpandasaspd\nimportmatplotlib.pyplotasplt\n\nimporttensorflowastf\nprint('TF: {}'.format(tf.__version__))\n\nimportapache_beamasbeam\nprint('Beam: {}'.format(beam.__version__))\n\nimporttensorflow_transformastft\nimporttensorflow_transform.beamastft_beam\nfromtensorflow_transform.keras_libimport tf_keras\nprint('Transform: {}'.format(tft.__version__))\n\nfromtfx_bsl.publicimport tfxio\nfromtfx_bsl.coders.example_coderimport RecordBatchToExamplesEncoder\n\n```\n```\n2024-04-30 10:48:55.479069: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:48:55.479126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:48:55.480629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTF: 2.15.1\nBeam: 2.55.1\nTransform: 1.15.0\n\n```\n\nNext download the data files:\n```\n!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\n!wget https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\n\ntrain_path = './adult.data'\ntest_path = './adult.test'\n\n```\n```\n--2024-04-30 10:48:57--  https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.data\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 108.177.120.207, 142.250.103.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3974305 (3.8M) [application/octet-stream]\nSaving to: ‘adult.data’\n\nadult.data          100%[===================>]   3.79M  --.-KB/s    in 0.02s   \n\n2024-04-30 10:48:58 (165 MB/s) - ‘adult.data’ saved [3974305/3974305]\n\n--2024-04-30 10:48:58--  https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/census/adult.test\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.206.207, 108.177.120.207, 142.250.103.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.206.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2003153 (1.9M) [application/octet-stream]\nSaving to: ‘adult.test’\n\nadult.test          100%[===================>]   1.91M  --.-KB/s    in 0.01s   \n\n2024-04-30 10:48:58 (145 MB/s) - ‘adult.test’ saved [2003153/2003153]\n\n```\n\n### Name our columns\nWe'll create some handy lists for referencing the columns in our dataset.\n```\nCATEGORICAL_FEATURE_KEYS = [\n    'workclass',\n    'education',\n    'marital-status',\n    'occupation',\n    'relationship',\n    'race',\n    'sex',\n    'native-country',\n]\n\nNUMERIC_FEATURE_KEYS = [\n    'age',\n    'capital-gain',\n    'capital-loss',\n    'hours-per-week',\n    'education-num'\n]\n\nORDERED_CSV_COLUMNS = [\n    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'label'\n]\n\nLABEL_KEY = 'label'\n\n```\n\nHere's a quick preview of the data:\n```\npandas_train = pd.read_csv(train_path, header=None, names=ORDERED_CSV_COLUMNS)\n\npandas_train.head(5)\n\n```\n```\none_row = dict(pandas_train.loc[0])\n\n```\n```\nCOLUMN_DEFAULTS = [\n  '' if isinstance(v, str) else 0.0\n  for v in  dict(pandas_train.loc[1]).values()]\n\n```\n\nThe test data has 1 header line that needs to be skipped, and a trailing \".\" at the end of each line.\n```\npandas_test = pd.read_csv(test_path, header=1, names=ORDERED_CSV_COLUMNS)\n\npandas_test.head(5)\n\n```\n```\ntesting = os.getenv(\"WEB_TEST_BROWSER\", False)\nif testing:\n  pandas_train = pandas_train.loc[:1]\n  pandas_test = pandas_test.loc[:1]\n\n```\n\n### Define our features and schema\nLet's define a schema based on what types the columns are in our input. Among other things this will help with importing them correctly.\n```\nRAW_DATA_FEATURE_SPEC = dict(\n    [(name, tf.io.FixedLenFeature([], tf.string))\n     for name in CATEGORICAL_FEATURE_KEYS] +\n    [(name, tf.io.FixedLenFeature([], tf.float32))\n     for name in NUMERIC_FEATURE_KEYS] + \n    [(LABEL_KEY, tf.io.FixedLenFeature([], tf.string))]\n)\n\nSCHEMA = tft.DatasetMetadata.from_feature_spec(RAW_DATA_FEATURE_SPEC).schema\n\n```\n\n### [Optional] Encode and decode tf.train.Example protos\nThis tutorial needs to convert examples from the dataset to and from [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos in a few places. \nThe hidden `encode_example` function below converts a dictionary of features forom the dataset to a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example).\nToggle code```\ndefencode_example(input_features):\n  input_features = dict(input_features)\n  output_features = {}\n\n  for key in CATEGORICAL_FEATURE_KEYS:\n    value = input_features[key]\n    feature = tf.train.Feature(\n        bytes_list=tf.train.BytesList(value=[value.strip().encode()]))\n    output_features[key] = feature \n\n  for key in NUMERIC_FEATURE_KEYS:\n    value = input_features[key]\n    feature = tf.train.Feature(\n        float_list=tf.train.FloatList(value=[value]))\n    output_features[key] = feature \n\n  label_value = input_features.get(LABEL_KEY, None)\n  if label_value is not None:\n    output_features[LABEL_KEY]  = tf.train.Feature(\n        bytes_list = tf.train.BytesList(value=[label_value.strip().encode()]))\n\n  example = tf.train.Example(\n      features = tf.train.Features(feature=output_features)\n  )\n  return example\n\n```\n\nNow you can convert dataset examples into `Example` protos:\n```\ntf_example = encode_example(pandas_train.loc[0])\ntf_example.features.feature['age']\n\n```\n```\nfloat_list {\n  value: 39.0\n}\n\n```\n```\nserialized_example_batch = tf.constant([\n  encode_example(pandas_train.loc[i]).SerializeToString()\n  for i in range(3)\n])\n\nserialized_example_batch\n\n```\n```\n<tf.Tensor: shape=(3,), dtype=string, numpy=\narray([b'\\n\\xf9\\x02\\n\\x0f\\n\\x03age\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x1cB\\n\\x12\\n\\x05label\\x12\\t\\n\\x07\\n\\x05<=50K\\n\\x1a\\n\\x0ehours-per-week\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00 B\\n#\\n\\x0enative-country\\x12\\x11\\n\\x0f\\n\\rUnited-States\\n\\x1a\\n\\tworkclass\\x12\\r\\n\\x0b\\n\\tState-gov\\n\\x0f\\n\\x03sex\\x12\\x08\\n\\x06\\n\\x04Male\\n\\x18\\n\\x0ccapital-loss\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n\\x19\\n\\reducation-num\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00PA\\n!\\n\\x0crelationship\\x12\\x11\\n\\x0f\\n\\rNot-in-family\\n\\x1e\\n\\noccupation\\x12\\x10\\n\\x0e\\n\\x0cAdm-clerical\\n#\\n\\x0emarital-status\\x12\\x11\\n\\x0f\\n\\rNever-married\\n\\x11\\n\\x04race\\x12\\t\\n\\x07\\n\\x05White\\n\\x1a\\n\\teducation\\x12\\r\\n\\x0b\\n\\tBachelors\\n\\x18\\n\\x0ccapital-gain\\x12\\x08\\x12\\x06\\n\\x04\\x00\\xe0\\x07E',\n       b'\\n\\x82\\x03\\n\\x12\\n\\x05label\\x12\\t\\n\\x07\\n\\x05<=50K\\n\\x1a\\n\\teducation\\x12\\r\\n\\x0b\\n\\tBachelors\\n\\x18\\n\\x0ccapital-gain\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n(\\n\\x0emarital-status\\x12\\x16\\n\\x14\\n\\x12Married-civ-spouse\\n\\x1b\\n\\x0crelationship\\x12\\x0b\\n\\t\\n\\x07Husband\\n\\x19\\n\\reducation-num\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00PA\\n\\x11\\n\\x04race\\x12\\t\\n\\x07\\n\\x05White\\n\\x18\\n\\x0ccapital-loss\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n#\\n\\x0enative-country\\x12\\x11\\n\\x0f\\n\\rUnited-States\\n!\\n\\noccupation\\x12\\x13\\n\\x11\\n\\x0fExec-managerial\\n!\\n\\tworkclass\\x12\\x14\\n\\x12\\n\\x10Self-emp-not-inc\\n\\x0f\\n\\x03age\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00HB\\n\\x1a\\n\\x0ehours-per-week\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00PA\\n\\x0f\\n\\x03sex\\x12\\x08\\n\\x06\\n\\x04Male',\n       b'\\n\\xf5\\x02\\n\\x19\\n\\reducation-num\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x10A\\n!\\n\\x0crelationship\\x12\\x11\\n\\x0f\\n\\rNot-in-family\\n#\\n\\noccupation\\x12\\x15\\n\\x13\\n\\x11Handlers-cleaners\\n\\x0f\\n\\x03age\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x18B\\n\\x18\\n\\tworkclass\\x12\\x0b\\n\\t\\n\\x07Private\\n\\x18\\n\\x0ccapital-gain\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n\\x18\\n\\x0ccapital-loss\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00\\x00\\x00\\n\\x12\\n\\x05label\\x12\\t\\n\\x07\\n\\x05<=50K\\n\\x0f\\n\\x03sex\\x12\\x08\\n\\x06\\n\\x04Male\\n\\x1a\\n\\x0ehours-per-week\\x12\\x08\\x12\\x06\\n\\x04\\x00\\x00 B\\n\\x18\\n\\teducation\\x12\\x0b\\n\\t\\n\\x07HS-grad\\n\\x11\\n\\x04race\\x12\\t\\n\\x07\\n\\x05White\\n\\x1e\\n\\x0emarital-status\\x12\\x0c\\n\\n\\n\\x08Divorced\\n#\\n\\x0enative-country\\x12\\x11\\n\\x0f\\n\\rUnited-States'],\n      dtype=object)>\n\n```\n\nYou can also convert batches of serialized Example protos back into a dictionary of tensors:\n```\ndecoded_tensors = tf.io.parse_example(\n    serialized_example_batch,\n    features=RAW_DATA_FEATURE_SPEC\n)\n\n```\n\nIn some cases the label will not be passed in, so the encode function is written so that the label is optional:\n```\nfeatures_dict = dict(pandas_train.loc[0])\nfeatures_dict.pop(LABEL_KEY)\n\nLABEL_KEY in features_dict\n\n```\n```\nFalse\n\n```\n\nWhen creating an `Example` proto it will simply not contain the label key. \n```\nno_label_example = encode_example(features_dict)\n\nLABEL_KEY in no_label_example.features.feature.keys()\n\n```\n```\nFalse\n\n```\n\n### Setting hyperparameters and basic housekeeping\nConstants and hyperparameters used for training.\n```\nNUM_OOV_BUCKETS = 1\n\nEPOCH_SPLITS = 10\nTRAIN_NUM_EPOCHS = 2*EPOCH_SPLITS\nNUM_TRAIN_INSTANCES = len(pandas_train)\nNUM_TEST_INSTANCES = len(pandas_test)\n\nBATCH_SIZE = 128\n\nSTEPS_PER_TRAIN_EPOCH = tf.math.ceil(NUM_TRAIN_INSTANCES/BATCH_SIZE/EPOCH_SPLITS)\nEVALUATION_STEPS = tf.math.ceil(NUM_TEST_INSTANCES/BATCH_SIZE)\n\n# Names of temp files\nTRANSFORMED_TRAIN_DATA_FILEBASE = 'train_transformed'\nTRANSFORMED_TEST_DATA_FILEBASE = 'test_transformed'\nEXPORTED_MODEL_DIR = 'exported_model_dir'\n\n```\n```\nif testing:\n  TRAIN_NUM_EPOCHS = 1\n\n```\n\n## Preprocessing with `tf.Transform`\n### Create a `tf.Transform` preprocessing_fn\nThe _preprocessing function_ is the most important concept of tf.Transform. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a [`Tensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor) or [`SparseTensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor). There are two main groups of API calls that typically form the heart of a preprocessing function:\n  1. **TensorFlow Ops:** Any function that accepts and returns tensors, which usually means TensorFlow ops. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time. These will run for every example, during both training and serving.\n  2. **Tensorflow Transform Analyzers/Mappers:** Any of the analyzers/mappers provided by tf.Transform. These also accept and return tensors, and typically contain a combination of Tensorflow ops and Beam computation, but unlike TensorFlow ops they only run in the Beam pipeline during analysis requiring a full pass over the entire training dataset. The Beam computation runs only once, (prior to training, during analysis), and typically make a full pass over the entire training dataset. They create [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) tensors, which are added to your graph. For example, [`tft.min`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/min) computes the minimum of a tensor over the training dataset.\n\n\nHere is a `preprocessing_fn` for this dataset. It does several things:\n  1. Using [`tft.scale_to_0_1`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/scale_to_0_1), it scales the numeric features to the `[0,1]` range.\n  2. Using [`tft.compute_and_apply_vocabulary`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/compute_and_apply_vocabulary), it computes a vocabulary for each of the categorical features, and returns the integer IDs for each input as an [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64). This applies both to string and integer categorical-inputs.\n  3. It applies some manual transformations to the data using standard TensorFlow operations. Here these operations are applied to the label but could transform the features as well. The TensorFlow operations do several things: \n     * They build a lookup table for the label (the [`tf.init_scope`](https://www.tensorflow.org/api_docs/python/tf/init_scope) ensures that the table is only created the first time the function is called).\n     * They normalize the text of the label.\n     * They convert the label to a one-hot. \n\n```\ndefpreprocessing_fn(inputs):\n\"\"\"Preprocess input columns into transformed columns.\"\"\"\n  # Since we are modifying some features and leaving others unchanged, we\n  # start by setting `outputs` to a copy of `inputs.\n  outputs = inputs.copy()\n\n  # Scale numeric columns to have range [0, 1].\n  for key in NUMERIC_FEATURE_KEYS:\n    outputs[key] = tft.scale_to_0_1(inputs[key])\n\n  # For all categorical columns except the label column, we generate a\n  # vocabulary but do not modify the feature.  This vocabulary is instead\n  # used in the trainer, by means of a feature column, to convert the feature\n  # from a string to an integer id.\n  for key in CATEGORICAL_FEATURE_KEYS:\n    outputs[key] = tft.compute_and_apply_vocabulary(\n        tf.strings.strip(inputs[key]),\n        num_oov_buckets=NUM_OOV_BUCKETS,\n        vocab_filename=key)\n\n  # For the label column we provide the mapping from string to index.\n  table_keys = ['>50K', '<=50K']\n  with tf.init_scope():\n    initializer = tf.lookup.KeyValueTensorInitializer(\n        keys=table_keys,\n        values=tf.cast(tf.range(len(table_keys)), tf.int64),\n        key_dtype=tf.string,\n        value_dtype=tf.int64)\n    table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n\n  # Remove trailing periods for test data when the data is read with tf.data.\n  # label_str  = tf.sparse.to_dense(inputs[LABEL_KEY])\n  label_str = inputs[LABEL_KEY]\n  label_str = tf.strings.regex_replace(label_str, r'\\.$', '')\n  label_str = tf.strings.strip(label_str)\n  data_labels = table.lookup(label_str)\n  transformed_label = tf.one_hot(\n      indices=data_labels, depth=len(table_keys), on_value=1.0, off_value=0.0)\n  outputs[LABEL_KEY] = tf.reshape(transformed_label, [-1, len(table_keys)])\n\n  return outputs\n\n```\n\n## Syntax\nYou're almost ready to put everything together and use \nApache Beam uses a \n```\nresult = pass_this | 'name this step' >> to_this_call\n\n```\n\nThe method `to_this_call` is being invoked and passed the object called `pass_this`, and `to_this_call` is returned in `result`. You will often see stages of a pipeline chained together like this:\n```\nresult = apache_beam.Pipeline() | 'first step' >> do_this_first() | 'second step' >> do_this_last()\n\n```\n\nand since that started with a new pipeline, you can continue like this:\n```\nnext_result = result | 'doing more stuff' >> another_function()\n\n```\n\n### Transform the data\nNow we're ready to start transforming our data in an Apache Beam pipeline.\n  1. Read in the data using the [`tfxio.CsvTFXIO`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/CsvTFXIO) CSV reader (to process lines of text in a pipeline use [`tfxio.BeamRecordCsvTFXIO`](https://www.tensorflow.org/tfx/tfx_bsl/api_docs/python/tfx_bsl/public/tfxio/BeamRecordCsvTFXIO) instead).\n  2. Analyse and transform the data using the `preprocessing_fn` defined above.\n  3. Write out the result as a `TFRecord` of `Example` protos, which we will use for training a model later\n\n```\ndeftransform_data(train_data_file, test_data_file, working_dir):\n\"\"\"Transform the data and write out as a TFRecord of Example protos.\n\n  Read in the data using the CSV reader, and transform it using a\n  preprocessing pipeline that scales numeric data and converts categorical data\n  from strings to int64 values indices, by creating a vocabulary for each\n  category.\n\n  Args:\n    train_data_file: File containing training data\n    test_data_file: File containing test data\n    working_dir: Directory to write transformed data and metadata to\n  \"\"\"\n\n  # The \"with\" block will create a pipeline, and run that pipeline at the exit\n  # of the block.\n  with beam.Pipeline() as pipeline:\n    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n      # Create a TFXIO to read the census data with the schema. To do this we\n      # need to list all columns in order since the schema doesn't specify the\n      # order of columns in the csv.\n      # We first read CSV files and use BeamRecordCsvTFXIO whose .BeamSource()\n      # accepts a PCollection[bytes] because we need to patch the records first\n      # (see \"FixCommasTrainData\" below). Otherwise, tfxio.CsvTFXIO can be used\n      # to both read the CSV files and parse them to TFT inputs:\n      # csv_tfxio = tfxio.CsvTFXIO(...)\n      # raw_data = (pipeline | 'ToRecordBatches' >> csv_tfxio.BeamSource())\n      train_csv_tfxio = tfxio.CsvTFXIO(\n          file_pattern=train_data_file,\n          telemetry_descriptors=[],\n          column_names=ORDERED_CSV_COLUMNS,\n          schema=SCHEMA)\n\n      # Read in raw data and convert using CSV TFXIO.\n      raw_data = (\n          pipeline |\n          'ReadTrainCsv' >> train_csv_tfxio.BeamSource())\n\n      # Combine data and schema into a dataset tuple.  Note that we already used\n      # the schema to read the CSV data, but we also need it to interpret\n      # raw_data.\n      cfg = train_csv_tfxio.TensorAdapterConfig()\n      raw_dataset = (raw_data, cfg)\n\n      # The TFXIO output format is chosen for improved performance.\n      transformed_dataset, transform_fn = (\n          raw_dataset | tft_beam.AnalyzeAndTransformDataset(\n              preprocessing_fn, output_record_batches=True))\n\n      # Transformed metadata is not necessary for encoding.\n      transformed_data, _ = transformed_dataset\n\n      # Extract transformed RecordBatches, encode and write them to the given\n      # directory.\n      coder = RecordBatchToExamplesEncoder()\n      _ = (\n          transformed_data\n          | 'EncodeTrainData' >>\n          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n          | 'WriteTrainData' >> beam.io.WriteToTFRecord(\n              os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)))\n\n      # Now apply transform function to test data.  In this case we remove the\n      # trailing period at the end of each line, and also ignore the header line\n      # that is present in the test data file.\n      test_csv_tfxio = tfxio.CsvTFXIO(\n          file_pattern=test_data_file,\n          skip_header_lines=1,\n          telemetry_descriptors=[],\n          column_names=ORDERED_CSV_COLUMNS,\n          schema=SCHEMA)\n      raw_test_data = (\n          pipeline\n          | 'ReadTestCsv' >> test_csv_tfxio.BeamSource())\n\n      raw_test_dataset = (raw_test_data, test_csv_tfxio.TensorAdapterConfig())\n\n      # The TFXIO output format is chosen for improved performance.\n      transformed_test_dataset = (\n          (raw_test_dataset, transform_fn)\n          | tft_beam.TransformDataset(output_record_batches=True))\n\n      # Transformed metadata is not necessary for encoding.\n      transformed_test_data, _ = transformed_test_dataset\n\n      # Extract transformed RecordBatches, encode and write them to the given\n      # directory.\n      _ = (\n          transformed_test_data\n          | 'EncodeTestData' >>\n          beam.FlatMapTuple(lambda batch, _: coder.encode(batch))\n          | 'WriteTestData' >> beam.io.WriteToTFRecord(\n              os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)))\n\n      # Will write a SavedModel and metadata to working_dir, which can then\n      # be read by the tft.TFTransformOutput class.\n      _ = (\n          transform_fn\n          | 'WriteTransformFn' >> tft_beam.WriteTransformFn(working_dir))\n\n```\n\nRun the pipeline:\n```\nimporttempfile\nimportpathlib\n\noutput_dir = os.path.join(tempfile.mkdtemp(), 'keras')\n\n\ntransform_data(train_path, test_path, output_dir)\n\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpui2ti1wk/tftransform_tmp/c6e2397d5edb4102a64777cdf8d1b9bb/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpui2ti1wk/tftransform_tmp/c6e2397d5edb4102a64777cdf8d1b9bb/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpui2ti1wk/tftransform_tmp/58d7642780cb4ce0964fc9e2deb91d67/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpui2ti1wk/tftransform_tmp/58d7642780cb4ce0964fc9e2deb91d67/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n\n```\n\nWrap up the output directory as a [`tft.TFTransformOutput`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TFTransformOutput):\n```\ntf_transform_output = tft.TFTransformOutput(output_dir)\n\n```\n```\ntf_transform_output.transformed_feature_spec()\n\n```\n```\n{'age': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n 'capital-gain': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n 'capital-loss': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n 'education': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'education-num': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n 'hours-per-week': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n 'label': FixedLenFeature(shape=[2], dtype=tf.float32, default_value=None),\n 'marital-status': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'native-country': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'occupation': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'race': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'relationship': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'sex': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n 'workclass': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None)}\n\n```\n\nIf you look in the directory you'll see it contains three things:\n  1. The `train_transformed` and `test_transformed` data files\n  2. The `transform_fn` directory (a [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model))\n  3. The `transformed_metadata`\n\n\nThe followning sections show how to use these artifacts to train a model.\n```\nls{output_dir}\n```\n```\ntotal 15704\n-rw-rw-r-- 1 kbuilder kbuilder  5356449 Apr 30 10:49 test_transformed-00000-of-00001\n-rw-rw-r-- 1 kbuilder kbuilder 10712569 Apr 30 10:49 train_transformed-00000-of-00001\ndrwxr-xr-x 4 kbuilder kbuilder     4096 Apr 30 10:49 transform_fn\ndrwxr-xr-x 2 kbuilder kbuilder     4096 Apr 30 10:49 transformed_metadata\n\n```\n\n## Using our preprocessed data to train a model using tf_keras\nTo show how `tf.Transform` enables us to use the same code for both training and serving, and thus prevent skew, we're going to train a model. To train our model and prepare our trained model for production we need to create input functions. The main difference between our training input function and our serving input function is that training data contains the labels, and production data does not. The arguments and returns are also somewhat different.\n### Create an input function for training\nRunning the pipeline in the previous section created `TFRecord` files containing the transformed data.\nThe following code uses [`tf.data.experimental.make_batched_features_dataset`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset) and [`tft.TFTransformOutput.transformed_feature_spec`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TFTransformOutput#transformed_feature_spec) to read these data files as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset):\n```\ndef_make_training_input_fn(tf_transform_output, train_file_pattern,\n                            batch_size):\n\"\"\"An input function reading from transformed data, converting to model input.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    transformed_examples: Base filename of examples.\n    batch_size: Batch size.\n\n  Returns:\n    The input data for training or eval, in the form of k.\n  \"\"\"\n  definput_fn():\n    return tf.data.experimental.make_batched_features_dataset(\n        file_pattern=train_file_pattern,\n        batch_size=batch_size,\n        features=tf_transform_output.transformed_feature_spec(),\n        reader=tf.data.TFRecordDataset,\n        label_key=LABEL_KEY,\n        shuffle=True)\n\n  return input_fn\n\n```\n```\ntrain_file_pattern = pathlib.Path(output_dir)/f'{TRANSFORMED_TRAIN_DATA_FILEBASE}*'\n\ninput_fn = _make_training_input_fn(\n    tf_transform_output=tf_transform_output,\n    train_file_pattern = str(train_file_pattern),\n    batch_size = 10\n)\n\n```\n\nBelow you can see a transformed sample of the data. Note how the numeric columns like `education-num` and `hourd-per-week` are converted to floats with a range of [0,1], and the string columns have been converted to IDs:\n```\nfor example, label in input_fn().take(1):\n  break\n\npd.DataFrame(example)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n\n```\n```\nlabel\n\n```\n```\n<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\narray([[0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [1., 0.],\n       [0., 1.],\n       [0., 1.],\n       [0., 1.]], dtype=float32)>\n\n```\n\n### Train, Evaluate the model\nBuild the model\n```\ndefbuild_keras_model(working_dir):\n  inputs = build_keras_inputs(working_dir)\n\n  encoded_inputs = encode_inputs(inputs)\n\n  stacked_inputs = tf.concat(tf.nest.flatten(encoded_inputs), axis=1)\n  output = tf_keras.layers.Dense(100, activation='relu')(stacked_inputs)\n  output = tf_keras.layers.Dense(50, activation='relu')(output)\n  output = tf_keras.layers.Dense(2)(output)\n  model = tf_keras.Model(inputs=inputs, outputs=output)\n\n  return model\n\n```\n```\ndefbuild_keras_inputs(working_dir):\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  feature_spec = tf_transform_output.transformed_feature_spec().copy()\n  feature_spec.pop(LABEL_KEY)\n\n  # Build the `keras.Input` objects.\n  inputs = {}\n  for key, spec in feature_spec.items():\n    if isinstance(spec, tf.io.VarLenFeature):\n      inputs[key] = tf_keras.layers.Input(\n          shape=[None], name=key, dtype=spec.dtype, sparse=True)\n    elif isinstance(spec, tf.io.FixedLenFeature):\n      inputs[key] = tf_keras.layers.Input(\n          shape=spec.shape, name=key, dtype=spec.dtype)\n    else:\n      raise ValueError('Spec type is not supported: ', key, spec)\n\n  return inputs\n\n```\n```\ndefencode_inputs(inputs):\n  encoded_inputs = {}\n  for key in inputs:\n    feature = tf.expand_dims(inputs[key], -1)\n    if key in CATEGORICAL_FEATURE_KEYS:\n      num_buckets = tf_transform_output.num_buckets_for_transformed_feature(key)\n      encoding_layer = (\n          tf_keras.layers.CategoryEncoding(\n              num_tokens=num_buckets, output_mode='binary', sparse=False))\n      encoded_inputs[key] = encoding_layer(feature)\n    else:\n      encoded_inputs[key] = feature\n\n  return encoded_inputs\n\n```\n```\nmodel = build_keras_model(output_dir)\n\ntf_keras.utils.plot_model(model,rankdir='LR', show_shapes=True)\n\n```\n\nBuild the datasets\n```\ndefget_dataset(working_dir, filebase):\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  data_path_pattern = os.path.join(\n      working_dir,\n      filebase + '*')\n\n  input_fn = _make_training_input_fn(\n      tf_transform_output,\n      data_path_pattern,\n      batch_size=BATCH_SIZE)\n\n  dataset = input_fn()\n\n  return dataset\n\n```\n\nTrain and evaluate the model:\n```\ndeftrain_and_evaluate(\n    model,\n    working_dir):\n\"\"\"Train the model on training data and evaluate on test data.\n\n  Args:\n    working_dir: The location of the Transform output.\n    num_train_instances: Number of instances in train set\n    num_test_instances: Number of instances in test set\n\n  Returns:\n    The results from the estimator's 'evaluate' method\n  \"\"\"\n  train_dataset = get_dataset(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE)\n  validation_dataset = get_dataset(working_dir, TRANSFORMED_TEST_DATA_FILEBASE)\n\n  model = build_keras_model(working_dir)\n\n  history = train_model(model, train_dataset, validation_dataset)\n\n  metric_values = model.evaluate(validation_dataset,\n                                 steps=EVALUATION_STEPS,\n                                 return_dict=True)\n  return model, history, metric_values\n\n```\n```\ndeftrain_model(model, train_dataset, validation_dataset):\n  model.compile(optimizer='adam',\n                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  history = model.fit(train_dataset, validation_data=validation_dataset,\n      epochs=TRAIN_NUM_EPOCHS,\n      steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n      validation_steps=EVALUATION_STEPS)\n  return history\n\n```\n```\nmodel, history, metric_values = train_and_evaluate(model, output_dir)\n\n```\n```\nEpoch 1/20\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1714474167.542556  187132 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n26/26 [==============================] - 4s 70ms/step - loss: 0.5136 - accuracy: 0.7578 - val_loss: 0.4207 - val_accuracy: 0.8198\nEpoch 2/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3934 - accuracy: 0.8185 - val_loss: 0.3671 - val_accuracy: 0.8317\nEpoch 3/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3696 - accuracy: 0.8272 - val_loss: 0.3548 - val_accuracy: 0.8365\nEpoch 4/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3499 - accuracy: 0.8314 - val_loss: 0.3528 - val_accuracy: 0.8383\nEpoch 5/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3503 - accuracy: 0.8401 - val_loss: 0.3478 - val_accuracy: 0.8408\nEpoch 6/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3506 - accuracy: 0.8416 - val_loss: 0.3453 - val_accuracy: 0.8411\nEpoch 7/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3511 - accuracy: 0.8380 - val_loss: 0.3430 - val_accuracy: 0.8410\nEpoch 8/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3349 - accuracy: 0.8434 - val_loss: 0.3441 - val_accuracy: 0.8375\nEpoch 9/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3473 - accuracy: 0.8296 - val_loss: 0.3390 - val_accuracy: 0.8425\nEpoch 10/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3377 - accuracy: 0.8389 - val_loss: 0.3472 - val_accuracy: 0.8401\nEpoch 11/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3446 - accuracy: 0.8383 - val_loss: 0.3403 - val_accuracy: 0.8413\nEpoch 12/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3343 - accuracy: 0.8471 - val_loss: 0.3335 - val_accuracy: 0.8447\nEpoch 13/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3303 - accuracy: 0.8534 - val_loss: 0.3384 - val_accuracy: 0.8416\nEpoch 14/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3400 - accuracy: 0.8407 - val_loss: 0.3340 - val_accuracy: 0.8453\nEpoch 15/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3374 - accuracy: 0.8410 - val_loss: 0.3347 - val_accuracy: 0.8448\nEpoch 16/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3279 - accuracy: 0.8459 - val_loss: 0.3326 - val_accuracy: 0.8450\nEpoch 17/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3184 - accuracy: 0.8474 - val_loss: 0.3341 - val_accuracy: 0.8447\nEpoch 18/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3393 - accuracy: 0.8410 - val_loss: 0.3332 - val_accuracy: 0.8433\nEpoch 19/20\n26/26 [==============================] - 1s 26ms/step - loss: 0.3356 - accuracy: 0.8368 - val_loss: 0.3300 - val_accuracy: 0.8454\nEpoch 20/20\n26/26 [==============================] - 1s 27ms/step - loss: 0.3283 - accuracy: 0.8438 - val_loss: 0.3298 - val_accuracy: 0.8434\n128/128 [==============================] - 1s 4ms/step - loss: 0.3303 - accuracy: 0.8433\n\n```\n```\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Eval')\nplt.ylim(0,max(plt.ylim()))\nplt.legend()\nplt.title('Loss');\n\n```\n\n### Transform new data\nIn the previous section the training process used the hard-copies of the transformed data that were generated by [`tft_beam.AnalyzeAndTransformDataset`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/AnalyzeAndTransformDataset) in the `transform_dataset` function. \nFor operating on new data you'll need to load final version of the `preprocessing_fn` that was saved by [`tft_beam.WriteTransformFn`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft_beam/WriteTransformFn). \nThe [`TFTransformOutput.transform_features_layer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TFTransformOutput#transform_features_layer) method loads the `preprocessing_fn` SavedModel from the output directory.\nHere's a function to load new, unprocessed batches from a source file:\n```\ndefread_csv(file_name, batch_size):\n  return tf.data.experimental.make_csv_dataset(\n        file_pattern=file_name,\n        batch_size=batch_size,\n        column_names=ORDERED_CSV_COLUMNS,\n        column_defaults=COLUMN_DEFAULTS,\n        prefetch_buffer_size=0,\n        ignore_errors=True)\n\n```\n```\nfor ex in read_csv(test_path, batch_size=5):\n  break\n\npd.DataFrame(ex)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.ignore_errors` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/readers.py:573: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.ignore_errors` instead.\n\n```\n\nLoad the [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) to transform this data with the `preprocessing_fn`:\n```\nex2 = ex.copy()\nex2.pop('fnlwgt')\n\ntft_layer = tf_transform_output.transform_features_layer()\nt_ex = tft_layer(ex2)\n\nlabel = t_ex.pop(LABEL_KEY)\npd.DataFrame(t_ex)\n\n```\n```\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n\n```\n\nThe `tft_layer` is smart enough to still execute the transformation if only a subset of features are passed in. For example, if you only pass in two features, you'll get just the transformed versions of those features back: \n```\nex2 = pd.DataFrame(ex)[['education', 'hours-per-week']]\nex2\n\n```\n```\npd.DataFrame(tft_layer(dict(ex2)))\n\n```\n\nHere's a more robust version that drops features that are not in the feature-spec, and returns a `(features, label)` pair if the label is in the provided features:\n```\nclassTransform(tf.Module):\n  def__init__(self, working_dir):\n    self.working_dir = working_dir\n    self.tf_transform_output = tft.TFTransformOutput(working_dir)\n    self.tft_layer = tf_transform_output.transform_features_layer()\n\n  @tf.function\n  def__call__(self, features):\n    raw_features = {}\n\n    for key, val in features.items():\n      # Skip unused keys\n      if key not in RAW_DATA_FEATURE_SPEC:\n        continue\n\n      raw_features[key] = val\n\n    # Apply the `preprocessing_fn`.\n    transformed_features = tft_layer(raw_features)\n\n    if LABEL_KEY in transformed_features:\n      # Pop the label and return a (features, labels) pair.\n      data_labels = transformed_features.pop(LABEL_KEY)\n      return (transformed_features, data_labels)\n    else:\n      return transformed_features\n\n```\n```\ntransform = Transform(output_dir)\n\n```\n```\nt_ex, t_label = transform(ex)\n\n```\n```\npd.DataFrame(t_ex)\n\n```\n\nNow you can use [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) to apply that transformation, on the fly to new data:\n```\nmodel.evaluate(\n    read_csv(test_path, batch_size=5).map(transform),\n    steps=EVALUATION_STEPS,\n    return_dict=True\n)\n\n```\n```\n128/128 [==============================] - 1s 4ms/step - loss: 0.2992 - accuracy: 0.8547\n{'loss': 0.2991926074028015, 'accuracy': 0.854687511920929}\n\n```\n\n### Export the model\nSo you have a trained model, and a method to apply the `preprocessing_fn` to new data. Assemble them into a new model that accepts serialized [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos as input.\n```\nclassServingModel(tf.Module):\n  def__init__(self, model, working_dir):\n    self.model = model\n    self.working_dir = working_dir\n    self.transform = Transform(working_dir)\n\n  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n  def__call__(self, serialized_tf_examples):\n    # parse the tf.train.Example\n    feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n    feature_spec.pop(LABEL_KEY)\n    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n    # Apply the `preprocessing_fn`\n    transformed_features = self.transform(parsed_features)\n    # Run the model\n    outputs = self.model(transformed_features)\n    # Format the output\n    classes_names = tf.constant([['0', '1']])\n    classes = tf.tile(classes_names, [tf.shape(outputs)[0], 1])\n    return {'classes': classes, 'scores': outputs}\n\n  defexport(self, output_dir):\n    # Increment the directory number. This is required in order to make this\n    # model servable with model_server.\n    save_model_dir = pathlib.Path(output_dir)/'model'\n    number_dirs = [int(p.name) for p in save_model_dir.glob('*')\n                  if p.name.isdigit()]\n    id = max([0] + number_dirs)+1\n    save_model_dir = save_model_dir/str(id)\n\n    # Set the signature to make it visible for serving.\n    concrete_serving_fn = self.__call__.get_concrete_function()\n    signatures = {'serving_default': concrete_serving_fn}\n\n    # Export the model.\n    tf.saved_model.save(\n        self,\n        str(save_model_dir),\n        signatures=signatures)\n\n    return save_model_dir\n\n```\n\nBuild the model and test-run it on the batch of serialized examples:\n```\nserving_model = ServingModel(model, output_dir)\n\nserving_model(serialized_example_batch)\n\n```\n```\n{'classes': <tf.Tensor: shape=(3, 2), dtype=string, numpy=\n array([[b'0', b'1'],\n        [b'0', b'1'],\n        [b'0', b'1']], dtype=object)>,\n 'scores': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-1.6049761e+00,  9.7535902e-01],\n        [-5.3329688e-01, -1.6330201e-03],\n        [-1.8765860e+00,  1.5198938e+00]], dtype=float32)>}\n\n```\n\nExport the model as a SavedModel:\n```\nsaved_model_dir = serving_model.export(output_dir)\nsaved_model_dir\n\n```\n```\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpckiw2b8s/keras/model/1/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpckiw2b8s/keras/model/1/assets\nPosixPath('/tmpfs/tmp/tmpckiw2b8s/keras/model/1')\n\n```\n\nReload the model and test it on the same batch of examples:\n```\nreloaded = tf.saved_model.load(str(saved_model_dir))\nrun_model = reloaded.signatures['serving_default']\n\n```\n```\nrun_model(serialized_example_batch)\n\n```\n```\n{'classes': <tf.Tensor: shape=(3, 2), dtype=string, numpy=\n array([[b'0', b'1'],\n        [b'0', b'1'],\n        [b'0', b'1']], dtype=object)>,\n 'scores': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n array([[-1.6049761e+00,  9.7535902e-01],\n        [-5.3329688e-01, -1.6330201e-03],\n        [-1.8765860e+00,  1.5198938e+00]], dtype=float32)>}\n\n```\n\n## What we did\nIn this example we used `tf.Transform` to preprocess a dataset of census data, and train a model with the cleaned and transformed data. We also created an input function that we could use when we deploy our trained model in a production environment to perform inference. By using the same code for both training and inference we avoid any issues with data skew. Along the way we learned about creating an Apache Beam transform to perform the transformation that we needed for cleaning the data. We also saw how to use this transformed data to train a model using `tf_keras`. This is just a small piece of what TensorFlow Transform can do! We encourage you to dive into `tf.Transform` and discover what it can do for you.\n## [Optional] Using our preprocessed data to train a model using tf.estimator\nShow Section\n### Create an input function for training\n```\ndef_make_training_input_fn(tf_transform_output, transformed_examples,\n                            batch_size):\n\"\"\"Creates an input function reading from transformed data.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n    transformed_examples: Base filename of examples.\n    batch_size: Batch size.\n\n  Returns:\n    The input function for training or eval.\n  \"\"\"\n  definput_fn():\n\"\"\"Input function for training and eval.\"\"\"\n    dataset = tf.data.experimental.make_batched_features_dataset(\n        file_pattern=transformed_examples,\n        batch_size=batch_size,\n        features=tf_transform_output.transformed_feature_spec(),\n        reader=tf.data.TFRecordDataset,\n        shuffle=True)\n\n    transformed_features = tf.compat.v1.data.make_one_shot_iterator(\n        dataset).get_next()\n\n    # Extract features and label from the transformed tensors.\n    transformed_labels = tf.where(\n        tf.equal(transformed_features.pop(LABEL_KEY), 1))\n\n    return transformed_features, transformed_labels[:,1]\n\n  return input_fn\n\n```\n\n### Create an input function for serving\nLet's create an input function that we could use in production, and prepare our trained model for serving.\n```\ndef_make_serving_input_fn(tf_transform_output):\n\"\"\"Creates an input function reading from raw data.\n\n  Args:\n    tf_transform_output: Wrapper around output of tf.Transform.\n\n  Returns:\n    The serving input function.\n  \"\"\"\n  raw_feature_spec = RAW_DATA_FEATURE_SPEC.copy()\n  # Remove label since it is not available during serving.\n  raw_feature_spec.pop(LABEL_KEY)\n\n  defserving_input_fn():\n\"\"\"Input function for serving.\"\"\"\n    # Get raw features by generating the basic serving input_fn and calling it.\n    # Here we generate an input_fn that expects a parsed Example proto to be fed\n    # to the model at serving time.  See also\n    # tf.estimator.export.build_raw_serving_input_receiver_fn.\n    raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n        raw_feature_spec, default_batch_size=None)\n    serving_input_receiver = raw_input_fn()\n\n    # Apply the transform function that was used to generate the materialized\n    # data.\n    raw_features = serving_input_receiver.features\n    transformed_features = tf_transform_output.transform_raw_features(\n        raw_features)\n\n    return tf.estimator.export.ServingInputReceiver(\n        transformed_features, serving_input_receiver.receiver_tensors)\n\n  return serving_input_fn\n\n```\n\n### Wrap our input data in FeatureColumns\nOur model will expect our data in TensorFlow FeatureColumns.\n```\ndefget_feature_columns(tf_transform_output):\n\"\"\"Returns the FeatureColumns for the model.\n\n  Args:\n    tf_transform_output: A `TFTransformOutput` object.\n\n  Returns:\n    A list of FeatureColumns.\n  \"\"\"\n  # Wrap scalars as real valued columns.\n  real_valued_columns = [tf.feature_column.numeric_column(key, shape=())\n                         for key in NUMERIC_FEATURE_KEYS]\n\n  # Wrap categorical columns.\n  one_hot_columns = [\n      tf.feature_column.indicator_column(\n          tf.feature_column.categorical_column_with_identity(\n              key=key,\n              num_buckets=(NUM_OOV_BUCKETS +\n                  tf_transform_output.vocabulary_size_by_name(\n                      vocab_filename=key))))\n      for key in CATEGORICAL_FEATURE_KEYS]\n\n  return real_valued_columns + one_hot_columns\n\n```\n\n### Train, Evaluate, and Export our model\n```\ndeftrain_and_evaluate(working_dir, num_train_instances=NUM_TRAIN_INSTANCES,\n                       num_test_instances=NUM_TEST_INSTANCES):\n\"\"\"Train the model on training data and evaluate on test data.\n\n  Args:\n    working_dir: Directory to read transformed data and metadata from and to\n        write exported model to.\n    num_train_instances: Number of instances in train set\n    num_test_instances: Number of instances in test set\n\n  Returns:\n    The results from the estimator's 'evaluate' method\n  \"\"\"\n  tf_transform_output = tft.TFTransformOutput(working_dir)\n\n  run_config = tf.estimator.RunConfig()\n\n  estimator = tf.estimator.LinearClassifier(\n      feature_columns=get_feature_columns(tf_transform_output),\n      config=run_config,\n      loss_reduction=tf.losses.Reduction.SUM)\n\n  # Fit the model using the default optimizer.\n  train_input_fn = _make_training_input_fn(\n      tf_transform_output,\n      os.path.join(working_dir, TRANSFORMED_TRAIN_DATA_FILEBASE + '*'),\n      batch_size=BATCH_SIZE)\n  estimator.train(\n      input_fn=train_input_fn,\n      max_steps=TRAIN_NUM_EPOCHS * num_train_instances / BATCH_SIZE)\n\n  # Evaluate model on test dataset.\n  eval_input_fn = _make_training_input_fn(\n      tf_transform_output,\n      os.path.join(working_dir, TRANSFORMED_TEST_DATA_FILEBASE + '*'),\n      batch_size=1)\n\n  # Export the model.\n  serving_input_fn = _make_serving_input_fn(tf_transform_output)\n  exported_model_dir = os.path.join(working_dir, EXPORTED_MODEL_DIR)\n  estimator.export_saved_model(exported_model_dir, serving_input_fn)\n\n  return estimator.evaluate(input_fn=eval_input_fn, steps=num_test_instances)\n\n```\n\n### Put it all together\nWe've created all the stuff we need to preprocess our census data, train a model, and prepare it for serving. So far we've just been getting things ready. It's time to start running!\n```\nimporttempfile\ntemp = temp = os.path.join(tempfile.mkdtemp(),'estimator')\n\ntransform_data(train_path, test_path, temp)\nresults = train_and_evaluate(temp)\n\n```\n```\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpvfol9yyw/tftransform_tmp/7f57f74495a24870877a207197967bb1/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpvfol9yyw/tftransform_tmp/7f57f74495a24870877a207197967bb1/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpvfol9yyw/tftransform_tmp/50532d4a7a7844099ecd59a9a8bb3b64/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpvfol9yyw/tftransform_tmp/50532d4a7a7844099ecd59a9a8bb3b64/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/871689286.py:16: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/871689286.py:16: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:11: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:11: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:17: categorical_column_with_identity (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:17: categorical_column_with_identity (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:16: indicator_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/2648502843.py:16: indicator_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/871689286.py:18: LinearClassifierV2.__init__ (from tensorflow_estimator.python.estimator.canned.linear) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/871689286.py:18: LinearClassifierV2.__init__ (from tensorflow_estimator.python.estimator.canned.linear) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/head_utils.py:54: BinaryClassHead.__init__ (from tensorflow_estimator.python.estimator.head.binary_class_head) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/head_utils.py:54: BinaryClassHead.__init__ (from tensorflow_estimator.python.estimator.head.binary_class_head) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:944: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:944: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmp5z0b2qd4\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmp5z0b2qd4\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmp5z0b2qd4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmp5z0b2qd4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:385: StopAtStepHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:385: StopAtStepHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/legacy/ftrl.py:173: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/legacy/ftrl.py:173: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/model_fn.py:250: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/model_fn.py:250: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1416: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1416: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1419: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1419: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/basic_session_run_hooks.py:232: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/basic_session_run_hooks.py:232: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1456: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1456: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:579: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:579: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmp5z0b2qd4/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmp5z0b2qd4/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:loss = 88.72284, step = 0\nINFO:tensorflow:loss = 88.72284, step = 0\nINFO:tensorflow:global_step/sec: 217.048\nINFO:tensorflow:global_step/sec: 217.048\nINFO:tensorflow:loss = 38.05179, step = 100 (0.463 sec)\nINFO:tensorflow:loss = 38.05179, step = 100 (0.463 sec)\nINFO:tensorflow:global_step/sec: 309.278\nINFO:tensorflow:global_step/sec: 309.278\nINFO:tensorflow:loss = 62.872578, step = 200 (0.323 sec)\nINFO:tensorflow:loss = 62.872578, step = 200 (0.323 sec)\nINFO:tensorflow:global_step/sec: 306.322\nINFO:tensorflow:global_step/sec: 306.322\nINFO:tensorflow:loss = 43.058277, step = 300 (0.327 sec)\nINFO:tensorflow:loss = 43.058277, step = 300 (0.327 sec)\nINFO:tensorflow:global_step/sec: 307.682\nINFO:tensorflow:global_step/sec: 307.682\nINFO:tensorflow:loss = 33.610596, step = 400 (0.325 sec)\nINFO:tensorflow:loss = 33.610596, step = 400 (0.325 sec)\nINFO:tensorflow:global_step/sec: 306.892\nINFO:tensorflow:global_step/sec: 306.892\nINFO:tensorflow:loss = 49.49376, step = 500 (0.326 sec)\nINFO:tensorflow:loss = 49.49376, step = 500 (0.326 sec)\nINFO:tensorflow:global_step/sec: 309.289\nINFO:tensorflow:global_step/sec: 309.289\nINFO:tensorflow:loss = 39.562958, step = 600 (0.323 sec)\nINFO:tensorflow:loss = 39.562958, step = 600 (0.323 sec)\nINFO:tensorflow:global_step/sec: 311.884\nINFO:tensorflow:global_step/sec: 311.884\nINFO:tensorflow:loss = 39.649498, step = 700 (0.320 sec)\nINFO:tensorflow:loss = 39.649498, step = 700 (0.320 sec)\nINFO:tensorflow:global_step/sec: 311.451\nINFO:tensorflow:global_step/sec: 311.451\nINFO:tensorflow:loss = 40.63858, step = 800 (0.321 sec)\nINFO:tensorflow:loss = 40.63858, step = 800 (0.321 sec)\nINFO:tensorflow:global_step/sec: 310.801\nINFO:tensorflow:global_step/sec: 310.801\nINFO:tensorflow:loss = 56.933117, step = 900 (0.322 sec)\nINFO:tensorflow:loss = 56.933117, step = 900 (0.322 sec)\nINFO:tensorflow:global_step/sec: 310.947\nINFO:tensorflow:global_step/sec: 310.947\nINFO:tensorflow:loss = 43.414566, step = 1000 (0.321 sec)\nINFO:tensorflow:loss = 43.414566, step = 1000 (0.321 sec)\nINFO:tensorflow:global_step/sec: 307.503\nINFO:tensorflow:global_step/sec: 307.503\nINFO:tensorflow:loss = 46.722263, step = 1100 (0.326 sec)\nINFO:tensorflow:loss = 46.722263, step = 1100 (0.326 sec)\nINFO:tensorflow:global_step/sec: 310.43\nINFO:tensorflow:global_step/sec: 310.43\nINFO:tensorflow:loss = 42.71798, step = 1200 (0.322 sec)\nINFO:tensorflow:loss = 42.71798, step = 1200 (0.322 sec)\nINFO:tensorflow:global_step/sec: 306.606\nINFO:tensorflow:global_step/sec: 306.606\nINFO:tensorflow:loss = 32.245277, step = 1300 (0.326 sec)\nINFO:tensorflow:loss = 32.245277, step = 1300 (0.326 sec)\nINFO:tensorflow:global_step/sec: 304.767\nINFO:tensorflow:global_step/sec: 304.767\nINFO:tensorflow:loss = 39.286648, step = 1400 (0.328 sec)\nINFO:tensorflow:loss = 39.286648, step = 1400 (0.328 sec)\nINFO:tensorflow:global_step/sec: 311.309\nINFO:tensorflow:global_step/sec: 311.309\nINFO:tensorflow:loss = 47.270004, step = 1500 (0.321 sec)\nINFO:tensorflow:loss = 47.270004, step = 1500 (0.321 sec)\nINFO:tensorflow:global_step/sec: 312.664\nINFO:tensorflow:global_step/sec: 312.664\nINFO:tensorflow:loss = 41.641903, step = 1600 (0.320 sec)\nINFO:tensorflow:loss = 41.641903, step = 1600 (0.320 sec)\nINFO:tensorflow:global_step/sec: 314.642\nINFO:tensorflow:global_step/sec: 314.642\nINFO:tensorflow:loss = 39.352055, step = 1700 (0.318 sec)\nINFO:tensorflow:loss = 39.352055, step = 1700 (0.318 sec)\nINFO:tensorflow:global_step/sec: 308.436\nINFO:tensorflow:global_step/sec: 308.436\nINFO:tensorflow:loss = 42.981514, step = 1800 (0.324 sec)\nINFO:tensorflow:loss = 42.981514, step = 1800 (0.324 sec)\nINFO:tensorflow:global_step/sec: 304.007\nINFO:tensorflow:global_step/sec: 304.007\nINFO:tensorflow:loss = 39.558506, step = 1900 (0.329 sec)\nINFO:tensorflow:loss = 39.558506, step = 1900 (0.329 sec)\nINFO:tensorflow:global_step/sec: 308.174\nINFO:tensorflow:global_step/sec: 308.174\nINFO:tensorflow:loss = 36.912056, step = 2000 (0.325 sec)\nINFO:tensorflow:loss = 36.912056, step = 2000 (0.325 sec)\nINFO:tensorflow:global_step/sec: 305.635\nINFO:tensorflow:global_step/sec: 305.635\nINFO:tensorflow:loss = 50.084297, step = 2100 (0.327 sec)\nINFO:tensorflow:loss = 50.084297, step = 2100 (0.327 sec)\nINFO:tensorflow:global_step/sec: 304.925\nINFO:tensorflow:global_step/sec: 304.925\nINFO:tensorflow:loss = 34.076836, step = 2200 (0.328 sec)\nINFO:tensorflow:loss = 34.076836, step = 2200 (0.328 sec)\nINFO:tensorflow:global_step/sec: 304.67\nINFO:tensorflow:global_step/sec: 304.67\nINFO:tensorflow:loss = 42.80255, step = 2300 (0.328 sec)\nINFO:tensorflow:loss = 42.80255, step = 2300 (0.328 sec)\nINFO:tensorflow:global_step/sec: 304.428\nINFO:tensorflow:global_step/sec: 304.428\nINFO:tensorflow:loss = 43.28376, step = 2400 (0.328 sec)\nINFO:tensorflow:loss = 43.28376, step = 2400 (0.328 sec)\nINFO:tensorflow:global_step/sec: 306.855\nINFO:tensorflow:global_step/sec: 306.855\nINFO:tensorflow:loss = 52.975185, step = 2500 (0.326 sec)\nINFO:tensorflow:loss = 52.975185, step = 2500 (0.326 sec)\nINFO:tensorflow:global_step/sec: 301.499\nINFO:tensorflow:global_step/sec: 301.499\nINFO:tensorflow:loss = 38.57332, step = 2600 (0.332 sec)\nINFO:tensorflow:loss = 38.57332, step = 2600 (0.332 sec)\nINFO:tensorflow:global_step/sec: 304.658\nINFO:tensorflow:global_step/sec: 304.658\nINFO:tensorflow:loss = 42.026337, step = 2700 (0.328 sec)\nINFO:tensorflow:loss = 42.026337, step = 2700 (0.328 sec)\nINFO:tensorflow:global_step/sec: 304.471\nINFO:tensorflow:global_step/sec: 304.471\nINFO:tensorflow:loss = 49.812424, step = 2800 (0.329 sec)\nINFO:tensorflow:loss = 49.812424, step = 2800 (0.329 sec)\nINFO:tensorflow:global_step/sec: 301.243\nINFO:tensorflow:global_step/sec: 301.243\nINFO:tensorflow:loss = 38.365997, step = 2900 (0.332 sec)\nINFO:tensorflow:loss = 38.365997, step = 2900 (0.332 sec)\nINFO:tensorflow:global_step/sec: 303.047\nINFO:tensorflow:global_step/sec: 303.047\nINFO:tensorflow:loss = 46.136482, step = 3000 (0.330 sec)\nINFO:tensorflow:loss = 46.136482, step = 3000 (0.330 sec)\nINFO:tensorflow:global_step/sec: 309.327\nINFO:tensorflow:global_step/sec: 309.327\nINFO:tensorflow:loss = 39.838882, step = 3100 (0.323 sec)\nINFO:tensorflow:loss = 39.838882, step = 3100 (0.323 sec)\nINFO:tensorflow:global_step/sec: 314.267\nINFO:tensorflow:global_step/sec: 314.267\nINFO:tensorflow:loss = 41.79177, step = 3200 (0.318 sec)\nINFO:tensorflow:loss = 41.79177, step = 3200 (0.318 sec)\nINFO:tensorflow:global_step/sec: 301.294\nINFO:tensorflow:global_step/sec: 301.294\nINFO:tensorflow:loss = 41.994194, step = 3300 (0.332 sec)\nINFO:tensorflow:loss = 41.994194, step = 3300 (0.332 sec)\nINFO:tensorflow:global_step/sec: 308.412\nINFO:tensorflow:global_step/sec: 308.412\nINFO:tensorflow:loss = 41.158104, step = 3400 (0.324 sec)\nINFO:tensorflow:loss = 41.158104, step = 3400 (0.324 sec)\nINFO:tensorflow:global_step/sec: 305.302\nINFO:tensorflow:global_step/sec: 305.302\nINFO:tensorflow:loss = 35.35069, step = 3500 (0.328 sec)\nINFO:tensorflow:loss = 35.35069, step = 3500 (0.328 sec)\nINFO:tensorflow:global_step/sec: 303.808\nINFO:tensorflow:global_step/sec: 303.808\nINFO:tensorflow:loss = 49.999313, step = 3600 (0.329 sec)\nINFO:tensorflow:loss = 49.999313, step = 3600 (0.329 sec)\nINFO:tensorflow:global_step/sec: 312.812\nINFO:tensorflow:global_step/sec: 312.812\nINFO:tensorflow:loss = 44.52297, step = 3700 (0.320 sec)\nINFO:tensorflow:loss = 44.52297, step = 3700 (0.320 sec)\nINFO:tensorflow:global_step/sec: 311.422\nINFO:tensorflow:global_step/sec: 311.422\nINFO:tensorflow:loss = 31.237823, step = 3800 (0.321 sec)\nINFO:tensorflow:loss = 31.237823, step = 3800 (0.321 sec)\nINFO:tensorflow:global_step/sec: 311.942\nINFO:tensorflow:global_step/sec: 311.942\nINFO:tensorflow:loss = 40.837013, step = 3900 (0.321 sec)\nINFO:tensorflow:loss = 40.837013, step = 3900 (0.321 sec)\nINFO:tensorflow:global_step/sec: 310.278\nINFO:tensorflow:global_step/sec: 310.278\nINFO:tensorflow:loss = 48.289017, step = 4000 (0.322 sec)\nINFO:tensorflow:loss = 48.289017, step = 4000 (0.322 sec)\nINFO:tensorflow:global_step/sec: 305.809\nINFO:tensorflow:global_step/sec: 305.809\nINFO:tensorflow:loss = 42.82827, step = 4100 (0.327 sec)\nINFO:tensorflow:loss = 42.82827, step = 4100 (0.327 sec)\nINFO:tensorflow:global_step/sec: 309.371\nINFO:tensorflow:global_step/sec: 309.371\nINFO:tensorflow:loss = 49.08073, step = 4200 (0.323 sec)\nINFO:tensorflow:loss = 49.08073, step = 4200 (0.323 sec)\nINFO:tensorflow:global_step/sec: 313.159\nINFO:tensorflow:global_step/sec: 313.159\nINFO:tensorflow:loss = 43.150997, step = 4300 (0.319 sec)\nINFO:tensorflow:loss = 43.150997, step = 4300 (0.319 sec)\nINFO:tensorflow:global_step/sec: 317.596\nINFO:tensorflow:global_step/sec: 317.596\nINFO:tensorflow:loss = 46.704082, step = 4400 (0.315 sec)\nINFO:tensorflow:loss = 46.704082, step = 4400 (0.315 sec)\nINFO:tensorflow:global_step/sec: 316.261\nINFO:tensorflow:global_step/sec: 316.261\nINFO:tensorflow:loss = 42.477634, step = 4500 (0.316 sec)\nINFO:tensorflow:loss = 42.477634, step = 4500 (0.316 sec)\nINFO:tensorflow:global_step/sec: 319.902\nINFO:tensorflow:global_step/sec: 319.902\nINFO:tensorflow:loss = 47.049324, step = 4600 (0.313 sec)\nINFO:tensorflow:loss = 47.049324, step = 4600 (0.313 sec)\nINFO:tensorflow:global_step/sec: 323.097\nINFO:tensorflow:global_step/sec: 323.097\nINFO:tensorflow:loss = 28.26455, step = 4700 (0.310 sec)\nINFO:tensorflow:loss = 28.26455, step = 4700 (0.310 sec)\nINFO:tensorflow:global_step/sec: 318.749\nINFO:tensorflow:global_step/sec: 318.749\nINFO:tensorflow:loss = 30.772062, step = 4800 (0.314 sec)\nINFO:tensorflow:loss = 30.772062, step = 4800 (0.314 sec)\nINFO:tensorflow:global_step/sec: 323.13\nINFO:tensorflow:global_step/sec: 323.13\nINFO:tensorflow:loss = 42.176075, step = 4900 (0.310 sec)\nINFO:tensorflow:loss = 42.176075, step = 4900 (0.310 sec)\nINFO:tensorflow:global_step/sec: 321.773\nINFO:tensorflow:global_step/sec: 321.773\nINFO:tensorflow:loss = 52.00352, step = 5000 (0.311 sec)\nINFO:tensorflow:loss = 52.00352, step = 5000 (0.311 sec)\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5088...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 5088...\nINFO:tensorflow:Saving checkpoints for 5088 into /tmpfs/tmp/tmp5z0b2qd4/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 5088 into /tmpfs/tmp/tmp5z0b2qd4/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5088...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 5088...\nINFO:tensorflow:Loss for final step: 33.25688.\nINFO:tensorflow:Loss for final step: 33.25688.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/3233312620.py:20: build_parsing_serving_input_receiver_fn (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_186972/3233312620.py:20: build_parsing_serving_input_receiver_fn (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/export/export.py:312: ServingInputReceiver.__new__ (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/export/export.py:312: ServingInputReceiver.__new__ (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nWARNING:tensorflow:Loading a TF2 SavedModel but eager mode seems disabled.\nWARNING:tensorflow:Loading a TF2 SavedModel but eager mode seems disabled.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/base_head.py:786: ClassificationOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/base_head.py:786: ClassificationOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:561: RegressionOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:561: RegressionOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:563: PredictOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:563: PredictOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:168: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:168: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py:83: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py:83: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nINFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\nINFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\nINFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\nINFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp4ti4zdkp/estimator/exported_model_dir/temp-1714474233/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp4ti4zdkp/estimator/exported_model_dir/temp-1714474233/assets\nINFO:tensorflow:SavedModel written to: /tmpfs/tmp/tmp4ti4zdkp/estimator/exported_model_dir/temp-1714474233/saved_model.pb\nINFO:tensorflow:SavedModel written to: /tmpfs/tmp/tmp4ti4zdkp/estimator/exported_model_dir/temp-1714474233/saved_model.pb\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2024-04-30T10:50:35\nINFO:tensorflow:Starting evaluation at 2024-04-30T10:50:35\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/evaluation.py:260: FinalOpsHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/evaluation.py:260: FinalOpsHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Evaluation [1628/16280]\nINFO:tensorflow:Evaluation [1628/16280]\nINFO:tensorflow:Evaluation [3256/16280]\nINFO:tensorflow:Evaluation [3256/16280]\nINFO:tensorflow:Evaluation [4884/16280]\nINFO:tensorflow:Evaluation [4884/16280]\nINFO:tensorflow:Evaluation [6512/16280]\nINFO:tensorflow:Evaluation [6512/16280]\nINFO:tensorflow:Evaluation [8140/16280]\nINFO:tensorflow:Evaluation [8140/16280]\nINFO:tensorflow:Evaluation [9768/16280]\nINFO:tensorflow:Evaluation [9768/16280]\nINFO:tensorflow:Evaluation [11396/16280]\nINFO:tensorflow:Evaluation [11396/16280]\nINFO:tensorflow:Evaluation [13024/16280]\nINFO:tensorflow:Evaluation [13024/16280]\nINFO:tensorflow:Evaluation [14652/16280]\nINFO:tensorflow:Evaluation [14652/16280]\nINFO:tensorflow:Evaluation [16280/16280]\nINFO:tensorflow:Evaluation [16280/16280]\nINFO:tensorflow:Inference Time : 49.09539s\nINFO:tensorflow:Inference Time : 49.09539s\nINFO:tensorflow:Finished evaluation at 2024-04-30-10:51:24\nINFO:tensorflow:Finished evaluation at 2024-04-30-10:51:24\nINFO:tensorflow:Saving dict for global step 5088: accuracy = 0.85110563, accuracy_baseline = 0.7637592, auc = 0.90211606, auc_precision_recall = 0.96728647, average_loss = 0.32371244, global_step = 5088, label/mean = 0.7637592, loss = 0.32371244, precision = 0.88235295, prediction/mean = 0.75723934, recall = 0.9289046\nINFO:tensorflow:Saving dict for global step 5088: accuracy = 0.85110563, accuracy_baseline = 0.7637592, auc = 0.90211606, auc_precision_recall = 0.96728647, average_loss = 0.32371244, global_step = 5088, label/mean = 0.7637592, loss = 0.32371244, precision = 0.88235295, prediction/mean = 0.75723934, recall = 0.9289046\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 5088: /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 5088: /tmpfs/tmp/tmp5z0b2qd4/model.ckpt-5088\n\n```\n```\npprint.pprint(results)\n\n```\n```\n{'accuracy': 0.85110563,\n 'accuracy_baseline': 0.7637592,\n 'auc': 0.90211606,\n 'auc_precision_recall': 0.96728647,\n 'average_loss': 0.32371244,\n 'global_step': 5088,\n 'label/mean': 0.7637592,\n 'loss': 0.32371244,\n 'precision': 0.88235295,\n 'prediction/mean': 0.75723934,\n 'recall': 0.9289046}\n\n```\n\n",
  "https://www.tensorflow.org/tfx/tutorials/data_validation/chicago_taxi": "**_An Example of a Key Component of TensorFlow Extended_**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)  \n---  \nThis example colab notebook illustrates how TensorFlow Data Validation (TFDV) can be used to investigate and visualize your dataset. That includes looking at descriptive statistics, inferring a schema, checking for and fixing anomalies, and checking for drift and skew in our dataset. It's important to understand your dataset's characteristics, including how it might change over time in your production pipeline. It's also important to look for anomalies in your data, and to compare your training, evaluation, and serving datasets to make sure that they're consistent.\nWe'll use data from the \nThe columns in the dataset are: \npickup_community_area | fare | trip_start_month  \n---|---|---  \ntrip_start_hour | trip_start_day | trip_start_timestamp  \npickup_latitude | pickup_longitude | dropoff_latitude  \ndropoff_longitude | trip_miles | pickup_census_tract  \ndropoff_census_tract | payment_type | company  \ntrip_seconds | dropoff_community_area | tips  \n## Install and import packages\nInstall the packages for TensorFlow Data Validation.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install Data Validation packages\nInstall the TensorFlow Data Validation packages and dependencies, which takes a few minutes. You may see warnings and errors regarding incompatible dependency versions, which you will resolve in the next section.\n```\nprint('Installing TensorFlow Data Validation')\n!pip install --upgrade 'tensorflow_data_validation[visualization]<2'\n\n```\n\n### Import TensorFlow and reload updated packages\nThe prior step updates the default packages in the Gooogle Colab environment, so you must reload the package resources to resolve the new dependencies.\n```\nimportpkg_resources\nimportimportlib\nimportlib.reload(pkg_resources)\n\n```\n```\n/tmpfs/tmp/ipykernel_183404/3239164719.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n<module 'pkg_resources' from '/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/pkg_resources/__init__.py'>\n\n```\n\nCheck the versions of TensorFlow and the Data Validation before proceeding. \n```\nimporttensorflowastf\nimporttensorflow_data_validationastfdv\nprint('TF version:', tf.__version__)\nprint('TFDV version:', tfdv.version.__version__)\n\n```\n```\n2024-04-30 10:45:22.158704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:45:22.158751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:45:22.160265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nTF version: 2.15.1\nTFDV version: 1.15.1\n\n```\n\n## Load the dataset\nWe will download our dataset from Google Cloud Storage.\n```\nimportos\nimporttempfile,urllib,zipfile\n\n# Set up some globals for our file paths\nBASE_DIR = tempfile.mkdtemp()\nDATA_DIR = os.path.join(BASE_DIR, 'data')\nOUTPUT_DIR = os.path.join(BASE_DIR, 'chicago_taxi_output')\nTRAIN_DATA = os.path.join(DATA_DIR, 'train', 'data.csv')\nEVAL_DATA = os.path.join(DATA_DIR, 'eval', 'data.csv')\nSERVING_DATA = os.path.join(DATA_DIR, 'serving', 'data.csv')\n\n# Download the zip file from GCP and unzip it\nzip, headers = urllib.request.urlretrieve('https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip')\nzipfile.ZipFile(zip).extractall(BASE_DIR)\nzipfile.ZipFile(zip).close()\n\nprint(\"Here's what we downloaded:\")\n!ls -R {os.path.join(BASE_DIR, 'data')}\n\n```\n```\nHere's what we downloaded:\n/tmpfs/tmp/tmp7em4lo4u/data:\neval  serving  train\n\n/tmpfs/tmp/tmp7em4lo4u/data/eval:\ndata.csv\n\n/tmpfs/tmp/tmp7em4lo4u/data/serving:\ndata.csv\n\n/tmpfs/tmp/tmp7em4lo4u/data/train:\ndata.csv\n\n```\n\n## Compute and visualize statistics\nFirst we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\nTFDV can compute descriptive \nInternally, TFDV uses \n```\ntrain_stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_data_validation/utils/artifacts_io_impl.py:93: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse eager execution and: \n`tf.data.TFRecordDataset(path)`\n\n```\n\nNow let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses \n  * Notice that numeric features and catagorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n  * Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features. The percentage is the percentage of examples that have missing or zero values for that feature.\n  * Notice that there are no examples with values for `pickup_census_tract`. This is an opportunity for dimensionality reduction!\n  * Try clicking \"expand\" above the charts to change the display\n  * Try hovering over bars in the charts to display bucket ranges and counts\n  * Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n  * Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages\n\n```\n# docs-infra: no-execute\ntfdv.visualize_statistics(train_stats)\n\n```\n\n## Infer a schema\nNow let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data. A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data. For categorical features the schema also defines the domain - the list of acceptable values. Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\nGetting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct. The schema also provides documentation for the data, and so is useful when different developers work on the same data. Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it.\n```\nschema = tfdv.infer_schema(statistics=train_stats)\ntfdv.display_schema(schema=schema)\n\n```\n\n## Check evaluation data for errors\nSo far we've only been looking at the training data. It's important that our evaluation data is consistent with our training data, including that it uses the same schema. It's also important that the evaluation data includes examples of roughly the same ranges of values for our numerical features as our training data, so that our coverage of the loss surface during evaluation is roughly the same as during training. The same is true for categorical features. Otherwise, we may have training issues that are not identified during evaluation, because we didn't evaluate part of our loss surface.\n  * Notice that each feature now includes statistics for both the training and evaluation datasets.\n  * Notice that the charts now have both the training and evaluation datasets overlaid, making it easy to compare them.\n  * Notice that the charts now include a percentages view, which can be combined with log or the default linear scales.\n  * Notice that the mean and median for `trip_miles` are different for the training versus the evaluation datasets. Will that cause problems?\n  * Wow, the max `tips` is very different for the training versus the evaluation datasets. Will that cause problems?\n  * Click expand on the Numeric Features chart, and select the log scale. Review the `trip_seconds` feature, and notice the difference in the max. Will evaluation miss parts of the loss surface?\n\n```\n# Compute stats for evaluation data\neval_stats = tfdv.generate_statistics_from_csv(data_location=EVAL_DATA)\n\n```\n```\n# docs-infra: no-execute\n# Compare evaluation data with training data\ntfdv.visualize_statistics(lhs_statistics=eval_stats, rhs_statistics=train_stats,\n                          lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n\n```\n\n## Check for evaluation anomalies\nDoes our evaluation dataset match the schema from our training dataset? This is especially important for categorical features, where we want to identify the range of acceptable values.\n```\n# Check eval data for errors by validating the eval data stats using the previously inferred schema.\nanomalies = tfdv.validate_statistics(statistics=eval_stats, schema=schema)\ntfdv.display_anomalies(anomalies)\n\n```\n\n## Fix evaluation anomalies in the schema\nOops! It looks like we have some new values for `company` in our evaluation data, that we didn't have in our training data. We also have a new value for `payment_type`. These should be considered anomalies, but what we decide to do about them depends on our domain knowledge of the data. If an anomaly truly indicates a data error, then the underlying data should be fixed. Otherwise, we can simply update the schema to include the values in the eval dataset.\nUnless we change our evaluation dataset we can't fix everything, but we can fix things in the schema that we're comfortable accepting. That includes relaxing our view of what is and what is not an anomaly for particular features, as well as updating our schema to include missing values for categorical features. TFDV has enabled us to discover what we need to fix.\nLet's make those fixes now, and then review one more time.\n```\n# Relax the minimum fraction of values that must come from the domain for feature company.\ncompany = tfdv.get_feature(schema, 'company')\ncompany.distribution_constraints.min_domain_mass = 0.9\n\n# Add new value to the domain of feature payment_type.\npayment_type_domain = tfdv.get_domain(schema, 'payment_type')\npayment_type_domain.value.append('Prcard')\n\n# Validate eval stats after updating the schema \nupdated_anomalies = tfdv.validate_statistics(eval_stats, schema)\ntfdv.display_anomalies(updated_anomalies)\n\n```\n\nHey, look at that! We verified that the training and evaluation data are now consistent! Thanks TFDV ;)\n## Schema Environments\nWe also split off a 'serving' dataset for this example, so we should check that too. By default all datasets in a pipeline should use the same schema, but there are often exceptions. For example, in supervised learning we need to include labels in our dataset, but when we serve the model for inference the labels will not be included. In some cases introducing slight schema variations is necessary.\n**Environments** can be used to express such requirements. In particular, features in schema can be associated with a set of environments using `default_environment`, `in_environment` and `not_in_environment`.\nFor example, in this dataset the `tips` feature is included as the label for training, but it's missing in the serving data. Without environment specified, it will show up as an anomaly.\n```\nserving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n\n```\n\nWe'll deal with the `tips` feature below. We also have an INT value in our trip seconds, where our schema expected a FLOAT. By making us aware of that difference, TFDV helps uncover inconsistencies in the way the data is generated for training and serving. It's very easy to be unaware of problems like that until model performance suffers, sometimes catastrophically. It may or may not be a significant issue, but in any case this should be cause for further investigation.\nIn this case, we can safely convert INT values to FLOATs, so we want to tell TFDV to use our schema to infer the type. Let's do that now.\n```\noptions = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True)\nserving_stats = tfdv.generate_statistics_from_csv(SERVING_DATA, stats_options=options)\nserving_anomalies = tfdv.validate_statistics(serving_stats, schema)\n\ntfdv.display_anomalies(serving_anomalies)\n\n```\n\nNow we just have the `tips` feature (which is our label) showing up as an anomaly ('Column dropped'). Of course we don't expect to have labels in our serving data, so let's tell TFDV to ignore that.\n```\n# All features are by default in both TRAINING and SERVING environments.\nschema.default_environment.append('TRAINING')\nschema.default_environment.append('SERVING')\n\n# Specify that 'tips' feature is not in SERVING environment.\ntfdv.get_feature(schema, 'tips').not_in_environment.append('SERVING')\n\nserving_anomalies_with_env = tfdv.validate_statistics(\n    serving_stats, schema, environment='SERVING')\n\ntfdv.display_anomalies(serving_anomalies_with_env)\n\n```\n\n## Check for drift and skew\nIn addition to checking whether a dataset conforms to the expectations set in the schema, TFDV also provides functionalities to detect drift and skew. TFDV performs this check by comparing the statistics of the different datasets based on the drift/skew comparators specified in the schema.\n### Drift\nDrift detection is supported for categorical features and between consecutive spans of data (i.e., between span N and span N+1), such as between different days of training data. We express drift in terms of \n### Skew\nTFDV can detect three different kinds of skew in your data - schema skew, feature skew, and distribution skew.\n#### Schema Skew\nSchema skew occurs when the training and serving data do not conform to the same schema. Both training and serving data are expected to adhere to the same schema. Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n#### Feature Skew\nFeature skew occurs when the feature values that a model trains on are different from the feature values that it sees at serving time. For example, this can happen when:\n  * A data source that provides some feature values is modified between training and serving time\n  * There is different logic for generating features between training and serving. For example, if you apply some transformation only in one of the two code paths.\n\n\n#### Distribution Skew\nDistribution skew occurs when the distribution of the training dataset is significantly different from the distribution of the serving dataset. One of the key causes for distribution skew is using different code or different data sources to generate the training dataset. Another reason is a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on.\n```\n# Add skew comparator for 'payment_type' feature.\npayment_type = tfdv.get_feature(schema, 'payment_type')\npayment_type.skew_comparator.infinity_norm.threshold = 0.01\n\n# Add drift comparator for 'company' feature.\ncompany=tfdv.get_feature(schema, 'company')\ncompany.drift_comparator.infinity_norm.threshold = 0.001\n\nskew_anomalies = tfdv.validate_statistics(train_stats, schema,\n                                          previous_statistics=eval_stats,\n                                          serving_statistics=serving_stats)\n\ntfdv.display_anomalies(skew_anomalies)\n\n```\n\nIn this example we do see some drift, but it is well below the threshold that we've set.\n## Freeze the schema\nNow that the schema has been reviewed and curated, we will store it in a file to reflect its \"frozen\" state.\n```\nfromtensorflow.python.lib.ioimport file_io\nfromgoogle.protobufimport text_format\n\nfile_io.recursive_create_dir(OUTPUT_DIR)\nschema_file = os.path.join(OUTPUT_DIR, 'schema.pbtxt')\ntfdv.write_schema_text(schema, schema_file)\n\n!cat {schema_file}\n\n```\n```\nfeature {\n  name: \"pickup_community_area\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"fare\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_month\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_hour\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_day\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"trip_start_timestamp\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_latitude\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_longitude\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"dropoff_latitude\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"dropoff_longitude\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: FLOAT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"trip_miles\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"pickup_census_tract\"\n  type: BYTES\n  presence {\n    min_count: 0\n  }\n}\nfeature {\n  name: \"dropoff_census_tract\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: INT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"payment_type\"\n  type: BYTES\n  domain: \"payment_type\"\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  skew_comparator {\n    infinity_norm {\n      threshold: 0.01\n    }\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"company\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: BYTES\n  domain: \"company\"\n  presence {\n    min_count: 1\n  }\n  distribution_constraints {\n    min_domain_mass: 0.9\n  }\n  drift_comparator {\n    infinity_norm {\n      threshold: 0.001\n    }\n  }\n}\nfeature {\n  name: \"trip_seconds\"\n  type: INT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nfeature {\n  name: \"dropoff_community_area\"\n  value_count {\n    min: 1\n    max: 1\n  }\n  type: INT\n  presence {\n    min_count: 1\n  }\n}\nfeature {\n  name: \"tips\"\n  type: FLOAT\n  presence {\n    min_fraction: 1.0\n    min_count: 1\n  }\n  not_in_environment: \"SERVING\"\n  shape {\n    dim {\n      size: 1\n    }\n  }\n}\nstring_domain {\n  name: \"payment_type\"\n  value: \"Cash\"\n  value: \"Credit Card\"\n  value: \"Dispute\"\n  value: \"No Charge\"\n  value: \"Pcard\"\n  value: \"Unknown\"\n  value: \"Prcard\"\n}\nstring_domain {\n  name: \"company\"\n  value: \"0118 - 42111 Godfrey S.Awir\"\n  value: \"0694 - 59280 Chinesco Trans Inc\"\n  value: \"1085 - 72312 N and W Cab Co\"\n  value: \"2733 - 74600 Benny Jona\"\n  value: \"2809 - 95474 C & D Cab Co Inc.\"\n  value: \"3011 - 66308 JBL Cab Inc.\"\n  value: \"3152 - 97284 Crystal Abernathy\"\n  value: \"3201 - C&D Cab Co Inc\"\n  value: \"3201 - CID Cab Co Inc\"\n  value: \"3253 - 91138 Gaither Cab Co.\"\n  value: \"3385 - 23210 Eman Cab\"\n  value: \"3623 - 72222 Arrington Enterprises\"\n  value: \"3897 - Ilie Malec\"\n  value: \"4053 - Adwar H. Nikola\"\n  value: \"4197 - 41842 Royal Star\"\n  value: \"4615 - 83503 Tyrone Henderson\"\n  value: \"4615 - Tyrone Henderson\"\n  value: \"4623 - Jay Kim\"\n  value: \"5006 - 39261 Salifu Bawa\"\n  value: \"5006 - Salifu Bawa\"\n  value: \"5074 - 54002 Ahzmi Inc\"\n  value: \"5074 - Ahzmi Inc\"\n  value: \"5129 - 87128\"\n  value: \"5129 - 98755 Mengisti Taxi\"\n  value: \"5129 - Mengisti Taxi\"\n  value: \"5724 - KYVI Cab Inc\"\n  value: \"585 - Valley Cab Co\"\n  value: \"5864 - 73614 Thomas Owusu\"\n  value: \"5864 - Thomas Owusu\"\n  value: \"5874 - 73628 Sergey Cab Corp.\"\n  value: \"5997 - 65283 AW Services Inc.\"\n  value: \"5997 - AW Services Inc.\"\n  value: \"6488 - 83287 Zuha Taxi\"\n  value: \"6743 - Luhak Corp\"\n  value: \"Blue Ribbon Taxi Association Inc.\"\n  value: \"C & D Cab Co Inc\"\n  value: \"Chicago Elite Cab Corp.\"\n  value: \"Chicago Elite Cab Corp. (Chicago Carriag\"\n  value: \"Chicago Medallion Leasing INC\"\n  value: \"Chicago Medallion Management\"\n  value: \"Choice Taxi Association\"\n  value: \"Dispatch Taxi Affiliation\"\n  value: \"KOAM Taxi Association\"\n  value: \"Northwest Management LLC\"\n  value: \"Taxi Affiliation Services\"\n  value: \"Top Cab Affiliation\"\n}\ndefault_environment: \"TRAINING\"\ndefault_environment: \"SERVING\"\n\n```\n\n## When to use TFDV\nIt's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here's a few more:\n  * Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n  * Validating new data for inference to make sure that our model has trained on that part of the decision surface\n  * Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/transform/get_started)) to make sure we haven't done something wrong\n\n\n",
  "https://www.tensorflow.org/tfx/tutorials/transform/simple": "**_The Feature Engineering Component of TensorFlow Extended (TFX)_**\n[ View on TensorFlow.org](https://www.tensorflow.org/tfx/tutorials/transform/simple)  \n---  \nThis example colab notebook provides a very simple example of how [TensorFlow Transform (`tf.Transform`)](https://www.tensorflow.org/tfx/transform/get_started/) can be used to preprocess data using exactly the same code for both training a model and serving inferences in production.\nTensorFlow Transform is a library for preprocessing input data for TensorFlow, including creating features that require a full pass over the training dataset. For example, using TensorFlow Transform you could:\n  * Normalize an input value by using the mean and standard deviation\n  * Convert strings to integers by generating a vocabulary over all of the input values\n  * Convert floats to integers by assigning them to buckets, based on the observed data distribution\n\n\nTensorFlow has built-in support for manipulations on a single example or a batch of examples. `tf.Transform` extends these capabilities to support full passes over the entire training dataset.\nThe output of `tf.Transform` is exported as a TensorFlow graph which you can use for both training and serving. Using the same graph for both training and serving can prevent skew, since the same transformations are applied in both stages.\n### Upgrade Pip\nTo avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab. Local systems can of course be upgraded separately.\n```\ntry:\n  importcolab\n  !pip install --upgrade pip\nexcept:\n  pass\n\n```\n\n### Install TensorFlow Transform\n```\npip\n```\n```\n# This cell is only necessary because packages were installed while python was\n# running. It avoids the need to restart the runtime when running in Colab.\nimportpkg_resources\nimportimportlib\n\nimportlib.reload(pkg_resources)\n\n```\n```\n/tmpfs/tmp/ipykernel_192169/639106435.py:3: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  import pkg_resources\n<module 'pkg_resources' from '/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/pkg_resources/__init__.py'>\n\n```\n\n## Imports\n```\nimportpathlib\nimportpprint\nimporttempfile\n\nimporttensorflowastf\nimporttensorflow_transformastft\n\nimporttensorflow_transform.beamastft_beam\nfromtensorflow_transform.tf_metadataimport dataset_metadata\nfromtensorflow_transform.tf_metadataimport schema_utils\nfromtensorflow_transform.keras_libimport tf_keras\n\n```\n```\n2024-04-30 10:54:48.029467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-30 10:54:48.029516: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-30 10:54:48.030987: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\n## Data: Create some dummy data\nWe'll create some simple dummy data for our simple example:\n  * `raw_data` is the initial raw data that we're going to preprocess\n  * `raw_data_metadata` contains the schema that tells us the types of each of the columns in `raw_data`. In this case, it's very simple.\n\n```\nraw_data = [\n      {'x': 1, 'y': 1, 's': 'hello'},\n      {'x': 2, 'y': 2, 's': 'world'},\n      {'x': 3, 'y': 3, 's': 'hello'}\n  ]\n\nraw_data_metadata = dataset_metadata.DatasetMetadata(\n    schema_utils.schema_from_feature_spec({\n        'y': tf.io.FixedLenFeature([], tf.float32),\n        'x': tf.io.FixedLenFeature([], tf.float32),\n        's': tf.io.FixedLenFeature([], tf.string),\n    }))\n\n```\n\n## Transform: Create a preprocessing function\nThe _preprocessing function_ is the most important concept of tf.Transform. A preprocessing function is where the transformation of the dataset really happens. It accepts and returns a dictionary of tensors, where a tensor means a [`Tensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/Tensor) or [`SparseTensor`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/SparseTensor). There are two main groups of API calls that typically form the heart of a preprocessing function:\n  1. **TensorFlow Ops:** Any function that accepts and returns tensors, which usually means TensorFlow ops. These add TensorFlow operations to the graph that transforms raw data into transformed data one feature vector at a time. These will run for every example, during both training and serving.\n  2. **Tensorflow Transform Analyzers/Mappers:** Any of the analyzers/mappers provided by tf.Transform. These also accept and return tensors, and typically contain a combination of Tensorflow ops and Beam computation, but unlike TensorFlow ops they only run in the Beam pipeline during analysis requiring a full pass over the entire training dataset. The Beam computation runs only once, (prior to training, during analysis), and typically make a full pass over the entire training dataset. They create [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) tensors, which are added to your graph. For example, [`tft.min`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/min) computes the minimum of a tensor over the training dataset.\n\n```\ndefpreprocessing_fn(inputs):\n\"\"\"Preprocess input columns into transformed columns.\"\"\"\n    x = inputs['x']\n    y = inputs['y']\n    s = inputs['s']\n    x_centered = x - tft.mean(x)\n    y_normalized = tft.scale_to_0_1(y)\n    s_integerized = tft.compute_and_apply_vocabulary(s)\n    x_centered_times_y_normalized = (x_centered * y_normalized)\n    return {\n        'x_centered': x_centered,\n        'y_normalized': y_normalized,\n        's_integerized': s_integerized,\n        'x_centered_times_y_normalized': x_centered_times_y_normalized,\n    }\n\n```\n\n## Syntax\nYou're almost ready to put everything together and use \nApache Beam uses a \n```\nresult = pass_this | 'name this step' >> to_this_call\n\n```\n\nThe method `to_this_call` is being invoked and passed the object called `pass_this`, and `to_this_call` is returned in `result`. You will often see stages of a pipeline chained together like this:\n```\nresult = apache_beam.Pipeline() | 'first step' >> do_this_first() | 'second step' >> do_this_last()\n\n```\n\nand since that started with a new pipeline, you can continue like this:\n```\nnext_result = result | 'doing more stuff' >> another_function()\n\n```\n\n## Putting it all together\nNow we're ready to transform our data. We'll use Apache Beam with a direct runner, and supply three inputs:\n  1. `raw_data` - The raw input data that we created above\n  2. `raw_data_metadata` - The schema for the raw data\n  3. `preprocessing_fn` - The function that we created to do our transformation\n\n```\ndefmain(output_dir):\n  # Ignore the warnings\n  with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n    transformed_dataset, transform_fn = (  # pylint: disable=unused-variable\n        (raw_data, raw_data_metadata) | tft_beam.AnalyzeAndTransformDataset(\n            preprocessing_fn))\n\n  transformed_data, transformed_metadata = transformed_dataset  # pylint: disable=unused-variable\n\n  # Save the transform_fn to the output_dir\n  _ = (\n      transform_fn\n      | 'WriteTransformFn' >> tft_beam.WriteTransformFn(output_dir))\n\n  return transformed_data, transformed_metadata\n\n```\n```\noutput_dir = pathlib.Path(tempfile.mkdtemp())\n\ntransformed_data, transformed_metadata = main(str(output_dir))\n\nprint('\\nRaw data:\\n{}\\n'.format(pprint.pformat(raw_data)))\nprint('Transformed data:\\n{}'.format(pprint.pformat(transformed_data)))\n\n```\n```\nWARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\nWARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:absl:You are passing instance dicts and DatasetMetadata to TFT which will not provide optimal performance. Consider following the TFT guide to upgrade to the TFXIO format (Apache Arrow RecordBatch).\nWARNING:absl:You are outputting instance dicts from `TransformDataset` which will not provide optimal performance. Consider setting  `output_record_batches=True` to upgrade to the TFXIO format (Apache Arrow RecordBatch). Encoding functionality in this module works with both formats.\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpgsoge9im.json', '--HistoryManager.hist_file=:memory:']\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8s0_zhbm/tftransform_tmp/c576d13575254973b6f7263cfcf3ffc3/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8s0_zhbm/tftransform_tmp/c576d13575254973b6f7263cfcf3ffc3/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8s0_zhbm/tftransform_tmp/b9fda3835766458d8e33d05f6357bed2/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmp8s0_zhbm/tftransform_tmp/b9fda3835766458d8e33d05f6357bed2/assets\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\nWARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py', '-f', '/tmpfs/tmp/tmpgsoge9im.json', '--HistoryManager.hist_file=:memory:']\nRaw data:\n[{'s': 'hello', 'x': 1, 'y': 1},\n {'s': 'world', 'x': 2, 'y': 2},\n {'s': 'hello', 'x': 3, 'y': 3}]\n\nTransformed data:\n[{'s_integerized': 0,\n  'x_centered': -1.0,\n  'x_centered_times_y_normalized': -0.0,\n  'y_normalized': 0.0},\n {'s_integerized': 1,\n  'x_centered': 0.0,\n  'x_centered_times_y_normalized': 0.0,\n  'y_normalized': 0.5},\n {'s_integerized': 0,\n  'x_centered': 1.0,\n  'x_centered_times_y_normalized': 1.0,\n  'y_normalized': 1.0}]\n\n```\n\n## Is this the right answer?\nPreviously, we used `tf.Transform` to do this:\n```\nx_centered = x - tft.mean(x)\ny_normalized = tft.scale_to_0_1(y)\ns_integerized = tft.compute_and_apply_vocabulary(s)\nx_centered_times_y_normalized = (x_centered * y_normalized)\n\n```\n\n  * **x_centered** - With input of `[1, 2, 3]` the mean of x is 2, and we subtract it from x to center our x values at 0. So our result of `[-1.0, 0.0, 1.0]` is correct.\n  * **y_normalized** - We wanted to scale our y values between 0 and 1. Our input was `[1, 2, 3]` so our result of `[0.0, 0.5, 1.0]` is correct.\n  * **s_integerized** - We wanted to map our strings to indexes in a vocabulary, and there were only 2 words in our vocabulary (\"hello\" and \"world\"). So with input of `[\"hello\", \"world\", \"hello\"]` our result of `[0, 1, 0]` is correct. Since \"hello\" occurs most frequently in this data, it will be the first entry in the vocabulary.\n  * **x_centered_times_y_normalized** - We wanted to create a new feature by crossing `x_centered` and `y_normalized` using multiplication. Note that this multiplies the results, not the original values, and our new result of `[-0.0, 0.0, 1.0]` is correct.\n\n\n## Use the resulting `transform_fn`\n```\nls{output_dir}\n```\n```\ntotal 8\ndrwxr-xr-x 4 kbuilder kbuilder 4096 Apr 30 10:54 transform_fn\ndrwxr-xr-x 2 kbuilder kbuilder 4096 Apr 30 10:54 transformed_metadata\n\n```\n\nThe `transform_fn/` directory contains a `tf.saved_model` implementing with all the constants tensorflow-transform analysis results built into the graph. \nIt is possible to load this directly with [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load), but this not easy to use:\n```\nloaded = tf.saved_model.load(str(output_dir/'transform_fn'))\nloaded.signatures['serving_default']\n\n```\n```\n<ConcreteFunction (*, inputs: TensorSpec(shape=(None,), dtype=tf.string, name='inputs'), inputs_1: TensorSpec(shape=(None,), dtype=tf.float32, name='inputs_1'), inputs_2: TensorSpec(shape=(None,), dtype=tf.float32, name='inputs_2')) -> Dict[['x_centered', TensorSpec(shape=(None,), dtype=tf.float32, name='x_centered')], ['s_integerized', TensorSpec(shape=<unknown>, dtype=tf.int64, name='s_integerized')], ['x_centered_times_y_normalized', TensorSpec(shape=(None,), dtype=tf.float32, name='x_centered_times_y_normalized')], ['y_normalized', TensorSpec(shape=(None,), dtype=tf.float32, name='y_normalized')]] at 0x7F452C2C6400>\n\n```\n\nA better approach is to load it using [`tft.TFTransformOutput`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TFTransformOutput). The [`TFTransformOutput.transform_features_layer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TFTransformOutput#transform_features_layer) method returns a [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) object that can be used to apply the transformation:\n```\ntf_transform_output = tft.TFTransformOutput(output_dir)\n\ntft_layer = tf_transform_output.transform_features_layer()\ntft_layer\n\n```\n```\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:struct2tensor is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_decision_forests is not available.\nINFO:tensorflow:tensorflow_text is not available.\nINFO:tensorflow:tensorflow_text is not available.\n<tensorflow_transform.output_wrapper.TransformFeaturesLayer at 0x7f46bc272700>\n\n```\n\nThis [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) expects a dictionary of batched features. So create a `Dict[str, tf.Tensor]` from the `List[Dict[str, Any]]` in `raw_data`:\n```\nraw_data_batch = {\n    's': tf.constant([ex['s'] for ex in raw_data]),\n    'x': tf.constant([ex['x'] for ex in raw_data], dtype=tf.float32),\n    'y': tf.constant([ex['y'] for ex in raw_data], dtype=tf.float32),\n}\n\n```\n\nYou can use the [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) on its own:\n```\ntransformed_batch = tft_layer(raw_data_batch)\n\n{key: value.numpy() for key, value in transformed_batch.items()}\n\n```\n```\n{'x_centered': array([-1.,  0.,  1.], dtype=float32),\n 'x_centered_times_y_normalized': array([-0.,  0.,  1.], dtype=float32),\n 'y_normalized': array([0. , 0.5, 1. ], dtype=float32),\n 's_integerized': array([0, 1, 0])}\n\n```\n\n## Export\nA more typical use case would use `tf.Transform` to apply the transformation to the training and evaluation datasets (see the [next tutorial](https://www.tensorflow.org/tfx/tutorials/transform/census) for an example). Then, after training, before exporting the model attach the [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) as the first layer so that you can export it as part of your [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model). For a concrete example, keep reading.\n### An example training model\nBelow is a model that:\n  1. takes the transformed batch,\n  2. stacks them all together into a simple `(batch, features)` matrix,\n  3. runs them through a few dense layers, and\n  4. produces 10 linear outputs.\n\n\nIn a real use case you would apply a one-hot to the `s_integerized` feature.\nYou could train this model on a dataset transformed by `tf.Transform`:\n```\nclassStackDict(tf_keras.layers.Layer):\n  defcall(self, inputs):\n    values = [\n        tf.cast(v, tf.float32)\n        for k,v in sorted(inputs.items(), key=lambda kv: kv[0])]\n    return tf.stack(values, axis=1)\n\n```\n```\nclassTrainedModel(tf_keras.Model):\n  def__init__(self):\n    super().__init__(self)\n    self.concat = StackDict()\n    self.body = tf_keras.Sequential([\n        tf_keras.layers.Dense(64, activation='relu'),\n        tf_keras.layers.Dense(64, activation='relu'),\n        tf_keras.layers.Dense(10),\n    ])\n\n  defcall(self, inputs, training=None):\n    x = self.concat(inputs)\n    return self.body(x, training)\n\n```\n```\ntrained_model = TrainedModel()\n\n```\n\nImagine we trained the model.\n```\ntrained_model.compile(loss=..., optimizer='adam')\ntrained_model.fit(...)\n\n```\n\nThis model runs on the transformed inputs\n```\ntrained_model_output = trained_model(transformed_batch)\ntrained_model_output.shape\n\n```\n```\nTensorShape([3, 10])\n\n```\n\n### An example export wrapper\nImagine you've trained the above model and want to export it.\nYou'll want to include the transform function in the exported model:\n```\nclassExportModel(tf.Module):\n  def__init__(self, trained_model, input_transform):\n    self.trained_model = trained_model\n    self.input_transform = input_transform\n\n  @tf.function\n  def__call__(self, inputs, training=None):\n    x = self.input_transform(inputs)\n    return self.trained_model(x)\n\n```\n```\nexport_model = ExportModel(trained_model=trained_model,\n                           input_transform=tft_layer)\n\n```\n\nThis combined model works on the raw data, and produces exactly the same results as calling the trained model directly:\n```\nexport_model_output = export_model(raw_data_batch)\nexport_model_output.shape\n\n```\n```\nTensorShape([3, 10])\n\n```\n```\ntf.reduce_max(abs(export_model_output - trained_model_output)).numpy()\n\n```\n```\n0.0\n\n```\n\nThis `export_model` includes the [`tft.TransformFeaturesLayer`](https://www.tensorflow.org/tfx/transform/api_docs/python/tft/TransformFeaturesLayer) and is entirely self-contained. You can save it and restore it in another environment and still get exactly the same result:\n```\nimporttempfile\nmodel_dir = tempfile.mkdtemp(suffix='tft')\n\ntf.saved_model.save(export_model, model_dir)\n\n```\n```\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpjz93eylstft/assets\nINFO:tensorflow:Assets written to: /tmpfs/tmp/tmpjz93eylstft/assets\n\n```\n```\nreloaded = tf.saved_model.load(model_dir)\n\nreloaded_model_output = reloaded(raw_data_batch)\nreloaded_model_output.shape\n\n```\n```\nTensorShape([3, 10])\n\n```\n```\ntf.reduce_max(abs(export_model_output - reloaded_model_output)).numpy()\n\n```\n```\n0.0\n\n```\n\n",
  "https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html": "\n",
  "https://www.tensorflow.org/tutorials/load_data/tfrecord": "[View on TensorFlow.org](https://www.tensorflow.org/tutorials/load_data/tfrecord)  \n---  \nThe TFRecord format is a simple format for storing a sequence of binary records.\nProtocol messages are defined by `.proto` files, these are often the easiest way to understand a message type.\nThe [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message (or protobuf) is a flexible message type that represents a `{\"string\": value}` mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as [TFX](https://www.tensorflow.org/tfx/).\nThis notebook demonstrates how to create, parse, and use the [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message, and then serialize, write, and read [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) messages to and from `.tfrecord` files.\n## Setup\n```\nimporttensorflowastf\n\nimportnumpyasnp\nimportIPython.displayasdisplay\n\n```\n```\n2024-08-16 07:04:00.479365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-16 07:04:00.500427: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-16 07:04:00.506819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\n## [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)\n### Data types for [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)\nFundamentally, a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) is a `{\"string\": tf.train.Feature}` mapping.\nThe [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature) message type can accept one of the following three types (See the \n  1. [`tf.train.BytesList`](https://www.tensorflow.org/api_docs/python/tf/train/BytesList) (the following types can be coerced)\n     * `string`\n     * `byte`\n  2. [`tf.train.FloatList`](https://www.tensorflow.org/api_docs/python/tf/train/FloatList) (the following types can be coerced)\n     * `float` (`float32`)\n     * `double` (`float64`)\n  3. [`tf.train.Int64List`](https://www.tensorflow.org/api_docs/python/tf/train/Int64List) (the following types can be coerced)\n     * `bool`\n     * `enum`\n     * `int32`\n     * `uint32`\n     * `int64`\n     * `uint64`\n\n\nIn order to convert a standard TensorFlow type to a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)-compatible [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature), you can use the shortcut functions below. Note that each function takes a scalar input value and returns a [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature) containing one of the three `list` types above:\n```\n# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef_bytes_feature(value):\n\"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef_float_feature(value):\n\"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef_int64_feature(value):\n\"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n```\n\nBelow are some examples of how these functions work. Note the varying input types and the standardized output types. If the input type for a function does not match one of the coercible types stated above, the function will raise an exception (e.g. `_int64_feature(1.0)` will error out because `1.0` is a float—therefore, it should be used with the `_float_feature` function instead):\n```\nprint(_bytes_feature(b'test_string'))\nprint(_bytes_feature(u'test_bytes'.encode('utf-8')))\n\nprint(_float_feature(np.exp(1)))\n\nprint(_int64_feature(True))\nprint(_int64_feature(1))\n\n```\n```\nbytes_list {\n  value: \"test_string\"\n}\n\nbytes_list {\n  value: \"test_bytes\"\n}\n\nfloat_list {\n  value: 2.7182817459106445\n}\n\nint64_list {\n  value: 1\n}\n\nint64_list {\n  value: 1\n}\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723791843.092175  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.095539  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.099144  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.102704  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.113978  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.117024  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.120462  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.123843  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.126718  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.129679  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.133016  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791843.136422  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.360431  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.362573  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.364572  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.366645  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.368643  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.370639  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.372538  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.374538  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.376473  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.378464  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.380358  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.382352  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.421137  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.423231  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.425187  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.427219  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.429165  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.431166  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.433071  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.435053  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.436984  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.439359  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.441678  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723791844.444034  194894 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n\n```\n\nAll proto messages can be serialized to a binary-string using the `.SerializeToString` method:\n```\nfeature = _float_feature(np.exp(1))\n\nfeature.SerializeToString()\n\n```\n```\nb'\\x12\\x06\\n\\x04T\\xf8-@'\n\n```\n\n### Creating a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message\nSuppose you want to create a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message from existing data. In practice, the dataset may come from anywhere, but the procedure of creating the [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message from a single observation will be the same:\n  1. Within each observation, each value needs to be converted to a [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature) containing one of the 3 compatible types, using one of the functions above.\n  2. You create a map (dictionary) from the feature name string to the encoded feature value produced in #1.\n  3. The map produced in step 2 is converted to a \n\n\nIn this notebook, you will create a dataset using NumPy.\nThis dataset will have 4 features:\n  * a boolean feature, `False` or `True` with equal probability\n  * an integer feature uniformly randomly chosen from `[0, 5]`\n  * a string feature generated from a string table by using the integer feature as an index\n  * a float feature from a standard normal distribution\n\n\nConsider a sample consisting of 10,000 independently and identically distributed observations from each of the above distributions:\n```\n# The number of observations in the dataset.\nn_observations = int(1e4)\n\n# Boolean feature, encoded as False or True.\nfeature0 = np.random.choice([False, True], n_observations)\n\n# Integer feature, random from 0 to 4.\nfeature1 = np.random.randint(0, 5, n_observations)\n\n# String feature.\nstrings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\nfeature2 = strings[feature1]\n\n# Float feature, from a standard normal distribution.\nfeature3 = np.random.randn(n_observations)\n\n```\n\nEach of these features can be coerced into a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)-compatible type using one of `_bytes_feature`, `_float_feature`, `_int64_feature`. You can then create a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message from these encoded features:\n```\n@tf.py_function(Tout=tf.string)\ndefserialize_example(feature0, feature1, feature2, feature3):\n\"\"\"\n  Creates a tf.train.Example message ready to be written to a file.\n  \"\"\"\n  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n  # data type.\n  feature = {\n      'feature0': _int64_feature(feature0),\n      'feature1': _int64_feature(feature1),\n      'feature2': _bytes_feature(feature2),\n      'feature3': _float_feature(feature3),\n  }\n\n  # Create a Features message using tf.train.Example.\n\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()\n\n```\n\nFor example, suppose you have a single observation from the dataset, `[False, 4, bytes('goat'), 0.9876]`. You can create and print the `tf.train.Example` message for this observation using `serialize_example()`. Each single observation will be written as a `Features` message as per the above. Note that the `tf.train.Example` `Features` message:\n```\n# This is an example observation from the dataset.\n\nexample_observation = [False, 4, b'goat', 0.9876]\nserialized_example = serialize_example(*example_observation)\nserialized_example\n\n```\n```\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04[\\xd3|?\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat'>\n\n```\n\nTo decode the message use the [`tf.train.Example.FromString`](https://www.tensorflow.org/api_docs/python/tf/train/Example#FromString) method.\n```\nexample_proto = tf.train.Example.FromString(serialized_example.numpy())\nexample_proto\n\n```\n```\nfeatures {\n  feature {\n    key: \"feature0\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"feature1\"\n    value {\n      int64_list {\n        value: 4\n      }\n    }\n  }\n  feature {\n    key: \"feature2\"\n    value {\n      bytes_list {\n        value: \"goat\"\n      }\n    }\n  }\n  feature {\n    key: \"feature3\"\n    value {\n      float_list {\n        value: 0.9876000285148621\n      }\n    }\n  }\n}\n\n```\n\n## TFRecords format details\nA TFRecord file contains a sequence of records. The file can only be read sequentially.\nEach record contains a byte-string, for the data-payload, plus the data-length, and CRC-32C (\nEach record is stored in the following formats:\n```\nuint64length\nuint32masked_crc32_of_length\nbytedata[length]\nuint32masked_crc32_of_data\n\n```\n\nThe records are concatenated together to produce the file. CRCs are \n```\nmasked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul\n\n```\n\n## Reading and writing TFRecord files\nThe [`tf.io`](https://www.tensorflow.org/api_docs/python/tf/io) module also contains pure-Python functions for reading and writing TFRecord files.\n### Writing a TFRecord file\nNext, write the 10,000 observations to the file `test.tfrecord`. Each observation is converted to a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message, then written to file. You can then verify that the file `test.tfrecord` has been created:\n```\nfilename = 'test.tfrecord'\n\n```\n```\n# Write the `tf.train.Example` observations to the file.\nwith tf.io.TFRecordWriter(filename) as writer:\n  for i in range(n_observations):\n    example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])\n    writer.write(example.numpy())\n\n```\n```\ndu{filename}\n```\n```\n984K    test.tfrecord\n\n```\n\n### Reading a TFRecord file in python\nThese serialized tensors can be easily parsed using [`tf.train.Example.ParseFromString`](https://www.tensorflow.org/api_docs/python/tf/train/Example#ParseFromString):\n```\nfilenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset\n\n```\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n```\n```\nfor raw_record in raw_dataset.take(1):\n  example = tf.train.Example()\n  example.ParseFromString(raw_record.numpy())\n  print(example)\n\n```\n```\nfeatures {\n  feature {\n    key: \"feature0\"\n    value {\n      int64_list {\n        value: 0\n      }\n    }\n  }\n  feature {\n    key: \"feature1\"\n    value {\n      int64_list {\n        value: 3\n      }\n    }\n  }\n  feature {\n    key: \"feature2\"\n    value {\n      bytes_list {\n        value: \"horse\"\n      }\n    }\n  }\n  feature {\n    key: \"feature3\"\n    value {\n      float_list {\n        value: -0.6452258229255676\n      }\n    }\n  }\n}\n\n```\n\nThat returns a [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) proto which is dificult to use as is, but it's fundamentally a representation of a:\n```\nDict[str,\n     Union[List[float],\nList[int],\nList[str]]]\n\n```\n\nThe following code manually converts the `Example` to a dictionary of NumPy arrays, without using TensorFlow Ops. Refer to \n```\nresult = {}\n# example.features.feature is the dictionary\nfor key, feature in example.features.feature.items():\n  # The values are the Feature objects which contain a `kind` which contains:\n  # one of three fields: bytes_list, float_list, int64_list\n\n  kind = feature.WhichOneof('kind')\n  result[key] = np.array(getattr(feature, kind).value)\n\nresult\n\n```\n```\n{'feature2': array([b'horse'], dtype='|S5'),\n 'feature1': array([3]),\n 'feature0': array([0]),\n 'feature3': array([-0.64522582])}\n\n```\n\n### Reading a TFRecord file Using tf.data\nYou can also read the TFRecord file using the [`tf.data.TFRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) class.\nMore information on consuming TFRecord files using [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) can be found in the [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data#consuming_tfrecord_data) guide.\nUsing `TFRecordDataset`s can be useful for standardizing input data and optimizing performance.\n```\nfilenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset\n\n```\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n```\n\nAt this point the dataset contains serialized [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) messages. When iterated over it returns these as scalar string tensors.\nUse the `.take` method to only show the first 10 records.\n```\nfor raw_record in raw_dataset.take(10):\n  print(repr(raw_record))\n\n```\n```\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x85-%\\xbf'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xc0,\\xec\\xbe\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xd6O\\xb8?\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nR\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\xf3\\xbf\\xa0\\xbf\\n\\x14\\n\\x08feature2\\x12\\x08\\n\\x06\\n\\x04goat\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x04\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nS\\n\\x15\\n\\x08feature2\\x12\\t\\n\\x07\\n\\x05horse\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x97\\x1aE\\xbe\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x03'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04~<\\xe0\\xbe\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nU\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x02\\n\\x17\\n\\x08feature2\\x12\\x0b\\n\\t\\n\\x07chicken\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x0cW\\x18>\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x8d\\xdd\\xcb\\xbf\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x95\\xdf=?\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x01\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03dog\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x01'>\n<tf.Tensor: shape=(), dtype=string, numpy=b'\\nQ\\n\\x14\\n\\x08feature3\\x12\\x08\\x12\\x06\\n\\x04\\x8dC\\xb8\\xbd\\n\\x11\\n\\x08feature0\\x12\\x05\\x1a\\x03\\n\\x01\\x00\\n\\x13\\n\\x08feature2\\x12\\x07\\n\\x05\\n\\x03cat\\n\\x11\\n\\x08feature1\\x12\\x05\\x1a\\x03\\n\\x01\\x00'>\n\n```\n\nThese tensors can be parsed using the function below. Note that the `feature_description` is necessary here because [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)s use graph-execution, and need this description to build their shape and type signature:\n```\n# Create a description of the features.\nfeature_description = {\n    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n}\n\ndef_parse_function(example_proto):\n  # Parse the input `tf.train.Example` proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, feature_description)\n\n```\n\nAlternatively, use `tf.parse_example` to parse the whole batch at once. Apply this function to each item in the dataset using the [`tf.data.Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method:\n```\nparsed_dataset = raw_dataset.map(_parse_function)\nparsed_dataset\n\n```\n```\n<_MapDataset element_spec={'feature0': TensorSpec(shape=(), dtype=tf.int64, name=None), 'feature1': TensorSpec(shape=(), dtype=tf.int64, name=None), 'feature2': TensorSpec(shape=(), dtype=tf.string, name=None), 'feature3': TensorSpec(shape=(), dtype=tf.float32, name=None)}>\n\n```\n\nUse eager execution to display the observations in the dataset. There are 10,000 observations in this dataset, but you will only display the first 10. The data is displayed as a dictionary of features. Each item is a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), and the `numpy` element of this tensor displays the value of the feature:\n```\nfor parsed_record in parsed_dataset.take(10):\n  print(repr(parsed_record))\n\n```\n```\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6452258>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.46127892>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=1.4399364>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=4>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'goat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.2558578>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=3>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'horse'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.19248424>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.43796152>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=2>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'chicken'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.14876956>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-1.5926987>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=1>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'dog'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=0.74169284>}\n{'feature0': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature1': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'feature2': <tf.Tensor: shape=(), dtype=string, numpy=b'cat'>, 'feature3': <tf.Tensor: shape=(), dtype=float32, numpy=-0.08997259>}\n\n```\n\nHere, the `tf.parse_example` function unpacks the [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) fields into standard tensors.\n## Walkthrough: Reading and writing image data\nThis is an end-to-end example of how to read and write image data using TFRecords. Using an image as input data, you will write the data as a TFRecord file, then read the file back and display the image.\nThis can be useful if, for example, you want to use several models on the same input dataset. Instead of storing the image data raw, it can be preprocessed into the TFRecords format, and that can be used in all further processing and modelling.\nFirst, let's download \n### Fetch the images\n```\ncat_in_snow  = tf.keras.utils.get_file(\n    '320px-Felis_catus-cat_on_snow.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n\nwilliamsburg_bridge = tf.keras.utils.get_file(\n    '194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')\n\n```\n```\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\n17858/17858 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg\n15477/15477 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n```\n```\ndisplay.display(display.Image(filename=cat_in_snow))\ndisplay.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))\n\n```\n\n```\ndisplay.display(display.Image(filename=williamsburg_bridge))\ndisplay.display(display.HTML('<a \"href=https://commons.wikimedia.org/wiki/File:New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg\">From Wikimedia</a>'))\n\n```\n\n### Write the TFRecord file\nAs before, encode the features as types compatible with [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example). This stores the raw image string feature, as well as the height, width, depth, and arbitrary `label` feature. The latter is used when you write the file to distinguish between the cat image and the bridge image. Use `0` for the cat image, and `1` for the bridge image:\n```\nimage_labels = {\n    cat_in_snow : 0,\n    williamsburg_bridge : 1,\n}\n\n```\n```\n# This is an example, just using the cat image.\nimage_string = open(cat_in_snow, 'rb').read()\n\nlabel = image_labels[cat_in_snow]\n\n# Create a dictionary with features that may be relevant.\ndefimage_example(image_string, label):\n  image_shape = tf.io.decode_jpeg(image_string).shape\n\n  feature = {\n      'height': _int64_feature(image_shape[0]),\n      'width': _int64_feature(image_shape[1]),\n      'depth': _int64_feature(image_shape[2]),\n      'label': _int64_feature(label),\n      'image_raw': _bytes_feature(image_string),\n  }\n\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\nfor line in str(image_example(image_string, label)).split('\\n')[:15]:\n  print(line)\nprint('...')\n\n```\n```\nfeatures {\n  feature {\n    key: \"depth\"\n    value {\n      int64_list {\n        value: 3\n      }\n    }\n  }\n  feature {\n    key: \"height\"\n    value {\n      int64_list {\n        value: 213\n      }\n...\n\n```\n\nNotice that all of the features are now stored in the [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) message. Next, functionalize the code above and write the example messages to a file named `images.tfrecords`:\n```\n# Write the raw image files to `images.tfrecords`.\n# First, process the two images into `tf.train.Example` messages.\n# Then, write to a `.tfrecords` file.\nrecord_file = 'images.tfrecords'\nwith tf.io.TFRecordWriter(record_file) as writer:\n  for filename, label in image_labels.items():\n    image_string = open(filename, 'rb').read()\n    tf_example = image_example(image_string, label)\n    writer.write(tf_example.SerializeToString())\n\n```\n```\ndu{record_file}\n```\n```\n36K images.tfrecords\n\n```\n\n### Read the TFRecord file\nYou now have the file—`images.tfrecords`—and can now iterate over the records in it to read back what you wrote. Given that in this example you will only reproduce the image, the only feature you will need is the raw image string. Extract it using the getters described above, namely `example.features.feature['image_raw'].bytes_list.value[0]`. You can also use the labels to determine which record is the cat and which one is the bridge:\n```\nraw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n\n# Create a dictionary describing the features.\nimage_feature_description = {\n    'height': tf.io.FixedLenFeature([], tf.int64),\n    'width': tf.io.FixedLenFeature([], tf.int64),\n    'depth': tf.io.FixedLenFeature([], tf.int64),\n    'label': tf.io.FixedLenFeature([], tf.int64),\n    'image_raw': tf.io.FixedLenFeature([], tf.string),\n}\n\ndef_parse_image_function(example_proto):\n  # Parse the input tf.train.Example proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, image_feature_description)\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\nparsed_image_dataset\n\n```\n```\n<_MapDataset element_spec={'depth': TensorSpec(shape=(), dtype=tf.int64, name=None), 'height': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image_raw': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None), 'width': TensorSpec(shape=(), dtype=tf.int64, name=None)}>\n\n```\n\nRecover the images from the TFRecord file:\n```\nfor image_features in parsed_image_dataset:\n  image_raw = image_features['image_raw'].numpy()\n  display.display(display.Image(data=image_raw))\n\n```\n\n",
  "https://blog.tensorflow.org/2022/08/content-moderation-using-machine-learning-a-dual-approach.html": "\n",
  "https://blog.tensorflow.org/2021/02/how-openx-trains-and-serves-for-million-queries-per-second.html": "\n",
  "https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html": "\n",
  "https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model": "[ TensorFlow 2 version](https://www.tensorflow.org/api_docs/python/tf/saved_model)  \n---  \nPublic API for tf.saved_model namespace.\n## Modules\n[`builder`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/builder) module: SavedModel builder.\n[`constants`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/constants) module: Constants for SavedModel save and restore operations.\n[`experimental`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/experimental) module: Public API for tf.saved_model.experimental namespace.\n[`loader`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/loader) module: Loader functionality for SavedModel with hermetic, language-neutral exports.\n[`main_op`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/main_op) module: SavedModel main op.\n[`signature_constants`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/signature_constants) module: Signature constants for SavedModel save and restore operations.\n[`signature_def_utils`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/signature_def_utils) module: SignatureDef utility functions.\n[`tag_constants`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/tag_constants) module: Common tags used for graphs in SavedModel.\n[`utils`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/utils) module: SavedModel utility functions.\n## Classes\n[`class Builder`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/Builder): Builds the `SavedModel` protocol buffer and saves variables and assets.\n## Functions\n[`build_signature_def(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/build_signature_def): Utility function to build a SignatureDef protocol buffer.\n[`build_tensor_info(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/build_tensor_info): Utility function to build TensorInfo proto from a Tensor. (deprecated)\n[`classification_signature_def(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/classification_signature_def): Creates classification signature from given examples and predictions.\n[`contains_saved_model(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/contains_saved_model): Checks whether the provided export directory could contain a SavedModel.\n[`get_tensor_from_tensor_info(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/get_tensor_from_tensor_info): Returns the Tensor or CompositeTensor described by a TensorInfo proto. (deprecated)\n[`is_valid_signature(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/is_valid_signature): Determine whether a SignatureDef can be served by TensorFlow Serving.\n[`load(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/load): Loads the model from a SavedModel as specified by tags. (deprecated)\n[`load_v2(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/load_v2): Load a SavedModel from `export_dir`.\n[`main_op_with_restore(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/main_op_with_restore): Returns a main op to init variables, tables and restore the graph. (deprecated)\n[`maybe_saved_model_directory(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/contains_saved_model): Checks whether the provided export directory could contain a SavedModel.\n[`predict_signature_def(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/predict_signature_def): Creates prediction signature from given inputs and outputs.\n[`regression_signature_def(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/regression_signature_def): Creates regression signature from given examples and predictions.\n[`save(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/save): Exports the Trackable object `obj` to \n[`simple_save(...)`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/simple_save): Convenience function to build a SavedModel suitable for serving. (deprecated)\n## Other Members\n  * `ASSETS_DIRECTORY = 'assets'`\n  * `ASSETS_KEY = 'saved_model_assets'`\n  * `CLASSIFY_INPUTS = 'inputs'`\n  * `CLASSIFY_METHOD_NAME = 'tensorflow/serving/classify'`\n  * `CLASSIFY_OUTPUT_CLASSES = 'classes'`\n  * `CLASSIFY_OUTPUT_SCORES = 'scores'`\n  * `DEFAULT_SERVING_SIGNATURE_DEF_KEY = 'serving_default'`\n  * `GPU = 'gpu'`\n  * `LEGACY_INIT_OP_KEY = 'legacy_init_op'`\n  * `MAIN_OP_KEY = 'saved_model_main_op'`\n  * `PREDICT_INPUTS = 'inputs'`\n  * `PREDICT_METHOD_NAME = 'tensorflow/serving/predict'`\n  * `PREDICT_OUTPUTS = 'outputs'`\n  * `REGRESS_INPUTS = 'inputs'`\n  * `REGRESS_METHOD_NAME = 'tensorflow/serving/regress'`\n  * `REGRESS_OUTPUTS = 'outputs'`\n  * `SAVED_MODEL_FILENAME_PB = 'saved_model.pb'`\n  * `SAVED_MODEL_FILENAME_PBTXT = 'saved_model.pbtxt'`\n  * `SAVED_MODEL_SCHEMA_VERSION = 1`\n  * `SERVING = 'serve'`\n  * `TPU = 'tpu'`\n  * `TRAINING = 'train'`\n  * `VARIABLES_DIRECTORY = 'variables'`\n  * `VARIABLES_FILENAME = 'variables'`\n\n\n",
  "https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef": "A ProtocolMessage\nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.MetaGraphDef`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef)\n## Attributes  \n---  \n`asset_file_def` |  `repeated AssetFileDef asset_file_def`  \n`collection_def` |  `repeated CollectionDefEntry collection_def`  \n`graph_def` |  `GraphDef graph_def`  \n`meta_info_def` |  `MetaInfoDef meta_info_def`  \n`object_graph_def` |  `SavedObjectGraph object_graph_def`  \n`saver_def` |  `SaverDef saver_def`  \n`signature_def` |  `repeated SignatureDefEntry signature_def`  \n## Child Classes\n[`class CollectionDefEntry`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef/CollectionDefEntry)\n[`class MetaInfoDef`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef/MetaInfoDef)\n[`class SignatureDefEntry`](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef/SignatureDefEntry)\n",
  "https://blog.tensorflow.org/2022/08/jax-on-web-with-tensorflowjs.html": "\n",
  "https://magenta.tensorflow.org/demos/performance_rnn/index.html": "\n",
  "https://discuss.tensorflow.org/tag/tfx": "\n",
  "https://blog.tensorflow.org/2022/09/content-moderation-using-machine-learning-the-server-side-part.html": "\n",
  "https://blog.tensorflow.org/2023/03/how-adobe-used-web-ml-with-tensorflowjs-to-enhance-photoshop-for-web.html": "\n",
  "https://blog.tensorflow.org/search?label=TFX&max-results=20": "\n",
  "https://tensorflow.org/hub": "```\n  !pip install --upgrade tensorflow_hub\n\n  importtensorflow_hubashub\n\n  model = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\n  embeddings = model([\"The rain in Spain.\", \"falls\",\n                      \"mainly\", \"In the plain!\"])\n\n  print(embeddings.shape)  #(4,128)\n```\n\nTensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like BERT and Faster R-CNN with just a few lines of code. \n  * ####  [ See the guide ](https://www.tensorflow.org/hub/overview)\nLearn about how to use TensorFlow Hub and how it works. \n  * ####  [ See tutorials ](https://www.tensorflow.org/hub/tutorials)\nTutorials show you end-to-end examples using TensorFlow Hub. \n  * Find trained TF, TFLite, and TF.js models for your use case. \n\n\nCheck out BERT for NLP tasks including text classification and question answering. \nUse the Faster R-CNN Inception ResNet V2 640x640 model for detecting objects in images. \nTransfer the style of one image to another using the image style transfer model. \nUse this TFLite model to classify photos of food on a mobile device. \nLearn how you can use TensorFlow Hub to build ML solutions with real world impact. \nTo explore ML solutions for your mobile and web apps including TensorFlow Hub, visit the Google on-device machine learning page. \n###  [ Making BERT Easier with Preprocessing Models From TensorFlow Hub ](https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html)\nTensorFlow Hub makes BERT simple to use with new preprocessing models. \n[Read the blogarrow_forward](https://blog.tensorflow.org/2020/12/making-bert-easier-with-preprocessing-models-from-tensorflow-hub.html)\n###  [ From singing to musical scores: Estimating pitch with SPICE and Tensorflow Hub ](https://blog.tensorflow.org/2020/06/estimating-pitch-with-spice-and-tensorflow-hub.html)\nLearn how to use the SPICE model to automatically transcribe sheet music from live audio. \n[Read the blogarrow_forward](https://blog.tensorflow.org/2020/06/estimating-pitch-with-spice-and-tensorflow-hub.html)\n[ publish  ](https://www.tensorflow.org/hub/publish)\n###  [ Contribute models ](https://www.tensorflow.org/hub/publish)\n",
  "https://www.tensorflow.org": "An end-to-end platform for machine learning \n[ Install TensorFlow ](https://www.tensorflow.org/install)\n##  Get started with TensorFlow \nTensorFlow makes it easy to create ML models that can run in any environment. Learn how to use the intuitive APIs through interactive code samples.\n```\nimporttensorflowastf\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam',\n  loss='sparse_categorical_crossentropy',\n  metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=5)\nmodel.evaluate(x_test, y_test)\n```\n\n##  Solve real-world problems with ML \nExplore examples of how TensorFlow is used to advance research and build AI-powered applications.\nTENSORFLOW.JS \nCatch up on the latest from the Web AI Summit \nExplore the latest advancements in running models client-side with speakers from Chrome, MediaPipe, Intel, Hugging Face, Microsoft, LangChain, and more.\n_close_\nTensorFlow GNN \n[ Analyze relational data using graph neural networks ](https://blog.tensorflow.org/2024/02/graph-neural-networks-in-tensorflow.html)\nGNNs can process complex relationships between objects, making them a powerful technique for traffic forecasting, medical discovery, and more.\nTensorFlow Agents \n[ Build recommendation systems with reinforcement learning ](https://blog.tensorflow.org/2023/10/simulated-spotify-listening-experiences-reinforcement-learning-tensorflow-tf-agents.html)\nLearn how Spotify uses the TensorFlow ecosystem to design an extendable offline simulator and train RL Agents to generate playlists.\n##  What's new in TensorFlow \nRead the latest announcements from the TensorFlow team and community.\n### [What's new in TensorFlow 2.19](https://blog.tensorflow.org/2025/03/whats-new-in-tensorflow-2-19.html)\nUpdated March 13, 2025\n### [Introducing Wake Vision: A High-Quality, Large-Scale Dataset for TinyML Computer Vision Applications](https://blog.tensorflow.org/2024/12/introducing-wake-vision-new-dataset-for-person-detection-in-tinyml.html)\nUpdated December 5, 2024\n### [MLSysBook.AI: Principles and Practices of Machine Learning Systems Engineering](https://blog.tensorflow.org/2024/11/mlsysbookai-principles-and-practices-of-machine-learning-systems-engineering.html)\nUpdated November 19, 2024\n### [What's new in TensorFlow 2.18](https://blog.tensorflow.org/2024/10/whats-new-in-tensorflow-218.html)\nUpdated October 28, 2024\n### [What's new in TensorFlow 2.17](https://blog.tensorflow.org/2024/07/whats-new-in-tensorflow-217.html)\nUpdated July 18, 2024\n### [Faster Dynamically Quantized Inference with XNNPack](https://blog.tensorflow.org/2024/04/faster-dynamically-quantized-inference-with-xnnpack.html)\nUpdated April 9, 2024\n##  Explore the ecosystem \nDiscover production-tested tools to accelerate modeling, deployment, and other workflows.\n  * [ Library  TensorFlow.js  Train and run models directly in the browser using JavaScript or Node.js.  ](https://www.tensorflow.org/js)\n  * [ API  tf.data  Preprocess data and create input pipelines for ML models.  ](https://www.tensorflow.org/guide/data)\n  * [ Library  TFX  Create production ML pipelines and implement MLOps best practices.  ](https://www.tensorflow.org/tfx)\n  * [ API  tf.keras  Create ML models with TensorFlow's high-level API.  ](https://www.tensorflow.org/guide/keras)\n  * [ Resource  TensorFlow Datasets  Browse the collection of standard datasets for initial training and validation.  ](https://www.tensorflow.org/datasets)\n  * [ Tool  TensorBoard  Visualize and track development of ML models.  ](https://www.tensorflow.org/tensorboard)\n\n\n### [ ML models & datasets  Pretrained models and ready-to-use datasets for image, text, audio, and video use cases.  ](https://www.tensorflow.org/resources/models-datasets)\n### [ Libraries & extensions  Packages for domain-specific applications and APIs for languages other than Python.  ](https://www.tensorflow.org/resources/libraries-extensions)\n### [ Developer tools  Tools to evaluate models, optimize performance, and productionize ML workflows.  ](https://www.tensorflow.org/resources/tools)\nJoin the community \nCollaborate, find support, and share your projects by joining interest groups or attending developer events.\nLearn ML \nNew to machine learning? Begin with TensorFlow's curated curriculums or browse the resource library of books, online courses, and videos.\n##  Stay connected \nLearn the latest in machine learning and TensorFlow by following our channels or signing up for the newsletter. View past newsletters in the [archive](https://www.tensorflow.org/subscribe/latest). \n[ Forum ](https://discuss.tensorflow.org/)\n[ Forum ](https://discuss.tensorflow.org/)\n##  Start building with TensorFlow \n[ Install TensorFlow ](https://www.tensorflow.org/install) [ Explore tutorials ](https://www.tensorflow.org/tutorials)\n",
  "https://www.tensorflow.org/about/case-studies?filter=TFX": "#  Learn how TensorFlow solves real, everyday machine learning problems \nExplore how various companies from a wide variety of industries implement ML to solve their biggest problems. From [healthcare](https://blog.tensorflow.org/2019/03/intelligent-scanning-using-deep-learning.html) to [social networks](https://blog.tensorflow.org/2019/03/ranking-tweets-with-tensorflow.html) and even [ecommerce](https://blog.tensorflow.org/2019/09/using-tensorflow-to-predict-product.html), ML can be integrated into your industry and company.\nCase studies \nAll TensorFlow  TensorFlow Lite  TensorFlow.js  TFX \nTFX \n[ Airbus uses TensorFlow to extract information from their satellite images and deliver valuable insights to clients ](https://blog.tensorflow.org/2020/04/how-airbus-detects-anomalies-iss-telemetry-data-tfx.html)\nML helps with monitoring changes to the Earth's surface for urban planning, fighting illegal construction and mapping damage and landscape changes caused by natural catastrophes.\nTFX \nKakao Mobility uses TensorFlow and TensorFlow Serving to predict the probability of trip completed rates when drivers are dispatched to fulfill ride-hailing requests.\nTFX \n[ OpenX prioritizes traffic for high volume requests using TFX ](https://blog.tensorflow.org/2021/02/how-openx-trains-and-serves-for-million-queries-per-second.html)\nOpenX integrates TFX and Google Cloud Platform in their ad exchange to process more than one million requests every second and serve responses in under 15 milliseconds.\nTFX \nSpotify leverages TFX and Kubeflow pipelines in its Paved Road for ML systems, an opinionated set of products and configurations to deploy an end-to-end machine learning solution targeted at teams starting out on their ML journeys.\nTFX \n[ Ranking tweets with TensorFlow ](https://blog.tensorflow.org/2019/03/ranking-tweets-with-tensorflow.html)\nTwitter used TensorFlow to build their \"Ranked Timeline,\" allowing users to ensure that they don't miss their most important tweets even if they follow thousands of users.\n##  Build, deploy, and experiment easily with TensorFlow \n",
  "https://www.tensorflow.org/api_docs/python/tf/GradientTape": "Record operations for automatic differentiation.\n#### View aliases\n**Main aliases**\n[`tf.autodiff.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n```\ntf.GradientTape(\n    persistent=False, watch_accessed_variables=True\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff)\n  * [Advanced automatic differentiation](https://www.tensorflow.org/guide/advanced_autodiff)\n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n  * [TensorFlow basics](https://www.tensorflow.org/guide/basics)\n  * [Effective Tensorflow 2](https://www.tensorflow.org/guide/effective_tf2)\n\n| \n  * [Deep Convolutional Generative Adversarial Network](https://www.tensorflow.org/tutorials/generative/dcgan)\n  * [pix2pix: Image-to-image translation with a conditional GAN](https://www.tensorflow.org/tutorials/generative/pix2pix)\n  * [Neural style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)\n  * [Custom training: walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough)\n\n  \nOperations are recorded if they are executed within this context manager and at least one of their inputs is being \"watched\".\nTrainable variables (created by [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.compat.v1.get_variable`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_variable), where `trainable=True` is default in both cases) are automatically watched. Tensors can be manually watched by invoking the `watch` method on this context manager.\nFor example, consider the function `y = x * x`. The gradient at `x = 3.0` can be computed as:\n```\nx = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n```\n\nGradientTapes can be nested to compute higher-order derivatives. For example,\n```\nx = tf.constant(5.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  with tf.GradientTape() as gg:\n    gg.watch(x)\n    y = x * x\n  dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\nd2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\nprint(dy_dx)\ntf.Tensor(10.0, shape=(), dtype=float32)\nprint(d2y_dx2)\ntf.Tensor(2.0, shape=(), dtype=float32)\n```\n\nBy default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. For example:\n```\nx = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as g:\n  g.watch(x)\n  y = x * x\n  z = y * y\ndz_dx = g.gradient(z, x)  # (4*x^3 at x = 3)\nprint(dz_dx)\ntf.Tensor(108.0, shape=(), dtype=float32)\ndy_dx = g.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(6.0, shape=(), dtype=float32)\n```\n\nBy default GradientTape will automatically watch any trainable variables that are accessed inside the context. If you want fine grained control over which variables are watched you can disable automatic tracking by passing `watch_accessed_variables=False` to the tape constructor:\n```\nx = tf.Variable(2.0)\nw = tf.Variable(5.0)\nwith tf.GradientTape(\n    watch_accessed_variables=False, persistent=True) as tape:\n  tape.watch(x)\n  y = x ** 2  # Gradients will be available for `x`.\n  z = w ** 3  # No gradients will be available as `w` isn't being watched.\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\ntf.Tensor(4.0, shape=(), dtype=float32)\n# No gradients will be available as `w` isn't being watched.\ndz_dw = tape.gradient(z, w)\nprint(dz_dw)\nNone\n```\n\nNote that when using models you should ensure that your variables exist when using `watch_accessed_variables=False`. Otherwise it's quite easy to make your first iteration not have any gradients:\n```\na = tf.keras.layers.Dense(32)\nb = tf.keras.layers.Dense(32)\n\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n  tape.watch(a.variables)  # Since `a.build` has not been called at this point\n                           # `a.variables` will return an empty list and the\n                           # tape will not be watching anything.\n  result = b(a(inputs))\n  tape.gradient(result, a.variables)  # The result of this computation will be\n                                      # a list of `None`s since a's variables\n                                      # are not being watched.\n\n```\n\nNote that only tensors with real or complex dtypes are differentiable.\n## Args  \n---  \n`persistent` |  Boolean controlling whether a persistent gradient tape is created. False by default, which means at most one call can be made to the gradient() method on this object.   \n`watch_accessed_variables` |  Boolean controlling whether the tape will automatically `watch` any (trainable) variables accessed while the tape is active. Defaults to True meaning gradients can be requested from any result computed in the tape derived from reading a trainable `Variable`. If False users must explicitly `watch` any `Variable`s they want to request gradients from.   \n## Methods\n### `batch_jacobian`\n```\nbatch_jacobian(\n    target,\n    source,\n    unconnected_gradients=[tf.UnconnectedGradients.NONE](https://www.tensorflow.org/api_docs/python/tf/UnconnectedGradients#NONE),\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n\n```\n\nComputes and stacks per-example jacobians.\nSee \n`tf.stack([self.jacobian(y[i], x[i]) for i in range(x.shape[0])])`.\nNote that compared to [`GradientTape.jacobian`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian) which computes gradient of each output value w.r.t each input value, this function is useful when `target[i,...]` is independent of `source[j,...]` for `j != i`. This assumption allows more efficient computation as compared to [`GradientTape.jacobian`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian). The output, as well as intermediate activations, are lower dimensional and avoid a bunch of redundant zeros which would result in the jacobian computation given the independence assumption.\n#### Example usage:\n```\nwith tf.GradientTape() as g:\n  x = tf.constant([[1., 2.], [3., 4.]], dtype=tf.float32)\n  g.watch(x)\n  y = x * x\nbatch_jacobian = g.batch_jacobian(y, x)\n# batch_jacobian is [[[2,  0], [0,  4]], [[6,  0], [0,  8]]]\n\n```\nArgs  \n---  \n`target` |  A tensor with rank 2 or higher and with shape [b, y1, ..., y_n]. `target[i,...]` should only depend on `source[i,...]`.   \n`source` |  A tensor with rank 2 or higher and with shape [b, x1, ..., x_m].   \n`unconnected_gradients` |  a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.   \n`parallel_iterations` |  A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.   \n`experimental_use_pfor` |  If true, uses pfor for computing the Jacobian. Else uses a tf.while_loop.   \nReturns  \n---  \nA tensor `t` with shape [b, y_1, ..., y_n, x1, ..., x_m] where `t[i, ...]` is the jacobian of `target[i, ...]` w.r.t. `source[i, ...]`, i.e. stacked per-example jacobians.   \nRaises  \n---  \n`RuntimeError` |  If called on a used, non-persistent tape.   \n`RuntimeError` |  If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.   \n`ValueError` |  If vectorization of jacobian computation fails or if first dimension of `target` and `source` do not match.   \n### `gradient`\n```\ngradient(\n    target,\n    sources,\n    output_gradients=None,\n    unconnected_gradients=[tf.UnconnectedGradients.NONE](https://www.tensorflow.org/api_docs/python/tf/UnconnectedGradients#NONE)\n)\n\n```\n\nComputes the gradient using operations recorded in context of this tape.\nIn addition to Tensors, gradient also supports RaggedTensors. For example,\n```\nx = tf.ragged.constant([[1.0, 2.0], [3.0]])\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ng.gradient(y, x)\n<tf.RaggedTensor [[2.0, 4.0], [6.0]]>\n```\nArgs  \n---  \n`target` |  a list or nested structure of Tensors or Variables or CompositeTensors to be differentiated.   \n`sources` |  a list or nested structure of Tensors or Variables or CompositeTensors. `target` will be differentiated against elements in `sources`.   \n`output_gradients` |  a list of gradients, one for each differentiable element of target. Defaults to None.   \n`unconnected_gradients` |  a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.   \nReturns  \n---  \na list or nested structure of Tensors (or IndexedSlices, or None, or CompositeTensor), one for each element in `sources`. Returned structure is the same as the structure of `sources`.   \nRaises  \n---  \n`RuntimeError` |  If called on a used, non-persistent tape.   \n`RuntimeError` |  If called inside the context of the tape.   \n`TypeError` |  If the target is a None object.   \n`ValueError` |  If the target is a variable or if unconnected gradients is called with an unknown value.   \n### `jacobian`\n```\njacobian(\n    target,\n    sources,\n    unconnected_gradients=[tf.UnconnectedGradients.NONE](https://www.tensorflow.org/api_docs/python/tf/UnconnectedGradients#NONE),\n    parallel_iterations=None,\n    experimental_use_pfor=True\n)\n\n```\n\nComputes the jacobian using operations recorded in context of this tape.\nSee\n#### Example usage:\n```\nwith tf.GradientTape() as g:\n  x  = tf.constant([1.0, 2.0])\n  g.watch(x)\n  y = x * x\njacobian = g.jacobian(y, x)\n# jacobian value is [[2., 0.], [0., 4.]]\n\n```\nArgs  \n---  \n`target` |  Tensor to be differentiated.   \n`sources` |  a list or nested structure of Tensors or Variables. `target` will be differentiated against elements in `sources`.   \n`unconnected_gradients` |  a value which can either hold 'none' or 'zero' and alters the value which will be returned if the target and sources are unconnected. The possible values and effects are detailed in 'UnconnectedGradients' and it defaults to 'none'.   \n`parallel_iterations` |  A knob to control how many iterations are dispatched in parallel. This knob can be used to control the total memory usage.   \n`experimental_use_pfor` |  If true, vectorizes the jacobian computation. Else falls back to a sequential while_loop. Vectorization can sometimes fail or lead to excessive memory usage. This option can be used to disable vectorization in such cases.   \nReturns  \n---  \nA list or nested structure of Tensors (or None), one for each element in `sources`. Returned structure is the same as the structure of `sources`. Note if any gradient is sparse (IndexedSlices), jacobian function currently makes it dense and returns a Tensor instead. This may change in the future.   \nRaises  \n---  \n`RuntimeError` |  If called on a used, non-persistent tape.   \n`RuntimeError` |  If called on a non-persistent tape with eager execution enabled and without enabling experimental_use_pfor.   \n`ValueError` |  If vectorization of jacobian computation fails.   \n### `reset`\n```\nreset()\n\n```\n\nClears all information stored in this tape.\nEquivalent to exiting and reentering the tape context manager with a new tape. For example, the two following code blocks are equivalent:\n```\nwith tf.GradientTape() as t:\n  loss = loss_fn()\nwith tf.GradientTape() as t:\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n\n# The following is equivalent to the above\nwith tf.GradientTape() as t:\n  loss = loss_fn()\n  t.reset()\n  loss += other_loss_fn()\nt.gradient(loss, ...)  # Only differentiates other_loss_fn, not loss_fn\n\n```\n\nThis is useful if you don't want to exit the context manager for the tape, or can't because the desired reset point is inside a control flow construct:\n```\nwith tf.GradientTape() as t:\n  loss = ...\n  if loss > k:\n    t.reset()\n\n```\n\n### `stop_recording`\n```\n@tf_contextlib.contextmanager\nstop_recording()\n\n```\n\nTemporarily stops recording operations on this tape.\nOperations executed while this context manager is active will not be recorded on the tape. This is useful for reducing the memory used by tracing all computations.\n#### For example:\n```\nx = tf.constant(4.0)\nwith tf.GradientTape() as tape:\n  with tape.stop_recording():\n    y = x ** 2\ndy_dx = tape.gradient(y, x)\nprint(dy_dx)\nNone\n```\nYields  \n---  \nNone   \nRaises  \n---  \n`RuntimeError` |  if the tape is not currently recording.   \n### `watch`\n```\nwatch(\n    tensor\n)\n\n```\n\nEnsures that `tensor` is being traced by this tape.\nArgs  \n---  \n`tensor` |  a Tensor/Variable or list of Tensors/Variables.   \nRaises  \n---  \n`ValueError` |  if it encounters something that is not a tensor.   \n### `watched_variables`\n```\nwatched_variables()\n\n```\n\nReturns variables watched by this tape in order of construction.\n### `__enter__`\n```\n__enter__()\n\n```\n\nEnters a context inside which operations are recorded on this tape.\n### `__exit__`\n```\n__exit__(\n    typ, value, traceback\n)\n\n```\n\nExits the recording context, no further operations are traced.\n",
  "https://www.tensorflow.org/api_docs/python/tf/IndexedSlices": "A sparse representation of a set of tensor slices at given indices.\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices)\n```\ntf.IndexedSlices(\n    values, indices, dense_shape=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Client-efficient large-model federated learning via `federated_select` and sparse aggregation](https://www.tensorflow.org/federated/tutorials/sparse_federated_learning)\n\n  \nThis class is a simple wrapper for a pair of `Tensor` objects:\n  * `values`: A `Tensor` of any dtype with shape `[D0, D1, ..., Dn]`.\n  * `indices`: A 1-D integer `Tensor` with shape `[D0]`.\n\n\nAn `IndexedSlices` is typically used to represent a subset of a larger tensor `dense` of shape `[LARGE0, D1, .. , DN]` where `LARGE0 >> D0`. The values in `indices` are the indices in the first dimension of the slices that have been extracted from the larger tensor.\nThe dense tensor `dense` represented by an `IndexedSlices` `slices` has\n```\ndense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]\n\n```\n\nThe `IndexedSlices` class is used principally in the definition of gradients for operations that have sparse gradients (e.g. [`tf.gather`](https://www.tensorflow.org/api_docs/python/tf/gather)).\n```\nv = tf.Variable([[0.,1, 2], [2, 3, 4], [4, 5, 6], [6, 7, 8]])\nwith tf.GradientTape() as tape:\n  r = tf.gather(v, [1,3])\nindex_slices = tape.gradient(r,v)\nindex_slices\n<...IndexedSlices object ...>\nindex_slices.indices.numpy()\narray([1, 3], dtype=int32)\nindex_slices.values.numpy()\narray([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)\n```\n\nContrast this representation with [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), which uses multi-dimensional indices and scalar values.\n## Attributes  \n---  \n`dense_shape` |  A 1-D `Tensor` containing the shape of the corresponding dense tensor.   \n`device` |  The name of the device on which `values` will be produced, or `None`.   \n`dtype` |  The `DType` of elements in this tensor.   \n`graph` |  The `Graph` that contains the values, indices, and shape tensors.   \n`indices` |  A 1-D `Tensor` containing the indices of the slices.   \n`name` |  The name of this `IndexedSlices`.   \n`op` |  The `Operation` that produces `values` as an output.   \n`shape` |  Gets the [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) representing the shape of the dense tensor.   \n`values` |  A `Tensor` containing the values of the slices.   \n## Methods\n### `consumers`\n```\nconsumers()\n\n```\n\n### `__neg__`\n```\n__neg__()\n\n```\n\n",
  "https://tensorflow.org/lite": "LiteRT (short for Lite Runtime), formerly known as TensorFlow Lite, is Google's high-performance runtime for on-device AI. You can find ready-to-run LiteRT models for a wide range of ML/AI tasks, or convert and run TensorFlow, PyTorch, and JAX models to the TFLite format using the AI Edge conversion and optimization tools.\n## Key features\n  * **Optimized for on-device machine learning** : LiteRT addresses five key ODML constraints: latency (there's no round-trip to a server), privacy (no personal data leaves the device), connectivity (internet connectivity is not required), size (reduced model and binary size) and power consumption (efficient inference and a lack of network connections).\n  * **Multi-platform support** : Compatible with [Android](https://ai.google.dev/edge/litert/android) and [iOS](https://ai.google.dev/edge/litert/ios/quickstart) devices, [embedded Linux](https://ai.google.dev/edge/litert/microcontrollers/python), and [microcontrollers](https://ai.google.dev/edge/litert/microcontrollers/overview).\n  * **Multi-framework model options** : AI Edge provides tools to convert models from TensorFlow, PyTorch, and JAX models into the FlatBuffers format (`.tflite`), enabling you to use a wide range of state-of-the-art models on LiteRT. You also have access to model optimization tools that can handle quantization and metadata.\n  * **Diverse language support** : Includes SDKs for Java/Kotlin, Swift, Objective-C, C++, and Python.\n  * **High performance** : [Hardware acceleration](https://ai.google.dev/edge/litert/performance/delegates) through specialized delegates like GPU and iOS Core ML.\n\n\n## Development workflow\nThe LiteRT development workflow involves identifying an ML/AI problem, choosing a model that solves that problem, and implementing the model on-device. The following steps walk you through the workflow and provides links to further instructions.\n### 1. Identify the most suitable solution to the ML problem\nLiteRT offers users a high level of flexibility and customizability when it comes to solving machine learning problems, making it a good fit for users who require a specific model or a specialized implementation. Users looking for plug-and-play solutions may prefer [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks), which provides ready-made solutions for common machine learning tasks like object detection, text classification, and LLM inference.\nChoose one of the following AI Edge frameworks:\n  * **LiteRT** : Flexible and customizable runtime that can run a wide range of models. Choose a model for your use case, convert it to the LiteRT format (if necessary), and run it on-device. If you intend to use LiteRT, keep reading.\n  * **MediaPipe Tasks** : Plug-and-play solutions with default models that allow for customization. Choose the task that solves your AI/ML problem, and implement it on multiple platforms. If you intend to use MediaPipe Tasks, refer to the [MediaPipe Tasks](https://ai.google.dev/edge/mediapipe/solutions/tasks) documentation.\n\n\n### 2. Choose a model\nA LiteRT model is represented in an efficient portable format known as `.tflite` file extension.\nYou can use a LiteRT model in the following ways:\n  * **Use an existing LiteRT model:** The simplest approach is to use a LiteRT model already in the `.tflite` format. These models do not require any added conversion steps. You can find LiteRT models on \n  * **Convert a model into a LiteRT model:** You can use the [TensorFlow Converter](https://ai.google.dev/edge/litert/models/convert_tf), [PyTorch Converter](https://ai.google.dev/edge/litert/models/convert_pytorch), or [JAX converter](https://ai.google.dev/edge/litert/models/convert_jax) to convert models to the FlatBuffers format (`.tflite`) and run them in LiteRT. To get started, you can find models on the following sites:\n    * **TensorFlow models** on \n    * **PyTorch models** on \n    * **JAX models** on \n\n\nA LiteRT model can optionally include _metadata_ that contains human-readable model descriptions and machine-readable data for automatic generation of pre- and post-processing pipelines during on-device inference. Refer to [Add metadata](https://ai.google.dev/edge/litert/models/metadata) for more details.\n### 3. Integrate the model into your app\nYou can implement your LiteRT models to run inferences completely on-device on web, embedded, and mobile devices. LiteRT contains APIs for [Python](https://ai.google.dev/edge/api/tflite/python/tf/lite), [Java and Kotlin](https://ai.google.dev/edge/api/tflite/java/org/tensorflow/lite/package-summary) for Android, [Swift](https://ai.google.dev/edge/api/tflite/swift/Classes) for iOS, and [C++](https://ai.google.dev/edge/api/tflite/cc) for micro-devices.\nUse the following guides to implement a LiteRT model on your preferred platform:\n  * [Run on Android](https://ai.google.dev/edge/litert/android/index): Run models on Android devices using the Java/Kotlin APIs.\n  * [Run on iOS](https://ai.google.dev/edge/litert/ios/quickstart): Run models on iOS devices using the Swift APIs.\n  * [Run on Micro](https://ai.google.dev/edge/litert/microcontrollers/overview): Run models on embedded devices using the C++ APIs.\n\n\nOn Android and iOS devices, you can improve performance using hardware acceleration. On either platform you can use a [GPU Delegate](https://ai.google.dev/edge/litert/performance/gpu), and on iOS you can use the [Core ML Delegate](https://ai.google.dev/edge/litert/ios/coreml). To add support for new hardware accelerators, you can [define your own delegate](https://ai.google.dev/edge/litert/performance/implementing_delegate).\nYou can run inference in the following ways based on the model type:\n  * **Models without metadata** : Use the [LiteRT Interpreter](https://ai.google.dev/edge/litert/inference) API. Supported on multiple platforms and languages such as Java, Swift, C++, Objective-C and Python.\n  * **Models with metadata** : You can build custom inference pipelines with the [LiteRT Support Library](https://ai.google.dev/edge/litert/android/metadata/lite_support).\n\n\n## Migrate from TF Lite\nApplications that use TF Lite libraries will continue to function, but all new active development and updates will only be included in LiteRT packages. The LiteRT APIs contain the same method names as the TF Lite APIs, so migrating to LiteRT does not require detailed code changes.\nFor more information, refer to the [migration guide](https://ai.google.dev/edge/litert/migration).\n## Next steps\nNew users should get started with the [LiteRT quickstart](https://ai.google.dev/edge/litert/inference). For specific information, see the following sections:\n**Model conversion**\n  * [Convert TensorFlow models](https://ai.google.dev/edge/litert/models/convert_tf)\n  * [Convert PyTorch models](https://ai.google.dev/edge/litert/models/convert_pytorch)\n  * [Convert PyTorch Generative AI models](https://ai.google.dev/edge/litert/models/edge_generative)\n\n\n**Platform guides**\n\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/Graph": "A TensorFlow computation, represented as a dataflow graph.\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)\n```\ntf.Graph() -> None\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrating model checkpoints](https://www.tensorflow.org/guide/migrate/migrating_checkpoints)\n  * [Validating correctness & numerical equivalence](https://www.tensorflow.org/guide/migrate/validate_correctness)\n  * [Migrate the SavedModel workflow](https://www.tensorflow.org/guide/migrate/saved_model)\n  * [Debug a TensorFlow 2 migrated training pipeline](https://www.tensorflow.org/guide/migrate/migration_debugging)\n  * [Migrating your TFLite code to TF2](https://www.tensorflow.org/guide/migrate/tflite)\n\n| \n  * [Generating Images with Little Data Using S3GAN](https://www.tensorflow.org/hub/tutorials/s3gan_generation_with_tf_hub)\n  * [Exploring the TF-Hub CORD-19 Swivel Embeddings](https://www.tensorflow.org/hub/tutorials/cord_19_embeddings)\n  * [Wiki40B Language Models](https://www.tensorflow.org/hub/tutorials/wiki40b_lm)\n  * [Migrating tf.summary usage to TF 2.x](https://www.tensorflow.org/tensorboard/migrate)\n\n  \nGraphs are used by [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s to represent the function's computations. Each graph contains a set of [`tf.Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation) objects, which represent units of computation; and [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects, which represent the units of data that flow between operations.\n### Using graphs directly (deprecated)\nA [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) can be constructed and used directly without a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), as was required in TensorFlow 1, but this is deprecated and it is recommended to use a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) instead. If a graph is directly used, other deprecated TensorFlow 1 classes are also required to execute the graph, such as a [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session).\nA default graph can be registered with the [`tf.Graph.as_default`](https://www.tensorflow.org/api_docs/python/tf/Graph#as_default) context manager. Then, operations will be added to the graph instead of being executed eagerly. For example:\n```\ng = tf.Graph()\nwith g.as_default():\n  # Define operations and tensors in `g`.\n  c = tf.constant(30.0)\n  assert c.graph is g\n\n```\n\n[`tf.compat.v1.get_default_graph()`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/get_default_graph) can be used to obtain the default graph.\nImportant note: This class _is not_ thread-safe for graph construction. All operations should be created from a single thread, or external synchronization must be provided. Unless otherwise specified, all methods are not thread-safe.\nA `Graph` instance supports an arbitrary number of \"collections\" that are identified by name. For convenience when building a large graph, collections can store groups of related objects: for example, the [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) uses a collection (named `tf.GraphKeys.GLOBAL_VARIABLES`) for all variables that are created during the construction of a graph. The caller may define additional collections by specifying a new name.\n## Attributes  \n---  \n`building_function` |  Returns True iff this graph represents a function.   \n`collections` |  Returns the names of the collections known to this graph.   \n`finalized` |  True if this graph has been finalized.   \n`graph_def_versions` |  The GraphDef version information of this graph.For details on the meaning of each version, see [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).   \n`operations`  \n`seed` |  The graph-level random seed of this graph.   \n`version`  \n## Methods\n### `Dismantle`\n```\nDismantle()\n\n```\n\n(self: handle) -> None\n### `add_to_collection`\n```\nadd_to_collection(\n    name, value\n) -> None\n\n```\n\nStores `value` in the collection with the given `name`.\nNote that collections are not sets, so it is possible to add a value to a collection several times.\nArgs  \n---  \n`name` |  The key for the collection. The `GraphKeys` class contains many standard names for collections.   \n`value` |  The value to add to the collection.   \n### `add_to_collections`\n```\nadd_to_collections(\n    names, value\n) -> None\n\n```\n\nStores `value` in the collections given by `names`.\nNote that collections are not sets, so it is possible to add a value to a collection several times. This function makes sure that duplicates in `names` are ignored, but it will not check for pre-existing membership of `value` in any of the collections in `names`.\n`names` can be any iterable, but if `names` is a string, it is treated as a single collection name.\nArgs  \n---  \n`names` |  The keys for the collections to add to. The `GraphKeys` class contains many standard names for collections.   \n`value` |  The value to add to the collections.   \n### `as_default`\n```\nas_default() -> ContextManager['Graph']\n\n```\n\nReturns a context manager that makes this `Graph` the default graph.\nThis method should be used if you want to create multiple graphs in the same process. For convenience, a global default graph is provided, and all ops will be added to this graph if you do not create a new graph explicitly.\nUse this method with the `with` keyword to specify that ops created within the scope of a block should be added to this graph. In this case, once the scope of the `with` is exited, the previous default graph is set again as default. There is a stack, so it's ok to have multiple nested levels of `as_default` calls.\nThe default graph is a property of the current thread. If you create a new thread, and wish to use the default graph in that thread, you must explicitly add a `with g.as_default():` in that thread's function.\nThe following code examples are equivalent:\n```\n# 1. Using Graph.as_default():\ng = tf.Graph()\nwith g.as_default():\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n# 2. Constructing and making default:\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  assert c.graph is g\n\n```\n\nIf eager execution is enabled ops created under this context manager will be added to the graph instead of executed eagerly.\nReturns  \n---  \nA context manager for using this graph as the default graph.   \n### `as_graph_def`\n```\nas_graph_def(\n    from_version=None, add_shapes=False, use_pybind11_proto=False\n) -> [tf.compat.v1.GraphDef](https://www.tensorflow.org/api_docs/python/tf/compat/v1/GraphDef)\n\n```\n\nReturns a serialized `GraphDef` representation of this graph.\nThe serialized `GraphDef` can be imported into another `Graph` (using [`tf.import_graph_def`](https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def)) or used with the [C++ Session API](https://www.tensorflow.org/api_docs/api_docs/cc/index).\nThis method is thread-safe.\nArgs  \n---  \n`from_version` |  Optional. If this is set, returns a `GraphDef` containing only the nodes that were added to this graph since its `version` property had the given value.   \n`add_shapes` |  If true, adds an \"_output_shapes\" list attr to each node with the inferred shapes of each of its outputs.   \n`use_pybind11_proto` |  If true, If true, uses the c++ pybind11_proto api to get the GraphDef proto directly from c++, instead of through a TF buffer. See   \nReturns  \n---  \nA [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto) protocol buffer.   \nRaises  \n---  \n`ValueError` |  If the `graph_def` would be too large.   \n### `as_graph_element`\n```\nas_graph_element(\n    obj, allow_tensor=True, allow_operation=True\n) -> Union[tensor_lib.Tensor, 'Operation']\n\n```\n\nReturns the object referred to by `obj`, as an `Operation` or `Tensor`.\nThis function validates that `obj` represents an element of this graph, and gives an informative error message if it is not.\nThis function is the canonical way to get/validate an object of one of the allowed types from an external argument reference in the Session API.\nThis method may be called concurrently from multiple threads.\nArgs  \n---  \n`obj` |  A `Tensor`, an `Operation`, or the name of a tensor or operation. Can also be any object with an `_as_graph_element()` method that returns a value of one of these types. Note: `_as_graph_element` will be called inside the graph's lock and so may not modify the graph.   \n`allow_tensor` |  If true, `obj` may refer to a `Tensor`.   \n`allow_operation` |  If true, `obj` may refer to an `Operation`.   \nReturns  \n---  \nThe `Tensor` or `Operation` in the Graph corresponding to `obj`.   \nRaises  \n---  \n`TypeError` |  If `obj` is not a type we support attempting to convert to types.   \n`ValueError` |  If `obj` is of an appropriate type but invalid. For example, an invalid string.   \n`KeyError` |  If `obj` is not an object in the graph.   \n### `clear_collection`\n```\nclear_collection(\n    name\n) -> None\n\n```\n\nClears all values in a collection.\nArgs  \n---  \n`name` |  The key for the collection. The `GraphKeys` class contains many standard names for collections.   \n### `colocate_with`\n```\n@tf_contextlib.contextmanager\ncolocate_with(\n    op, ignore_existing=False\n) -> Iterator[None]\n\n```\n\nReturns a context manager that specifies an op to colocate with.\n#### For example:\n```\na = tf.Variable([1.0])\nwith g.colocate_with(a):\n  b = tf.constant(1.0)\n  c = tf.add(a, b)\n\n```\n\n`b` and `c` will always be colocated with `a`, no matter where `a` is eventually placed.\nIf `op` is `None` then `ignore_existing` must be `True` and the new scope resets all colocation and device constraints.\nArgs  \n---  \n`op` |  The op to colocate all created ops with, or `None`.   \n`ignore_existing` |  If true, only applies colocation of this op within the context, rather than applying all colocation properties on the stack. If `op` is `None`, this value must be `True`.   \nRaises  \n---  \n`ValueError` |  if op is None but ignore_existing is False.   \nYields  \n---  \nA context manager that specifies the op with which to colocate newly created ops.   \n### `container`\n```\n@tf_contextlib.contextmanager\ncontainer(\n    container_name\n) -> Iterator[str]\n\n```\n\nReturns a context manager that specifies the resource container to use.\nStateful operations, such as variables and queues, can maintain their states on devices so that they can be shared by multiple processes. A resource container is a string name under which these stateful operations are tracked. These resources can be released or cleared with `tf.Session.reset()`.\n#### For example:\n```\nwith g.container('experiment0'):\n  # All stateful Operations constructed in this context will be placed\n  # in resource container \"experiment0\".\n  v1 = tf.Variable([1.0])\n  v2 = tf.Variable([2.0])\n  with g.container(\"experiment1\"):\n    # All stateful Operations constructed in this context will be\n    # placed in resource container \"experiment1\".\n    v3 = tf.Variable([3.0])\n    q1 = tf.queue.FIFOQueue(10, tf.float32)\n  # All stateful Operations constructed in this context will be\n  # be created in the \"experiment0\".\n  v4 = tf.Variable([4.0])\n  q1 = tf.queue.FIFOQueue(20, tf.float32)\n  with g.container(\"\"):\n    # All stateful Operations constructed in this context will be\n    # be placed in the default resource container.\n    v5 = tf.Variable([5.0])\n    q3 = tf.queue.FIFOQueue(30, tf.float32)\n\n# Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n# will become undefined (such as uninitialized).\ntf.Session.reset(target, [\"experiment0\"])\n\n```\nArgs  \n---  \n`container_name` |  container name string.   \nReturns  \n---  \nA context manager for defining resource containers for stateful ops, yields the container name.   \n### `control_dependencies`\n```\ncontrol_dependencies(\n    control_inputs\n) -> _ControlDependenciesController\n\n```\n\nReturns a context manager that specifies control dependencies.\nUse with the `with` keyword to specify that all operations constructed within the context should have control dependencies on `control_inputs`. For example:\n```\nwith g.control_dependencies([a, b, c]):\n  # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n  d = ...\n  e = ...\n\n```\n\nMultiple calls to `control_dependencies()` can be nested, and in that case a new `Operation` will have control dependencies on the union of `control_inputs` from all active contexts.\n```\nwith g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies([c, d]):\n    # Ops constructed here run after `a`, `b`, `c`, and `d`.\n\n```\n\nYou can pass None to clear the control dependencies:\n```\nwith g.control_dependencies([a, b]):\n  # Ops constructed here run after `a` and `b`.\n  with g.control_dependencies(None):\n    # Ops constructed here run normally, not waiting for either `a` or `b`.\n    with g.control_dependencies([c, d]):\n      # Ops constructed here run after `c` and `d`, also not waiting\n      # for either `a` or `b`.\n\n```\n```\n# WRONG\ndefmy_func(pred, tensor):\n  t = tf.matmul(tensor, tensor)\n  with tf.control_dependencies([pred]):\n    # The matmul op is created outside the context, so no control\n    # dependency will be added.\n    return t\n\n# RIGHT\ndefmy_func(pred, tensor):\n  with tf.control_dependencies([pred]):\n    # The matmul op is created in the context, so a control dependency\n    # will be added.\n    return tf.matmul(tensor, tensor)\n\n```\n\nAlso note that though execution of ops created under this scope will trigger execution of the dependencies, the ops created under this scope might still be pruned from a normal tensorflow graph. For example, in the following snippet of code the dependencies are never executed:\n```\n  loss = model.loss()\n  with tf.control_dependencies(dependencies):\n    loss = loss + tf.constant(1)  # note: dependencies ignored in the\n                                  # backward pass\n  return tf.gradients(loss, model.variables)\n\n```\n\nThis is because evaluating the gradient graph does not require evaluating the constant(1) op created in the forward pass.\nArgs  \n---  \n`control_inputs` |  A list of `Operation` or `Tensor` objects which must be executed or computed before running the operations defined in the context. Can also be `None` to clear the control dependencies.   \nReturns  \n---  \nA context manager that specifies control dependencies for all operations constructed within the context.   \nRaises  \n---  \n`TypeError` |  If `control_inputs` is not a list of `Operation` or `Tensor` objects.   \n### `create_op`\n```\ncreate_op(\n    op_type,\n    inputs,\n    dtypes=None,\n    input_types=None,\n    name=None,\n    attrs=None,\n    op_def=None,\n    compute_shapes=True,\n    compute_device=True\n) -> 'Operation'\n\n```\n\nCreates an `Operation` in this graph. (deprecated arguments)\nThis is a low-level interface for creating an `Operation`. Most programs will not call this method directly, and instead use the Python op constructors, such as [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant), which add ops to the default graph.\nArgs  \n---  \n`op_type` |  The `Operation` type to create. This corresponds to the `OpDef.name` field for the proto that defines the operation.   \n`inputs` |  A list of `Tensor` objects that will be inputs to the `Operation`.   \n`dtypes` |  (Optional) A list of `DType` objects that will be the types of the tensors that the operation produces.   \n`input_types` |  (Optional.) A list of `DType`s that will be the types of the tensors that the operation consumes. By default, uses the base `DType` of each input in `inputs`. Operations that expect reference-typed inputs must specify `input_types` explicitly.   \n`name` |  (Optional.) A string name for the operation. If not specified, a name is generated based on `op_type`.   \n`attrs` |  (Optional.) A dictionary where the key is the attribute name (a string) and the value is the respective `attr` attribute of the `NodeDef` proto that will represent the operation (an `AttrValue` proto).   \n`op_def` |  (Optional.) The `OpDef` proto that describes the `op_type` that the operation will have.   \n`compute_shapes` |  (Optional.) Deprecated. Has no effect (shapes are always computed).   \n`compute_device` |  (Optional.) If True, device functions will be executed to compute the device property of the Operation.   \nRaises  \n---  \n`TypeError` |  if any of the inputs is not a `Tensor`.   \n`ValueError` |  if colocation conflicts with existing device assignment.   \nReturns  \n---  \nAn `Operation` object.   \n### `device`\n```\n@tf_contextlib.contextmanager\ndevice(\n    device_name_or_function\n) -> Iterator[None]\n\n```\n\nReturns a context manager that specifies the default device to use.\nThe `device_name_or_function` argument may either be a device name string, a device function, or None:\n  * If it is a device name string, all operations constructed in this context will be assigned to the device with that name, unless overridden by a nested `device()` context.\n  * If it is a function, it will be treated as a function from Operation objects to device name strings, and invoked each time a new Operation is created. The Operation will be assigned to the device with the returned name.\n  * If it is None, all `device()` invocations from the enclosing context will be ignored.\n\n\nFor information about the valid syntax of device name strings, see the documentation in [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n#### For example:\n```\nwith g.device('/device:GPU:0'):\n  # All operations constructed in this context will be placed\n  # on GPU 0.\n  with g.device(None):\n    # All operations constructed in this context will have no\n    # assigned device.\n\n# Defines a function from `Operation` to device string.\ndefmatmul_on_gpu(n):\n  if n.type == \"MatMul\":\n    return \"/device:GPU:0\"\n  else:\n    return \"/cpu:0\"\n\nwith g.device(matmul_on_gpu):\n  # All operations of type \"MatMul\" constructed in this context\n  # will be placed on GPU 0; all other operations will be placed\n  # on CPU 0.\n\n```\nArgs  \n---  \n`device_name_or_function` |  The device name or function to use in the context.   \nYields  \n---  \nA context manager that specifies the default device to use for newly created ops.   \nRaises  \n---  \n`RuntimeError` |  If device scopes are not properly nested.   \n### `finalize`\n```\nfinalize() -> None\n\n```\n\nFinalizes this graph, making it read-only.\nAfter calling `g.finalize()`, no new operations can be added to `g`. This method is used to ensure that no operations are added to a graph when it is shared between multiple threads, for example when using a [`tf.compat.v1.train.QueueRunner`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/QueueRunner).\n### `get`\n```\nget() -> GraphType\n\n```\n\n### `get_all_collection_keys`\n```\nget_all_collection_keys() -> list[str]\n\n```\n\nReturns a list of collections used in this graph.\n### `get_collection`\n```\nget_collection(\n    name, scope=None\n) -> list[Any]\n\n```\n\nReturns a list of values in the collection with the given `name`.\nThis is different from `get_collection_ref()` which always returns the actual collection list if it exists in that it returns a new list each time it is called.\nArgs  \n---  \n`name` |  The key for the collection. For example, the `GraphKeys` class contains many standard names for collections.   \n`scope` |  (Optional.) A string. If supplied, the resulting list is filtered to include only items whose `name` attribute matches `scope` using `re.match`. Items without a `name` attribute are never returned if a scope is supplied. The choice of `re.match` means that a `scope` without special tokens filters by prefix.   \nReturns  \n---  \nThe list of values in the collection with the given `name`, or an empty list if no value has been added to that collection. The list contains the values in the order under which they were collected.   \n### `get_collection_ref`\n```\nget_collection_ref(\n    name\n) -> list[Any]\n\n```\n\nReturns a list of values in the collection with the given `name`.\nIf the collection exists, this returns the list itself, which can be modified in place to change the collection. If the collection does not exist, it is created as an empty list and the list is returned.\nThis is different from `get_collection()` which always returns a copy of the collection list if it exists and never creates an empty collection.\nArgs  \n---  \n`name` |  The key for the collection. For example, the `GraphKeys` class contains many standard names for collections.   \nReturns  \n---  \nThe list of values in the collection with the given `name`, or an empty list if no value has been added to that collection.   \n### `get_name_scope`\n```\nget_name_scope() -> str\n\n```\n\nReturns the current name scope.\n#### For example:\n```\nwith tf.name_scope('scope1'):\n  with tf.name_scope('scope2'):\n    print(tf.compat.v1.get_default_graph().get_name_scope())\n\n```\n\nwould print the string `scope1/scope2`.\nReturns  \n---  \nA string representing the current name scope.   \n### `get_operation_by_name`\n```\nget_operation_by_name(\n    name\n) -> 'Operation'\n\n```\n\nReturns the `Operation` with the given `name`.\nThis method may be called concurrently from multiple threads.\nArgs  \n---  \n`name` |  The name of the `Operation` to return.   \nReturns  \n---  \nThe `Operation` with the given `name`.   \nRaises  \n---  \n`TypeError` |  If `name` is not a string.   \n`KeyError` |  If `name` does not correspond to an operation in this graph.   \n### `get_operations`\n```\nget_operations()\n\n```\n\n(self: handle) -> list\n### `get_tensor_by_name`\n```\nget_tensor_by_name(\n    name\n) -> [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor)\n\n```\n\nReturns the `Tensor` with the given `name`.\nThis method may be called concurrently from multiple threads.\nArgs  \n---  \n`name` |  The name of the `Tensor` to return.   \nReturns  \n---  \nThe `Tensor` with the given `name`.   \nRaises  \n---  \n`TypeError` |  If `name` is not a string.   \n`KeyError` |  If `name` does not correspond to a tensor in this graph.   \n### `gradient_override_map`\n```\n@tf_contextlib.contextmanager\ngradient_override_map(\n    op_type_map\n) -> Iterator[None]\n\n```\n\nEXPERIMENTAL: A context manager for overriding gradient functions.\nThis context manager can be used to override the gradient function that will be used for ops within the scope of the context.\n#### For example:\n```\n@tf.RegisterGradient(\"CustomSquare\")\ndef_custom_square_grad(op, grad):\n  # ...\n\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0)\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n                          # gradient of s_2.\n\n```\nArgs  \n---  \n`op_type_map` |  A dictionary mapping op type strings to alternative op type strings.   \nReturns  \n---  \nA context manager that sets the alternative op type to be used for one or more ops created in that context.   \nRaises  \n---  \n`TypeError` |  If `op_type_map` is not a dictionary mapping strings to strings.   \n### `is_feedable`\n```\nis_feedable(\n    tensor\n) -> bool\n\n```\n\nReturns `True` if and only if `tensor` is feedable.\n### `is_fetchable`\n```\nis_fetchable(\n    tensor_or_op\n) -> bool\n\n```\n\nReturns `True` if and only if `tensor_or_op` is fetchable.\n### `name_scope`\n```\n@tf_contextlib.contextmanager\nname_scope(\n    name\n) -> Iterator[str]\n\n```\n\nReturns a context manager that creates hierarchical names for operations.\nA graph maintains a stack of name scopes. A `with name_scope(...):` statement pushes a new name onto the stack for the lifetime of the context.\nThe `name` argument will be interpreted as follows:\n  * A string (not ending with '/') will create a new name scope, in which `name` is appended to the prefix of all operations created in the context. If `name` has been used before, it will be made unique by calling `self.unique_name(name)`.\n  * A scope previously captured from a `with g.name_scope(...) as scope:` statement will be treated as an \"absolute\" name scope, which makes it possible to re-enter existing scopes.\n  * A value of `None` or the empty string will reset the current name scope to the top-level (empty) name scope.\n\n\n#### For example:\n```\nwith tf.Graph().as_default() as g:\n  c = tf.constant(5.0, name=\"c\")\n  assert c.op.name == \"c\"\n  c_1 = tf.constant(6.0, name=\"c\")\n  assert c_1.op.name == \"c_1\"\n\n  # Creates a scope called \"nested\"\n  with g.name_scope(\"nested\") as scope:\n    nested_c = tf.constant(10.0, name=\"c\")\n    assert nested_c.op.name == \"nested/c\"\n\n    # Creates a nested scope called \"inner\".\n    with g.name_scope(\"inner\"):\n      nested_inner_c = tf.constant(20.0, name=\"c\")\n      assert nested_inner_c.op.name == \"nested/inner/c\"\n\n    # Create a nested scope called \"inner_1\".\n    with g.name_scope(\"inner\"):\n      nested_inner_1_c = tf.constant(30.0, name=\"c\")\n      assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n\n      # Treats `scope` as an absolute name scope, and\n      # switches to the \"nested/\" scope.\n      with g.name_scope(scope):\n        nested_d = tf.constant(40.0, name=\"d\")\n        assert nested_d.op.name == \"nested/d\"\n\n        with g.name_scope(\"\"):\n          e = tf.constant(50.0, name=\"e\")\n          assert e.op.name == \"e\"\n\n```\n\nThe name of the scope itself can be captured by `with g.name_scope(...) as scope:`, which stores the name of the scope in the variable `scope`. This value can be used to name an operation that represents the overall result of executing the ops in a scope. For example:\n```\ninputs = tf.constant(...)\nwith g.name_scope('my_layer') as scope:\n  weights = tf.Variable(..., name=\"weights\")\n  biases = tf.Variable(..., name=\"biases\")\n  affine = tf.matmul(inputs, weights) + biases\n  output = tf.nn.relu(affine, name=scope)\n\n```\n```\n[A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)\n[A-Za-z0-9_.\\-/]* (for other scopes)\n\n```\nArgs  \n---  \n`name` |  A name for the scope.   \nReturns  \n---  \nA context manager that installs `name` as a new name scope.   \nRaises  \n---  \n`ValueError` |  If `name` is not a valid scope name, according to the rules above.   \n### `new_operations`\n```\nnew_operations()\n\n```\n\n(self: handle) -> List[TF_Operation]\n### `num_operations`\n```\nnum_operations()\n\n```\n\n(self: handle) -> int\n### `op_def_for_type`\n```\nop_def_for_type(\n    type\n) -> op_def_pb2.OpDef\n\n```\n\nReturns the `OpDef` proto for `type`. `type` is a string.\n### `prevent_feeding`\n```\nprevent_feeding(\n    tensor\n) -> None\n\n```\n\nMarks the given `tensor` as unfeedable in this graph.\n### `prevent_fetching`\n```\nprevent_fetching(\n    op\n) -> None\n\n```\n\nMarks the given `op` as unfetchable in this graph.\n### `switch_to_thread_local`\n```\nswitch_to_thread_local() -> None\n\n```\n\nMake device, colocation and dependencies stacks thread-local.\nDevice, colocation and dependencies stacks are not thread-local be default. If multiple threads access them, then the state is shared. This means that one thread may affect the behavior of another thread.\nAfter this method is called, the stacks become thread-local. If multiple threads access them, then the state is not shared. Each thread uses its own value; a thread doesn't affect other threads by mutating such a stack.\nThe initial value for every thread's stack is set to the current value of the stack when `switch_to_thread_local()` was first called.\n### `unique_name`\n```\nunique_name(\n    name, mark_as_used=True\n) -> str\n\n```\n\nReturn a unique operation name for `name`.\n`unique_name` is used to generate structured names, separated by `\"/\"`, to help identify operations when debugging a graph. Operation names are displayed in error messages reported by the TensorFlow runtime, and in various visualization tools such as TensorBoard.\nIf `mark_as_used` is set to `True`, which is the default, a new unique name is created and marked as in use. If it's set to `False`, the unique name is returned without actually being marked as used. This is useful when the caller simply wants to know what the name to be created will be.\nArgs  \n---  \n`name` |  The name for an operation.   \n`mark_as_used` |  Whether to mark this name as being used.   \nReturns  \n---  \nA string to be passed to `create_op()` that will be used to name the operation being created.   \n### `__enter__`\n```\n__enter__() -> GraphType\n\n```\n\n### `__exit__`\n```\n__exit__(\n    *args\n) -> None\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf": "## TensorFlow  \n```\npip install tensorflow\n\n```\n\n## Modules\n[`audio`](https://www.tensorflow.org/api_docs/python/tf/audio) module: Public API for tf._api.v2.audio namespace\n[`autodiff`](https://www.tensorflow.org/api_docs/python/tf/autodiff) module: Public API for tf._api.v2.autodiff namespace\n[`autograph`](https://www.tensorflow.org/api_docs/python/tf/autograph) module: Public API for tf._api.v2.autograph namespace\n[`bitwise`](https://www.tensorflow.org/api_docs/python/tf/bitwise) module: Public API for tf._api.v2.bitwise namespace\n[`compat`](https://www.tensorflow.org/api_docs/python/tf/compat) module: Public API for tf._api.v2.compat namespace\n[`config`](https://www.tensorflow.org/api_docs/python/tf/config) module: Public API for tf._api.v2.config namespace\n[`data`](https://www.tensorflow.org/api_docs/python/tf/data) module: Public API for tf._api.v2.data namespace\n[`debugging`](https://www.tensorflow.org/api_docs/python/tf/debugging) module: Public API for tf._api.v2.debugging namespace\n[`distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) module: Public API for tf._api.v2.distribute namespace\n[`dtypes`](https://www.tensorflow.org/api_docs/python/tf/dtypes) module: Public API for tf._api.v2.dtypes namespace\n[`errors`](https://www.tensorflow.org/api_docs/python/tf/errors) module: Public API for tf._api.v2.errors namespace\n[`experimental`](https://www.tensorflow.org/api_docs/python/tf/experimental) module: Public API for tf._api.v2.experimental namespace\n[`feature_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column) module: Public API for tf._api.v2.feature_column namespace\n[`graph_util`](https://www.tensorflow.org/api_docs/python/tf/graph_util) module: Public API for tf._api.v2.graph_util namespace\n[`image`](https://www.tensorflow.org/api_docs/python/tf/image) module: Public API for tf._api.v2.image namespace\n[`io`](https://www.tensorflow.org/api_docs/python/tf/io) module: Public API for tf._api.v2.io namespace\n[`keras`](https://www.tensorflow.org/api_docs/python/tf/keras) module: DO NOT EDIT.\n[`linalg`](https://www.tensorflow.org/api_docs/python/tf/linalg) module: Public API for tf._api.v2.linalg namespace\n[`lite`](https://www.tensorflow.org/api_docs/python/tf/lite) module: Public API for tf._api.v2.lite namespace\n[`lookup`](https://www.tensorflow.org/api_docs/python/tf/lookup) module: Public API for tf._api.v2.lookup namespace\n[`math`](https://www.tensorflow.org/api_docs/python/tf/math) module: Public API for tf._api.v2.math namespace\n[`mlir`](https://www.tensorflow.org/api_docs/python/tf/mlir) module: Public API for tf._api.v2.mlir namespace\n[`nest`](https://www.tensorflow.org/api_docs/python/tf/nest) module: Public API for tf._api.v2.nest namespace\n[`nn`](https://www.tensorflow.org/api_docs/python/tf/nn) module: Public API for tf._api.v2.nn namespace\n[`profiler`](https://www.tensorflow.org/api_docs/python/tf/profiler) module: Public API for tf._api.v2.profiler namespace\n[`quantization`](https://www.tensorflow.org/api_docs/python/tf/quantization) module: Public API for tf._api.v2.quantization namespace\n[`queue`](https://www.tensorflow.org/api_docs/python/tf/queue) module: Public API for tf._api.v2.queue namespace\n[`ragged`](https://www.tensorflow.org/api_docs/python/tf/ragged) module: Public API for tf._api.v2.ragged namespace\n[`random`](https://www.tensorflow.org/api_docs/python/tf/random) module: Public API for tf._api.v2.random namespace\n[`raw_ops`](https://www.tensorflow.org/api_docs/python/tf/raw_ops) module: Public API for tf._api.v2.raw_ops namespace\n[`saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model) module: Public API for tf._api.v2.saved_model namespace\n[`sets`](https://www.tensorflow.org/api_docs/python/tf/sets) module: Public API for tf._api.v2.sets namespace\n[`signal`](https://www.tensorflow.org/api_docs/python/tf/signal) module: Public API for tf._api.v2.signal namespace\n[`sparse`](https://www.tensorflow.org/api_docs/python/tf/sparse) module: Public API for tf._api.v2.sparse namespace\n[`strings`](https://www.tensorflow.org/api_docs/python/tf/strings) module: Public API for tf._api.v2.strings namespace\n[`summary`](https://www.tensorflow.org/api_docs/python/tf/summary) module: Public API for tf._api.v2.summary namespace\n[`sysconfig`](https://www.tensorflow.org/api_docs/python/tf/sysconfig) module: Public API for tf._api.v2.sysconfig namespace\n[`test`](https://www.tensorflow.org/api_docs/python/tf/test) module: Public API for tf._api.v2.test namespace\n[`tpu`](https://www.tensorflow.org/api_docs/python/tf/tpu) module: Public API for tf._api.v2.tpu namespace\n[`train`](https://www.tensorflow.org/api_docs/python/tf/train) module: Public API for tf._api.v2.train namespace\n[`types`](https://www.tensorflow.org/api_docs/python/tf/types) module: Public API for tf._api.v2.types namespace\n[`version`](https://www.tensorflow.org/api_docs/python/tf/version) module: Public API for tf._api.v2.version namespace\n[`xla`](https://www.tensorflow.org/api_docs/python/tf/xla) module: Public API for tf._api.v2.xla namespace\n## Classes\n[`class AggregationMethod`](https://www.tensorflow.org/api_docs/python/tf/AggregationMethod): A class listing aggregation methods used to combine gradients.\n[`class CriticalSection`](https://www.tensorflow.org/api_docs/python/tf/CriticalSection): Critical section.\n[`class DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType): Represents the type of the elements in a `Tensor`.\n[`class DeviceSpec`](https://www.tensorflow.org/api_docs/python/tf/DeviceSpec): Represents a (possibly partial) specification for a TensorFlow device.\n[`class GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape): Record operations for automatic differentiation.\n[`class Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph): A TensorFlow computation, represented as a dataflow graph.\n[`class IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices): A sparse representation of a set of tensor slices at given indices.\n[`class IndexedSlicesSpec`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlicesSpec): Type specification for a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).\n[`class Module`](https://www.tensorflow.org/api_docs/python/tf/Module): Base neural network module class.\n[`class Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation): Represents a graph node that performs computation on tensors.\n[`class OptionalSpec`](https://www.tensorflow.org/api_docs/python/tf/OptionalSpec): Type specification for [`tf.experimental.Optional`](https://www.tensorflow.org/api_docs/python/tf/experimental/Optional).\n[`class RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor): Represents a ragged tensor.\n[`class RaggedTensorSpec`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensorSpec): Type specification for a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).\n[`class RegisterGradient`](https://www.tensorflow.org/api_docs/python/tf/RegisterGradient): A decorator for registering the gradient function for an op type.\n[`class SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor): Represents a sparse tensor.\n[`class SparseTensorSpec`](https://www.tensorflow.org/api_docs/python/tf/SparseTensorSpec): Type specification for a [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor).\n[`class Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor): A [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) represents a multidimensional array of elements.\n[`class TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray): Class wrapping dynamic-sized, per-time-step, Tensor arrays.\n[`class TensorArraySpec`](https://www.tensorflow.org/api_docs/python/tf/TensorArraySpec): Type specification for a [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray).\n[`class TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape): Represents the shape of a `Tensor`.\n[`class TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec): Describes the type of a tf.Tensor.\n[`class TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec): Specifies a TensorFlow value type.\n[`class UnconnectedGradients`](https://www.tensorflow.org/api_docs/python/tf/UnconnectedGradients): Controls how gradient computation behaves when y does not depend on x.\n[`class Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable): See the [variable guide](https://tensorflow.org/guide/variable).\n[`class VariableAggregation`](https://www.tensorflow.org/api_docs/python/tf/VariableAggregation): Indicates how a distributed variable will be aggregated.\n[`class VariableSynchronization`](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization): Indicates when a distributed variable will be synced.\n[`class constant_initializer`](https://www.tensorflow.org/api_docs/python/tf/constant_initializer): Initializer that generates tensors with constant values.\n[`class name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope): A context manager for use when defining a Python op.\n[`class ones_initializer`](https://www.tensorflow.org/api_docs/python/tf/ones_initializer): Initializer that generates tensors initialized to 1.\n[`class random_normal_initializer`](https://www.tensorflow.org/api_docs/python/tf/random_normal_initializer): Initializer that generates tensors with a normal distribution.\n[`class random_uniform_initializer`](https://www.tensorflow.org/api_docs/python/tf/random_uniform_initializer): Initializer that generates tensors with a uniform distribution.\n[`class zeros_initializer`](https://www.tensorflow.org/api_docs/python/tf/zeros_initializer): Initializer that generates tensors initialized to 0.\n## Functions\n[`Assert(...)`](https://www.tensorflow.org/api_docs/python/tf/debugging/Assert): Asserts that the given condition is true.\n[`abs(...)`](https://www.tensorflow.org/api_docs/python/tf/math/abs): Computes the absolute value of a tensor.\n[`acos(...)`](https://www.tensorflow.org/api_docs/python/tf/math/acos): Computes acos of x element-wise.\n[`acosh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/acosh): Computes inverse hyperbolic cosine of x element-wise.\n[`add(...)`](https://www.tensorflow.org/api_docs/python/tf/math/add): Returns x + y element-wise.\n[`add_n(...)`](https://www.tensorflow.org/api_docs/python/tf/math/add_n): Returns the element-wise sum of a list of tensors.\n[`approx_top_k(...)`](https://www.tensorflow.org/api_docs/python/tf/approx_top_k): Returns min/max k values and their indices of the input operand in an approximate manner.\n[`argmax(...)`](https://www.tensorflow.org/api_docs/python/tf/math/argmax): Returns the index with the largest value across axes of a tensor.\n[`argmin(...)`](https://www.tensorflow.org/api_docs/python/tf/math/argmin): Returns the index with the smallest value across axes of a tensor.\n[`argsort(...)`](https://www.tensorflow.org/api_docs/python/tf/argsort): Returns the indices of a tensor that give its sorted order along an axis.\n[`as_dtype(...)`](https://www.tensorflow.org/api_docs/python/tf/dtypes/as_dtype): Converts the given `type_value` to a [`tf.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType).\n[`as_string(...)`](https://www.tensorflow.org/api_docs/python/tf/strings/as_string): Converts each entry in the given tensor to strings.\n[`asin(...)`](https://www.tensorflow.org/api_docs/python/tf/math/asin): Computes the trignometric inverse sine of x element-wise.\n[`asinh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/asinh): Computes inverse hyperbolic sine of x element-wise.\n[`assert_equal(...)`](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_equal): Assert the condition `x == y` holds element-wise.\n[`assert_greater(...)`](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_greater): Assert the condition `x > y` holds element-wise.\n[`assert_less(...)`](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_less): Assert the condition `x < y` holds element-wise.\n[`assert_rank(...)`](https://www.tensorflow.org/api_docs/python/tf/debugging/assert_rank): Assert that `x` has rank equal to `rank`.\n[`atan(...)`](https://www.tensorflow.org/api_docs/python/tf/math/atan): Computes the trignometric inverse tangent of x element-wise.\n[`atan2(...)`](https://www.tensorflow.org/api_docs/python/tf/math/atan2): Computes arctangent of `y/x` element-wise, respecting signs of the arguments.\n[`atanh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/atanh): Computes inverse hyperbolic tangent of x element-wise.\n[`batch_to_space(...)`](https://www.tensorflow.org/api_docs/python/tf/batch_to_space): BatchToSpace for N-D tensors of type T.\n[`bitcast(...)`](https://www.tensorflow.org/api_docs/python/tf/bitcast): Bitcasts a tensor from one type to another without copying data.\n[`boolean_mask(...)`](https://www.tensorflow.org/api_docs/python/tf/boolean_mask): Apply boolean mask to tensor.\n[`broadcast_dynamic_shape(...)`](https://www.tensorflow.org/api_docs/python/tf/broadcast_dynamic_shape): Computes the shape of a broadcast given symbolic shapes.\n[`broadcast_static_shape(...)`](https://www.tensorflow.org/api_docs/python/tf/broadcast_static_shape): Computes the shape of a broadcast given known shapes.\n[`broadcast_to(...)`](https://www.tensorflow.org/api_docs/python/tf/broadcast_to): Broadcast an array for a compatible shape.\n[`case(...)`](https://www.tensorflow.org/api_docs/python/tf/case): Create a case operation.\n[`cast(...)`](https://www.tensorflow.org/api_docs/python/tf/cast): Casts a tensor to a new type.\n[`clip_by_global_norm(...)`](https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm): Clips values of multiple tensors by the ratio of the sum of their norms.\n[`clip_by_norm(...)`](https://www.tensorflow.org/api_docs/python/tf/clip_by_norm): Clips tensor values to a maximum L2-norm.\n[`clip_by_value(...)`](https://www.tensorflow.org/api_docs/python/tf/clip_by_value): Clips tensor values to a specified min and max.\n[`complex(...)`](https://www.tensorflow.org/api_docs/python/tf/dtypes/complex): Converts two real numbers to a complex number.\n[`concat(...)`](https://www.tensorflow.org/api_docs/python/tf/concat): Concatenates tensors along one dimension.\n[`cond(...)`](https://www.tensorflow.org/api_docs/python/tf/cond): Return `true_fn()` if the predicate `pred` is true else `false_fn()`.\n[`constant(...)`](https://www.tensorflow.org/api_docs/python/tf/constant): Creates a constant tensor from a tensor-like object.\n[`control_dependencies(...)`](https://www.tensorflow.org/api_docs/python/tf/control_dependencies): Wrapper for [`Graph.control_dependencies()`](https://www.tensorflow.org/api_docs/python/tf/Graph#control_dependencies) using the default graph.\n[`conv(...)`](https://www.tensorflow.org/api_docs/python/tf/conv): Computes a N-D convolution given (N+1+batch_dims)-D `input` and (N+2)-D `filter` tensors.\n[`conv2d_backprop_filter_v2(...)`](https://www.tensorflow.org/api_docs/python/tf/conv2d_backprop_filter_v2): Computes the gradients of convolution with respect to the filter.\n[`conv2d_backprop_input_v2(...)`](https://www.tensorflow.org/api_docs/python/tf/conv2d_backprop_input_v2): Computes the gradients of convolution with respect to the input.\n[`convert_to_tensor(...)`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor): Converts the given `value` to a `Tensor`.\n[`cos(...)`](https://www.tensorflow.org/api_docs/python/tf/math/cos): Computes cos of x element-wise.\n[`cosh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/cosh): Computes hyperbolic cosine of x element-wise.\n[`cumsum(...)`](https://www.tensorflow.org/api_docs/python/tf/math/cumsum): Compute the cumulative sum of the tensor `x` along `axis`.\n[`custom_gradient(...)`](https://www.tensorflow.org/api_docs/python/tf/custom_gradient): Decorator to define a function with a custom gradient.\n[`device(...)`](https://www.tensorflow.org/api_docs/python/tf/device): Specifies the device for ops created/executed in this context.\n[`divide(...)`](https://www.tensorflow.org/api_docs/python/tf/math/divide): Computes Python style division of `x` by `y`.\n[`dynamic_partition(...)`](https://www.tensorflow.org/api_docs/python/tf/dynamic_partition): Partitions `data` into `num_partitions` tensors using indices from `partitions`.\n[`dynamic_stitch(...)`](https://www.tensorflow.org/api_docs/python/tf/dynamic_stitch): Interleave the values from the `data` tensors into a single tensor.\n[`edit_distance(...)`](https://www.tensorflow.org/api_docs/python/tf/edit_distance): Computes the Levenshtein distance between sequences.\n[`eig(...)`](https://www.tensorflow.org/api_docs/python/tf/linalg/eig): Computes the eigen decomposition of a batch of matrices.\n[`eigvals(...)`](https://www.tensorflow.org/api_docs/python/tf/linalg/eigvals): Computes the eigenvalues of one or more matrices.\n[`einsum(...)`](https://www.tensorflow.org/api_docs/python/tf/einsum): Tensor contraction over specified indices and outer product.\n[`ensure_shape(...)`](https://www.tensorflow.org/api_docs/python/tf/ensure_shape): Updates the shape of a tensor and checks at runtime that the shape holds.\n[`equal(...)`](https://www.tensorflow.org/api_docs/python/tf/math/equal): Returns the truth value of (x == y) element-wise.\n[`executing_eagerly(...)`](https://www.tensorflow.org/api_docs/python/tf/executing_eagerly): Checks whether the current thread has eager execution enabled.\n[`exp(...)`](https://www.tensorflow.org/api_docs/python/tf/math/exp): Computes exponential of x element-wise. y=ex\n[`expand_dims(...)`](https://www.tensorflow.org/api_docs/python/tf/expand_dims): Returns a tensor with a length 1 axis inserted at index `axis`.\n[`extract_volume_patches(...)`](https://www.tensorflow.org/api_docs/python/tf/extract_volume_patches): Extract `patches` from `input` and put them in the `\"depth\"` output dimension. 3D extension of `extract_image_patches`.\n[`eye(...)`](https://www.tensorflow.org/api_docs/python/tf/eye): Construct an identity matrix, or a batch of matrices.\n[`fftnd(...)`](https://www.tensorflow.org/api_docs/python/tf/fftnd): ND fast Fourier transform.\n[`fill(...)`](https://www.tensorflow.org/api_docs/python/tf/fill): Creates a tensor filled with a scalar value.\n[`fingerprint(...)`](https://www.tensorflow.org/api_docs/python/tf/fingerprint): Generates fingerprint values.\n[`floor(...)`](https://www.tensorflow.org/api_docs/python/tf/math/floor): Returns element-wise largest integer not greater than x.\n[`foldl(...)`](https://www.tensorflow.org/api_docs/python/tf/foldl): foldl on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n[`foldr(...)`](https://www.tensorflow.org/api_docs/python/tf/foldr): foldr on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n[`function(...)`](https://www.tensorflow.org/api_docs/python/tf/function): Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments)\n[`gather(...)`](https://www.tensorflow.org/api_docs/python/tf/gather): Gather slices from params axis `axis` according to indices. (deprecated arguments)\n[`gather_nd(...)`](https://www.tensorflow.org/api_docs/python/tf/gather_nd): Gather slices from `params` into a Tensor with shape specified by `indices`.\n[`get_current_name_scope(...)`](https://www.tensorflow.org/api_docs/python/tf/get_current_name_scope): Returns current full name scope specified by [`tf.name_scope(...)`](https://www.tensorflow.org/api_docs/python/tf/name_scope)s.\n[`get_logger(...)`](https://www.tensorflow.org/api_docs/python/tf/get_logger): Return TF logger instance.\n[`get_static_value(...)`](https://www.tensorflow.org/api_docs/python/tf/get_static_value): Returns the constant value of the given tensor, if efficiently calculable.\n[`grad_pass_through(...)`](https://www.tensorflow.org/api_docs/python/tf/grad_pass_through): Creates a grad-pass-through op with the forward behavior provided in f.\n[`gradients(...)`](https://www.tensorflow.org/api_docs/python/tf/gradients): Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`.\n[`greater(...)`](https://www.tensorflow.org/api_docs/python/tf/math/greater): Returns the truth value of (x > y) element-wise.\n[`greater_equal(...)`](https://www.tensorflow.org/api_docs/python/tf/math/greater_equal): Returns the truth value of (x >= y) element-wise.\n[`group(...)`](https://www.tensorflow.org/api_docs/python/tf/group): Create an op that groups multiple operations.\n[`guarantee_const(...)`](https://www.tensorflow.org/api_docs/python/tf/guarantee_const): Promise to the TF runtime that the input tensor is a constant. (deprecated)\n[`hessians(...)`](https://www.tensorflow.org/api_docs/python/tf/hessians): Constructs the Hessian of sum of `ys` with respect to `x` in `xs`.\n[`histogram_fixed_width(...)`](https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width): Return histogram of values.\n[`histogram_fixed_width_bins(...)`](https://www.tensorflow.org/api_docs/python/tf/histogram_fixed_width_bins): Bins the given values for use in a histogram.\n[`identity(...)`](https://www.tensorflow.org/api_docs/python/tf/identity): Return a Tensor with the same shape and contents as input.\n[`identity_n(...)`](https://www.tensorflow.org/api_docs/python/tf/identity_n): Returns a list of tensors with the same shapes and contents as the input\n[`ifftnd(...)`](https://www.tensorflow.org/api_docs/python/tf/ifftnd): ND inverse fast Fourier transform.\n[`import_graph_def(...)`](https://www.tensorflow.org/api_docs/python/tf/graph_util/import_graph_def): Imports the graph from `graph_def` into the current default `Graph`. (deprecated arguments)\n[`init_scope(...)`](https://www.tensorflow.org/api_docs/python/tf/init_scope): A context manager that lifts ops out of control-flow scopes and function-building graphs.\n[`inside_function(...)`](https://www.tensorflow.org/api_docs/python/tf/inside_function): Indicates whether the caller code is executing inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\n[`irfftnd(...)`](https://www.tensorflow.org/api_docs/python/tf/irfftnd): ND inverse real fast Fourier transform.\n[`is_symbolic_tensor(...)`](https://www.tensorflow.org/api_docs/python/tf/is_symbolic_tensor): Test if `tensor` is a symbolic Tensor.\n[`is_tensor(...)`](https://www.tensorflow.org/api_docs/python/tf/is_tensor): Checks whether `x` is a TF-native type that can be passed to many TF ops.\n[`less(...)`](https://www.tensorflow.org/api_docs/python/tf/math/less): Returns the truth value of (x < y) element-wise.\n[`less_equal(...)`](https://www.tensorflow.org/api_docs/python/tf/math/less_equal): Returns the truth value of (x <= y) element-wise.\n[`linspace(...)`](https://www.tensorflow.org/api_docs/python/tf/linspace): Generates evenly-spaced values in an interval along a given axis.\n[`load_library(...)`](https://www.tensorflow.org/api_docs/python/tf/load_library): Loads a TensorFlow plugin.\n[`load_op_library(...)`](https://www.tensorflow.org/api_docs/python/tf/load_op_library): Loads a TensorFlow plugin, containing custom ops and kernels.\n[`logical_and(...)`](https://www.tensorflow.org/api_docs/python/tf/math/logical_and): Returns the truth value of x AND y element-wise.\n[`logical_not(...)`](https://www.tensorflow.org/api_docs/python/tf/math/logical_not): Returns the truth value of `NOT x` element-wise.\n[`logical_or(...)`](https://www.tensorflow.org/api_docs/python/tf/math/logical_or): Returns the truth value of x OR y element-wise.\n[`make_ndarray(...)`](https://www.tensorflow.org/api_docs/python/tf/make_ndarray): Create a numpy ndarray from a tensor.\n[`make_tensor_proto(...)`](https://www.tensorflow.org/api_docs/python/tf/make_tensor_proto): Create a TensorProto.\n[`map_fn(...)`](https://www.tensorflow.org/api_docs/python/tf/map_fn): Transforms `elems` by applying `fn` to each element unstacked on axis 0. (deprecated arguments)\n[`matmul(...)`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul): Multiplies matrix `a` by matrix `b`, producing `a` * `b`.\n[`matrix_square_root(...)`](https://www.tensorflow.org/api_docs/python/tf/linalg/sqrtm): Computes the matrix square root of one or more square matrices:\n[`maximum(...)`](https://www.tensorflow.org/api_docs/python/tf/math/maximum): Returns the max of x and y (i.e. x > y ? x : y) element-wise.\n[`meshgrid(...)`](https://www.tensorflow.org/api_docs/python/tf/meshgrid): Broadcasts parameters for evaluation on an N-D grid.\n[`minimum(...)`](https://www.tensorflow.org/api_docs/python/tf/math/minimum): Returns the min of x and y (i.e. x < y ? x : y) element-wise.\n[`multiply(...)`](https://www.tensorflow.org/api_docs/python/tf/math/multiply): Returns an element-wise x * y.\n[`negative(...)`](https://www.tensorflow.org/api_docs/python/tf/math/negative): Computes numerical negative value element-wise.\n[`no_gradient(...)`](https://www.tensorflow.org/api_docs/python/tf/no_gradient): Specifies that ops of type `op_type` is not differentiable.\n[`no_op(...)`](https://www.tensorflow.org/api_docs/python/tf/no_op): Does nothing. Only useful as a placeholder for control edges.\n[`nondifferentiable_batch_function(...)`](https://www.tensorflow.org/api_docs/python/tf/nondifferentiable_batch_function): Batches the computation done by the decorated function.\n[`norm(...)`](https://www.tensorflow.org/api_docs/python/tf/norm): Computes the norm of vectors, matrices, and tensors.\n[`not_equal(...)`](https://www.tensorflow.org/api_docs/python/tf/math/not_equal): Returns the truth value of (x != y) element-wise.\n[`numpy_function(...)`](https://www.tensorflow.org/api_docs/python/tf/numpy_function): Wraps a python function and uses it as a TensorFlow op.\n[`one_hot(...)`](https://www.tensorflow.org/api_docs/python/tf/one_hot): Returns a one-hot tensor.\n[`ones(...)`](https://www.tensorflow.org/api_docs/python/tf/ones): Creates a tensor with all elements set to one (1).\n[`ones_like(...)`](https://www.tensorflow.org/api_docs/python/tf/ones_like): Creates a tensor of all ones that has the same shape as the input.\n[`pad(...)`](https://www.tensorflow.org/api_docs/python/tf/pad): Pads a tensor.\n[`parallel_stack(...)`](https://www.tensorflow.org/api_docs/python/tf/parallel_stack): Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor in parallel.\n[`pow(...)`](https://www.tensorflow.org/api_docs/python/tf/math/pow): Computes the power of one value to another.\n[`print(...)`](https://www.tensorflow.org/api_docs/python/tf/print): Print the specified inputs.\n[`py_function(...)`](https://www.tensorflow.org/api_docs/python/tf/py_function): Wraps a python function into a TensorFlow op that executes it eagerly.\n[`ragged_fill_empty_rows(...)`](https://www.tensorflow.org/api_docs/python/tf/ragged_fill_empty_rows)\n[`ragged_fill_empty_rows_grad(...)`](https://www.tensorflow.org/api_docs/python/tf/ragged_fill_empty_rows_grad)\n[`random_index_shuffle(...)`](https://www.tensorflow.org/api_docs/python/tf/random_index_shuffle): Outputs the position of `value` in a permutation of [0, ..., max_index].\n[`range(...)`](https://www.tensorflow.org/api_docs/python/tf/range): Creates a sequence of numbers.\n[`rank(...)`](https://www.tensorflow.org/api_docs/python/tf/rank): Returns the rank of a tensor.\n[`realdiv(...)`](https://www.tensorflow.org/api_docs/python/tf/realdiv): Returns x / y element-wise for real types.\n[`recompute_grad(...)`](https://www.tensorflow.org/api_docs/python/tf/recompute_grad): Defines a function as a recompute-checkpoint for the tape auto-diff.\n[`reduce_all(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_all): Computes [`tf.math.logical_and`](https://www.tensorflow.org/api_docs/python/tf/math/logical_and) of elements across dimensions of a tensor.\n[`reduce_any(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_any): Computes [`tf.math.logical_or`](https://www.tensorflow.org/api_docs/python/tf/math/logical_or) of elements across dimensions of a tensor.\n[`reduce_logsumexp(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp): Computes log(sum(exp(elements across dimensions of a tensor))).\n[`reduce_max(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_max): Computes [`tf.math.maximum`](https://www.tensorflow.org/api_docs/python/tf/math/maximum) of elements across dimensions of a tensor.\n[`reduce_mean(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean): Computes the mean of elements across dimensions of a tensor.\n[`reduce_min(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_min): Computes the [`tf.math.minimum`](https://www.tensorflow.org/api_docs/python/tf/math/minimum) of elements across dimensions of a tensor.\n[`reduce_prod(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_prod): Computes [`tf.math.multiply`](https://www.tensorflow.org/api_docs/python/tf/math/multiply) of elements across dimensions of a tensor.\n[`reduce_sum(...)`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum): Computes the sum of elements across dimensions of a tensor.\n[`register_tensor_conversion_function(...)`](https://www.tensorflow.org/api_docs/python/tf/register_tensor_conversion_function): Registers a function for converting objects of `base_type` to `Tensor`.\n[`repeat(...)`](https://www.tensorflow.org/api_docs/python/tf/repeat): Repeat elements of `input`.\n[`required_space_to_batch_paddings(...)`](https://www.tensorflow.org/api_docs/python/tf/required_space_to_batch_paddings): Calculate padding required to make block_shape divide input_shape.\n[`reshape(...)`](https://www.tensorflow.org/api_docs/python/tf/reshape): Reshapes a tensor.\n[`reverse(...)`](https://www.tensorflow.org/api_docs/python/tf/reverse): Reverses specific dimensions of a tensor.\n[`reverse_sequence(...)`](https://www.tensorflow.org/api_docs/python/tf/reverse_sequence): Reverses variable length slices.\n[`rfftnd(...)`](https://www.tensorflow.org/api_docs/python/tf/rfftnd): ND fast real Fourier transform.\n[`roll(...)`](https://www.tensorflow.org/api_docs/python/tf/roll): Rolls the elements of a tensor along an axis.\n[`round(...)`](https://www.tensorflow.org/api_docs/python/tf/math/round): Rounds the values of a tensor to the nearest integer, element-wise.\n[`saturate_cast(...)`](https://www.tensorflow.org/api_docs/python/tf/dtypes/saturate_cast): Performs a safe saturating cast of `value` to `dtype`.\n[`scalar_mul(...)`](https://www.tensorflow.org/api_docs/python/tf/math/scalar_mul): Multiplies a scalar times a `Tensor` or `IndexedSlices` object.\n[`scan(...)`](https://www.tensorflow.org/api_docs/python/tf/scan): scan on the list of tensors unpacked from `elems` on dimension 0. (deprecated argument values)\n[`scatter_nd(...)`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd): Scatters `updates` into a tensor of shape `shape` according to `indices`.\n[`searchsorted(...)`](https://www.tensorflow.org/api_docs/python/tf/searchsorted): Searches for where a value would go in a sorted sequence.\n[`sequence_mask(...)`](https://www.tensorflow.org/api_docs/python/tf/sequence_mask): Returns a mask tensor representing the first N positions of each cell.\n[`shape(...)`](https://www.tensorflow.org/api_docs/python/tf/shape): Returns a tensor containing the shape of the input tensor.\n[`shape_n(...)`](https://www.tensorflow.org/api_docs/python/tf/shape_n): Returns shape of a list of tensors.\n[`sigmoid(...)`](https://www.tensorflow.org/api_docs/python/tf/math/sigmoid): Computes sigmoid of `x` element-wise.\n[`sign(...)`](https://www.tensorflow.org/api_docs/python/tf/math/sign): Returns an element-wise indication of the sign of a number.\n[`sin(...)`](https://www.tensorflow.org/api_docs/python/tf/math/sin): Computes sine of x element-wise.\n[`sinh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/sinh): Computes hyperbolic sine of x element-wise.\n[`size(...)`](https://www.tensorflow.org/api_docs/python/tf/size): Returns the size of a tensor.\n[`slice(...)`](https://www.tensorflow.org/api_docs/python/tf/slice): Extracts a slice from a tensor.\n[`sort(...)`](https://www.tensorflow.org/api_docs/python/tf/sort): Sorts a tensor.\n[`space_to_batch(...)`](https://www.tensorflow.org/api_docs/python/tf/space_to_batch): SpaceToBatch for N-D tensors of type T.\n[`space_to_batch_nd(...)`](https://www.tensorflow.org/api_docs/python/tf/space_to_batch_nd): SpaceToBatch for N-D tensors of type T.\n[`split(...)`](https://www.tensorflow.org/api_docs/python/tf/split): Splits a tensor `value` into a list of sub tensors.\n[`sqrt(...)`](https://www.tensorflow.org/api_docs/python/tf/math/sqrt): Computes element-wise square root of the input tensor.\n[`square(...)`](https://www.tensorflow.org/api_docs/python/tf/math/square): Computes square of x element-wise.\n[`squeeze(...)`](https://www.tensorflow.org/api_docs/python/tf/squeeze): Removes dimensions of size 1 from the shape of a tensor.\n[`stack(...)`](https://www.tensorflow.org/api_docs/python/tf/stack): Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n[`stop_gradient(...)`](https://www.tensorflow.org/api_docs/python/tf/stop_gradient): Stops gradient computation.\n[`strided_slice(...)`](https://www.tensorflow.org/api_docs/python/tf/strided_slice): Extracts a strided slice of a tensor (generalized Python array indexing).\n[`subtract(...)`](https://www.tensorflow.org/api_docs/python/tf/math/subtract): Returns x - y element-wise.\n[`switch_case(...)`](https://www.tensorflow.org/api_docs/python/tf/switch_case): Create a switch/case operation, i.e.\n[`tan(...)`](https://www.tensorflow.org/api_docs/python/tf/math/tan): Computes tan of x element-wise.\n[`tanh(...)`](https://www.tensorflow.org/api_docs/python/tf/math/tanh): Computes hyperbolic tangent of `x` element-wise.\n[`tensor_scatter_nd_add(...)`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_add): Adds sparse `updates` to an existing tensor according to `indices`.\n[`tensor_scatter_nd_max(...)`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_max): Apply a sparse update to a tensor taking the element-wise maximum.\n[`tensor_scatter_nd_min(...)`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_min)\n[`tensor_scatter_nd_sub(...)`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_sub): Subtracts sparse `updates` from an existing tensor according to `indices`.\n[`tensor_scatter_nd_update(...)`](https://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update): Scatter `updates` into an existing tensor according to `indices`.\n[`tensordot(...)`](https://www.tensorflow.org/api_docs/python/tf/tensordot): Tensor contraction of a and b along specified axes and outer product.\n[`tile(...)`](https://www.tensorflow.org/api_docs/python/tf/tile): Constructs a tensor by tiling a given tensor.\n[`timestamp(...)`](https://www.tensorflow.org/api_docs/python/tf/timestamp): Provides the time since epoch in seconds.\n[`transpose(...)`](https://www.tensorflow.org/api_docs/python/tf/transpose): Transposes `a`, where `a` is a Tensor.\n[`truediv(...)`](https://www.tensorflow.org/api_docs/python/tf/math/truediv): Divides x / y elementwise (using Python 3 division operator semantics).\n[`truncatediv(...)`](https://www.tensorflow.org/api_docs/python/tf/truncatediv): Returns x / y element-wise, rounded towards zero.\n[`truncatemod(...)`](https://www.tensorflow.org/api_docs/python/tf/truncatemod): Returns element-wise remainder of division.\n[`tuple(...)`](https://www.tensorflow.org/api_docs/python/tf/tuple): Groups tensors together.\n[`type_spec_from_value(...)`](https://www.tensorflow.org/api_docs/python/tf/type_spec_from_value): Returns a [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) that represents the given `value`.\n[`unique(...)`](https://www.tensorflow.org/api_docs/python/tf/unique): Finds unique elements in a 1-D tensor.\n[`unique_with_counts(...)`](https://www.tensorflow.org/api_docs/python/tf/unique_with_counts): Finds unique elements in a 1-D tensor.\n[`unravel_index(...)`](https://www.tensorflow.org/api_docs/python/tf/unravel_index): Converts an array of flat indices into a tuple of coordinate arrays.\n[`unstack(...)`](https://www.tensorflow.org/api_docs/python/tf/unstack): Unpacks the given dimension of a rank-`R` tensor into rank-`(R-1)` tensors.\n[`variable_creator_scope(...)`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope): Scope which defines a variable creation function to be used by variable().\n[`vectorized_map(...)`](https://www.tensorflow.org/api_docs/python/tf/vectorized_map): Parallel map on the list of tensors unpacked from `elems` on dimension 0.\n[`where(...)`](https://www.tensorflow.org/api_docs/python/tf/where): Returns the indices of non-zero elements, or multiplexes `x` and `y`.\n[`while_loop(...)`](https://www.tensorflow.org/api_docs/python/tf/while_loop): Repeat `body` while the condition `cond` is true. (deprecated argument values)\n[`zeros(...)`](https://www.tensorflow.org/api_docs/python/tf/zeros): Creates a tensor with all elements set to zero.\n[`zeros_like(...)`](https://www.tensorflow.org/api_docs/python/tf/zeros_like): Creates a tensor with all elements set to zero.\n## Other Members  \n---  \n**version** |  `'2.16.1'`  \nbfloat16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)16-bit bfloat (brain floating point).   \nbool  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Boolean.   \ncomplex128  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)128-bit complex.   \ncomplex64  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)64-bit complex.   \ndouble  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)64-bit (double precision) floating-point.   \nfloat16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)16-bit (half precision) floating-point.   \nfloat32  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)32-bit (single precision) floating-point.   \nfloat64  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)64-bit (double precision) floating-point.   \nhalf  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)16-bit (half precision) floating-point.   \nint16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed 16-bit integer.   \nint32  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed 32-bit integer.   \nint64  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed 64-bit integer.   \nint8  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed 8-bit integer.   \nnewaxis  |  `None`  \nqint16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed quantized 16-bit integer.   \nqint32  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)signed quantized 32-bit integer.   \nqint8  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Signed quantized 8-bit integer.   \nquint16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned quantized 16-bit integer.   \nquint8  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned quantized 8-bit integer.   \nresource  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Handle to a mutable, dynamically allocated resource.   \nstring  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Variable-length string, represented as byte array.   \nuint16  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned 16-bit (word) integer.   \nuint32  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned 32-bit (dword) integer.   \nuint64  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned 64-bit (qword) integer.   \nuint8  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Unsigned 8-bit (byte) integer.   \nvariant  |  Instance of [`tf.dtypes.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType)Data of arbitrary type (known at runtime). \n",
  "https://www.tensorflow.org/api_docs/python/tf/Module": "Base neural network module class.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.Module`](https://www.tensorflow.org/api_docs/python/tf/Module)\n```\ntf.Module(\n    name=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Introduction to modules, layers, and models](https://www.tensorflow.org/guide/intro_to_modules)\n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n  * [Distributed training with Core APIs and DTensor](https://www.tensorflow.org/guide/core/distribution)\n  * [Logistic regression for binary classification with Core APIs](https://www.tensorflow.org/guide/core/logistic_regression_core)\n  * [Multilayer perceptrons for digit recognition with Core APIs](https://www.tensorflow.org/guide/core/mlp_core)\n\n| \n  * [Distributed training with DTensors](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)\n  * [Simple audio recognition: Recognizing keywords](https://www.tensorflow.org/tutorials/audio/simple_audio)\n  * [Neural machine translation with attention](https://www.tensorflow.org/text/tutorials/nmt_with_attention)\n  * [Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)\n\n  \nA module is a named container for [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s, other [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module)s and functions which apply to user input. For example a dense layer in a neural network might be implemented as a [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module):\n```\nclassDense(tf.Module):\n  def__init__(self, input_dim, output_size, name=None):\n    super().__init__(name=name)\n    self.w = tf.Variable(\n      tf.random.normal([input_dim, output_size]), name='w')\n    self.b = tf.Variable(tf.zeros([output_size]), name='b')\n  def__call__(self, x):\n    y = tf.matmul(x, self.w) + self.b\n    return tf.nn.relu(y)\n```\n\nYou can use the Dense layer as you would expect:\n```\nd = Dense(input_dim=3, output_size=2)\nd(tf.ones([1, 3]))\n<tf.Tensor: shape=(1, 2), dtype=float32, numpy=..., dtype=float32)>\n```\n\nBy subclassing [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) instead of `object` any [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) instances assigned to object properties can be collected using the `variables`, `trainable_variables` or `submodules` property:\n```\nd.variables\n    (<tf.Variable 'b:0' shape=(2,) dtype=float32, numpy=...,\n    dtype=float32)>,\n    <tf.Variable 'w:0' shape=(3, 2) dtype=float32, numpy=..., dtype=float32)>)\n```\n\nSubclasses of [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) can also take advantage of the `_flatten` method which can be used to implement tracking of any other types.\nAll [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) classes have an associated [`tf.name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope) which can be used to group operations in TensorBoard and create hierarchies for variable names which can help with debugging. We suggest using the name scope when creating nested submodules/parameters or for forward methods whose graph you might want to inspect in TensorBoard. You can enter the name scope explicitly using `with self.name_scope:` or you can annotate methods (apart from `__init__`) with [`@tf.Module.with_name_scope`](https://www.tensorflow.org/api_docs/python/tf/Module#with_name_scope).\n```\nclassMLP(tf.Module):\n  def__init__(self, input_size, sizes, name=None):\n    super().__init__(name=name)\n    self.layers = []\n    with self.name_scope:\n      for size in sizes:\n        self.layers.append(Dense(input_dim=input_size, output_size=size))\n        input_size = size\n  @tf.Module.with_name_scope\n  def__call__(self, x):\n    for layer in self.layers:\n      x = layer(x)\n    return x\n```\n```\nmodule = MLP(input_size=5, sizes=[5, 5])\nmodule.variables\n(<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)>,\n<tf.Variable 'mlp/b:0' shape=(5,) dtype=float32, numpy=..., dtype=float32)>,\n<tf.Variable 'mlp/w:0' shape=(5, 5) dtype=float32, numpy=...,\n   dtype=float32)>)\n```\n\n## Attributes  \n---  \n`name` |  Returns the name of this module as passed or determined in the ctor.  \n`name_scope` |  Returns a [`tf.name_scope`](https://www.tensorflow.org/api_docs/python/tf/name_scope) instance for this class.   \n`non_trainable_variables` |  Sequence of non-trainable variables owned by this module and its submodules.  \n`submodules` |  Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ```\na = tf.Module()\nb = tf.Module()\nc = tf.Module()\na.b = b\nb.c = c\nlist(a.submodules) == [b, c]\nTrue\nlist(b.submodules) == [c]\nTrue\nlist(c.submodules) == []\nTrue\n```\n  \n`trainable_variables` |  Sequence of trainable variables owned by this module and its submodules.  \n`variables` |  Sequence of variables owned by this module and its submodules.  \n## Methods\n### `with_name_scope`\n```\n@classmethod\nwith_name_scope(\n    method\n)\n\n```\n\nDecorator to automatically enter the module name scope.\n```\nclassMyModule(tf.Module):\n  @tf.Module.with_name_scope\n  def__call__(self, x):\n    if not hasattr(self, 'w'):\n      self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n    return tf.matmul(x, self.w)\n```\n\nUsing the above module would produce [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s and [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)s whose names included the module name:\n```\nmod = MyModule()\nmod(tf.ones([1, 2]))\n<tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\nmod.w\n<tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\nnumpy=..., dtype=float32)>\n```\n\nArgs  \n---  \n`method` |  The method to wrap.   \nReturns  \n---  \nThe original method wrapped such that it enters the module's name scope. \n",
  "https://www.tensorflow.org/api_docs/python/tf/TypeSpec": "Specifies a TensorFlow value type.\nInherits From: [`TraceType`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType)\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec), [`tf.compat.v1.data.experimental.Structure`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec)\nA [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) provides metadata describing an object accepted or returned by TensorFlow APIs. Concrete subclasses, such as [`tf.TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec) and [`tf.RaggedTensorSpec`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensorSpec), are used to describe different value types.\nFor example, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)'s `input_signature` argument accepts a list (or nested structure) of `TypeSpec`s.\nCreating new subclasses of `TypeSpec` (outside of TensorFlow core) is not currently supported. In particular, we may make breaking changes to the private methods and properties defined by this base class.\n#### Example:\n```\nspec = tf.TensorSpec(shape=[None, None], dtype=tf.int32)\n@tf.function(input_signature=[spec])\ndefdouble(x):\n  return x * 2\ndouble(tf.constant([[1, 2], [3, 4]]))\n<tf.Tensor: shape=(2, 2), dtype=int32,\n    numpy=array([[2, 4], [6, 8]], dtype=int32)>\n```\n\n## Attributes  \n---  \n`value_type` |  The Python type for values that are compatible with this TypeSpec.In particular, all values that are compatible with this TypeSpec must be an instance of this type.   \n## Methods\n### `experimental_as_proto`\n```\nexperimental_as_proto() -> struct_pb2.TypeSpecProto\n\n```\n\nReturns a proto representation of the TypeSpec instance.\nDo NOT override for custom non-TF types.\n### `experimental_from_proto`\n```\n@classmethod\nexperimental_from_proto(\n    proto: struct_pb2.TypeSpecProto\n) -> 'TypeSpec'\n\n```\n\nReturns a TypeSpec instance based on the serialized proto.\nDo NOT override for custom non-TF types.\nArgs  \n---  \n`proto` |  Proto generated using 'experimental_as_proto'.   \n### `experimental_type_proto`\n```\n@classmethod\nexperimental_type_proto() -> Type[struct_pb2.TypeSpecProto]\n\n```\n\nReturns the type of proto associated with TypeSpec serialization.\nDo NOT override for custom non-TF types.\n### `is_compatible_with`\n```\nis_compatible_with(\n    spec_or_value\n)\n\n```\n\nReturns true if `spec_or_value` is compatible with this TypeSpec.\nPrefer using \"is_subtype_of\" and \"most_specific_common_supertype\" wherever possible.\nArgs  \n---  \n`spec_or_value` |  A TypeSpec or TypeSpec associated value to compare against.   \n### `is_subtype_of`\n```\nis_subtype_of(\n    other: [tf.types.experimental.TraceType](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType)\n) -> bool\n\n```\n\nReturns True if `self` is a subtype of `other`.\nImplements the tf.types.experimental.func.TraceType interface.\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\nArgs  \n---  \n`other` |  A TraceType object.   \n### `most_specific_common_supertype`\n```\nmost_specific_common_supertype(\n    others: Sequence[[tf.types.experimental.TraceType](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType)]\n) -> Optional['TypeSpec']\n\n```\n\nReturns the most specific supertype TypeSpec of `self` and `others`.\nImplements the tf.types.experimental.func.TraceType interface.\nIf not overridden by a subclass, the default behavior is to assume the TypeSpec is covariant upon attributes that implement TraceType and invariant upon rest of the attributes as well as the structure and type of the TypeSpec.\nArgs  \n---  \n`others` |  A sequence of TraceTypes.   \n### `most_specific_compatible_type`\n```\nmost_specific_compatible_type(\n    other: 'TypeSpec'\n) -> 'TypeSpec'\n\n```\n\nReturns the most specific TypeSpec compatible with `self` and `other`. (deprecated)\nDeprecated. Please use `most_specific_common_supertype` instead. Do not override this function.\nArgs  \n---  \n`other` |  A `TypeSpec`.   \nRaises  \n---  \n`ValueError` |  If there is no TypeSpec that is compatible with both `self` and `other`.   \n### `__eq__`\n```\n__eq__(\n    other\n) -> bool\n\n```\n\nReturn self==value.\n### `__ne__`\n```\n__ne__(\n    other\n) -> bool\n\n```\n\nReturn self!=value.\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef": "A ProtocolMessage\n## Attributes  \n---  \n`asset_file_def` |  `repeated AssetFileDef asset_file_def`  \n`collection_def` |  `repeated CollectionDefEntry collection_def`  \n`graph_def` |  `GraphDef graph_def`  \n`meta_info_def` |  `MetaInfoDef meta_info_def`  \n`object_graph_def` |  `SavedObjectGraph object_graph_def`  \n`saver_def` |  `SaverDef saver_def`  \n`signature_def` |  `repeated SignatureDefEntry signature_def`  \n## Child Classes\n[`class CollectionDefEntry`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef/CollectionDefEntry)\n[`class MetaInfoDef`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef/MetaInfoDef)\n[`class SignatureDefEntry`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/MetaGraphDef/SignatureDefEntry)\n",
  "https://www.tensorflow.org/api_docs/python/tf/Tensor": "A [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) represents a multidimensional array of elements.  \n#### View aliases\n**Main aliases**\n[`tf.experimental.numpy.ndarray`](https://www.tensorflow.org/api_docs/python/tf/Tensor)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)\nAll elements are of a single known data type.\nWhen writing a TensorFlow program, the main object that is manipulated and passed around is the [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor).\nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) has the following properties:\n  * a single data type (float32, int32, or string, for example)\n  * a shape\n\n\nTensorFlow supports eager execution and graph execution. In eager execution, operations are evaluated immediately. In graph execution, a computational graph is constructed for later evaluation.\nTensorFlow defaults to eager execution. In the example below, the matrix multiplication results are calculated immediately.\n```\n# Compute some values using a Tensor\nc = tf.constant([[1.0, 2.0], [3.0, 4.0]])\nd = tf.constant([[1.0, 1.0], [0.0, 1.0]])\ne = tf.matmul(c, d)\nprint(e)\ntf.Tensor(\n[[1. 3.]\n [3. 7.]], shape=(2, 2), dtype=float32)\n```\n\nNote that during eager execution, you may discover your `Tensors` are actually of type `EagerTensor`. This is an internal detail, but it does give you access to a useful function, `numpy`:\n```\ntype(e)\n<class'...ops.EagerTensor'>\nprint(e.numpy())\n  [[1. 3.]\n   [3. 7.]]\n```\n\nIn TensorFlow, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s are a common way to define graph execution.\nA Tensor's shape (that is, the rank of the Tensor and the size of each dimension) may not always be fully known. In [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) definitions, the shape may only be partially known.\nMost operations produce tensors of fully-known shapes if the shapes of their inputs are also fully known, but in some cases it's only possible to find the shape of a tensor at execution time.\nA number of specialized tensors are available: see [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant), `tf.placeholder`, [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), and [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).\n```\na = np.array([1, 2, 3])\nb = tf.constant(a)\na[0] = 4\nprint(b)  # tf.Tensor([4 2 3], shape=(3,), dtype=int64)\n\n```\n\nFor more on Tensors, see the [guide](https://tensorflow.org/guide/tensor).\n## Attributes  \n---  \n`dtype` |  The `DType` of elements in this tensor.   \n`name`  \n`ndim`  \n`shape` |  Returns a [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) that represents the shape of this tensor.```\nt = tf.constant([1,2,3,4,5])\nt.shape\nTensorShape([5])\n```\n[`tf.Tensor.shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#shape) is equivalent to [`tf.Tensor.get_shape()`](https://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape). In a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) or when building a model using [`tf.keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input), they return the build-time shape of the tensor, which may be partially unknown. A [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) is not a tensor. Use [`tf.shape(t)`](https://www.tensorflow.org/api_docs/python/tf/shape) to get a tensor containing the shape, calculated at runtime. See [`tf.Tensor.get_shape()`](https://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape), and [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) for details and examples.   \n## Methods\n### `eval`\n```\neval(\n    feed_dict=None, session=None\n)\n\n```\n\nEvaluates this tensor in a `Session`.\nCalling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.\nArgs  \n---  \n`feed_dict` |  A dictionary that maps `Tensor` objects to feed values. See `tf.Session.run` for a description of the valid feed values.   \n`session` |  (Optional.) The `Session` to be used to evaluate this tensor. If none, the default session will be used.   \nReturns  \n---  \nA numpy array corresponding to the value of this tensor.   \n### `experimental_ref`\n```\nexperimental_ref()\n\n```\n\nDEPRECATED FUNCTION\n### `get_shape`\n```\nget_shape() -> [tf.TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape)\n\n```\n\nReturns a [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) that represents the shape of this tensor.\nIn eager execution the shape is always fully-known.\n```\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nprint(a.shape)\n(2, 3)\n```\n\n[`tf.Tensor.get_shape()`](https://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape) is equivalent to [`tf.Tensor.shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#shape).\nWhen executing in a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) or building a model using [`tf.keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [`Tensor.shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#shape) may return a partial shape (including `None` for unknown dimensions). See [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) for more details.\n```\ninputs = tf.keras.Input(shape = [10])\n# Unknown batch size\nprint(inputs.shape)\n(None, 10)\n```\n\nThe shape is computed using shape inference functions that are registered for each [`tf.Operation`](https://www.tensorflow.org/api_docs/python/tf/Operation).\nThe returned [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) is determined at _build_ time, without executing the underlying kernel. It is not a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor). If you need a shape _tensor_ , either convert the [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) to a [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant), or use the [`tf.shape(tensor)`](https://www.tensorflow.org/api_docs/python/tf/shape) function, which returns the tensor's shape at _execution_ time.\nThis is useful for debugging and providing early errors. For example, when tracing a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), no ops are being executed, shapes may be unknown (See the [Concrete Functions Guide](https://www.tensorflow.org/guide/concrete_function) for details).\n```\n@tf.function\ndefmy_matmul(a, b):\n  result = a@b\n  # the `print` executes during tracing.\n  print(\"Result shape: \", result.shape)\n  return result\n```\n\nThe shape inference functions propagate shapes to the extent possible:\n```\nf = my_matmul.get_concrete_function(\n  tf.TensorSpec([None,3]),\n  tf.TensorSpec([3,5]))\nResult shape: (None, 5)\n```\n\nTracing may fail if a shape missmatch can be detected:\n```\ncf = my_matmul.get_concrete_function(\n  tf.TensorSpec([None,3]),\n  tf.TensorSpec([4,5]))\nTraceback (most recent call last):\nValueError: Dimensions must be equal, but are 3 and 4 for 'matmul' (op:\n'MatMul') with input shapes: [?,3], [4,5].\n```\n\nIn some cases, the inferred shape may have unknown dimensions. If the caller has additional information about the values of these dimensions, [`tf.ensure_shape`](https://www.tensorflow.org/api_docs/python/tf/ensure_shape) or [`Tensor.set_shape()`](https://www.tensorflow.org/api_docs/python/tf/Tensor#set_shape) can be used to augment the inferred shape.\n```\n@tf.function\ndefmy_fun(a):\n  a = tf.ensure_shape(a, [5, 5])\n  # the `print` executes during tracing.\n  print(\"Result shape: \", a.shape)\n  return a\n```\n```\ncf = my_fun.get_concrete_function(\n  tf.TensorSpec([None, None]))\nResult shape: (5, 5)\n```\nReturns  \n---  \nA [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) representing the shape of this tensor.   \n### `ref`\n```\nref()\n\n```\n\nReturns a hashable reference object to this Tensor.\nThe primary use case for this API is to put tensors in a set/dictionary. We can't put tensors in a set/dictionary as `tensor.__hash__()` is no longer available starting Tensorflow 2.0.\nThe following will raise an exception starting 2.0\n```\nx = tf.constant(5)\ny = tf.constant(10)\nz = tf.constant(10)\ntensor_set = {x, y, z}\nTraceback (most recent call last):\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\ntensor_dict = {x: 'five', y: 'ten'}\nTraceback (most recent call last):\nTypeError: Tensor is unhashable. Instead, use tensor.ref() as the key.\n```\n\nInstead, we can use `tensor.ref()`.\n```\ntensor_set = {x.ref(), y.ref(), z.ref()}\nx.ref() in tensor_set\nTrue\ntensor_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\ntensor_dict[y.ref()]\n'ten'\n```\n\nAlso, the reference object provides `.deref()` function that returns the original Tensor.\n```\nx = tf.constant(5)\nx.ref().deref()\n<tf.Tensor: shape=(), dtype=int32, numpy=5>\n```\n\n### `set_shape`\n```\nset_shape(\n    shape\n)\n\n```\n\nUpdates the shape of this tensor.\nWith eager execution this operates as a shape assertion. Here the shapes match:\n```\nt = tf.constant([[1,2,3]])\nt.set_shape([1, 3])\n```\n\nPassing a `None` in the new shape allows any value for that axis:\n```\nt.set_shape([1,None])\n```\n\nAn error is raised if an incompatible shape is passed.\n```\nt.set_shape([1,5])\nTraceback (most recent call last):\nValueError: Tensor's shape (1, 3) is not compatible with supplied\nshape [1, 5]\n```\n\nWhen executing in a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), or building a model using [`tf.keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [`Tensor.set_shape`](https://www.tensorflow.org/api_docs/python/tf/Tensor#set_shape) will _merge_ the given `shape` with the current shape of this tensor, and set the tensor's shape to the merged value (see [`tf.TensorShape.merge_with`](https://www.tensorflow.org/api_docs/python/tf/TensorShape#merge_with) for details):\n```\nt = tf.keras.Input(shape=[None, None, 3])\nprint(t.shape)\n(None, None, None, 3)\n```\n\nDimensions set to `None` are not updated:\n```\nt.set_shape([None, 224, 224, None])\nprint(t.shape)\n(None, 224, 224, 3)\n```\n\nThe main use case for this is to provide additional shape information that cannot be inferred from the graph alone.\nFor example if you know all the images in a dataset have shape [28,28,3] you can set it with `tf.set_shape`:\n```\n@tf.function\ndefload_image(filename):\n  raw = tf.io.read_file(filename)\n  image = tf.image.decode_png(raw, channels=3)\n  # the `print` executes during tracing.\n  print(\"Initial shape: \", image.shape)\n  image.set_shape([28, 28, 3])\n  print(\"Final shape: \", image.shape)\n  return image\n```\n\nTrace the function, see the [Concrete Functions Guide](https://www.tensorflow.org/guide/concrete_function) for details.\n```\ncf = load_image.get_concrete_function(\n    tf.TensorSpec([], dtype=tf.string))\nInitial shape:  (None, None, 3)\nFinal shape: (28, 28, 3)\n```\n\nSimilarly the [`tf.io.parse_tensor`](https://www.tensorflow.org/api_docs/python/tf/io/parse_tensor) function could return a tensor with any shape, even the [`tf.rank`](https://www.tensorflow.org/api_docs/python/tf/rank) is unknown. If you know that all your serialized tensors will be 2d, set it with `set_shape`:\n```\n@tf.function\ndefmy_parse(string_tensor):\n  result = tf.io.parse_tensor(string_tensor, out_type=tf.float32)\n  # the `print` executes during tracing.\n  print(\"Initial shape: \", result.shape)\n  result.set_shape([None, None])\n  print(\"Final shape: \", result.shape)\n  return result\n```\n\nTrace the function\n```\nconcrete_parse = my_parse.get_concrete_function(\n    tf.TensorSpec([], dtype=tf.string))\nInitial shape:  <unknown>\nFinal shape:  (None, None)\n```\n\n#### Make sure it works:\n```\nt = tf.ones([5,3], dtype=tf.float32)\nserialized = tf.io.serialize_tensor(t)\nprint(serialized.dtype)\n<dtype: 'string'>\nprint(serialized.shape)\n()\nt2 = concrete_parse(serialized)\nprint(t2.shape)\n(5, 3)\n```\n```\n# Serialize a rank-3 tensor\nt = tf.ones([5,5,5], dtype=tf.float32)\nserialized = tf.io.serialize_tensor(t)\n# The function still runs, even though it `set_shape([None,None])`\nt2 = concrete_parse(serialized)\nprint(t2.shape)\n(5, 5, 5)\n```\nArgs  \n---  \n`shape` |  A `TensorShape` representing the shape of this tensor, a `TensorShapeProto`, a list, a tuple, or None.   \nRaises  \n---  \n`ValueError` |  If `shape` is not compatible with the current shape of this tensor.   \n### `__abs__`\n```\n__abs__(\n    name=None\n)\n\n```\n\n### `__add__`\n```\n__add__(\n    y\n)\n\n```\n\n### `__and__`\n```\n__and__(\n    y\n)\n\n```\n\n### `__array__`\n```\n__array__(\n    dtype=None\n)\n\n```\n\n### `__bool__`\n```\n__bool__()\n\n```\n\nDummy method to prevent a tensor from being used as a Python `bool`.\nThis overload raises a `TypeError` when the user inadvertently treats a `Tensor` as a boolean (most commonly in an `if` or `while` statement), in code that was not converted by AutoGraph. For example:\n```\nif tf.constant(True):  # Will raise.\n  # ...\n\nif tf.constant(5) < tf.constant(7):  # Will raise.\n  # ...\n\n```\nRaises  \n---  \n`TypeError`.   \n### `__div__`\n```\n__div__(\n    y\n)\n\n```\n\n### `__eq__`\n```\n__eq__(\n    other\n)\n\n```\n\n### `__floordiv__`\n```\n__floordiv__(\n    y\n)\n\n```\n\n### `__ge__`\n```\n__ge__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x >= y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==> [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==> [True, False, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__getitem__`\n```\n__getitem__(\n    slice_spec, var=None\n)\n\n```\n\nOverload for Tensor.**getitem**.\nThis operation extracts the specified region from the tensor. The notation is similar to NumPy with the restriction that currently only support basic indexing. That means that using a non-scalar tensor as input is not currently allowed.\n#### Some useful examples:\n```\n# Strip leading and trailing 2 elements\nfoo = tf.constant([1,2,3,4,5,6])\nprint(foo[2:-2])  # => [3,4]\n\n# Skip every other row and reverse the order of the columns\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[::2,::-1])  # => [[3,2,1], [9,8,7]]\n\n# Use scalar tensors as indices on both dimensions\nprint(foo[tf.constant(0), tf.constant(2)])  # => 3\n\n# Insert another dimension\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[tf.newaxis, :, :]) # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[:, tf.newaxis, :]) # => [[[1,2,3]], [[4,5,6]], [[7,8,9]]]\nprint(foo[:, :, tf.newaxis]) # => [[[1],[2],[3]], [[4],[5],[6]],\n[[7],[8],[9]]]\n\n# Ellipses (3 equivalent operations)\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[tf.newaxis, :, :])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[tf.newaxis, ...])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\nprint(foo[tf.newaxis])  # => [[[1,2,3], [4,5,6], [7,8,9]]]\n\n# Masks\nfoo = tf.constant([[1,2,3], [4,5,6], [7,8,9]])\nprint(foo[foo > 2])  # => [3, 4, 5, 6, 7, 8, 9]\n\n```\nNotes  \n---  \n  * [`tf.newaxis`](https://www.tensorflow.org/api_docs/python/tf#newaxis) is `None` as in NumPy.\n  * An implicit ellipsis is placed at the end of the `slice_spec`\n  * NumPy advanced indexing is currently not supported. \n\n  \nPurpose in the API  \n---  \nThis method is exposed in TensorFlow's API so that library developers can register dispatching for [`Tensor.**getitem**`](https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__)to allow it to handle custom composite tensors& other custom objects.The API symbol is not intended to be called by users directly and does appear in TensorFlow's generated documentation.   \nArgs  \n---  \n`tensor` |  An tensor.Tensor object.   \n`slice_spec` |  The arguments to Tensor.**getitem**.   \n`var` |  In the case of variable slice assignment, the Variable object to slice (i.e. tensor is the read-only view of this variable).   \nReturns  \n---  \nThe appropriate slice of \"tensor\", based on \"slice_spec\".   \nRaises  \n---  \n`ValueError` |  If a slice range is negative size.   \n`TypeError` |  If the slice indices aren't int, slice, ellipsis, tf.newaxis or scalar int32/int64 tensors.   \n### `__gt__`\n```\n__gt__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x > y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__invert__`\n```\n__invert__(\n    name=None\n)\n\n```\n\n### `__iter__`\n```\n__iter__()\n\n```\n\n### `__le__`\n```\n__le__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x <= y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__len__`\n```\n__len__()\n\n```\n\n### `__lt__`\n```\n__lt__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x < y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__matmul__`\n```\n__matmul__(\n    y\n)\n\n```\n\n### `__mod__`\n```\n__mod__(\n    y\n)\n\n```\n\n### `__mul__`\n```\n__mul__(\n    y\n)\n\n```\n\n### `__ne__`\n```\n__ne__(\n    other\n)\n\n```\n\n### `__neg__`\n```\n__neg__(\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nComputes numerical negative value element-wise.\nI.e., \\\\(y = -x\\\\).\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `x`.If `x` is a `SparseTensor`, returns `SparseTensor(x.indices, tf.math.negative(x.values, ...), x.dense_shape)`  \n### `__nonzero__`\n```\n__nonzero__()\n\n```\n\nDummy method to prevent a tensor from being used as a Python `bool`.\nThis is the Python 2.x counterpart to `__bool__()` above.\nRaises  \n---  \n`TypeError`.   \n### `__or__`\n```\n__or__(\n    y\n)\n\n```\n\n### `__pow__`\n```\n__pow__(\n    y\n)\n\n```\n\n### `__radd__`\n```\n__radd__(\n    x\n)\n\n```\n\n### `__rand__`\n```\n__rand__(\n    x\n)\n\n```\n\n### `__rdiv__`\n```\n__rdiv__(\n    x\n)\n\n```\n\n### `__rfloordiv__`\n```\n__rfloordiv__(\n    x\n)\n\n```\n\n### `__rmatmul__`\n```\n__rmatmul__(\n    x\n)\n\n```\n\n### `__rmod__`\n```\n__rmod__(\n    x\n)\n\n```\n\n### `__rmul__`\n```\n__rmul__(\n    x\n)\n\n```\n\n### `__ror__`\n```\n__ror__(\n    x\n)\n\n```\n\n### `__rpow__`\n```\n__rpow__(\n    x\n)\n\n```\n\n### `__rsub__`\n```\n__rsub__(\n    x\n)\n\n```\n\n### `__rtruediv__`\n```\n__rtruediv__(\n    x\n)\n\n```\n\n### `__rxor__`\n```\n__rxor__(\n    x\n)\n\n```\n\n### `__sub__`\n```\n__sub__(\n    y\n)\n\n```\n\n### `__truediv__`\n```\n__truediv__(\n    y\n)\n\n```\n\n### `__xor__`\n```\n__xor__(\n    y\n)\n\n```\n\n## Class Variables  \n---  \nOVERLOADABLE_OPERATORS  |  ```\n{\n '__abs__',\n '__add__',\n '__and__',\n '__div__',\n '__eq__',\n '__floordiv__',\n '__ge__',\n '__getitem__',\n '__gt__',\n '__invert__',\n '__le__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__or__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdiv__',\n '__rfloordiv__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__sub__',\n '__truediv__',\n '__xor__'\n}\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/disable_v2_behavior": "Disables TensorFlow 2.x behaviors.\n```\ntf.compat.v1.disable_v2_behavior()\n\n```\nMigrate to TF2\nUsing this function indicates that your software is not compatible with eager execution and [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) in TF2.\nTo migrate to TF2, rewrite your code to be compatible with eager execution. Please refer to the [migration guide](https://www.tensorflow.org/guide/migrate) for additional resource on the topic.\n## Description\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Generating Images with BigBiGAN](https://www.tensorflow.org/hub/tutorials/bigbigan_with_tf_hub)\n  * [Generating Images with BigGAN](https://www.tensorflow.org/hub/tutorials/biggan_generation_with_tf_hub)\n  * [Classify Flowers with Transfer Learning](https://www.tensorflow.org/hub/tutorials/image_feature_vector)\n  * [Generating Images with Little Data Using S3GAN](https://www.tensorflow.org/hub/tutorials/s3gan_generation_with_tf_hub)\n  * [Universal Sentence Encoder-Lite demo](https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder_lite)\n\n  \nThis function can be called at the beginning of the program (before `Tensors`, `Graphs` or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.\nUser can call this function to disable 2.x behavior during complex migrations.\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor": "A ProtocolMessage  \n## Attributes  \n---  \n`components` |  `repeated TensorInfo components`  \n`type_spec` |  `TypeSpecProto type_spec`\n",
  "https://www.tensorflow.org/api_docs/python/tf/Variable": "See the [variable guide](https://tensorflow.org/guide/variable).\n```\ntf.Variable(\n    initial_value=None,\n    trainable=None,\n    validate_shape=True,\n    caching_device=None,\n    name=None,\n    variable_def=None,\n    dtype=None,\n    import_scope=None,\n    constraint=None,\n    synchronization=[tf.VariableSynchronization.AUTO](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization#AUTO),\n    aggregation=[tf.compat.v1.VariableAggregation.NONE](https://www.tensorflow.org/api_docs/python/tf/compat/v1/VariableAggregation#NONE),\n    shape=None,\n    experimental_enable_variable_lifting=True\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrating model checkpoints](https://www.tensorflow.org/guide/migrate/migrating_checkpoints)\n  * [Introduction to gradients and automatic differentiation](https://www.tensorflow.org/guide/autodiff)\n  * [Introduction to Variables](https://www.tensorflow.org/guide/variable)\n  * [Advanced automatic differentiation](https://www.tensorflow.org/guide/advanced_autodiff)\n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n\n| \n  * [Scalable model compression](https://www.tensorflow.org/tutorials/optimization/compression)\n  * [Custom training loop with Keras and MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n  * [Neural style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)\n  * [Learned data compression](https://www.tensorflow.org/tutorials/generative/data_compression)\n  * [Learnable Distributions Zoo](https://www.tensorflow.org/probability/examples/Learnable_Distributions_Zoo)\n\n  \nA variable maintains shared, persistent state manipulated by a program.\nThe `Variable()` constructor requires an initial value for the variable, which can be a `Tensor` of any type and shape. This initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n```\nv = tf.Variable(1.)\nv.assign(2.)\n<tf.Variable ... shape=() dtype=float32, numpy=2.0>\nv.assign_add(0.5)\n<tf.Variable ... shape=() dtype=float32, numpy=2.5>\n```\n\nThe `shape` argument to `Variable`'s constructor allows you to construct a variable with a less defined shape than its `initial_value`:\n```\nv = tf.Variable(1., shape=tf.TensorShape(None))\nv.assign([[1.]])\n<tf.Variable ... shape=<unknown> dtype=float32, numpy=array([[1.]], ...)>\n```\n\nJust like any `Tensor`, variables created with `Variable()` can be used as inputs to operations. Additionally, all the operators overloaded for the `Tensor` class are carried over to variables.\n```\nw = tf.Variable([[1.], [2.]])\nx = tf.constant([[3., 4.]])\ntf.matmul(w, x)\n<tf.Tensor:... shape=(2, 2), ... numpy=\n  array([[3., 4.],\n         [6., 8.]], dtype=float32)>\ntf.sigmoid(w + x)\n<tf.Tensor:... shape=(2, 2), ...>\n```\n\nWhen building a machine learning model it is often convenient to distinguish between variables holding trainable model parameters and other variables such as a `step` variable used to count training steps. To make this easier, the variable constructor supports a `trainable=<bool>` parameter. [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) watches trainable variables by default:\n```\nwith tf.GradientTape(persistent=True) as tape:\n  trainable = tf.Variable(1.)\n  non_trainable = tf.Variable(2., trainable=False)\n  x1 = trainable * 2.\n  x2 = non_trainable * 3.\ntape.gradient(x1, trainable)\n<tf.Tensor:... shape=(), dtype=float32, numpy=2.0>\nassert tape.gradient(x2, non_trainable) is None  # Unwatched\n```\n\nVariables are automatically tracked when assigned to attributes of types inheriting from [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module).\n```\nm = tf.Module()\nm.v = tf.Variable([1.])\nm.trainable_variables\n(<tf.Variable ... shape=(1,) ... numpy=array([1.], dtype=float32)>,)\n```\n\nThis tracking then allows saving variable values to [training checkpoints](https://www.tensorflow.org/guide/checkpoint), or to [SavedModels](https://www.tensorflow.org/guide/saved_model) which include serialized TensorFlow graphs.\nVariables are often captured and manipulated by [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s. This works the same way the un-decorated function would have:\n```\nv = tf.Variable(0.)\nread_and_decrement = tf.function(lambda: v.assign_sub(0.1))\nread_and_decrement()\n<tf.Tensor: shape=(), dtype=float32, numpy=-0.1>\nread_and_decrement()\n<tf.Tensor: shape=(), dtype=float32, numpy=-0.2>\n```\n\nVariables created inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) must be owned outside the function and be created only once:\n```\nclassM(tf.Module):\n  @tf.function\n  def__call__(self, x):\n    if not hasattr(self, \"v\"):  # Or set self.v to None in __init__\n      self.v = tf.Variable(x)\n    return self.v * x\nm = M()\nm(2.)\n<tf.Tensor: shape=(), dtype=float32, numpy=4.0>\nm(3.)\n<tf.Tensor: shape=(), dtype=float32, numpy=6.0>\nm.v\n<tf.Variable ... shape=() dtype=float32, numpy=2.0>\n```\n\nSee the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) documentation for details.\n## Args  \n---  \n`initial_value` |  A `Tensor`, or Python object convertible to a `Tensor`, which is the initial value for the Variable. The initial value must have a shape specified unless `validate_shape` is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, `dtype` must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.)   \n`trainable` |  If `True`, GradientTapes automatically watch uses of this variable. Defaults to `True`, unless `synchronization` is set to `ON_READ`, in which case it defaults to `False`.   \n`validate_shape` |  If `False`, allows the variable to be initialized with a value of unknown shape. If `True`, the default, the shape of `initial_value` must be known.   \n`caching_device` |  Note: This argument is only valid when using a v1-style `Session`. Optional device string describing where the Variable should be cached for reading. Defaults to the Variable's device. If not `None`, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through `Switch` and other conditional statements.   \n`name` |  Optional name for the variable. Defaults to `'Variable'` and gets uniquified automatically.   \n`variable_def` |  `VariableDef` protocol buffer. If not `None`, recreates the Variable object with its contents, referencing the variable's nodes in the graph, which must already exist. The graph is not changed. `variable_def` and the other arguments are mutually exclusive.   \n`dtype` |  If set, initial_value will be converted to the given type. If `None`, either the datatype will be kept (if `initial_value` is a Tensor), or `convert_to_tensor` will decide.   \n`import_scope` |  Optional `string`. Name scope to add to the `Variable.` Only used when initializing from protocol buffer.   \n`constraint` |  An optional projection function to be applied to the variable after being updated by an `Optimizer` (e.g. used to implement norm constraints or value constraints for layer weights). The function must take as input the unprojected Tensor representing the value of the variable and return the Tensor for the projected value (which must have the same shape). Constraints are not safe to use when doing asynchronous distributed training.   \n`synchronization` |  Indicates when a distributed variable will be aggregated. Accepted values are constants defined in the class [`tf.VariableSynchronization`](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization). By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize.   \n`aggregation` |  Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class [`tf.VariableAggregation`](https://www.tensorflow.org/api_docs/python/tf/VariableAggregation).   \n`shape` |  (optional) The shape of this variable. If None, the shape of `initial_value` will be used. When setting this argument to [`tf.TensorShape(None)`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) (representing an unspecified shape), the variable can be assigned with values of different shapes.   \n`experimental_enable_variable_lifting` |  Whether to lift the variable out if it's in a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Default is `True`. When this argument is `True`, variable creation will follow the behavior and restrictions described [here](https://www.tensorflow.org/guide/function#creating_tfvariables). If this argument is `False`, that description doesn't apply, and you can freely create and use the variable in the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), as if it's a \"mutable [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)\". You can't return the variable though.   \n## Raises  \n---  \n`ValueError` |  If both `variable_def` and initial_value are specified.   \n`ValueError` |  If the initial value is not specified, or does not have a shape and `validate_shape` is `True`.   \n## Attributes  \n---  \n`aggregation`  \n`constraint` |  Returns the constraint function associated with this variable.   \n`device` |  The device of this variable.   \n`dtype` |  The `DType` of this variable.   \n`graph` |  The `Graph` of this variable.   \n`initial_value` |  Returns the Tensor used as the initial value for the variable.Note that this is different from `initialized_value()` which runs the op that initializes the variable before returning its value. This method returns the tensor that is used by the op that initializes the variable.   \n`initializer` |  The initializer operation for this variable.   \n`name` |  The name of this variable.   \n`op` |  The `Operation` of this variable.   \n`shape` |  The `TensorShape` of this variable.   \n`synchronization`  \n`trainable`  \n## Child Classes\n[`class SaveSliceInfo`](https://www.tensorflow.org/api_docs/python/tf/Variable/SaveSliceInfo)\n## Methods\n### `assign`\n```\nassign(\n    value, use_locking=False, name=None, read_value=True\n)\n\n```\n\nAssigns a new value to the variable.\nThis is essentially a shortcut for `assign(self, value)`.\nArgs  \n---  \n`value` |  A `Tensor`. The new value for this variable.   \n`use_locking` |  If `True`, use locking during the assignment.   \n`name` |  The name of the operation to be created   \n`read_value` |  if True, will return something which evaluates to the new value of the variable; if False will return the assign op.   \nReturns  \n---  \nThe updated variable. If `read_value` is false, instead returns None in Eager mode and the assign op in graph mode.   \n### `assign_add`\n```\nassign_add(\n    delta, use_locking=False, name=None, read_value=True\n)\n\n```\n\nAdds a value to this variable.\nThis is essentially a shortcut for `assign_add(self, delta)`.\nArgs  \n---  \n`delta` |  A `Tensor`. The value to add to this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  The name of the operation to be created   \n`read_value` |  if True, will return something which evaluates to the new value of the variable; if False will return the assign op.   \nReturns  \n---  \nThe updated variable. If `read_value` is false, instead returns None in Eager mode and the assign op in graph mode.   \n### `assign_sub`\n```\nassign_sub(\n    delta, use_locking=False, name=None, read_value=True\n)\n\n```\n\nSubtracts a value from this variable.\nThis is essentially a shortcut for `assign_sub(self, delta)`.\nArgs  \n---  \n`delta` |  A `Tensor`. The value to subtract from this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  The name of the operation to be created   \n`read_value` |  if True, will return something which evaluates to the new value of the variable; if False will return the assign op.   \nReturns  \n---  \nThe updated variable. If `read_value` is false, instead returns None in Eager mode and the assign op in graph mode.   \n### `batch_scatter_update`\n```\nbatch_scatter_update(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nAssigns [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to this variable batch-wise.\nAnalogous to `batch_gather`. This assumes that this variable and the sparse_delta IndexedSlices have a series of leading dimensions that are the same for all of them, and the updates are performed on the last dimension of indices. In other words, the dimensions should be the following:\n`num_prefix_dims = sparse_delta.indices.ndims - 1` `batch_dim = num_prefix_dims + 1` `sparse_delta.updates.shape = sparse_delta.indices.shape + var.shape[      batch_dim:]`\nwhere\n`sparse_delta.updates.shape[:num_prefix_dims]` `== sparse_delta.indices.shape[:num_prefix_dims]` `== var.shape[:num_prefix_dims]`\nAnd the operation performed can be expressed as:\n`var[i_1, ..., i_n,      sparse_delta.indices[i_1, ..., i_n, j]] = sparse_delta.updates[         i_1, ..., i_n, j]`\nWhen sparse_delta.indices is a 1D tensor, this operation is equivalent to `scatter_update`.\nTo avoid this operation one can looping over the first `ndims` of the variable and using `scatter_update` on the subtensors that result of slicing the first dimension. This is a valid option for `ndims = 1`, but less efficient than this implementation.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to be assigned to this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `count_up_to`\n```\ncount_up_to(\n    limit\n)\n\n```\n\nIncrements this variable until it reaches `limit`. (deprecated)\nWhen that Op is run it tries to increment the variable by `1`. If incrementing the variable would bring it above `limit` then the Op raises the exception `OutOfRangeError`.\nIf no error is raised, the Op outputs the value of the variable before the increment.\nThis is essentially a shortcut for `count_up_to(self, limit)`.\nArgs  \n---  \n`limit` |  value at which incrementing the variable raises an error.   \nReturns  \n---  \nA `Tensor` that will hold the variable value before the increment. If no other Op modifies this variable, the values produced will all be distinct.   \n### `eval`\n```\neval(\n    session=None\n)\n\n```\n\nIn a session, computes and returns the value of this variable.\nThis is not a graph construction method, it does not add ops to the graph.\nThis convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) for more information on launching a graph and on sessions.\n```\nv = tf.Variable([1, 2])\ninit = tf.compat.v1.global_variables_initializer()\n\nwith tf.compat.v1.Session() as sess:\n    sess.run(init)\n    # Usage passing the session explicitly.\n    print(v.eval(sess))\n    # Usage with the default session.  The 'with' block\n    # above makes 'sess' the default session.\n    print(v.eval())\n\n```\nArgs  \n---  \n`session` |  The session to use to evaluate this variable. If none, the default session is used.   \nReturns  \n---  \nA numpy `ndarray` with a copy of the value of this variable.   \n### `experimental_ref`\n```\nexperimental_ref()\n\n```\n\nDEPRECATED FUNCTION\n### `from_proto`\n```\n@staticmethod\nfrom_proto(\n    variable_def, import_scope=None\n)\n\n```\n\nReturns a `Variable` object created from `variable_def`.\n### `gather_nd`\n```\ngather_nd(\n    indices, name=None\n)\n\n```\n\nGather slices from `params` into a Tensor with shape specified by `indices`.\nSee tf.gather_nd for details.\nArgs  \n---  \n`indices` |  A `Tensor`. Must be one of the following types: `int32`, `int64`. Index tensor.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `params`.   \n### `get_shape`\n```\nget_shape() -> [tf.TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape)\n\n```\n\nAlias of [`Variable.shape`](https://www.tensorflow.org/api_docs/python/tf/Variable#shape).\n### `initialized_value`\n```\ninitialized_value()\n\n```\n\nReturns the value of the initialized variable. (deprecated)\nYou should use this instead of the variable itself to initialize another variable with a value that depends on the value of this variable.\n```\n# Initialize 'v' with a random tensor.\nv = tf.Variable(tf.random.truncated_normal([10, 40]))\n# Use `initialized_value` to guarantee that `v` has been\n# initialized before its value is used to initialize `w`.\n# The random values are picked only once.\nw = tf.Variable(v.initialized_value() * 2.0)\n\n```\nReturns  \n---  \nA `Tensor` holding the value of this variable after its initializer has run.   \n### `load`\n```\nload(\n    value, session=None\n)\n\n```\n\nLoad new value into this variable. (deprecated)\nWrites new value to variable's memory. Doesn't add ops to the graph.\nThis convenience method requires a session where the graph containing this variable has been launched. If no session is passed, the default session is used. See [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) for more information on launching a graph and on sessions.\n```\nv = tf.Variable([1, 2])\ninit = tf.compat.v1.global_variables_initializer()\n\nwith tf.compat.v1.Session() as sess:\n    sess.run(init)\n    # Usage passing the session explicitly.\n    v.load([2, 3], sess)\n    print(v.eval(sess)) # prints [2 3]\n    # Usage with the default session.  The 'with' block\n    # above makes 'sess' the default session.\n    v.load([3, 4], sess)\n    print(v.eval()) # prints [3 4]\n\n```\nArgs  \n---  \n`value` |  New variable value   \n`session` |  The session to use to evaluate this variable. If none, the default session is used.   \nRaises  \n---  \n`ValueError` |  Session is not passed and no default session   \n### `read_value`\n```\nread_value()\n\n```\n\nReturns the value of this variable, read in the current context.\nCan be different from value() if it's on another device, with control dependencies, etc.\nReturns  \n---  \nA `Tensor` containing the value of the variable.   \n### `ref`\n```\nref()\n\n```\n\nReturns a hashable reference object to this Variable.\nThe primary use case for this API is to put variables in a set/dictionary. We can't put variables in a set/dictionary as `variable.__hash__()` is no longer available starting Tensorflow 2.0.\nThe following will raise an exception starting 2.0\n```\nx = tf.Variable(5)\ny = tf.Variable(10)\nz = tf.Variable(10)\nvariable_set = {x, y, z}\nTraceback (most recent call last):\nTypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\nvariable_dict = {x: 'five', y: 'ten'}\nTraceback (most recent call last):\nTypeError: Variable is unhashable. Instead, use tensor.ref() as the key.\n```\n\nInstead, we can use `variable.ref()`.\n```\nvariable_set = {x.ref(), y.ref(), z.ref()}\nx.ref() in variable_set\nTrue\nvariable_dict = {x.ref(): 'five', y.ref(): 'ten', z.ref(): 'ten'}\nvariable_dict[y.ref()]\n'ten'\n```\n\nAlso, the reference object provides `.deref()` function that returns the original Variable.\n```\nx = tf.Variable(5)\nx.ref().deref()\n<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n```\n\n### `scatter_add`\n```\nscatter_add(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nAdds [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to this variable.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to be added to this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_div`\n```\nscatter_div(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nDivide this variable by [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to divide this variable by.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_max`\n```\nscatter_max(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nUpdates this variable with the max of [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) and itself.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to use as an argument of max with this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_min`\n```\nscatter_min(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nUpdates this variable with the min of [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) and itself.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to use as an argument of min with this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_mul`\n```\nscatter_mul(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nMultiply this variable by [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to multiply this variable by.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_nd_add`\n```\nscatter_nd_add(\n    indices, updates, name=None\n)\n\n```\n\nApplies sparse addition to individual values or slices in a Variable.\nThe Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n`indices` must be integer tensor, containing indices into self. It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\nThe innermost dimension of `indices` (with length `K`) corresponds to indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th dimension of self.\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n```\n[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n\n```\n\nFor example, say we want to add 4 scattered elements to a rank-1 tensor to 8 elements. In Python, that update would look like this:\n```\n    v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n    indices = tf.constant([[4], [3], [1] ,[7]])\n    updates = tf.constant([9, 10, 11, 12])\n    v.scatter_nd_add(indices, updates)\n    print(v)\n\n```\n\nThe resulting update to v would look like this:\n```\n[1, 13, 3, 14, 14, 6, 7, 20]\n\n```\n\nSee [`tf.scatter_nd`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) for more details about how to make updates to slices.\nArgs  \n---  \n`indices` |  The indices to be used in the operation.   \n`updates` |  The values to be used in the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \n### `scatter_nd_sub`\n```\nscatter_nd_sub(\n    indices, updates, name=None\n)\n\n```\n\nApplies sparse subtraction to individual values or slices in a Variable.\nAssuming the variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n`indices` must be integer tensor, containing indices into self. It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\nThe innermost dimension of `indices` (with length `K`) corresponds to indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th dimension of self.\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n```\n[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n\n```\n\nFor example, say we want to add 4 scattered elements to a rank-1 tensor to 8 elements. In Python, that update would look like this:\n```\n    v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n    indices = tf.constant([[4], [3], [1] ,[7]])\n    updates = tf.constant([9, 10, 11, 12])\n    v.scatter_nd_sub(indices, updates)\n    print(v)\n\n```\n\nAfter the update `v` would look like this:\n```\n[1, -9, 3, -6, -4, 6, 7, -4]\n\n```\n\nSee [`tf.scatter_nd`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) for more details about how to make updates to slices.\nArgs  \n---  \n`indices` |  The indices to be used in the operation.   \n`updates` |  The values to be used in the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \n### `scatter_nd_update`\n```\nscatter_nd_update(\n    indices, updates, name=None\n)\n\n```\n\nApplies sparse assignment to individual values or slices in a Variable.\nThe Variable has rank `P` and `indices` is a `Tensor` of rank `Q`.\n`indices` must be integer tensor, containing indices into self. It must be shape `[d_0, ..., d_{Q-2}, K]` where `0 < K <= P`.\nThe innermost dimension of `indices` (with length `K`) corresponds to indices into elements (if `K = P`) or slices (if `K < P`) along the `K`th dimension of self.\n`updates` is `Tensor` of rank `Q-1+P-K` with shape:\n```\n[d_0, ..., d_{Q-2}, self.shape[K], ..., self.shape[P-1]].\n\n```\n\nFor example, say we want to add 4 scattered elements to a rank-1 tensor to 8 elements. In Python, that update would look like this:\n```\n    v = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])\n    indices = tf.constant([[4], [3], [1] ,[7]])\n    updates = tf.constant([9, 10, 11, 12])\n    v.scatter_nd_update(indices, updates)\n    print(v)\n\n```\n\nThe resulting update to v would look like this:\n```\n[1, 11, 3, 10, 9, 6, 7, 12]\n\n```\n\nSee [`tf.scatter_nd`](https://www.tensorflow.org/api_docs/python/tf/scatter_nd) for more details about how to make updates to slices.\nArgs  \n---  \n`indices` |  The indices to be used in the operation.   \n`updates` |  The values to be used in the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \n### `scatter_sub`\n```\nscatter_sub(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nSubtracts [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) from this variable.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to be subtracted from this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `scatter_update`\n```\nscatter_update(\n    sparse_delta, use_locking=False, name=None\n)\n\n```\n\nAssigns [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to this variable.\nArgs  \n---  \n`sparse_delta` |  [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) to be assigned to this variable.   \n`use_locking` |  If `True`, use locking during the operation.   \n`name` |  the name of the operation.   \nReturns  \n---  \nThe updated variable.   \nRaises  \n---  \n`TypeError` |  if `sparse_delta` is not an `IndexedSlices`.   \n### `set_shape`\n```\nset_shape(\n    shape\n)\n\n```\n\nOverrides the shape for this variable.\nArgs  \n---  \n`shape` |  the `TensorShape` representing the overridden shape.   \n### `sparse_read`\n```\nsparse_read(\n    indices, name=None\n)\n\n```\n\nGather slices from params axis axis according to indices.\nThis function supports a subset of tf.gather, see tf.gather for details on usage.\nArgs  \n---  \n`indices` |  The index `Tensor`. Must be one of the following types: `int32`, `int64`. Must be in range `[0, params.shape[axis])`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `params`.   \n### `to_proto`\n```\nto_proto(\n    export_scope=None\n)\n\n```\n\nConverts a `Variable` to a `VariableDef` protocol buffer.\nArgs  \n---  \n`export_scope` |  Optional `string`. Name scope to remove.   \nReturns  \n---  \nA `VariableDef` protocol buffer, or `None` if the `Variable` is not in the specified name scope.   \n### `value`\n```\nvalue()\n\n```\n\nReturns the last snapshot of this variable.\nYou usually do not need to call this method as all ops that need the value of the variable call it automatically through a `convert_to_tensor()` call.\nReturns a `Tensor` which holds the value of the variable. You can not assign a new value to this tensor as it is not a reference to the variable.\nTo avoid copies, if the consumer of the returned value is on the same device as the variable, this actually returns the live value of the variable, not a copy. Updates to the variable are seen by the consumer. If the consumer is on a different device it will get a copy of the variable.\nReturns  \n---  \nA `Tensor` containing the value of the variable.   \n### `__abs__`\n```\n__abs__(\n    name=None\n)\n\n```\n\n### `__add__`\n```\n__add__(\n    y\n)\n\n```\n\n### `__and__`\n```\n__and__(\n    y\n)\n\n```\n\n### `__div__`\n```\n__div__(\n    y\n)\n\n```\n\n### `__eq__`\n```\n__eq__(\n    other\n)\n\n```\n\nCompares two variables element-wise for equality.\n### `__floordiv__`\n```\n__floordiv__(\n    y\n)\n\n```\n\n### `__ge__`\n```\n__ge__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x >= y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5, 2, 5, 10])\ntf.math.greater_equal(x, y) ==> [True, True, True, False]\n\nx = tf.constant([5, 4, 6, 7])\ny = tf.constant([5])\ntf.math.greater_equal(x, y) ==> [True, False, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__getitem__`\n```\n__getitem__(\n    slice_spec\n)\n\n```\n\nCreates a slice helper object given a variable.\nThis allows creating a sub-tensor from part of the current contents of a variable. See [`tf.Tensor.**getitem**`](https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__)for detailed examples of slicing.\nThis function in addition also allows assignment to a sliced range. This is similar to `__setitem__` functionality in Python. However, the syntax is different so that the user can capture the assignment operation for grouping or passing to `sess.run()` in TF1. For example,\n```\nimporttensorflowastf\nA = tf.Variable([[1,2,3], [4,5,6], [7,8,9]], dtype=tf.float32)\nprint(A[:2, :2])  # => [[1,2], [4,5]]\n\nA[:2,:2].assign(22. * tf.ones((2, 2))))\nprint(A) # => [[22, 22, 3], [22, 22, 6], [7,8,9]]\n\n```\n\nNote that assignments currently do not support NumPy broadcasting semantics.\nArgs  \n---  \n`var` |  An `ops.Variable` object.   \n`slice_spec` |  The arguments to [`Tensor.**getitem**`](https://www.tensorflow.org/api_docs/python/tf/Tensor#__getitem__).  \nReturns  \n---  \nThe appropriate slice of \"tensor\", based on \"slice_spec\". As an operator. The operator also has a `assign()` method that can be used to generate an assignment operator.   \nRaises  \n---  \n`ValueError` |  If a slice range is negative size.   \n`TypeError` |  TypeError: If the slice indices aren't int, slice, ellipsis, tf.newaxis or int32/int64 tensors.   \n### `__gt__`\n```\n__gt__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x > y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 2, 5])\ntf.math.greater(x, y) ==> [False, True, True]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.greater(x, y) ==> [False, False, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__invert__`\n```\n__invert__(\n    name=None\n)\n\n```\n\n### `__iter__`\n```\n__iter__()\n\n```\n\nWhen executing eagerly, iterates over the value of the variable.\n### `__le__`\n```\n__le__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x <= y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less_equal(x, y) ==> [True, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 6])\ntf.math.less_equal(x, y) ==> [True, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__lt__`\n```\n__lt__(\n    y: Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)],\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nReturns the truth value of (x < y) element-wise.\n#### Example:\n```\nx = tf.constant([5, 4, 6])\ny = tf.constant([5])\ntf.math.less(x, y) ==> [False, True, False]\n\nx = tf.constant([5, 4, 6])\ny = tf.constant([5, 6, 7])\ntf.math.less(x, y) ==> [False, True, True]\n\n```\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `int64`, `bfloat16`, `uint16`, `half`, `uint32`, `uint64`.   \nA `Tensor`. Must have the same type as `x`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor` of type `bool`.   \n### `__matmul__`\n```\n__matmul__(\n    y\n)\n\n```\n\n### `__mod__`\n```\n__mod__(\n    y\n)\n\n```\n\n### `__mul__`\n```\n__mul__(\n    y\n)\n\n```\n\n### `__ne__`\n```\n__ne__(\n    other\n)\n\n```\n\nCompares two variables element-wise for equality.\n### `__neg__`\n```\n__neg__(\n    name=None\n) -> Annotated[Any, [tf.raw_ops.Any](https://www.tensorflow.org/api_docs/python/tf/raw_ops/Any)]\n\n```\n\nComputes numerical negative value element-wise.\nI.e., \\\\(y = -x\\\\).\nArgs  \n---  \nA `Tensor`. Must be one of the following types: `bfloat16`, `half`, `float32`, `float64`, `int8`, `int16`, `int32`, `int64`, `complex64`, `complex128`.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `x`.   \n### `__or__`\n```\n__or__(\n    y\n)\n\n```\n\n### `__pow__`\n```\n__pow__(\n    y\n)\n\n```\n\n### `__radd__`\n```\n__radd__(\n    x\n)\n\n```\n\n### `__rand__`\n```\n__rand__(\n    x\n)\n\n```\n\n### `__rdiv__`\n```\n__rdiv__(\n    x\n)\n\n```\n\n### `__rfloordiv__`\n```\n__rfloordiv__(\n    x\n)\n\n```\n\n### `__rmatmul__`\n```\n__rmatmul__(\n    x\n)\n\n```\n\n### `__rmod__`\n```\n__rmod__(\n    x\n)\n\n```\n\n### `__rmul__`\n```\n__rmul__(\n    x\n)\n\n```\n\n### `__ror__`\n```\n__ror__(\n    x\n)\n\n```\n\n### `__rpow__`\n```\n__rpow__(\n    x\n)\n\n```\n\n### `__rsub__`\n```\n__rsub__(\n    x\n)\n\n```\n\n### `__rtruediv__`\n```\n__rtruediv__(\n    x\n)\n\n```\n\n### `__rxor__`\n```\n__rxor__(\n    x\n)\n\n```\n\n### `__sub__`\n```\n__sub__(\n    y\n)\n\n```\n\n### `__truediv__`\n```\n__truediv__(\n    y\n)\n\n```\n\n### `__xor__`\n```\n__xor__(\n    y\n)\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/distribute/experimental/ParameterServerStrategy": "An asynchronous multi-worker parameter server tf.distribute strategy.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/distribute/Strategy)\n```\ntf.compat.v1.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver=None\n)\n\n```\n\nThis strategy requires two roles: workers and parameter servers. Variables and updates to those variables will be assigned to parameter servers and other operations are assigned to workers.\nWhen each worker has more than one GPU, operations will be replicated on all GPUs. Even though operations may be replicated, variables are not and each worker shares a common view for which parameter server a variable is assigned to.\nBy default it uses `TFConfigClusterResolver` to detect configurations for multi-worker training. This requires a 'TF_CONFIG' environment variable and the 'TF_CONFIG' must have a cluster spec.\nThis class assumes each worker is running the same code independently, but parameter servers are running a standard server. This means that while each worker will synchronously compute a single gradient update across all GPUs, updates between workers proceed asynchronously. Operations that occur only on the first replica (such as incrementing the global step), will occur on the first replica _of every worker_.\nIt is expected to call `call_for_each_replica(fn, ...)` for any operations which potentially can be replicated across replicas (i.e. multiple GPUs) even if there is only CPU or one GPU. When defining the `fn`, extra caution needs to be taken:\n1) It is generally not recommended to open a device scope under the strategy's scope. A device scope (i.e. calling [`tf.device`](https://www.tensorflow.org/api_docs/python/tf/device)) will be merged with or override the device for operations but will not change the device for variables.\n2) It is also not recommended to open a colocation scope (i.e. calling [`tf.compat.v1.colocate_with`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/colocate_with)) under the strategy's scope. For colocating variables, use `strategy.extended.colocate_vars_with` instead. Colocation of ops will possibly create device assignment conflicts.\n#### For Example:\n```\nstrategy = tf.distribute.experimental.ParameterServerStrategy()\nrun_config = tf.estimator.RunConfig(\n    experimental_distribute.train_distribute=strategy)\nestimator = tf.estimator.Estimator(config=run_config)\ntf.estimator.train_and_evaluate(estimator,...)\n\n```\n\n## Args  \n---  \n`cluster_resolver` |  Optional [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) object. Defaults to a [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver).   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.In general, when using a multi-worker [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), there is a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) must set the relevant attribute, or override this property; otherwise, `None` is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver), and in those cases this property will return `None`. The [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, ```\n\nos.environ['TF_CONFIG'] = json.dumps({\n  'cluster': {\n      'worker': [\"localhost:12345\", \"localhost:23456\"],\n      'ps': [\"localhost:34567\"]\n  },\n  'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. Since we set this\n  # as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. Since we\n  # set this as a worker above, this block will not run on this particular\n  # instance.\n\n```\nFor more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s API docstring.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `experimental_make_numpy_dataset`\n```\nexperimental_make_numpy_dataset(\n    numpy_input, session=None\n)\n\n```\n\nMakes a tf.data.Dataset for input provided via a numpy array.\nThis avoids adding `numpy_input` as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.\nNote that you will likely need to use tf.distribute.Strategy.experimental_distribute_dataset with the returned dataset to further distribute it with the strategy.\n#### Example:\n```\nnumpy_input = np.ones([10], dtype=np.float32)\ndataset = strategy.experimental_make_numpy_dataset(numpy_input)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n```\n\nArgs  \n---  \n`numpy_input` |  A nest of NumPy input arrays that will be converted into a dataset. Note that lists of Numpy arrays are stacked, as that is normal [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) behavior.   \n`session` |  (TensorFlow v1.x graph execution only) A session used for initialization.   \nReturns  \n---  \nA [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) representing `numpy_input`.   \n### `experimental_run`\n```\nexperimental_run(\n    fn, input_iterator=None\n)\n\n```\n\nRuns ops in `fn` on each replica, with inputs from `input_iterator`. (deprecated)\nWhen eager execution is enabled, executes ops specified by `fn` on each replica. Otherwise, builds a graph to execute the ops on each replica.\nEach replica will take a single, different input from the inputs provided by one `get_next` call on the input iterator.\n`fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `replica_id_in_sync_group`.\nArgs  \n---  \n`fn` |  The function to run. The inputs to the function must match the outputs of `input_iterator.get_next()`. The output must be a [`tf.nest`](https://www.tensorflow.org/api_docs/python/tf/nest) of `Tensor`s.   \n`input_iterator` |  (Optional) input iterator from which the inputs are taken.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be `PerReplica` (if the values are unsynchronized), `Mirrored` (if the values are kept in sync), or `Tensor` (if running on a single replica).   \n### `make_dataset_iterator`\n```\nmake_dataset_iterator(\n    dataset\n)\n\n```\n\nMakes an iterator for input provided via `dataset`.\nData from the given dataset will be distributed evenly across all the compute replicas. We will assume that the input dataset is batched by the global batch size. With this assumption, we will make a best effort to divide each batch across all the replicas (one or more workers). If this effort fails, an error will be thrown, and the user should instead use `make_input_fn_iterator` which provides more control to the user, and does not try to divide a batch across replicas.\nThe user could also use `make_input_fn_iterator` if they want to customize which input is fed to which replica/worker etc.\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be distributed evenly across all replicas.   \nReturns  \n---  \nAn `tf.distribute.InputIterator` which returns inputs for each step of the computation. User should call `initialize` on the returned iterator.   \n### `make_input_fn_iterator`\n```\nmake_input_fn_iterator(\n    input_fn,\n    replication_mode=[tf.distribute.InputReplicationMode.PER_WORKER](https://www.tensorflow.org/api_docs/python/tf/distribute/InputReplicationMode#PER_WORKER)\n)\n\n```\n\nReturns an iterator split across replicas created from an input function.\nThe `input_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) object where information about batching and input sharding can be accessed:\n```\ndefinput_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(input_context.num_input_pipelines,\n                 input_context.input_pipeline_id)\nwith strategy.scope():\n  iterator = strategy.make_input_fn_iterator(input_fn)\n  replica_results = strategy.experimental_run(replica_fn, iterator)\n\n```\n\nThe [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) returned by `input_fn` should have a per-replica batch size, which may be computed using `input_context.get_per_replica_batch_size`.\nArgs  \n---  \n`input_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) object and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`replication_mode` |  an enum value of [`tf.distribute.InputReplicationMode`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputReplicationMode). Only `PER_WORKER` is supported currently, which means there will be a single call to `input_fn` per worker. Replicas will dequeue from the local [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) on their worker.   \nReturns  \n---  \nAn iterator object that should first be `.initialize()`-ed. It may then either be passed to `strategy.experimental_run()` or you can `iterator.get_next()` to get the next value to pass to `strategy.extended.call_for_each_replica()`.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis=None\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nInvokes `fn` on each replica, with the given arguments.\nThis method is the primary way to distribute your computation with a tf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` is invoked under a replica context. `fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`. Please see the module-level docstring of tf.distribute for the concept of replica context.\nAll arguments in `args` or `kwargs` can be a nested structure of tensors, e.g. a list of tensors, in which case `args` and `kwargs` will be passed to the `fn` invoked on each replica. Or `args` or `kwargs` can be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors, i.e. [`tf.compat.v1.TensorInfo.CompositeTensor`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor), in which case each `fn` call will get the component of a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) corresponding to its replica. Note that arbitrary Python values that are not of the types above are not supported.\n#### Example usage:\n  1. Constant tensor input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndefreplica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n      1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n\n\n```\n\n  2. DistributedValues input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  defreplica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n    <tf.Tensor: shape=(), dtype=int32, numpy=4>\n\n```\n\n  3. Use [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) to allreduce values.\n```\nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndefrun():\n   defvalue_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   defreplica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\n         \"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n      1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n```\n\n\n\nArgs  \n---  \n`fn` |  The function to run on each replica.   \n`args` |  Optional positional arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`kwargs` |  Optional keyword arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`options` |  An optional instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\n\nReturns  \n---  \nA context manager.   \n### `update_config_proto`\n```\nupdate_config_proto(\n    config_proto\n)\n\n```\n\nReturns a copy of `config_proto` modified for use with this strategy.\nThe updated config has something needed to run a strategy, e.g. configuration to run collective ops, or device filters to improve distributed training performance.\nArgs  \n---  \n`config_proto` |  a `tf.ConfigProto` object.   \nReturns  \n---  \nThe updated copy of the `config_proto`. \n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/tag_constants": "Public API for tf._api.v2.saved_model.tag_constants namespace  \n## Other Members  \n---  \nGPU  |  `'gpu'`  \nSERVING  |  `'serve'`  \nTPU  |  `'tpu'`  \nTRAINING  |  `'train'`\n",
  "https://www.tensorflow.org/api_docs/python/tf/constant": "Creates a constant tensor from a tensor-like object.  \n```\ntf.constant(\n    value, dtype=None, shape=None, name='Const'\n) -> Union[[tf.Operation](https://www.tensorflow.org/api_docs/python/tf/Operation), ops._EagerTensorBase]\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n  * [TF-NumPy Type Promotion](https://www.tensorflow.org/guide/tf_numpy_type_promotion)\n  * [Introduction to Tensors](https://www.tensorflow.org/guide/tensor)\n  * [Introduction to graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs)\n  * [Migrate `tf.feature_column`s to Keras preprocessing layers](https://www.tensorflow.org/guide/migrate/migrating_feature_columns)\n\n| \n  * [Playing CartPole with the Actor-Critic method](https://www.tensorflow.org/tutorials/reinforcement_learning/actor_critic)\n  * [Neural style transfer](https://www.tensorflow.org/tutorials/generative/style_transfer)\n  * [Custom training loop with Keras and MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n\n  \nIf the argument `dtype` is not specified, then the type is inferred from the type of `value`.\n```\n# Constant 1-D Tensor from a python list.\ntf.constant([1, 2, 3, 4, 5, 6])\n<tf.Tensor: shape=(6,), dtype=int32,\n    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n# Or a numpy array\na = np.array([[1, 2, 3], [4, 5, 6]])\ntf.constant(a)\n<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n  array([[1, 2, 3],\n         [4, 5, 6]])>\n```\n\nIf `dtype` is specified, the resulting tensor values are cast to the requested `dtype`.\n```\ntf.constant([1, 2, 3, 4, 5, 6], dtype=tf.float64)\n<tf.Tensor: shape=(6,), dtype=float64,\n    numpy=array([1., 2., 3., 4., 5., 6.])>\n```\n\nIf `shape` is set, the `value` is reshaped to match. Scalars are expanded to fill the `shape`:\n```\ntf.constant(0, shape=(2, 3))\n  <tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[0, 0, 0],\n         [0, 0, 0]], dtype=int32)>\ntf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])\n<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n  array([[1, 2, 3],\n         [4, 5, 6]], dtype=int32)>\n```\n\n[`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) has no effect if an eager Tensor is passed as the `value`, it even transmits gradients:\n```\nv = tf.Variable([0.0])\nwith tf.GradientTape() as g:\n    loss = tf.constant(v + v)\ng.gradient(loss, v).numpy()\narray([2.], dtype=float32)\n```\n\nBut, since [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) embeds the value in the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) this fails for symbolic tensors:\n```\nwith tf.compat.v1.Graph().as_default():\n  i = tf.compat.v1.placeholder(shape=[None, None], dtype=tf.float32)\n  t = tf.constant(i)\nTraceback (most recent call last):\nTypeError: ...\n```\n\n[`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) will create tensors on the current device. Inputs which are already tensors maintain their placements unchanged.\n#### Related Ops:\n  * [`tf.convert_to_tensor`](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor) is similar but:\n    * It has no `shape` argument.\n    * Symbolic tensors are allowed to pass through.\n```\nwith tf.compat.v1.Graph().as_default():\n  i = tf.compat.v1.placeholder(shape=[None, None], dtype=tf.float32)\n  t = tf.convert_to_tensor(i)\n\n```\n\n  * [`tf.fill`](https://www.tensorflow.org/api_docs/python/tf/fill): differs in a few ways:\n    * [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) supports arbitrary constants, not just uniform scalar Tensors like [`tf.fill`](https://www.tensorflow.org/api_docs/python/tf/fill).\n    * [`tf.fill`](https://www.tensorflow.org/api_docs/python/tf/fill) creates an Op in the graph that is expanded at runtime, so it can efficiently represent large tensors.\n    * Since [`tf.fill`](https://www.tensorflow.org/api_docs/python/tf/fill) does not embed the value, it can produce dynamically sized outputs.\n\n\n## Args  \n---  \n`value` |  A constant value (or list) of output type `dtype`.   \n`dtype` |  The type of the elements of the resulting tensor.   \n`shape` |  Optional dimensions of resulting tensor.   \n`name` |  Optional name for the tensor.   \n## Returns  \n---  \nA Constant Tensor.   \n## Raises  \n---  \n`TypeError` |  if shape is incorrectly specified or unsupported.   \n`ValueError` |  if called on a symbolic tensor. \n",
  "https://www.tensorflow.org/api_docs/python/tf/data": "Public API for tf._api.v2.data namespace  \n## Modules\n[`experimental`](https://www.tensorflow.org/api_docs/python/tf/data/experimental) module: Public API for tf._api.v2.data.experimental namespace\n## Classes\n[`class Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset): Represents a potentially large set of elements.\n[`class DatasetSpec`](https://www.tensorflow.org/api_docs/python/tf/data/DatasetSpec): Type specification for [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n[`class FixedLengthRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/FixedLengthRecordDataset): A `Dataset` of fixed-length records from one or more binary files.\n[`class Iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator): Represents an iterator of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n[`class IteratorSpec`](https://www.tensorflow.org/api_docs/python/tf/data/IteratorSpec): Type specification for [`tf.data.Iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator).\n[`class NumpyIterator`](https://www.tensorflow.org/api_docs/python/tf/data/NumpyIterator): Iterator over a dataset with elements converted to numpy.\n[`class Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options): Represents options for [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n[`class TFRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset): A `Dataset` comprising records from one or more TFRecord files.\n[`class TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset): Creates a `Dataset` comprising lines from one or more text files.\n[`class ThreadingOptions`](https://www.tensorflow.org/api_docs/python/tf/data/ThreadingOptions): Represents options for dataset threading.\n## Other Members  \n---  \nAUTOTUNE  |  `-1`  \nINFINITE_CARDINALITY  |  `-1`  \nUNKNOWN_CARDINALITY  |  `-2`\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_def_utils": "Public API for tf._api.v2.saved_model.signature_def_utils namespace\n## Classes\n[`class MethodNameUpdater`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_def_utils/MethodNameUpdater): Updates the method name(s) of the SavedModel stored in the given path.\n## Functions\n[`build_signature_def(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/build_signature_def): Utility function to build a SignatureDef protocol buffer.\n[`classification_signature_def(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/classification_signature_def): Creates classification signature from given examples and predictions.\n[`is_valid_signature(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/is_valid_signature): Determine whether a SignatureDef can be served by TensorFlow Serving.\n[`predict_signature_def(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/predict_signature_def): Creates prediction signature from given inputs and outputs.\n[`regression_signature_def(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/regression_signature_def): Creates regression signature from given examples and predictions.\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/signature_constants": "Public API for tf._api.v2.saved_model.signature_constants namespace  \n## Other Members  \n---  \nCLASSIFY_INPUTS  |  `'inputs'`  \nCLASSIFY_METHOD_NAME  |  `'tensorflow/serving/classify'`  \nCLASSIFY_OUTPUT_CLASSES  |  `'classes'`  \nCLASSIFY_OUTPUT_SCORES  |  `'scores'`  \nDEFAULT_SERVING_SIGNATURE_DEF_KEY  |  `'serving_default'`  \nPREDICT_INPUTS  |  `'inputs'`  \nPREDICT_METHOD_NAME  |  `'tensorflow/serving/predict'`  \nPREDICT_OUTPUTS  |  `'outputs'`  \nREGRESS_INPUTS  |  `'inputs'`  \nREGRESS_METHOD_NAME  |  `'tensorflow/serving/regress'`  \nREGRESS_OUTPUTS  |  `'outputs'`\n",
  "https://www.tensorflow.org/api_docs/python/tf/data/Dataset": "Represents a potentially large set of elements.\n```\ntf.data.Dataset(\n    variant_tensor\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n  * [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance)\n  * [Migrate from Estimator to Keras APIs](https://www.tensorflow.org/guide/migrate/migrating_estimator)\n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n| \n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  * [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training)\n  * [pix2pix: Image-to-image translation with a conditional GAN](https://www.tensorflow.org/tutorials/generative/pix2pix)\n\n  \nThe [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) API supports writing descriptive and efficient input pipelines. `Dataset` usage follows a common pattern:\n  1. Create a source dataset from your input data.\n  2. Apply dataset transformations to preprocess the data.\n  3. Iterate over the dataset and process the elements.\n\n\nIteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n#### Source Datasets:\nThe simplest way to create a dataset is to create it from a python `list`:\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n```\n\nTo process lines from files, use [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset):\n```\ndataset = tf.data.TextLineDataset([\"file1.txt\", \"file2.txt\"])\n```\n\nTo process records written in the `TFRecord` format, use `TFRecordDataset`:\n```\ndataset = tf.data.TFRecordDataset([\"file1.tfrecords\", \"file2.tfrecords\"])\n```\n\nTo create a dataset of all files matching a pattern, use [`tf.data.Dataset.list_files`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#list_files):\n```\ndataset = tf.data.Dataset.list_files(\"/path/*.txt\")\n\n```\n\nSee [`tf.data.FixedLengthRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/FixedLengthRecordDataset) and [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) for more ways to create datasets.\n#### Transformations:\nOnce you have a dataset, you can apply transformations to prepare the data for your model:\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.map(lambda x: x*2)\nlist(dataset.as_numpy_iterator())\n[2, 4, 6]\n```\n\n#### Common Terms:\n**Element** : A single output from calling `next()` on a dataset iterator. Elements may be nested structures containing multiple components. For example, the element `(1, (3, \"apple\"))` has one tuple nested in another tuple. The components are `1`, `3`, and `\"apple\"`.\n**Component** : The leaf in the nested structure of an element.\n#### Supported types:\nElements can be nested structures of tuples, named tuples, and dictionaries. Note that Python lists are _not_ treated as nested structures of components. Instead, lists are converted to tensors and treated as components. For example, the element `(1, [1, 2, 3])` has only two components; the tensor `1` and the tensor `[1, 2, 3]`. Element components can be of any type representable by [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec), including [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), and [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray).\n```\na = 1 # Integer element\nb = 2.0 # Float element\nc = (1, 2) # Tuple element with 2 components\nd = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\nPoint = collections.namedtuple(\"Point\", [\"x\", \"y\"])\ne = Point(1, 2) # Named tuple\nf = tf.data.Dataset.range(10) # Dataset element\n\n```\n\nFor more information, read [this guide](https://www.tensorflow.org/guide/data).\n## Args  \n---  \n`variant_tensor` |  A DT_VARIANT tensor that represents the dataset.   \n## Attributes  \n---  \n`element_spec` |  The type specification of an element of this dataset.```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n```\nFor more information, read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).   \n## Methods\n### `apply`\n```\napply(\n    transformation_func\n) -> 'DatasetV2'\n\n```\n\nApplies a transformation function to this dataset.\n`apply` enables chaining of custom `Dataset` transformations, which are represented as functions that take one `Dataset` argument and return a transformed `Dataset`.\n```\ndataset = tf.data.Dataset.range(100)\ndefdataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`transformation_func` |  A function that takes one `Dataset` argument and returns a `Dataset`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `as_numpy_iterator`\n```\nas_numpy_iterator()\n\n```\n\nReturns an iterator which converts all elements of the dataset to numpy.\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using `as_numpy_iterator`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n```\n\nThis method requires that you are running in eager mode and the dataset's element_spec contains only `TensorSpec` components.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n\n\n\n```\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n```\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n```\ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n```\n\nReturns  \n---  \nAn iterable over the elements of the dataset, with their tensors converted to numpy arrays.   \nRaises  \n---  \n`TypeError` |  if an element contains a non-`Tensor` value.   \n`RuntimeError` |  if eager execution is not enabled.   \n### `batch`\n```\nbatch(\n    batch_size,\n    drop_remainder=False,\n    num_parallel_calls=None,\n    deterministic=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into batches.\n```\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n```\n```\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n\nThe components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`num_parallel_calls` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of batches to compute asynchronously in parallel. If not specified, batches will be computed sequentially. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available resources.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `bucket_by_sequence_length`\n```\nbucket_by_sequence_length(\n    element_length_func,\n    bucket_boundaries,\n    bucket_batch_sizes,\n    padded_shapes=None,\n    padding_values=None,\n    pad_to_bucket_boundary=False,\n    no_padding=False,\n    drop_remainder=False,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that buckets elements in a `Dataset` by length.\nElements of the `Dataset` are grouped together by length and then are padded and batched.\nThis is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.\nBelow is an example to bucketize the input data to the 3 buckets \"[0, 3), [3, 5), [5, inf)\" based on sequence length, with batch size 2.\n```\nelements = [\n  [0], [1, 2, 3, 4], [5, 6, 7],\n  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, tf.int64, output_shapes=[None])\ndataset = dataset.bucket_by_sequence_length(\n        element_length_func=lambda elem: tf.shape(elem)[0],\n        bucket_boundaries=[3, 5],\n        bucket_batch_sizes=[2, 2, 2])\nfor elem in dataset.as_numpy_iterator():\n  print(elem)\n[[1 2 3 4]\n[5 6 7 0]]\n[[ 7  8  9 10 11  0]\n[13 14 15 16 19 20]]\n[[ 0  0]\n[21 22]]\n```\n\nArgs  \n---  \n`element_length_func` |  function from element in `Dataset` to [`tf.int32`](https://www.tensorflow.org/api_docs/python/tf#int32), determines the length of the element, which will determine the bucket it goes into.   \n`bucket_boundaries` |  `list<int>`, upper length boundaries of the buckets.   \n`bucket_batch_sizes` |  `list<int>`, batch size per bucket. Length should be `len(bucket_boundaries) + 1`.   \n`padded_shapes` |  Nested structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) to pass to [`tf.data.Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch). If not provided, will use `dataset.output_shapes`, which will result in variable length dimensions being padded out to the maximum length in each batch.   \n`padding_values` |  Values to pad with, passed to [`tf.data.Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch). Defaults to padding with 0.   \n`pad_to_bucket_boundary` |  bool, if `False`, will pad dimensions with unknown size to maximum length in batch. If `True`, will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source `Dataset` does not contain any elements with length longer than `max(bucket_boundaries)`.   \n`no_padding` |  `bool`, indicates whether to pad the batch features (features need to be either of type [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) or of same shape).   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  if `len(bucket_batch_sizes) != len(bucket_boundaries) + 1`.   \n### `cache`\n```\ncache(\n    filename='', name=None\n) -> 'DatasetV2'\n\n```\n\nCaches the elements in this dataset.\nThe first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n```\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n```\n\nWhen caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to `.cache()` will have no effect until the cache file is removed or the filename is changed.\n```\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")\nlist(dataset.as_numpy_iterator())\n# [0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file!\nlist(dataset.as_numpy_iterator())\n# [0, 1, 2, 3, 4]\n\n```\n\nArgs  \n---  \n`filename` |  A [`tf.string`](https://www.tensorflow.org/api_docs/python/tf#string) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `cardinality`\n```\ncardinality()\n\n```\n\nReturns the cardinality of the dataset, if known.\n`cardinality` may return [`tf.data.INFINITE_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#INFINITE_CARDINALITY) if the dataset contains an infinite number of elements or [`tf.data.UNKNOWN_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY) if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).\n```\ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n```\n\nReturns  \n---  \nA scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) `Tensor` representing the cardinality of the dataset. If the cardinality is infinite or unknown, `cardinality` returns the named constants [`tf.data.INFINITE_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#INFINITE_CARDINALITY) and [`tf.data.UNKNOWN_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY) respectively.   \n### `choose_from_datasets`\n```\n@staticmethod\nchoose_from_datasets(\n    datasets, choice_dataset, stop_on_empty_dataset=True\n) -> 'DatasetV2'\n\n```\n\nCreates a dataset that deterministically chooses elements from `datasets`.\nFor example, given the following datasets:\n```\ndatasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n            tf.data.Dataset.from_tensors(\"bar\").repeat(),\n            tf.data.Dataset.from_tensors(\"baz\").repeat()]\n\n# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\nchoice_dataset = tf.data.Dataset.range(3).repeat(3)\n\nresult = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n\n```\n\nThe elements of `result` will be:\n```\n\"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\n\n```\n\nArgs  \n---  \n`datasets` |  A non-empty list of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects with compatible structure.   \n`choice_dataset` |  A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) of scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) tensors between `0` and `len(datasets) - 1`.   \n`stop_on_empty_dataset` |  If `True`, selection stops if it encounters an empty dataset. If `False`, it skips empty datasets. It is recommended to set it to `True`. Otherwise, the selected elements start off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Defaults to `True`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`TypeError` |  If `datasets` or `choice_dataset` has the wrong type.   \n`ValueError` |  If `datasets` is empty.   \n### `concatenate`\n```\nconcatenate(\n    dataset, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n```\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have\n# compatible element specs.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n```\n\nArgs  \n---  \n`dataset` |  `Dataset` to be concatenated.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `counter`\n```\n@staticmethod\ncounter(\n    start=0,\n    step=1,\n    dtype=[tf.dtypes.int64](https://www.tensorflow.org/api_docs/python/tf/dtypes#int64),\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that counts from `start` in steps of size `step`.\nUnlike [`tf.data.Dataset.range`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#range), which stops at some ending number, [`tf.data.Dataset.counter`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#counter) produces elements indefinitely.\n```\ndataset = tf.data.experimental.Counter().take(5)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int64, name=None)\ndataset = tf.data.experimental.Counter(dtype=tf.int32)\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\ndataset = tf.data.experimental.Counter(start=2).take(5)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\ndataset = tf.data.experimental.Counter(start=2, step=5).take(5)\nlist(dataset.as_numpy_iterator())\n[2, 7, 12, 17, 22]\ndataset = tf.data.experimental.Counter(start=10, step=-1).take(5)\nlist(dataset.as_numpy_iterator())\n[10, 9, 8, 7, 6]\n```\n\nArgs  \n---  \n`start` |  (Optional.) The starting value for the counter. Defaults to 0.   \n`step` |  (Optional.) The step size for the counter. Defaults to 1.   \n`dtype` |  (Optional.) The data type for counter elements. Defaults to [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA `Dataset` of scalar `dtype` elements.   \n### `enumerate`\n```\nenumerate(\n    start=0, name=None\n) -> 'DatasetV2'\n\n```\n\nEnumerates the elements of this dataset.\nIt is similar to python's `enumerate`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n```\n```\n# The (nested) structure of the input dataset determines the\n# structure of elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n```\n\nArgs  \n---  \n`start` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the start value for enumeration.   \n`name` |  Optional. A name for the tf.data operations used by `enumerate`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `filter`\n```\nfilter(\n    predicate, name=None\n) -> 'DatasetV2'\n\n```\n\nFilters this dataset according to `predicate`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndeffilter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n```\n\nArgs  \n---  \n`predicate` |  A function mapping a dataset element to a boolean.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `fingerprint`\n```\nfingerprint()\n\n```\n\nComputes the fingerprint of this `Dataset`.\nIf two datasets have the same fingerprint, it is guaranteeed that they would produce identical elements as long as the content of the upstream input files does not change and they produce data deterministically.\nHowever, two datasets producing identical values does not always mean they would have the same fingerprint due to different graph constructs.\nIn other words, if two datasets have different fingerprints, they could still produce identical values.\nReturns  \n---  \nA scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) of type [`tf.uint64`](https://www.tensorflow.org/api_docs/python/tf#uint64).   \n### `flat_map`\n```\nflat_map(\n    map_func, name=None\n) -> 'DatasetV2'\n\n```\n\nMaps `map_func` across this dataset and flattens the result.\n#### The type signature is:\n```\ndefflat_map(\n  self: Dataset[T],\n  map_func: Callable[[T], Dataset[S]]\n) -> Dataset[S]\n\n```\n\nUse `flat_map` if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:\n```\ndataset = tf.data.Dataset.from_tensor_slices(\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n\n[`tf.data.Dataset.interleave()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) is a generalization of `flat_map`, since `flat_map` produces the same output as [`tf.data.Dataset.interleave(cycle_length=1)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave)\nArgs  \n---  \n`map_func` |  A function mapping a dataset element to a dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `from_generator`\n```\n@staticmethod\nfrom_generator(\n    generator,\n    output_types=None,\n    output_shapes=None,\n    args=None,\n    output_signature=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\nThe `generator` argument must be a callable object that returns an object that supports the `iter()` protocol (e.g. a generator function).\nThe elements generated by `generator` must be compatible with either the given `output_signature` argument or with the given `output_types` and (optionally) `output_shapes` arguments, whichever was specified.\nThe recommended way to call `from_generator` is to use the `output_signature` argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects from `output_signature` argument:\n```\ndefgen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n```\n\nThere is also a deprecated way to call `from_generator` by either with `output_types` argument alone or together with `output_shapes` argument. In this case the output of the function will be assumed to consist of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects with the types defined by `output_types` and with the shapes which are either unknown or defined by `output_shapes`.\nArgs  \n---  \n`generator` |  A callable object that returns an object that supports the `iter()` protocol. If `args` is not specified, `generator` must take no arguments; otherwise it must take as many arguments as there are values in `args`.   \n`output_types` |  (Optional.) A (nested) structure of [`tf.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) objects corresponding to each component of an element yielded by `generator`.   \n`output_shapes` |  (Optional.) A (nested) structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) objects corresponding to each component of an element yielded by `generator`.   \n`args` |  (Optional.) A tuple of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects that will be evaluated and passed to `generator` as NumPy-array arguments.   \n`output_signature` |  (Optional.) A (nested) structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects corresponding to each component of an element yielded by `generator`.   \n`name` |  (Optional.) A name for the tf.data operations used by `from_generator`.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `from_tensor_slices`\n```\n@staticmethod\nfrom_tensor_slices(\n    tensors, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` whose elements are slices of the given tensors.\nThe given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.\n```\n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n```\n```\n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n```\n```\n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n```\n```\n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n```\n```\n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n```\n\nNote that if `tensors` contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If `tensors` contains one or more large NumPy arrays, consider the alternative described in [this guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\nArgs  \n---  \n`tensors` |  A dataset element, whose components have the same first dimension. Supported values are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `from_tensors`\n```\n@staticmethod\nfrom_tensors(\n    tensors, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n`from_tensors` produces a dataset containing only a single element. To slice the input tensor into multiple elements, use `from_tensor_slices` instead.\n```\ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n```\n```\n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n```\n\nNote that if `tensors` contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If `tensors` contains one or more large NumPy arrays, consider the alternative described in [this guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\nArgs  \n---  \n`tensors` |  A dataset \"element\". Supported values are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `get_single_element`\n```\nget_single_element(\n    name=None\n)\n\n```\n\nReturns the single element of the `dataset`.\nThe function enables you to use a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) in a stateless \"tensor-in tensor-out\" expression, without creating an iterator. This facilitates the ease of data transformation on tensors using the optimized [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) abstraction on top of them.\nFor example, lets consider a `preprocessing_fn` which would take as an input the raw features and returns the processed feature along with it's label.\n```\ndefpreprocessing_fn(raw_feature):\n  # ... the raw_feature is preprocessed as per the use-case\n  return feature\n\nraw_features = ...  # input batch of BATCH_SIZE elements.\ndataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n          .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n          .batch(BATCH_SIZE))\n\nprocessed_features = dataset.get_single_element()\n\n```\n\nIn the above example, the `raw_features` tensor of length=BATCH_SIZE was converted to a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Next, each of the `raw_feature` was mapped using the `preprocessing_fn` and the processed features were grouped into a single batch. The final `dataset` contains only one element which is a batch of all the processed features.\nNow, instead of creating an iterator for the `dataset` and retrieving the batch of features, the `tf.data.get_single_element()` function is used to skip the iterator creation process and directly output the batch of features.\nThis can be particularly useful when your tensor transformations are expressed as [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) operations, and you want to use those transformations while serving your model.\n#### Keras\n```\n\nmodel = ... # A pre-built or custom model\n\nclassPreprocessingModel(tf.keras.Model):\n  def__init__(self, model):\n    super().__init__(self)\n    self.model = model\n\n  @tf.function(input_signature=[...])\n  defserving_fn(self, data):\n    ds = tf.data.Dataset.from_tensor_slices(data)\n    ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n    ds = ds.batch(batch_size=BATCH_SIZE)\n    return tf.argmax(self.model(ds.get_single_element()), axis=-1)\n\npreprocessing_model = PreprocessingModel(model)\nyour_exported_model_dir = ... # save the model to this path.\ntf.saved_model.save(preprocessing_model, your_exported_model_dir,\n              signatures={'serving_default': preprocessing_model.serving_fn}\n              )\n\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects, corresponding to the single element of `dataset`.   \nRaises  \n---  \n`InvalidArgumentError` |  (at runtime) if `dataset` does not contain exactly one element.   \n### `group_by_window`\n```\ngroup_by_window(\n    key_func, reduce_func, window_size=None, window_size_func=None, name=None\n) -> 'DatasetV2'\n\n```\n\nGroups windows of elements by key and reduces them.\nThis transformation maps each consecutive element in a dataset to a key using `key_func` and groups the elements by key. It then applies `reduce_func` to at most `window_size_func(key)` elements matching the same key. All except the final window for each key will contain `window_size_func(key)` elements; the final window may be smaller.\nYou may provide either a constant `window_size` or a window size determined by the key through `window_size_func`.\n```\ndataset = tf.data.Dataset.range(10)\nwindow_size = 5\nkey_func = lambda x: x%2\nreduce_func = lambda key, dataset: dataset.batch(window_size)\ndataset = dataset.group_by_window(\n          key_func=key_func,\n          reduce_func=reduce_func,\n          window_size=window_size)\nfor elem in dataset.as_numpy_iterator():\n  print(elem)\n[0 2 4 6 8]\n[1 3 5 7 9]\n```\n\nArgs  \n---  \n`key_func` |  A function mapping a nested structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to a scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) tensor.   \n`reduce_func` |  A function mapping a key and a dataset of up to `window_size` consecutive elements matching that key to another dataset.   \n`window_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements matching the same key to combine in a single batch, which will be passed to `reduce_func`. Mutually exclusive with `window_size_func`.   \n`window_size_func` |  A function mapping a key to a [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements matching the same key to combine in a single batch, which will be passed to `reduce_func`. Mutually exclusive with `window_size`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  if neither or both of {`window_size`, `window_size_func`} are passed.   \n### `ignore_errors`\n```\nignore_errors(\n    log_warning=False, name=None\n) -> 'DatasetV2'\n\n```\n\nDrops elements that cause errors.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\ndataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"\"))\nlist(dataset.as_numpy_iterator())\nTraceback (most recent call last):\nInvalidArgumentError: ... Tensor had Inf values\ndataset = dataset.ignore_errors()\nlist(dataset.as_numpy_iterator())\n[1.0, 0.5, 0.25]\n```\n\nArgs  \n---  \n`log_warning` |  (Optional.) A bool indicating whether or not ignored errors should be logged to stderr. Defaults to `False`.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `interleave`\n```\ninterleave(\n    map_func,\n    cycle_length=None,\n    block_length=None,\n    num_parallel_calls=None,\n    deterministic=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nMaps `map_func` across this dataset, and interleaves the results.\n#### The type signature is:\n```\ndefinterleave(\n  self: Dataset[T],\n  map_func: Callable[[T], Dataset[S]]\n) -> Dataset[S]\n\n```\n\nFor example, you can use [`Dataset.interleave()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) to process many input files concurrently:\n```\n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndefparse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n```\n\nThe `cycle_length` and `block_length` arguments control the order in which elements are produced. `cycle_length` controls the number of input elements that are processed concurrently. If you set `cycle_length` to 1, this transformation will handle one input element at a time, and will produce identical results to [`tf.data.Dataset.flat_map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map). In general, this transformation will apply `map_func` to `cycle_length` input elements, open iterators on the returned `Dataset` objects, and cycle through them producing `block_length` consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.\n#### For example:\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n```\n\nPerformance can often be improved by setting `num_parallel_calls` so that `interleave` will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set `deterministic=False`.\n```\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n```\n\nArgs  \n---  \n`map_func` |  A function that takes a dataset element and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`cycle_length` |  (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If `num_parallel_calls` is set to [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE), the `cycle_length` argument identifies the maximum degree of parallelism.   \n`block_length` |  (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.   \n`num_parallel_calls` |  (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available CPU.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `list_files`\n```\n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None, name=None\n) -> 'DatasetV2'\n\n```\n\nA dataset of all files matching one or more glob patterns.\nThe `file_pattern` argument should be a small number of glob patterns. If your filenames have already been globbed, use [`Dataset.from_tensor_slices(filenames)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) instead, as re-globbing every filename with `list_files` may result in poor performance with remote storage systems.\nExample  \n---  \nIf we had the following files on our filesystem:\n  * /path/to/dir/a.txt\n  * /path/to/dir/b.py\n  * /path/to/dir/c.py\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n  * /path/to/dir/b.py\n  * /path/to/dir/c.py \n\n  \nArgs  \n---  \n`file_pattern` |  A string, a list of strings, or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.   \n`shuffle` |  (Optional.) If `True`, the file names will be shuffled randomly. Defaults to `True`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`name` |  Optional. A name for the tf.data operations used by `list_files`.   \nReturns  \n---  \n`Dataset` |  A `Dataset` of strings corresponding to file names.   \n### `load`\n```\n@staticmethod\nload(\n    path, element_spec=None, compression=None, reader_func=None\n) -> 'DatasetV2'\n\n```\n\nLoads a previously saved dataset.\n#### Example usage:\n```\nimporttempfile\npath = os.path.join(tempfile.gettempdir(), \"saved_data\")\n# Save a dataset\ndataset = tf.data.Dataset.range(2)\ntf.data.Dataset.save(dataset, path)\nnew_dataset = tf.data.Dataset.load(path)\nfor elem in new_dataset:\n  print(elem)\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\n```\n\nIf the default option of sharding the saved dataset was used, the element order of the saved dataset will be preserved when loading it.\nThe `reader_func` argument can be used to specify a custom order in which elements should be loaded from the individual shards. The `reader_func` is expected to take a single argument -- a dataset of datasets, each containing elements of one of the shards -- and return a dataset of elements. For example, the order of shards can be shuffled when loading them as follows:\n```\ndefcustom_reader_func(datasets):\n  datasets = datasets.shuffle(NUM_SHARDS)\n  return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = tf.data.Dataset.load(\n    path=\"/path/to/data\", ..., reader_func=custom_reader_func)\n\n```\n\nArgs  \n---  \n`path` |  Required. A path pointing to a previously saved dataset.   \n`element_spec` |  Optional. A nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects matching the structure of an element of the saved dataset and specifying the type of individual element components. If not provided, the nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) saved with the saved dataset is used. Note that this argument is required in graph mode.   \n`compression` |  Optional. The algorithm to use to decompress the data when reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.   \n`reader_func` |  Optional. A function to control how to read data from shards. If present, the function will be traced and executed as graph computation.   \nReturns  \n---  \nA [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance.   \nRaises  \n---  \n`FileNotFoundError` |  If `element_spec` is not specified and the saved nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) can not be located with the saved dataset.   \n`ValueError` |  If `element_spec` is not specified and the method is executed in graph mode.   \n### `map`\n```\nmap(\n    map_func, num_parallel_calls=None, deterministic=None, name=None\n)\n\n```\n\nMaps `map_func` across the elements of this dataset.\nThis transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. `map_func` can be used to change both the values and the structure of a dataset's elements. Supported structure constructs are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).\nFor example, `map` can be used for adding 1 to each element, or projecting a subset of element components.\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n```\n\nThe input signature of `map_func` is determined by the structure of each element in this dataset.\n```\ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n```\n```\n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n```\n```\n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n```\n\nThe value or values returned by `map_func` determine the structure of each element in the returned dataset.\n```\ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndefg(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndefh(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndefi(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n```\n\n`map_func` can accept as arguments and return any type of dataset element.\nNote that irrespective of the context in which `map_func` is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:\n1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n2) Use [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function), which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:\n```\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndefupper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n```\n\n3) Use [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function), which also allows you to write arbitrary Python code. Note that [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) accepts [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whereas [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function) accepts numpy arrays and returns only numpy arrays. For example:\n```\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndefupper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n```\n\nNote that the use of [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function) and [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).\nPerformance can often be improved by setting `num_parallel_calls` so that `map` will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set `deterministic=False`.\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n```\n\nThe order of elements yielded by this transformation is deterministic if `deterministic=True`. If `map_func` contains stateful operations and `num_parallel_calls > 1`, the order in which that state is accessed is undefined, so the values of output elements may not be deterministic regardless of the `deterministic` flag value.\nArgs  \n---  \n`map_func` |  A function mapping a dataset element to another dataset element.   \n`num_parallel_calls` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available CPU.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `options`\n```\noptions()\n\n```\n\nReturns the options for this dataset and its inputs.\nReturns  \n---  \nA [`tf.data.Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options) object representing the dataset options.   \n### `padded_batch`\n```\npadded_batch(\n    batch_size,\n    padded_shapes=None,\n    padding_values=None,\n    drop_remainder=False,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into padded batches.\nThis transformation combines multiple consecutive elements of the input dataset into a single element.\nLike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nUnlike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in `padded_shapes`. The `padded_shapes` argument determines the resulting shape for each dimension of each component in an output element:\n  * If the dimension is a constant, the component will be padded out to that length in that dimension.\n  * If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.\n\n```\nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n```\n\nSee also [`tf.data.experimental.dense_to_sparse_batch`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/dense_to_sparse_batch), which combines elements that may have different shapes into a [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor).\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`padded_shapes` |  (Optional.) A (nested) structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) or [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. `padded_shapes` must be set if any component has an unknown rank.   \n`padding_values` |  (Optional.) A (nested) structure of scalar-shaped [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the padding values to use for the respective components. None represents that the (nested) structure should be padded with default values. Defaults are `0` for numeric types and the empty string for string types. The `padding_values` should have the same (nested) structure as the input dataset. If `padding_values` is a single element and the input dataset has multiple components, then the same `padding_values` will be used to pad every component of the dataset. If `padding_values` is a scalar, then its value will be broadcasted to match the shape of each component.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  If a component has an unknown rank, and the `padded_shapes` argument is not set.   \n`TypeError` |  If a component is of an unsupported type. The list of supported types is documented in <https://www.tensorflow.org/guide/data#dataset_structure>  \n### `prefetch`\n```\nprefetch(\n    buffer_size, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that prefetches elements from this dataset.\nMost dataset input pipelines should end with a call to `prefetch`. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n```\n\nArgs  \n---  \n`buffer_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.   \n`name` |  Optional. A name for the tf.data transformation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `ragged_batch`\n```\nragged_batch(\n    batch_size,\n    drop_remainder=False,\n    row_splits_dtype=[tf.dtypes.int64](https://www.tensorflow.org/api_docs/python/tf/dtypes#int64),\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor)s.\nLike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nUnlike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the input elements to be batched may have different shapes:\n  * If an input element is a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whose static [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) is fully defined, then it is batched as normal.\n  * If an input element is a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whose static [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) contains one or more axes with unknown size (i.e., `shape[i]=None`), then the output will contain a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) that is ragged up to any of such dimensions.\n  * If an input element is a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) or any other type, then it is batched as normal.\n\n\n#### Example:\n```\ndataset = tf.data.Dataset.range(6)\ndataset = dataset.map(lambda x: tf.range(x))\ndataset.element_spec.shape\nTensorShape([None])\ndataset = dataset.ragged_batch(2)\nfor batch in dataset:\n  print(batch)\n<tf.RaggedTensor [[], [0]]>\n<tf.RaggedTensor [[0, 1], [0, 1, 2]]>\n<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`row_splits_dtype` |  The dtype that should be used for the `row_splits` of any new ragged tensors. Existing [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) elements do not have their row_splits dtype changed.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `random`\n```\n@staticmethod\nrandom(\n    seed=None, rerandomize_each_iteration=None, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` of pseudorandom values.\nThe dataset generates a sequence of uniformly distributed integer values.\n`rerandomize_each_iteration` controls whether the sequence of random number generated should be re-randomized for each epoch. The default value is False where the dataset generates the same sequence of random numbers for each epoch.\n```\nds1 = tf.data.Dataset.random(seed=4).take(10)\nds2 = tf.data.Dataset.random(seed=4).take(10)\nprint(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\nTrue\n```\n```\nds3 = tf.data.Dataset.random(seed=4).take(10)\nds3_first_epoch = list(ds3.as_numpy_iterator())\nds3_second_epoch = list(ds3.as_numpy_iterator())\nprint(ds3_first_epoch == ds3_second_epoch)\nTrue\n```\n```\nds4 = tf.data.Dataset.random(\n    seed=4, rerandomize_each_iteration=True).take(10)\nds4_first_epoch = list(ds4.as_numpy_iterator())\nds4_second_epoch = list(ds4.as_numpy_iterator())\nprint(ds4_first_epoch == ds4_second_epoch)\nFalse\n```\n\nArgs  \n---  \n`seed` |  (Optional) If specified, the dataset produces a deterministic sequence of values.   \n`rerandomize_each_iteration` |  (Optional) If set to False, the dataset generates the same sequence of random numbers for each epoch. If set to True, it generates a different deterministic sequence of random numbers for each epoch. It is defaulted to False if left unspecified.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `range`\n```\n@staticmethod\nrange(\n    *args, **kwargs\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` of a step-separated range of values.\n```\nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n```\n\nArgs  \n---  \n`*args` |  follows the same semantics as python's range. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].   \n`**kwargs` | \n  * output_type: Its expected dtype. (Optional, default: [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64)).\n  * name: (Optional.) A name for the tf.data operation. \n\n  \nReturns  \n---  \n`Dataset` |  A `RangeDataset`.   \nRaises  \n---  \n`ValueError` |  if len(args) == 0.   \n### `rebatch`\n```\nrebatch(\n    batch_size, drop_remainder=False, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that rebatches the elements from this dataset.\n`rebatch(N)` is functionally equivalent to `unbatch().batch(N)`, but is more efficient, performing one copy instead of two.\n```\nds = tf.data.Dataset.range(6)\nds = ds.batch(2)\nds = ds.rebatch(3)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n```\nds = tf.data.Dataset.range(7)\nds = ds.batch(4)\nds = ds.rebatch(3)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6])]\n```\n```\nds = tf.data.Dataset.range(7)\nds = ds.batch(2)\nds = ds.rebatch(3, drop_remainder=True)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n\nIf the `batch_size` argument is a list, `rebatch` cycles through the list to determine the size of each batch.\n```\nds = tf.data.Dataset.range(8)\nds = ds.batch(4)\nds = ds.rebatch([2, 1, 1])\nlist(ds.as_numpy_iterator())\n[array([0, 1]), array([2]), array([3]), array([4, 5]), array([6]),\narray([7])]\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar or vector, representing the size of batches to produce. If this argument is a vector, these values are cycled through in round robin fashion.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size[cycle_index]` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA `Dataset` of scalar `dtype` elements.   \n### `reduce`\n```\nreduce(\n    initial_state, reduce_func, name=None\n)\n\n```\n\nReduces the input dataset to a single element.\nThe transformation calls `reduce_func` successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The `initial_state` argument is used for the initial state and the final state is returned as the result.\n```\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n```\n\nArgs  \n---  \n`initial_state` |  An element representing the initial state of the transformation.   \n`reduce_func` |  A function that maps `(old_state, input_element)` to `new_state`. It must take two arguments and return a new element The structure of `new_state` must match the structure of `initial_state`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA dataset element corresponding to the final state of the transformation.   \n### `rejection_resample`\n```\nrejection_resample(\n    class_func, target_dist, initial_dist=None, seed=None, name=None\n) -> 'DatasetV2'\n\n```\n\nResamples elements to reach a target distribution.\n```\ninitial_dist = [0.6, 0.4]\nn = 1000\nelems = np.random.choice(len(initial_dist), size=n, p=initial_dist)\ndataset = tf.data.Dataset.from_tensor_slices(elems)\nzero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n```\n\nFollowing from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4.\n```\ntarget_dist = [0.5, 0.5]\ndataset = dataset.rejection_resample(\n   class_func=lambda x: x,\n   target_dist=target_dist,\n   initial_dist=initial_dist)\ndataset = dataset.map(lambda class_func_result, data: data)\nzero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n```\n\nFollowing from `target_dist`, `zero` is ~0.5 and `one` is ~0.5.\nArgs  \n---  \n`class_func` |  A function mapping an element of the input dataset to a scalar [`tf.int32`](https://www.tensorflow.org/api_docs/python/tf#int32) tensor. Values should be in `[0, num_classes)`.   \n`target_dist` |  A floating point type tensor, shaped `[num_classes]`.   \n`initial_dist` |  (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion.   \n`seed` |  (Optional.) Python integer seed for the resampler.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `repeat`\n```\nrepeat(\n    count=None, name=None\n) -> 'DatasetV2'\n\n```\n\nRepeats this dataset so each original value is seen `count` times.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n```\n\nArgs  \n---  \n`count` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of times the dataset should be repeated. The default behavior (if `count` is `None` or `-1`) is for the dataset be repeated indefinitely.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `sample_from_datasets`\n```\n@staticmethod\nsample_from_datasets(\n    datasets,\n    weights=None,\n    seed=None,\n    stop_on_empty_dataset=False,\n    rerandomize_each_iteration=None\n) -> 'DatasetV2'\n\n```\n\nSamples elements at random from the datasets in `datasets`.\nCreates a dataset by interleaving elements of `datasets` with `weight[i]` probability of picking an element from dataset `i`. Sampling is done without replacement. For example, suppose we have 2 datasets:\n```\ndataset1 = tf.data.Dataset.range(0, 3)\ndataset2 = tf.data.Dataset.range(100, 103)\n\n```\n\nSuppose that we sample from these 2 datasets with the following weights:\n```\nsample_dataset = tf.data.Dataset.sample_from_datasets(\n    [dataset1, dataset2], weights=[0.5, 0.5])\n\n```\n\nOne possible outcome of elements in sample_dataset is:\n```\nprint(list(sample_dataset.as_numpy_iterator()))\n# [100, 0, 1, 101, 2, 102]\n\n```\n\nArgs  \n---  \n`datasets` |  A non-empty list of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects with compatible structure.   \n`weights` |  (Optional.) A list or Tensor of `len(datasets)` floating-point values where `weights[i]` represents the probability to sample from `datasets[i]`, or a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object where each element is such a list. Defaults to a uniform distribution across `datasets`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`stop_on_empty_dataset` |  If `True`, sampling stops if it encounters an empty dataset. If `False`, it continues sampling and skips any empty datasets. It is recommended to set it to `True`. Otherwise, the distribution of samples starts off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Default to `False` for backward compatibility.   \n`rerandomize_each_iteration` |  An optional `bool`. The boolean argument controls whether the sequence of random numbers used to determine which dataset to sample from will be rerandomized each epoch. That is, it determinies whether datasets will be sampled in the same order across different epochs (the default behavior) or not.   \nReturns  \n---  \nA dataset that interleaves elements from `datasets` at random, according to `weights` if provided, otherwise with uniform probability.   \nRaises  \n---  \n`TypeError` |  If the `datasets` or `weights` arguments have the wrong type.   \n`ValueError` | \n  * If `datasets` is empty, or\n  * If `weights` is specified and does not match the length of `datasets`. \n\n  \n### `save`\n```\nsave(\n    path, compression=None, shard_func=None, checkpoint_args=None\n)\n\n```\n\nSaves the content of the given dataset.\nExample usage:\n```\n  importtempfile\n  path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n  # Save a dataset\n  dataset = tf.data.Dataset.range(2)\n  dataset.save(path)\n  new_dataset = tf.data.Dataset.load(path)\n  for elem in new_dataset:\n    print(elem)\n    tf.Tensor(0, shape=(), dtype=int64)\n    tf.Tensor(1, shape=(), dtype=int64)\n  \n```\n\nThe saved dataset is saved in multiple file \"shards\". By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the `shard_func` function. For example, you can save the dataset to using a single shard as follows:\n```\n  dataset = make_dataset()\n  defcustom_shard_func(element):\n    return np.int64(0)\n  dataset.save(\n      path=\"/path/to/data\", ..., shard_func=custom_shard_func)\n\n```\n\nTo enable checkpointing, pass in `checkpoint_args` to the `save` method as follows:\n```\n  dataset = tf.data.Dataset.range(100)\n  save_dir = \"...\"\n  checkpoint_prefix = \"...\"\n  step_counter = tf.Variable(0, trainable=False)\n  checkpoint_args = {\n    \"checkpoint_interval\": 50,\n    \"step_counter\": step_counter,\n    \"directory\": checkpoint_prefix,\n    \"max_to_keep\": 20,\n  }\n  dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)\n\n```\n\nArgs  \n---  \n`path` |  Required. A directory to use for saving the dataset.   \n`compression` |  Optional. The algorithm to use to compress data when writing it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.   \n`shard_func` |  Optional. A function to control the mapping of dataset elements to file shards. The function is expected to map elements of the input dataset to int64 shard IDs. If present, the function will be traced and executed as graph computation.   \n`checkpoint_args` |  Optional args for checkpointing which will be passed into the [`tf.train.CheckpointManager`](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager). If `checkpoint_args` are not specified, then checkpointing will not be performed. The `save()` implementation creates a [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) object internally, so users should not set the `checkpoint` argument in `checkpoint_args`.   \nReturns  \n---  \nAn operation which when executed performs the save. When writing checkpoints, returns None. The return value is useful in unit tests.   \nRaises  \n---  \nValueError if `checkpoint` is passed into `checkpoint_args`.   \n### `scan`\n```\nscan(\n    initial_state, scan_func, name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that scans a function across an input dataset.\nThis transformation is a stateful relative of [`tf.data.Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). In addition to mapping `scan_func` across the elements of the input dataset, `scan()` accumulates one or more state tensors, whose initial values are `initial_state`.\n```\ndataset = tf.data.Dataset.range(10)\ninitial_state = tf.constant(0, dtype=tf.int64)\nscan_func = lambda state, i: (state + i, state + i)\ndataset = dataset.scan(initial_state=initial_state, scan_func=scan_func)\nlist(dataset.as_numpy_iterator())\n[0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n```\n\nArgs  \n---  \n`initial_state` |  A nested structure of tensors, representing the initial state of the accumulator.   \n`scan_func` |  A function that maps `(old_state, input_element)` to `(new_state, output_element)`. It must take two arguments and return a pair of nested structures of tensors. The `new_state` must match the structure of `initial_state`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `shard`\n```\nshard(\n    num_shards, index, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain all elements of A whose index mod n = i.\n```\nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n```\n\nThis dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.\nWhen reading a single input file, you can shard elements as follows:\n```\nd = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n```\n\n#### Important caveats:\n  * Be sure to shard before you use any randomizing operator (such as shuffle).\n  * Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:\n\n```\nd = Dataset.list_files(pattern, shuffle=False)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n```\n\nArgs  \n---  \n`num_shards` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of shards operating in parallel.   \n`index` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the worker index.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`InvalidArgumentError` |  if `num_shards` or `index` are illegal values.  \n### `shuffle`\n```\nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\n) -> 'DatasetV2'\n\n```\n\nRandomly shuffles the elements of this dataset.\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n`reshuffle_each_iteration` controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the `repeat` transformation:\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)\n# [1, 0, 2, 1, 2, 0]\n\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)\n# [1, 0, 2, 1, 0, 2]\n\n```\n\nIn TF 2.0, [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects are Python iterables which makes it possible to also create epochs through Python iteration:\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\nlist(dataset.as_numpy_iterator())\n# [1, 2, 0]\n\n```\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\n\n```\n\n#### Fully shuffling all the data\nTo shuffle an entire dataset, set `buffer_size=dataset.cardinality()`. This is equivalent to setting the `buffer_size` equal to the number of elements in the dataset, resulting in uniform shuffle.\n```\ndataset = tf.data.Dataset.range(20)\ndataset = dataset.shuffle(dataset.cardinality())\n# [18, 4, 9, 2, 17, 8, 5, 10, 0, 6, 16, 3, 19, 7, 14, 11, 15, 13, 12, 1]\n\n```\n\nArgs  \n---  \n`buffer_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements from this dataset from which the new dataset will sample. To uniformly shuffle the entire dataset, use `buffer_size=dataset.cardinality()`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`reshuffle_each_iteration` |  (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to `True`.)   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `skip`\n```\nskip(\n    count, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n```\n\nArgs  \n---  \n`count` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of this dataset that should be skipped to form the new dataset. If `count` is greater than the size of this dataset, the new dataset will contain no elements. If `count` is -1, skips the entire dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `snapshot`\n```\nsnapshot(\n    path,\n    compression='AUTO',\n    reader_func=None,\n    shard_func=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nAPI to persist the output of the input dataset.\nThe snapshot API allows users to transparently persist the output of their preprocessing pipeline to disk, and materialize the pre-processed data on a different training run.\nThis API enables repeated preprocessing steps to be consolidated, and allows re-use of already processed data, trading off disk storage and network bandwidth for freeing up more valuable CPU resources and accelerator compute time.\nUsers can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the `reader_func` and `shard_func` parameters.\n`shard_func` is a user specified function that maps input elements to snapshot shards.\nUsers may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential `shard_func` could be written.\n```\ndataset = ...\ndataset = dataset.enumerate()\ndataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n    shard_func=lambda x, y: x % NUM_SHARDS, ...)\ndataset = dataset.map(lambda x, y: y)\n\n```\n\n`reader_func` is a user specified function that accepts a single argument: (1) a Dataset of Datasets, each representing a \"split\" of elements of the original dataset. The cardinality of the input dataset matches the number of the shards specified in the `shard_func` (see above). The function should return a Dataset of elements of the original dataset.\nUsers may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.\nHere is an example of a standard reader function a user can define. This function enables both dataset shuffling and parallel reading of datasets:\n```\ndefuser_reader_func(datasets):\n  # shuffle the datasets splits\n  datasets = datasets.shuffle(NUM_CORES)\n  # read datasets in parallel and interleave their elements\n  return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n    reader_func=user_reader_func)\n\n```\n\nBy default, snapshot parallelizes reads by the number of cores available on the system, but will not attempt to shuffle the data.\nArgs  \n---  \n`path` |  Required. A directory to use for storing / loading the snapshot to / from.   \n`compression` |  Optional. The type of compression to apply to the snapshot written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None. Defaults to `AUTO`, which attempts to pick an appropriate compression algorithm for the dataset.   \n`reader_func` |  Optional. A function to control how to read data from snapshot shards.   \n`shard_func` |  Optional. A function to control how to shard data when writing a snapshot.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `sparse_batch`\n```\nsparse_batch(\n    batch_size, row_shape, name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements into [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)s.\nLike [`Dataset.padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch), this transformation combines multiple consecutive elements of the dataset, which might have different shapes, into a single element. The resulting element has three components (`indices`, `values`, and `dense_shape`), which comprise a [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) that represents the same data. The `row_shape` represents the dense shape of each row in the resulting [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), to which the effective batch size is prepended. For example:\n```\n# NOTE: The following examples use `{ ... }` to represent the\n# contents of a dataset.\na = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n\na.apply(tf.data.experimental.dense_to_sparse_batch(\n    batch_size=2, row_shape=[6])) ==\n{\n    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices\n     ['a', 'b', 'c', 'a', 'b'],                 # values\n     [2, 6]),                                   # dense_shape\n    ([[0, 0], [0, 1], [0, 2], [0, 3]],\n     ['a', 'b', 'c', 'd'],\n     [1, 6])\n}\n\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`row_shape` |  A [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) or [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) vector tensor-like object representing the equivalent dense shape of a row in the resulting [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor). Each element of this dataset must have the same rank as `row_shape`, and must have size less than or equal to `row_shape` in each dimension.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `take`\n```\ntake(\n    count, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n```\n\nArgs  \n---  \n`count` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of this dataset that should be taken to form the new dataset. If `count` is -1, or if `count` is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `take_while`\n```\ntake_while(\n    predicate, name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that stops dataset iteration based on a `predicate`.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take_while(lambda x: x < 5)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`predicate` |  A function that maps a nested structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to a scalar [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) tensor.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `unbatch`\n```\nunbatch(\n    name=None\n) -> 'DatasetV2'\n\n```\n\nSplits elements of a dataset into multiple elements.\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where `B` may vary for each input element, then for each element in the dataset, the unbatched dataset will contain `B` consecutive elements of shape `[a0, a1, ...]`.\n```\nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `unique`\n```\nunique(\n    name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that discards duplicate elements of a `Dataset`.\nUse this transformation to produce a dataset that contains one instance of each unique element in the input. For example:\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\ndataset = dataset.unique()\nsorted(list(dataset.as_numpy_iterator()))\n[1, 2, 37]\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `window`\n```\nwindow(\n    size, shift=None, stride=1, drop_remainder=False, name=None\n) -> 'DatasetV2'\n\n```\n\nReturns a dataset of \"windows\".\nEach \"window\" is a dataset that contains a subset of elements of the input dataset. These are finite datasets of size `size` (or possibly fewer if there are not enough input elements to fill the window and `drop_remainder` evaluates to `False`).\n#### For example:\n```\ndataset = tf.data.Dataset.range(7).window(3)\nfor window in dataset:\n  print(window)\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n```\n\nSince windows are datasets, they can be iterated over:\n```\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[3, 4, 5]\n[6]\n```\n\n#### Shift\nThe `shift` argument determines the number of input elements to shift between the start of each window. If windows and elements are both numbered starting at 0, the first element in window `k` will be element `k * shift` of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1,\n                                          drop_remainder=True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[1, 2, 3]\n[2, 3, 4]\n[3, 4, 5]\n[4, 5, 6]\n```\n\n#### Stride\nThe `stride` argument determines the stride between input elements within a window.\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2,\n                                          drop_remainder=True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n```\n\n#### Nested elements\nWhen the `window` transformation is applied to a dataset whos elements are nested structures, it produces a dataset where the elements have the same nested structure but each leaf is replaced by a window. In other words, the nesting is applied outside of the windows as opposed inside of them.\n#### The type signature is:\n```\ndefwindow(\n    self: Dataset[Nest[T]], ...\n) -> Dataset[Nest[Dataset[T]]]\n\n```\n\nApplying `window` to a `Dataset` of tuples gives a tuple of windows:\n```\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\n                                              [6, 7, 8, 9, 10]))\ndataset = dataset.window(2)\nwindows = next(iter(dataset))\nwindows\n(<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>,\n <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>)\n```\n```\ndefto_numpy(ds):\n  return list(ds.as_numpy_iterator())\nfor windows in dataset:\n  print(to_numpy(windows[0]), to_numpy(windows[1]))\n[1, 2] [6, 7]\n[3, 4] [8, 9]\n[5] [10]\n```\n\nApplying `window` to a `Dataset` of dictionaries gives a dictionary of `Datasets`:\n```\ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\n                                              'b': [4, 5, 6],\n                                              'c': [7, 8, 9]})\ndataset = dataset.window(2)\ndefto_numpy(ds):\n  return list(ds.as_numpy_iterator())\nfor windows in dataset:\n  print(tf.nest.map_structure(to_numpy, windows))\n{'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\n{'a': [3], 'b': [6], 'c': [9]}\n```\n\n#### Flatten a dataset of windows\nThe [`Dataset.flat_map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map) and [`Dataset.interleave`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) methods can be used to flatten a dataset of windows into a single dataset.\nThe argument to `flat_map` is a function that takes an element from the dataset and returns a `Dataset`. `flat_map` chains together the resulting datasets sequentially.\nFor example, to turn each window into a dense tensor:\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1,\n                                          drop_remainder=True)\nbatched = dataset.flat_map(lambda x:x.batch(3))\nfor batch in batched:\n  print(batch.numpy())\n[0 1 2]\n[1 2 3]\n[2 3 4]\n[3 4 5]\n[4 5 6]\n```\n\nArgs  \n---  \n`size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of the input dataset to combine into a window. Must be positive.   \n`shift` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of input elements by which the window moves in each iteration. Defaults to `size`. Must be positive.   \n`stride` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last windows should be dropped if their size is smaller than `size`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `with_options`\n```\nwith_options(\n    options, name=None\n) -> 'DatasetV2'\n\n```\n\nReturns a new [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) with the given options set.\nThe options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.\n```\nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.deterministic = False\nds = ds.with_options(options)\n```\n\nArgs  \n---  \n`options` |  A [`tf.data.Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options) that identifies the options the use.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  when an option is set more than once to a non-default value   \n### `zip`\n```\n@staticmethod\nzip(\n    *args, datasets=None, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` by zipping together the given datasets.\nThis method has similar semantics to the built-in `zip()` function in Python, with the main difference being that the `datasets` argument can be a (nested) structure of `Dataset` objects. The supported nesting mechanisms are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).\n```\n# The datasets or nested structure of datasets `*args` argument\n# determines the structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip(a, b)\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip(b, a)\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip(a, b, c)\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip(a, d)\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n```\n\nArgs  \n---  \n`*args` |  Datasets or nested structures of datasets to zip together. This can't be set if `datasets` is set.   \n`datasets` |  A (nested) structure of datasets. This can't be set if `*args` is set. Note that this exists only for backwards compatibility and it is preferred to use *args.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `__bool__`\n```\n__bool__()\n\n```\n\n### `__iter__`\n```\n__iter__() -> iterator_ops.OwnedIterator\n\n```\n\nCreates an iterator for elements of this dataset.\nThe returned iterator implements the Python Iterator protocol.\nReturns  \n---  \nAn [`tf.data.Iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) for the elements of this dataset.   \nRaises  \n---  \n`RuntimeError` |  If not inside of tf.function and not executing eagerly.   \n### `__len__`\n```\n__len__()\n\n```\n\nReturns the length of the dataset if it is known and finite.\nThis method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use [`tf.data.Dataset.cardinality`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality) instead.\nReturns  \n---  \nAn integer representing the length of the dataset.   \nRaises  \n---  \n`RuntimeError` |  If the dataset length is unknown or infinite, or if eager execution is not enabled.   \n### `__nonzero__`\n```\n__nonzero__()\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/utils": "Public API for tf._api.v2.saved_model.utils namespace\n## Functions\n[`build_tensor_info(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/build_tensor_info): Utility function to build TensorInfo proto from a Tensor. (deprecated)\n[`get_tensor_from_tensor_info(...)`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/saved_model/get_tensor_from_tensor_info): Returns the Tensor or CompositeTensor described by a TensorInfo proto. (deprecated)\n",
  "https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset": "A `Dataset` comprising records from one or more TFRecord files.\nInherits From: [`Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n```\ntf.data.TFRecordDataset(\n    filenames,\n    compression_type=None,\n    buffer_size=None,\n    num_parallel_reads=None,\n    name=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n\n| \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [Recommending Movies: Recommender Models in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/recommenders)\n  * [Introduction to Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Example_Colab)\n  * [Instance Segmentation with Model Garden](https://www.tensorflow.org/tfmodels/vision/instance_segmentation)\n\n  \nThis dataset loads TFRecords from the files as bytes, exactly as they were written.`TFRecordDataset` does not do any parsing or decoding on its own. Parsing and decoding can be done by applying [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) transformations after the `TFRecordDataset`.\nA minimal example is given below:\n```\nimporttempfile\nexample_path = os.path.join(tempfile.gettempdir(), \"example.tfrecords\")\nnp.random.seed(0)\n```\n```\n# Write the records to a file.\nwith tf.io.TFRecordWriter(example_path) as file_writer:\n  for _ in range(4):\n    x, y = np.random.random(), np.random.random()\n    record_bytes = tf.train.Example(features=tf.train.Features(feature={\n        \"x\": tf.train.Feature(float_list=tf.train.FloatList(value=[x])),\n        \"y\": tf.train.Feature(float_list=tf.train.FloatList(value=[y])),\n    })).SerializeToString()\n    file_writer.write(record_bytes)\n```\n```\n# Read the data back out.\ndefdecode_fn(record_bytes):\n  return tf.io.parse_single_example(\n      # Data\n      record_bytes,\n      # Schema\n      {\"x\": tf.io.FixedLenFeature([], dtype=tf.float32),\n       \"y\": tf.io.FixedLenFeature([], dtype=tf.float32)}\n\n```\n```\nfor batch in tf.data.TFRecordDataset([example_path]).map(decode_fn):\n  print(\"x = {x:.4f},  y = {y:.4f}\".format(**batch))\nx = 0.5488,  y = 0.7152\nx = 0.6028,  y = 0.5449\nx = 0.4237,  y = 0.6459\nx = 0.4376,  y = 0.8918\n```\n\n## Args  \n---  \n`filenames` |  A [`tf.string`](https://www.tensorflow.org/api_docs/python/tf#string) tensor or [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) containing one or more filenames.   \n`compression_type` |  (Optional.) A [`tf.string`](https://www.tensorflow.org/api_docs/python/tf#string) scalar evaluating to one of `\"\"` (no compression), `\"ZLIB\"`, or `\"GZIP\"`.   \n`buffer_size` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar representing the number of bytes in the read buffer. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value 1-100 MBs. If `None`, a sensible default for both local and remote file systems is used.   \n`num_parallel_reads` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar representing the number of files to read in parallel. If greater than one, the records of files read in parallel are outputted in an interleaved order. If your input pipeline is I/O bottlenecked, consider setting this parameter to a value greater than one to parallelize the I/O. If `None`, files will be read sequentially.   \n`name` |  (Optional.) A name for the tf.data operation.   \n## Raises  \n---  \n`TypeError` |  If any argument does not have the expected type.   \n`ValueError` |  If any argument does not have the expected shape.   \n## Attributes  \n---  \n`element_spec` |  The type specification of an element of this dataset.```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\n```\nFor more information, read [this guide](https://www.tensorflow.org/guide/data#dataset_structure).   \n## Methods\n### `apply`\n```\napply(\n    transformation_func\n) -> 'DatasetV2'\n\n```\n\nApplies a transformation function to this dataset.\n`apply` enables chaining of custom `Dataset` transformations, which are represented as functions that take one `Dataset` argument and return a transformed `Dataset`.\n```\ndataset = tf.data.Dataset.range(100)\ndefdataset_fn(ds):\n  return ds.filter(lambda x: x < 5)\ndataset = dataset.apply(dataset_fn)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`transformation_func` |  A function that takes one `Dataset` argument and returns a `Dataset`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `as_numpy_iterator`\n```\nas_numpy_iterator()\n\n```\n\nReturns an iterator which converts all elements of the dataset to numpy.\nUse `as_numpy_iterator` to inspect the content of your dataset. To see element shapes and types, print dataset elements directly instead of using `as_numpy_iterator`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset:\n  print(element)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\ntf.Tensor(3, shape=(), dtype=int32)\n```\n\nThis method requires that you are running in eager mode and the dataset's element_spec contains only `TensorSpec` components.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nfor element in dataset.as_numpy_iterator():\n  print(element)\n\n\n\n```\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nprint(list(dataset.as_numpy_iterator()))\n[1, 2, 3]\n```\n\n`as_numpy_iterator()` will preserve the nested structure of dataset elements.\n```\ndataset = tf.data.Dataset.from_tensor_slices({'a': ([1, 2], [3, 4]),\n                                              'b': [5, 6]})\nlist(dataset.as_numpy_iterator()) == [{'a': (1, 3), 'b': 5},\n                                      {'a': (2, 4), 'b': 6}]\nTrue\n```\n\nReturns  \n---  \nAn iterable over the elements of the dataset, with their tensors converted to numpy arrays.   \nRaises  \n---  \n`TypeError` |  if an element contains a non-`Tensor` value.   \n`RuntimeError` |  if eager execution is not enabled.   \n### `batch`\n```\nbatch(\n    batch_size,\n    drop_remainder=False,\n    num_parallel_calls=None,\n    deterministic=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into batches.\n```\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6, 7])]\n```\n```\ndataset = tf.data.Dataset.range(8)\ndataset = dataset.batch(3, drop_remainder=True)\nlist(dataset.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n\nThe components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`num_parallel_calls` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of batches to compute asynchronously in parallel. If not specified, batches will be computed sequentially. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available resources.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `bucket_by_sequence_length`\n```\nbucket_by_sequence_length(\n    element_length_func,\n    bucket_boundaries,\n    bucket_batch_sizes,\n    padded_shapes=None,\n    padding_values=None,\n    pad_to_bucket_boundary=False,\n    no_padding=False,\n    drop_remainder=False,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that buckets elements in a `Dataset` by length.\nElements of the `Dataset` are grouped together by length and then are padded and batched.\nThis is useful for sequence tasks in which the elements have variable length. Grouping together elements that have similar lengths reduces the total fraction of padding in a batch which increases training step efficiency.\nBelow is an example to bucketize the input data to the 3 buckets \"[0, 3), [3, 5), [5, inf)\" based on sequence length, with batch size 2.\n```\nelements = [\n  [0], [1, 2, 3, 4], [5, 6, 7],\n  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, tf.int64, output_shapes=[None])\ndataset = dataset.bucket_by_sequence_length(\n        element_length_func=lambda elem: tf.shape(elem)[0],\n        bucket_boundaries=[3, 5],\n        bucket_batch_sizes=[2, 2, 2])\nfor elem in dataset.as_numpy_iterator():\n  print(elem)\n[[1 2 3 4]\n[5 6 7 0]]\n[[ 7  8  9 10 11  0]\n[13 14 15 16 19 20]]\n[[ 0  0]\n[21 22]]\n```\n\nArgs  \n---  \n`element_length_func` |  function from element in `Dataset` to [`tf.int32`](https://www.tensorflow.org/api_docs/python/tf#int32), determines the length of the element, which will determine the bucket it goes into.   \n`bucket_boundaries` |  `list<int>`, upper length boundaries of the buckets.   \n`bucket_batch_sizes` |  `list<int>`, batch size per bucket. Length should be `len(bucket_boundaries) + 1`.   \n`padded_shapes` |  Nested structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) to pass to [`tf.data.Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch). If not provided, will use `dataset.output_shapes`, which will result in variable length dimensions being padded out to the maximum length in each batch.   \n`padding_values` |  Values to pad with, passed to [`tf.data.Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch). Defaults to padding with 0.   \n`pad_to_bucket_boundary` |  bool, if `False`, will pad dimensions with unknown size to maximum length in batch. If `True`, will pad dimensions with unknown size to bucket boundary minus 1 (i.e., the maximum length in each bucket), and caller must ensure that the source `Dataset` does not contain any elements with length longer than `max(bucket_boundaries)`.   \n`no_padding` |  `bool`, indicates whether to pad the batch features (features need to be either of type [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) or of same shape).   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  if `len(bucket_batch_sizes) != len(bucket_boundaries) + 1`.   \n### `cache`\n```\ncache(\n    filename='', name=None\n) -> 'DatasetV2'\n\n```\n\nCaches the elements in this dataset.\nThe first time the dataset is iterated over, its elements will be cached either in the specified file or in memory. Subsequent iterations will use the cached data.\n```\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.map(lambda x: x**2)\ndataset = dataset.cache()\n# The first time reading through the data will generate the data using\n# `range` and `map`.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n# Subsequent iterations read from the cache.\nlist(dataset.as_numpy_iterator())\n[0, 1, 4, 9, 16]\n```\n\nWhen caching to a file, the cached data will persist across runs. Even the first iteration through the data will read from the cache file. Changing the input pipeline before the call to `.cache()` will have no effect until the cache file is removed or the filename is changed.\n```\ndataset = tf.data.Dataset.range(5)\ndataset = dataset.cache(\"/path/to/file\")\nlist(dataset.as_numpy_iterator())\n# [0, 1, 2, 3, 4]\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.cache(\"/path/to/file\")  # Same file!\nlist(dataset.as_numpy_iterator())\n# [0, 1, 2, 3, 4]\n\n```\n\nArgs  \n---  \n`filename` |  A [`tf.string`](https://www.tensorflow.org/api_docs/python/tf#string) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the name of a directory on the filesystem to use for caching elements in this Dataset. If a filename is not provided, the dataset will be cached in memory.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `cardinality`\n```\ncardinality()\n\n```\n\nReturns the cardinality of the dataset, if known.\n`cardinality` may return [`tf.data.INFINITE_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#INFINITE_CARDINALITY) if the dataset contains an infinite number of elements or [`tf.data.UNKNOWN_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY) if the analysis fails to determine the number of elements in the dataset (e.g. when the dataset source is a file).\n```\ndataset = tf.data.Dataset.range(42)\nprint(dataset.cardinality().numpy())\n42\ndataset = dataset.repeat()\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.INFINITE_CARDINALITY).numpy())\nTrue\ndataset = dataset.filter(lambda x: True)\ncardinality = dataset.cardinality()\nprint((cardinality == tf.data.UNKNOWN_CARDINALITY).numpy())\nTrue\n```\n\nReturns  \n---  \nA scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) `Tensor` representing the cardinality of the dataset. If the cardinality is infinite or unknown, `cardinality` returns the named constants [`tf.data.INFINITE_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#INFINITE_CARDINALITY) and [`tf.data.UNKNOWN_CARDINALITY`](https://www.tensorflow.org/api_docs/python/tf/data#UNKNOWN_CARDINALITY) respectively.   \n### `choose_from_datasets`\n```\n@staticmethod\nchoose_from_datasets(\n    datasets, choice_dataset, stop_on_empty_dataset=True\n) -> 'DatasetV2'\n\n```\n\nCreates a dataset that deterministically chooses elements from `datasets`.\nFor example, given the following datasets:\n```\ndatasets = [tf.data.Dataset.from_tensors(\"foo\").repeat(),\n            tf.data.Dataset.from_tensors(\"bar\").repeat(),\n            tf.data.Dataset.from_tensors(\"baz\").repeat()]\n\n# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.\nchoice_dataset = tf.data.Dataset.range(3).repeat(3)\n\nresult = tf.data.Dataset.choose_from_datasets(datasets, choice_dataset)\n\n```\n\nThe elements of `result` will be:\n```\n\"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\", \"foo\", \"bar\", \"baz\"\n\n```\n\nArgs  \n---  \n`datasets` |  A non-empty list of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects with compatible structure.   \n`choice_dataset` |  A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) of scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) tensors between `0` and `len(datasets) - 1`.   \n`stop_on_empty_dataset` |  If `True`, selection stops if it encounters an empty dataset. If `False`, it skips empty datasets. It is recommended to set it to `True`. Otherwise, the selected elements start off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Defaults to `True`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`TypeError` |  If `datasets` or `choice_dataset` has the wrong type.   \n`ValueError` |  If `datasets` is empty.   \n### `concatenate`\n```\nconcatenate(\n    dataset, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` by concatenating the given dataset with this dataset.\n```\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 8)  # ==> [ 4, 5, 6, 7 ]\nds = a.concatenate(b)\nlist(ds.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7]\n# The input dataset and dataset to be concatenated should have\n# compatible element specs.\nc = tf.data.Dataset.zip((a, b))\na.concatenate(c)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and (tf.int64, tf.int64)\nd = tf.data.Dataset.from_tensor_slices([\"a\", \"b\", \"c\"])\na.concatenate(d)\nTraceback (most recent call last):\nTypeError: Two datasets to concatenate have different types\n<dtype: 'int64'> and <dtype: 'string'>\n```\n\nArgs  \n---  \n`dataset` |  `Dataset` to be concatenated.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `counter`\n```\n@staticmethod\ncounter(\n    start=0,\n    step=1,\n    dtype=[tf.dtypes.int64](https://www.tensorflow.org/api_docs/python/tf/dtypes#int64),\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that counts from `start` in steps of size `step`.\nUnlike [`tf.data.Dataset.range`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#range), which stops at some ending number, [`tf.data.Dataset.counter`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#counter) produces elements indefinitely.\n```\ndataset = tf.data.experimental.Counter().take(5)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int64, name=None)\ndataset = tf.data.experimental.Counter(dtype=tf.int32)\ndataset.element_spec\nTensorSpec(shape=(), dtype=tf.int32, name=None)\ndataset = tf.data.experimental.Counter(start=2).take(5)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\ndataset = tf.data.experimental.Counter(start=2, step=5).take(5)\nlist(dataset.as_numpy_iterator())\n[2, 7, 12, 17, 22]\ndataset = tf.data.experimental.Counter(start=10, step=-1).take(5)\nlist(dataset.as_numpy_iterator())\n[10, 9, 8, 7, 6]\n```\n\nArgs  \n---  \n`start` |  (Optional.) The starting value for the counter. Defaults to 0.   \n`step` |  (Optional.) The step size for the counter. Defaults to 1.   \n`dtype` |  (Optional.) The data type for counter elements. Defaults to [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA `Dataset` of scalar `dtype` elements.   \n### `enumerate`\n```\nenumerate(\n    start=0, name=None\n) -> 'DatasetV2'\n\n```\n\nEnumerates the elements of this dataset.\nIt is similar to python's `enumerate`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.enumerate(start=5)\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(5, 1)\n(6, 2)\n(7, 3)\n```\n```\n# The (nested) structure of the input dataset determines the\n# structure of elements in the resulting dataset.\ndataset = tf.data.Dataset.from_tensor_slices([(7, 8), (9, 10)])\ndataset = dataset.enumerate()\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(0, array([7, 8], dtype=int32))\n(1, array([ 9, 10], dtype=int32))\n```\n\nArgs  \n---  \n`start` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the start value for enumeration.   \n`name` |  Optional. A name for the tf.data operations used by `enumerate`.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `filter`\n```\nfilter(\n    predicate, name=None\n) -> 'DatasetV2'\n\n```\n\nFilters this dataset according to `predicate`.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.filter(lambda x: x < 3)\nlist(dataset.as_numpy_iterator())\n[1, 2]\n# `tf.math.equal(x, y)` is required for equality comparison\ndeffilter_fn(x):\n  return tf.math.equal(x, 1)\ndataset = dataset.filter(filter_fn)\nlist(dataset.as_numpy_iterator())\n[1]\n```\n\nArgs  \n---  \n`predicate` |  A function mapping a dataset element to a boolean.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `fingerprint`\n```\nfingerprint()\n\n```\n\nComputes the fingerprint of this `Dataset`.\nIf two datasets have the same fingerprint, it is guaranteeed that they would produce identical elements as long as the content of the upstream input files does not change and they produce data deterministically.\nHowever, two datasets producing identical values does not always mean they would have the same fingerprint due to different graph constructs.\nIn other words, if two datasets have different fingerprints, they could still produce identical values.\nReturns  \n---  \nA scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) of type [`tf.uint64`](https://www.tensorflow.org/api_docs/python/tf#uint64).   \n### `flat_map`\n```\nflat_map(\n    map_func, name=None\n) -> 'DatasetV2'\n\n```\n\nMaps `map_func` across this dataset and flattens the result.\n#### The type signature is:\n```\ndefflat_map(\n  self: Dataset[T],\n  map_func: Callable[[T], Dataset[S]]\n) -> Dataset[S]\n\n```\n\nUse `flat_map` if you want to make sure that the order of your dataset stays the same. For example, to flatten a dataset of batches into a dataset of their elements:\n```\ndataset = tf.data.Dataset.from_tensor_slices(\n    [[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndataset = dataset.flat_map(tf.data.Dataset.from_tensor_slices)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n```\n\n[`tf.data.Dataset.interleave()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) is a generalization of `flat_map`, since `flat_map` produces the same output as [`tf.data.Dataset.interleave(cycle_length=1)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave)\nArgs  \n---  \n`map_func` |  A function mapping a dataset element to a dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `from_generator`\n```\n@staticmethod\nfrom_generator(\n    generator,\n    output_types=None,\n    output_shapes=None,\n    args=None,\n    output_signature=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` whose elements are generated by `generator`. (deprecated arguments)\nThe `generator` argument must be a callable object that returns an object that supports the `iter()` protocol (e.g. a generator function).\nThe elements generated by `generator` must be compatible with either the given `output_signature` argument or with the given `output_types` and (optionally) `output_shapes` arguments, whichever was specified.\nThe recommended way to call `from_generator` is to use the `output_signature` argument. In this case the output will be assumed to consist of objects with the classes, shapes and types defined by [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects from `output_signature` argument:\n```\ndefgen():\n  ragged_tensor = tf.ragged.constant([[1, 2], [3]])\n  yield 42, ragged_tensor\ndataset = tf.data.Dataset.from_generator(\n     gen,\n     output_signature=(\n         tf.TensorSpec(shape=(), dtype=tf.int32),\n         tf.RaggedTensorSpec(shape=(2, None), dtype=tf.int32)))\nlist(dataset.take(1))\n[(<tf.Tensor: shape=(), dtype=int32, numpy=42>,\n<tf.RaggedTensor [[1, 2], [3]]>)]\n```\n\nThere is also a deprecated way to call `from_generator` by either with `output_types` argument alone or together with `output_shapes` argument. In this case the output of the function will be assumed to consist of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects with the types defined by `output_types` and with the shapes which are either unknown or defined by `output_shapes`.\nArgs  \n---  \n`generator` |  A callable object that returns an object that supports the `iter()` protocol. If `args` is not specified, `generator` must take no arguments; otherwise it must take as many arguments as there are values in `args`.   \n`output_types` |  (Optional.) A (nested) structure of [`tf.DType`](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) objects corresponding to each component of an element yielded by `generator`.   \n`output_shapes` |  (Optional.) A (nested) structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) objects corresponding to each component of an element yielded by `generator`.   \n`args` |  (Optional.) A tuple of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects that will be evaluated and passed to `generator` as NumPy-array arguments.   \n`output_signature` |  (Optional.) A (nested) structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects corresponding to each component of an element yielded by `generator`.   \n`name` |  (Optional.) A name for the tf.data operations used by `from_generator`.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `from_tensor_slices`\n```\n@staticmethod\nfrom_tensor_slices(\n    tensors, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` whose elements are slices of the given tensors.\nThe given tensors are sliced along their first dimension. This operation preserves the structure of the input tensors, removing the first dimension of each tensor and using it as the dataset dimension. All input tensors must have the same size in their first dimensions.\n```\n# Slicing a 1D tensor produces scalar tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[1, 2, 3]\n```\n```\n# Slicing a 2D tensor produces 1D tensor elements.\ndataset = tf.data.Dataset.from_tensor_slices([[1, 2], [3, 4]])\nlist(dataset.as_numpy_iterator())\n[array([1, 2], dtype=int32), array([3, 4], dtype=int32)]\n```\n```\n# Slicing a tuple of 1D tensors produces tuple elements containing\n# scalar tensors.\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2], [3, 4], [5, 6]))\nlist(dataset.as_numpy_iterator())\n[(1, 3, 5), (2, 4, 6)]\n```\n```\n# Dictionary structure is also preserved.\ndataset = tf.data.Dataset.from_tensor_slices({\"a\": [1, 2], \"b\": [3, 4]})\nlist(dataset.as_numpy_iterator()) == [{'a': 1, 'b': 3},\n                                      {'a': 2, 'b': 4}]\nTrue\n```\n```\n# Two tensors can be combined into one Dataset object.\nfeatures = tf.constant([[1, 3], [2, 1], [3, 3]]) # ==> 3x2 tensor\nlabels = tf.constant(['A', 'B', 'A']) # ==> 3x1 tensor\ndataset = Dataset.from_tensor_slices((features, labels))\n# Both the features and the labels tensors can be converted\n# to a Dataset object separately and combined after.\nfeatures_dataset = Dataset.from_tensor_slices(features)\nlabels_dataset = Dataset.from_tensor_slices(labels)\ndataset = Dataset.zip((features_dataset, labels_dataset))\n# A batched feature and label set can be converted to a Dataset\n# in similar fashion.\nbatched_features = tf.constant([[[1, 3], [2, 3]],\n                                [[2, 1], [1, 2]],\n                                [[3, 3], [3, 2]]], shape=(3, 2, 2))\nbatched_labels = tf.constant([['A', 'A'],\n                              ['B', 'B'],\n                              ['A', 'B']], shape=(3, 2, 1))\ndataset = Dataset.from_tensor_slices((batched_features, batched_labels))\nfor element in dataset.as_numpy_iterator():\n  print(element)\n(array([[1, 3],\n       [2, 3]], dtype=int32), array([[b'A'],\n       [b'A']], dtype=object))\n(array([[2, 1],\n       [1, 2]], dtype=int32), array([[b'B'],\n       [b'B']], dtype=object))\n(array([[3, 3],\n       [3, 2]], dtype=int32), array([[b'A'],\n       [b'B']], dtype=object))\n```\n\nNote that if `tensors` contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If `tensors` contains one or more large NumPy arrays, consider the alternative described in [this guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\nArgs  \n---  \n`tensors` |  A dataset element, whose components have the same first dimension. Supported values are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `from_tensors`\n```\n@staticmethod\nfrom_tensors(\n    tensors, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` with a single element, comprising the given tensors.\n`from_tensors` produces a dataset containing only a single element. To slice the input tensor into multiple elements, use `from_tensor_slices` instead.\n```\ndataset = tf.data.Dataset.from_tensors([1, 2, 3])\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32)]\ndataset = tf.data.Dataset.from_tensors(([1, 2, 3], 'A'))\nlist(dataset.as_numpy_iterator())\n[(array([1, 2, 3], dtype=int32), b'A')]\n```\n```\n# You can use `from_tensors` to produce a dataset which repeats\n# the same example many times.\nexample = tf.constant([1,2,3])\ndataset = tf.data.Dataset.from_tensors(example).repeat(2)\nlist(dataset.as_numpy_iterator())\n[array([1, 2, 3], dtype=int32), array([1, 2, 3], dtype=int32)]\n```\n\nNote that if `tensors` contains a NumPy array, and eager execution is not enabled, the values will be embedded in the graph as one or more [`tf.constant`](https://www.tensorflow.org/api_docs/python/tf/constant) operations. For large datasets (> 1 GB), this can waste memory and run into byte limits of graph serialization. If `tensors` contains one or more large NumPy arrays, consider the alternative described in [this guide](https://tensorflow.org/guide/data#consuming_numpy_arrays).\nArgs  \n---  \n`tensors` |  A dataset \"element\". Supported values are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `get_single_element`\n```\nget_single_element(\n    name=None\n)\n\n```\n\nReturns the single element of the `dataset`.\nThe function enables you to use a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) in a stateless \"tensor-in tensor-out\" expression, without creating an iterator. This facilitates the ease of data transformation on tensors using the optimized [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) abstraction on top of them.\nFor example, lets consider a `preprocessing_fn` which would take as an input the raw features and returns the processed feature along with it's label.\n```\ndefpreprocessing_fn(raw_feature):\n  # ... the raw_feature is preprocessed as per the use-case\n  return feature\n\nraw_features = ...  # input batch of BATCH_SIZE elements.\ndataset = (tf.data.Dataset.from_tensor_slices(raw_features)\n          .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n          .batch(BATCH_SIZE))\n\nprocessed_features = dataset.get_single_element()\n\n```\n\nIn the above example, the `raw_features` tensor of length=BATCH_SIZE was converted to a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Next, each of the `raw_feature` was mapped using the `preprocessing_fn` and the processed features were grouped into a single batch. The final `dataset` contains only one element which is a batch of all the processed features.\nNow, instead of creating an iterator for the `dataset` and retrieving the batch of features, the `tf.data.get_single_element()` function is used to skip the iterator creation process and directly output the batch of features.\nThis can be particularly useful when your tensor transformations are expressed as [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) operations, and you want to use those transformations while serving your model.\n#### Keras\n```\n\nmodel = ... # A pre-built or custom model\n\nclassPreprocessingModel(tf.keras.Model):\n  def__init__(self, model):\n    super().__init__(self)\n    self.model = model\n\n  @tf.function(input_signature=[...])\n  defserving_fn(self, data):\n    ds = tf.data.Dataset.from_tensor_slices(data)\n    ds = ds.map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)\n    ds = ds.batch(batch_size=BATCH_SIZE)\n    return tf.argmax(self.model(ds.get_single_element()), axis=-1)\n\npreprocessing_model = PreprocessingModel(model)\nyour_exported_model_dir = ... # save the model to this path.\ntf.saved_model.save(preprocessing_model, your_exported_model_dir,\n              signatures={'serving_default': preprocessing_model.serving_fn}\n              )\n\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects, corresponding to the single element of `dataset`.   \nRaises  \n---  \n`InvalidArgumentError` |  (at runtime) if `dataset` does not contain exactly one element.   \n### `group_by_window`\n```\ngroup_by_window(\n    key_func, reduce_func, window_size=None, window_size_func=None, name=None\n) -> 'DatasetV2'\n\n```\n\nGroups windows of elements by key and reduces them.\nThis transformation maps each consecutive element in a dataset to a key using `key_func` and groups the elements by key. It then applies `reduce_func` to at most `window_size_func(key)` elements matching the same key. All except the final window for each key will contain `window_size_func(key)` elements; the final window may be smaller.\nYou may provide either a constant `window_size` or a window size determined by the key through `window_size_func`.\n```\ndataset = tf.data.Dataset.range(10)\nwindow_size = 5\nkey_func = lambda x: x%2\nreduce_func = lambda key, dataset: dataset.batch(window_size)\ndataset = dataset.group_by_window(\n          key_func=key_func,\n          reduce_func=reduce_func,\n          window_size=window_size)\nfor elem in dataset.as_numpy_iterator():\n  print(elem)\n[0 2 4 6 8]\n[1 3 5 7 9]\n```\n\nArgs  \n---  \n`key_func` |  A function mapping a nested structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to a scalar [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) tensor.   \n`reduce_func` |  A function mapping a key and a dataset of up to `window_size` consecutive elements matching that key to another dataset.   \n`window_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements matching the same key to combine in a single batch, which will be passed to `reduce_func`. Mutually exclusive with `window_size_func`.   \n`window_size_func` |  A function mapping a key to a [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements matching the same key to combine in a single batch, which will be passed to `reduce_func`. Mutually exclusive with `window_size`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  if neither or both of {`window_size`, `window_size_func`} are passed.   \n### `ignore_errors`\n```\nignore_errors(\n    log_warning=False, name=None\n) -> 'DatasetV2'\n\n```\n\nDrops elements that cause errors.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])\ndataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, \"\"))\nlist(dataset.as_numpy_iterator())\nTraceback (most recent call last):\nInvalidArgumentError: ... Tensor had Inf values\ndataset = dataset.ignore_errors()\nlist(dataset.as_numpy_iterator())\n[1.0, 0.5, 0.25]\n```\n\nArgs  \n---  \n`log_warning` |  (Optional.) A bool indicating whether or not ignored errors should be logged to stderr. Defaults to `False`.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `interleave`\n```\ninterleave(\n    map_func,\n    cycle_length=None,\n    block_length=None,\n    num_parallel_calls=None,\n    deterministic=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nMaps `map_func` across this dataset, and interleaves the results.\n#### The type signature is:\n```\ndefinterleave(\n  self: Dataset[T],\n  map_func: Callable[[T], Dataset[S]]\n) -> Dataset[S]\n\n```\n\nFor example, you can use [`Dataset.interleave()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) to process many input files concurrently:\n```\n# Preprocess 4 files concurrently, and interleave blocks of 16 records\n# from each file.\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndefparse_fn(filename):\n  return tf.data.Dataset.range(10)\ndataset = dataset.interleave(lambda x:\n    tf.data.TextLineDataset(x).map(parse_fn, num_parallel_calls=1),\n    cycle_length=4, block_length=16)\n```\n\nThe `cycle_length` and `block_length` arguments control the order in which elements are produced. `cycle_length` controls the number of input elements that are processed concurrently. If you set `cycle_length` to 1, this transformation will handle one input element at a time, and will produce identical results to [`tf.data.Dataset.flat_map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map). In general, this transformation will apply `map_func` to `cycle_length` input elements, open iterators on the returned `Dataset` objects, and cycle through them producing `block_length` consecutive elements from each iterator, and consuming the next input element each time it reaches the end of an iterator.\n#### For example:\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n# NOTE: New lines indicate \"block\" boundaries.\ndataset = dataset.interleave(\n    lambda x: Dataset.from_tensors(x).repeat(6),\n    cycle_length=2, block_length=4)\nlist(dataset.as_numpy_iterator())\n[1, 1, 1, 1,\n 2, 2, 2, 2,\n 1, 1,\n 2, 2,\n 3, 3, 3, 3,\n 4, 4, 4, 4,\n 3, 3,\n 4, 4,\n 5, 5, 5, 5,\n 5, 5]\n```\n\nPerformance can often be improved by setting `num_parallel_calls` so that `interleave` will use multiple threads to fetch elements. If determinism isn't required, it can also improve performance to set `deterministic=False`.\n```\nfilenames = [\"/var/data/file1.txt\", \"/var/data/file2.txt\",\n             \"/var/data/file3.txt\", \"/var/data/file4.txt\"]\ndataset = tf.data.Dataset.from_tensor_slices(filenames)\ndataset = dataset.interleave(lambda x: tf.data.TFRecordDataset(x),\n    cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n```\n\nArgs  \n---  \n`map_func` |  A function that takes a dataset element and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`cycle_length` |  (Optional.) The number of input elements that will be processed concurrently. If not set, the tf.data runtime decides what it should be based on available CPU. If `num_parallel_calls` is set to [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE), the `cycle_length` argument identifies the maximum degree of parallelism.   \n`block_length` |  (Optional.) The number of consecutive elements to produce from each input element before cycling to another input element. If not set, defaults to 1.   \n`num_parallel_calls` |  (Optional.) If specified, the implementation creates a threadpool, which is used to fetch inputs from cycle elements asynchronously and in parallel. The default behavior is to fetch inputs from cycle elements synchronously with no parallelism. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available CPU.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `list_files`\n```\n@staticmethod\nlist_files(\n    file_pattern, shuffle=None, seed=None, name=None\n) -> 'DatasetV2'\n\n```\n\nA dataset of all files matching one or more glob patterns.\nThe `file_pattern` argument should be a small number of glob patterns. If your filenames have already been globbed, use [`Dataset.from_tensor_slices(filenames)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) instead, as re-globbing every filename with `list_files` may result in poor performance with remote storage systems.\nExample  \n---  \nIf we had the following files on our filesystem:\n  * /path/to/dir/a.txt\n  * /path/to/dir/b.py\n  * /path/to/dir/c.py\n\nIf we pass \"/path/to/dir/*.py\" as the directory, the dataset would produce:\n  * /path/to/dir/b.py\n  * /path/to/dir/c.py \n\n  \nArgs  \n---  \n`file_pattern` |  A string, a list of strings, or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) of string type (scalar or vector), representing the filename glob (i.e. shell wildcard) pattern(s) that will be matched.   \n`shuffle` |  (Optional.) If `True`, the file names will be shuffled randomly. Defaults to `True`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`name` |  Optional. A name for the tf.data operations used by `list_files`.   \nReturns  \n---  \n`Dataset` |  A `Dataset` of strings corresponding to file names.   \n### `load`\n```\n@staticmethod\nload(\n    path, element_spec=None, compression=None, reader_func=None\n) -> 'DatasetV2'\n\n```\n\nLoads a previously saved dataset.\n#### Example usage:\n```\nimporttempfile\npath = os.path.join(tempfile.gettempdir(), \"saved_data\")\n# Save a dataset\ndataset = tf.data.Dataset.range(2)\ntf.data.Dataset.save(dataset, path)\nnew_dataset = tf.data.Dataset.load(path)\nfor elem in new_dataset:\n  print(elem)\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\n```\n\nIf the default option of sharding the saved dataset was used, the element order of the saved dataset will be preserved when loading it.\nThe `reader_func` argument can be used to specify a custom order in which elements should be loaded from the individual shards. The `reader_func` is expected to take a single argument -- a dataset of datasets, each containing elements of one of the shards -- and return a dataset of elements. For example, the order of shards can be shuffled when loading them as follows:\n```\ndefcustom_reader_func(datasets):\n  datasets = datasets.shuffle(NUM_SHARDS)\n  return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = tf.data.Dataset.load(\n    path=\"/path/to/data\", ..., reader_func=custom_reader_func)\n\n```\n\nArgs  \n---  \n`path` |  Required. A path pointing to a previously saved dataset.   \n`element_spec` |  Optional. A nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects matching the structure of an element of the saved dataset and specifying the type of individual element components. If not provided, the nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) saved with the saved dataset is used. Note that this argument is required in graph mode.   \n`compression` |  Optional. The algorithm to use to decompress the data when reading it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.   \n`reader_func` |  Optional. A function to control how to read data from shards. If present, the function will be traced and executed as graph computation.   \nReturns  \n---  \nA [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance.   \nRaises  \n---  \n`FileNotFoundError` |  If `element_spec` is not specified and the saved nested structure of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) can not be located with the saved dataset.   \n`ValueError` |  If `element_spec` is not specified and the method is executed in graph mode.   \n### `map`\n```\nmap(\n    map_func, num_parallel_calls=None, deterministic=None, name=None\n)\n\n```\n\nMaps `map_func` across the elements of this dataset.\nThis transformation applies `map_func` to each element of this dataset, and returns a new dataset containing the transformed elements, in the same order as they appeared in the input. `map_func` can be used to change both the values and the structure of a dataset's elements. Supported structure constructs are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).\nFor example, `map` can be used for adding 1 to each element, or projecting a subset of element components.\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1)\nlist(dataset.as_numpy_iterator())\n[2, 3, 4, 5, 6]\n```\n\nThe input signature of `map_func` is determined by the structure of each element in this dataset.\n```\ndataset = Dataset.range(5)\n# `map_func` takes a single argument of type `tf.Tensor` with the same\n# shape and dtype.\nresult = dataset.map(lambda x: x + 1)\n```\n```\n# Each element is a tuple containing two `tf.Tensor` objects.\nelements = [(1, \"foo\"), (2, \"bar\"), (3, \"baz\")]\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, (tf.int32, tf.string))\n# `map_func` takes two arguments of type `tf.Tensor`. This function\n# projects out just the first component.\nresult = dataset.map(lambda x_int, y_str: x_int)\nlist(result.as_numpy_iterator())\n[1, 2, 3]\n```\n```\n# Each element is a dictionary mapping strings to `tf.Tensor` objects.\nelements =  ([{\"a\": 1, \"b\": \"foo\"},\n              {\"a\": 2, \"b\": \"bar\"},\n              {\"a\": 3, \"b\": \"baz\"}])\ndataset = tf.data.Dataset.from_generator(\n    lambda: elements, {\"a\": tf.int32, \"b\": tf.string})\n# `map_func` takes a single argument of type `dict` with the same keys\n# as the elements.\nresult = dataset.map(lambda d: str(d[\"a\"]) + d[\"b\"])\n```\n\nThe value or values returned by `map_func` determine the structure of each element in the returned dataset.\n```\ndataset = tf.data.Dataset.range(3)\n# `map_func` returns two `tf.Tensor` objects.\ndefg(x):\n  return tf.constant(37.0), tf.constant([\"Foo\", \"Bar\", \"Baz\"])\nresult = dataset.map(g)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(3,), dtype=tf.string, name=None))\n# Python primitives, lists, and NumPy arrays are implicitly converted to\n# `tf.Tensor`.\ndefh(x):\n  return 37.0, [\"Foo\", \"Bar\"], np.array([1.0, 2.0], dtype=np.float64)\nresult = dataset.map(h)\nresult.element_spec\n(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(2,), dtype=tf.string, name=None), TensorSpec(shape=(2,), dtype=tf.float64, name=None))\n# `map_func` can return nested structures.\ndefi(x):\n  return (37.0, [42, 16]), \"foo\"\nresult = dataset.map(i)\nresult.element_spec\n((TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(2,), dtype=tf.int32, name=None)),\n TensorSpec(shape=(), dtype=tf.string, name=None))\n```\n\n`map_func` can accept as arguments and return any type of dataset element.\nNote that irrespective of the context in which `map_func` is defined (eager vs. graph), tf.data traces the function and executes it as a graph. To use Python code inside of the function you have a few options:\n1) Rely on AutoGraph to convert Python code into an equivalent graph computation. The downside of this approach is that AutoGraph can convert some but not all Python code.\n2) Use [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function), which allows you to write arbitrary Python code but will generally result in worse performance than 1). For example:\n```\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\n# transform a string tensor to upper case string using a Python function\ndefupper_case_fn(t: tf.Tensor):\n  return t.numpy().decode('utf-8').upper()\nd = d.map(lambda x: tf.py_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n```\n\n3) Use [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function), which also allows you to write arbitrary Python code. Note that [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) accepts [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whereas [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function) accepts numpy arrays and returns only numpy arrays. For example:\n```\nd = tf.data.Dataset.from_tensor_slices(['hello', 'world'])\ndefupper_case_fn(t: np.ndarray):\n  return t.decode('utf-8').upper()\nd = d.map(lambda x: tf.numpy_function(func=upper_case_fn,\n          inp=[x], Tout=tf.string))\nlist(d.as_numpy_iterator())\n[b'HELLO', b'WORLD']\n```\n\nNote that the use of [`tf.numpy_function`](https://www.tensorflow.org/api_docs/python/tf/numpy_function) and [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) in general precludes the possibility of executing user-defined transformations in parallel (because of Python GIL).\nPerformance can often be improved by setting `num_parallel_calls` so that `map` will use multiple threads to process elements. If deterministic order isn't required, it can also improve performance to set `deterministic=False`.\n```\ndataset = Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\ndataset = dataset.map(lambda x: x + 1,\n    num_parallel_calls=tf.data.AUTOTUNE,\n    deterministic=False)\n```\n\nThe order of elements yielded by this transformation is deterministic if `deterministic=True`. If `map_func` contains stateful operations and `num_parallel_calls > 1`, the order in which that state is accessed is undefined, so the values of output elements may not be deterministic regardless of the `deterministic` flag value.\nArgs  \n---  \n`map_func` |  A function mapping a dataset element to another dataset element.   \n`num_parallel_calls` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number elements to process asynchronously in parallel. If not specified, elements will be processed sequentially. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the number of parallel calls is set dynamically based on available CPU.   \n`deterministic` |  (Optional.) When `num_parallel_calls` is specified, if this boolean is specified (`True` or `False`), it controls the order in which the transformation produces elements. If set to `False`, the transformation is allowed to yield elements out of order to trade determinism for performance. If not specified, the [`tf.data.Options.deterministic`](https://www.tensorflow.org/api_docs/python/tf/data/Options#deterministic) option (`True` by default) controls the behavior.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `options`\n```\noptions()\n\n```\n\nReturns the options for this dataset and its inputs.\nReturns  \n---  \nA [`tf.data.Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options) object representing the dataset options.   \n### `padded_batch`\n```\npadded_batch(\n    batch_size,\n    padded_shapes=None,\n    padding_values=None,\n    drop_remainder=False,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into padded batches.\nThis transformation combines multiple consecutive elements of the input dataset into a single element.\nLike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nUnlike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the input elements to be batched may have different shapes, and this transformation will pad each component to the respective shape in `padded_shapes`. The `padded_shapes` argument determines the resulting shape for each dimension of each component in an output element:\n  * If the dimension is a constant, the component will be padded out to that length in that dimension.\n  * If the dimension is unknown, the component will be padded out to the maximum length of all elements in that dimension.\n\n```\nA = (tf.data.Dataset\n     .range(1, 5, output_type=tf.int32)\n     .map(lambda x: tf.fill([x], x)))\n# Pad to the smallest per-batch size that fits all elements.\nB = A.padded_batch(2)\nfor element in B.as_numpy_iterator():\n  print(element)\n[[1 0]\n [2 2]]\n[[3 3 3 0]\n [4 4 4 4]]\n# Pad to a fixed size.\nC = A.padded_batch(2, padded_shapes=5)\nfor element in C.as_numpy_iterator():\n  print(element)\n[[1 0 0 0 0]\n [2 2 0 0 0]]\n[[3 3 3 0 0]\n [4 4 4 4 0]]\n# Pad with a custom value.\nD = A.padded_batch(2, padded_shapes=5, padding_values=-1)\nfor element in D.as_numpy_iterator():\n  print(element)\n[[ 1 -1 -1 -1 -1]\n [ 2  2 -1 -1 -1]]\n[[ 3  3  3 -1 -1]\n [ 4  4  4  4 -1]]\n# Components of nested elements can be padded independently.\nelements = [([1, 2, 3], [10]),\n            ([4, 5], [11, 12])]\ndataset = tf.data.Dataset.from_generator(\n    lambda: iter(elements), (tf.int32, tf.int32))\n# Pad the first component of the tuple to length 4, and the second\n# component to the smallest size that fits.\ndataset = dataset.padded_batch(2,\n    padded_shapes=([4], [None]),\n    padding_values=(-1, 100))\nlist(dataset.as_numpy_iterator())\n[(array([[ 1,  2,  3, -1], [ 4,  5, -1, -1]], dtype=int32),\n  array([[ 10, 100], [ 11,  12]], dtype=int32))]\n# Pad with a single value and multiple components.\nE = tf.data.Dataset.zip((A, A)).padded_batch(2, padding_values=-1)\nfor element in E.as_numpy_iterator():\n  print(element)\n(array([[ 1, -1],\n       [ 2,  2]], dtype=int32), array([[ 1, -1],\n       [ 2,  2]], dtype=int32))\n(array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32), array([[ 3,  3,  3, -1],\n       [ 4,  4,  4,  4]], dtype=int32))\n```\n\nSee also [`tf.data.experimental.dense_to_sparse_batch`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/dense_to_sparse_batch), which combines elements that may have different shapes into a [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor).\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`padded_shapes` |  (Optional.) A (nested) structure of [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) or [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) vector tensor-like objects representing the shape to which the respective component of each input element should be padded prior to batching. Any unknown dimensions will be padded to the maximum size of that dimension in each batch. If unset, all dimensions of all components are padded to the maximum size in the batch. `padded_shapes` must be set if any component has an unknown rank.   \n`padding_values` |  (Optional.) A (nested) structure of scalar-shaped [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the padding values to use for the respective components. None represents that the (nested) structure should be padded with default values. Defaults are `0` for numeric types and the empty string for string types. The `padding_values` should have the same (nested) structure as the input dataset. If `padding_values` is a single element and the input dataset has multiple components, then the same `padding_values` will be used to pad every component of the dataset. If `padding_values` is a scalar, then its value will be broadcasted to match the shape of each component.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  If a component has an unknown rank, and the `padded_shapes` argument is not set.   \n`TypeError` |  If a component is of an unsupported type. The list of supported types is documented in <https://www.tensorflow.org/guide/data#dataset_structure>  \n### `prefetch`\n```\nprefetch(\n    buffer_size, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that prefetches elements from this dataset.\nMost dataset input pipelines should end with a call to `prefetch`. This allows later elements to be prepared while the current element is being processed. This often improves latency and throughput, at the cost of using additional memory to store prefetched elements.\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.prefetch(2)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n```\n\nArgs  \n---  \n`buffer_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the maximum number of elements that will be buffered when prefetching. If the value [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data#AUTOTUNE) is used, then the buffer size is dynamically tuned.   \n`name` |  Optional. A name for the tf.data transformation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `ragged_batch`\n```\nragged_batch(\n    batch_size,\n    drop_remainder=False,\n    row_splits_dtype=[tf.dtypes.int64](https://www.tensorflow.org/api_docs/python/tf/dtypes#int64),\n    name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements of this dataset into [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor)s.\nLike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the components of the resulting element will have an additional outer dimension, which will be `batch_size` (or `N % batch_size` for the last element if `batch_size` does not divide the number of input elements `N` evenly and `drop_remainder` is `False`). If your program depends on the batches having the same outer dimension, you should set the `drop_remainder` argument to `True` to prevent the smaller batch from being produced.\nUnlike [`tf.data.Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch), the input elements to be batched may have different shapes:\n  * If an input element is a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whose static [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) is fully defined, then it is batched as normal.\n  * If an input element is a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) whose static [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) contains one or more axes with unknown size (i.e., `shape[i]=None`), then the output will contain a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) that is ragged up to any of such dimensions.\n  * If an input element is a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) or any other type, then it is batched as normal.\n\n\n#### Example:\n```\ndataset = tf.data.Dataset.range(6)\ndataset = dataset.map(lambda x: tf.range(x))\ndataset.element_spec.shape\nTensorShape([None])\ndataset = dataset.ragged_batch(2)\nfor batch in dataset:\n  print(batch)\n<tf.RaggedTensor [[], [0]]>\n<tf.RaggedTensor [[0, 1], [0, 1, 2]]>\n<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size` elements; the default behavior is not to drop the smaller batch.   \n`row_splits_dtype` |  The dtype that should be used for the `row_splits` of any new ragged tensors. Existing [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) elements do not have their row_splits dtype changed.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `random`\n```\n@staticmethod\nrandom(\n    seed=None, rerandomize_each_iteration=None, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` of pseudorandom values.\nThe dataset generates a sequence of uniformly distributed integer values.\n`rerandomize_each_iteration` controls whether the sequence of random number generated should be re-randomized for each epoch. The default value is False where the dataset generates the same sequence of random numbers for each epoch.\n```\nds1 = tf.data.Dataset.random(seed=4).take(10)\nds2 = tf.data.Dataset.random(seed=4).take(10)\nprint(list(ds1.as_numpy_iterator())==list(ds2.as_numpy_iterator()))\nTrue\n```\n```\nds3 = tf.data.Dataset.random(seed=4).take(10)\nds3_first_epoch = list(ds3.as_numpy_iterator())\nds3_second_epoch = list(ds3.as_numpy_iterator())\nprint(ds3_first_epoch == ds3_second_epoch)\nTrue\n```\n```\nds4 = tf.data.Dataset.random(\n    seed=4, rerandomize_each_iteration=True).take(10)\nds4_first_epoch = list(ds4.as_numpy_iterator())\nds4_second_epoch = list(ds4.as_numpy_iterator())\nprint(ds4_first_epoch == ds4_second_epoch)\nFalse\n```\n\nArgs  \n---  \n`seed` |  (Optional) If specified, the dataset produces a deterministic sequence of values.   \n`rerandomize_each_iteration` |  (Optional) If set to False, the dataset generates the same sequence of random numbers for each epoch. If set to True, it generates a different deterministic sequence of random numbers for each epoch. It is defaulted to False if left unspecified.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \n`Dataset` |  A `Dataset`.   \n### `range`\n```\n@staticmethod\nrange(\n    *args, **kwargs\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` of a step-separated range of values.\n```\nlist(Dataset.range(5).as_numpy_iterator())\n[0, 1, 2, 3, 4]\nlist(Dataset.range(2, 5).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2).as_numpy_iterator())\n[1, 3]\nlist(Dataset.range(1, 5, -2).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1).as_numpy_iterator())\n[]\nlist(Dataset.range(5, 1, -2).as_numpy_iterator())\n[5, 3]\nlist(Dataset.range(2, 5, output_type=tf.int32).as_numpy_iterator())\n[2, 3, 4]\nlist(Dataset.range(1, 5, 2, output_type=tf.float32).as_numpy_iterator())\n[1.0, 3.0]\n```\n\nArgs  \n---  \n`*args` |  follows the same semantics as python's range. len(args) == 1 -> start = 0, stop = args[0], step = 1. len(args) == 2 -> start = args[0], stop = args[1], step = 1. len(args) == 3 -> start = args[0], stop = args[1], step = args[2].   \n`**kwargs` | \n  * output_type: Its expected dtype. (Optional, default: [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64)).\n  * name: (Optional.) A name for the tf.data operation. \n\n  \nReturns  \n---  \n`Dataset` |  A `RangeDataset`.   \nRaises  \n---  \n`ValueError` |  if len(args) == 0.   \n### `rebatch`\n```\nrebatch(\n    batch_size, drop_remainder=False, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that rebatches the elements from this dataset.\n`rebatch(N)` is functionally equivalent to `unbatch().batch(N)`, but is more efficient, performing one copy instead of two.\n```\nds = tf.data.Dataset.range(6)\nds = ds.batch(2)\nds = ds.rebatch(3)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n```\nds = tf.data.Dataset.range(7)\nds = ds.batch(4)\nds = ds.rebatch(3)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5]), array([6])]\n```\n```\nds = tf.data.Dataset.range(7)\nds = ds.batch(2)\nds = ds.rebatch(3, drop_remainder=True)\nlist(ds.as_numpy_iterator())\n[array([0, 1, 2]), array([3, 4, 5])]\n```\n\nIf the `batch_size` argument is a list, `rebatch` cycles through the list to determine the size of each batch.\n```\nds = tf.data.Dataset.range(8)\nds = ds.batch(4)\nds = ds.rebatch([2, 1, 1])\nlist(ds.as_numpy_iterator())\n[array([0, 1]), array([2]), array([3]), array([4, 5]), array([6]),\narray([7])]\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar or vector, representing the size of batches to produce. If this argument is a vector, these values are cycled through in round robin fashion.   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last batch should be dropped in the case it has fewer than `batch_size[cycle_index]` elements; the default behavior is not to drop the smaller batch.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA `Dataset` of scalar `dtype` elements.   \n### `reduce`\n```\nreduce(\n    initial_state, reduce_func, name=None\n)\n\n```\n\nReduces the input dataset to a single element.\nThe transformation calls `reduce_func` successively on every element of the input dataset until the dataset is exhausted, aggregating information in its internal state. The `initial_state` argument is used for the initial state and the final state is returned as the result.\n```\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, _: x + 1).numpy()\n\ntf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y).numpy()\n10\n```\n\nArgs  \n---  \n`initial_state` |  An element representing the initial state of the transformation.   \n`reduce_func` |  A function that maps `(old_state, input_element)` to `new_state`. It must take two arguments and return a new element The structure of `new_state` must match the structure of `initial_state`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA dataset element corresponding to the final state of the transformation.   \n### `rejection_resample`\n```\nrejection_resample(\n    class_func, target_dist, initial_dist=None, seed=None, name=None\n) -> 'DatasetV2'\n\n```\n\nResamples elements to reach a target distribution.\n```\ninitial_dist = [0.6, 0.4]\nn = 1000\nelems = np.random.choice(len(initial_dist), size=n, p=initial_dist)\ndataset = tf.data.Dataset.from_tensor_slices(elems)\nzero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n```\n\nFollowing from `initial_dist`, `zero` is ~0.6 and `one` is ~0.4.\n```\ntarget_dist = [0.5, 0.5]\ndataset = dataset.rejection_resample(\n   class_func=lambda x: x,\n   target_dist=target_dist,\n   initial_dist=initial_dist)\ndataset = dataset.map(lambda class_func_result, data: data)\nzero, one = np.bincount(list(dataset.as_numpy_iterator())) / n\n```\n\nFollowing from `target_dist`, `zero` is ~0.5 and `one` is ~0.5.\nArgs  \n---  \n`class_func` |  A function mapping an element of the input dataset to a scalar [`tf.int32`](https://www.tensorflow.org/api_docs/python/tf#int32) tensor. Values should be in `[0, num_classes)`.   \n`target_dist` |  A floating point type tensor, shaped `[num_classes]`.   \n`initial_dist` |  (Optional.) A floating point type tensor, shaped `[num_classes]`. If not provided, the true class distribution is estimated live in a streaming fashion.   \n`seed` |  (Optional.) Python integer seed for the resampler.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `repeat`\n```\nrepeat(\n    count=None, name=None\n) -> 'DatasetV2'\n\n```\n\nRepeats this dataset so each original value is seen `count` times.\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\ndataset = dataset.repeat(3)\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n```\n\nArgs  \n---  \n`count` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of times the dataset should be repeated. The default behavior (if `count` is `None` or `-1`) is for the dataset be repeated indefinitely.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `sample_from_datasets`\n```\n@staticmethod\nsample_from_datasets(\n    datasets,\n    weights=None,\n    seed=None,\n    stop_on_empty_dataset=False,\n    rerandomize_each_iteration=None\n) -> 'DatasetV2'\n\n```\n\nSamples elements at random from the datasets in `datasets`.\nCreates a dataset by interleaving elements of `datasets` with `weight[i]` probability of picking an element from dataset `i`. Sampling is done without replacement. For example, suppose we have 2 datasets:\n```\ndataset1 = tf.data.Dataset.range(0, 3)\ndataset2 = tf.data.Dataset.range(100, 103)\n\n```\n\nSuppose that we sample from these 2 datasets with the following weights:\n```\nsample_dataset = tf.data.Dataset.sample_from_datasets(\n    [dataset1, dataset2], weights=[0.5, 0.5])\n\n```\n\nOne possible outcome of elements in sample_dataset is:\n```\nprint(list(sample_dataset.as_numpy_iterator()))\n# [100, 0, 1, 101, 2, 102]\n\n```\n\nArgs  \n---  \n`datasets` |  A non-empty list of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects with compatible structure.   \n`weights` |  (Optional.) A list or Tensor of `len(datasets)` floating-point values where `weights[i]` represents the probability to sample from `datasets[i]`, or a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object where each element is such a list. Defaults to a uniform distribution across `datasets`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`stop_on_empty_dataset` |  If `True`, sampling stops if it encounters an empty dataset. If `False`, it continues sampling and skips any empty datasets. It is recommended to set it to `True`. Otherwise, the distribution of samples starts off as the user intends, but may change as input datasets become empty. This can be difficult to detect since the dataset starts off looking correct. Default to `False` for backward compatibility.   \n`rerandomize_each_iteration` |  An optional `bool`. The boolean argument controls whether the sequence of random numbers used to determine which dataset to sample from will be rerandomized each epoch. That is, it determinies whether datasets will be sampled in the same order across different epochs (the default behavior) or not.   \nReturns  \n---  \nA dataset that interleaves elements from `datasets` at random, according to `weights` if provided, otherwise with uniform probability.   \nRaises  \n---  \n`TypeError` |  If the `datasets` or `weights` arguments have the wrong type.   \n`ValueError` | \n  * If `datasets` is empty, or\n  * If `weights` is specified and does not match the length of `datasets`. \n\n  \n### `save`\n```\nsave(\n    path, compression=None, shard_func=None, checkpoint_args=None\n)\n\n```\n\nSaves the content of the given dataset.\nExample usage:\n```\n  importtempfile\n  path = os.path.join(tempfile.gettempdir(), \"saved_data\")\n  # Save a dataset\n  dataset = tf.data.Dataset.range(2)\n  dataset.save(path)\n  new_dataset = tf.data.Dataset.load(path)\n  for elem in new_dataset:\n    print(elem)\n    tf.Tensor(0, shape=(), dtype=int64)\n    tf.Tensor(1, shape=(), dtype=int64)\n  \n```\n\nThe saved dataset is saved in multiple file \"shards\". By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the `shard_func` function. For example, you can save the dataset to using a single shard as follows:\n```\n  dataset = make_dataset()\n  defcustom_shard_func(element):\n    return np.int64(0)\n  dataset.save(\n      path=\"/path/to/data\", ..., shard_func=custom_shard_func)\n\n```\n\nTo enable checkpointing, pass in `checkpoint_args` to the `save` method as follows:\n```\n  dataset = tf.data.Dataset.range(100)\n  save_dir = \"...\"\n  checkpoint_prefix = \"...\"\n  step_counter = tf.Variable(0, trainable=False)\n  checkpoint_args = {\n    \"checkpoint_interval\": 50,\n    \"step_counter\": step_counter,\n    \"directory\": checkpoint_prefix,\n    \"max_to_keep\": 20,\n  }\n  dataset.save(dataset, save_dir, checkpoint_args=checkpoint_args)\n\n```\n\nArgs  \n---  \n`path` |  Required. A directory to use for saving the dataset.   \n`compression` |  Optional. The algorithm to use to compress data when writing it. Supported options are `GZIP` and `NONE`. Defaults to `NONE`.   \n`shard_func` |  Optional. A function to control the mapping of dataset elements to file shards. The function is expected to map elements of the input dataset to int64 shard IDs. If present, the function will be traced and executed as graph computation.   \n`checkpoint_args` |  Optional args for checkpointing which will be passed into the [`tf.train.CheckpointManager`](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager). If `checkpoint_args` are not specified, then checkpointing will not be performed. The `save()` implementation creates a [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) object internally, so users should not set the `checkpoint` argument in `checkpoint_args`.   \nReturns  \n---  \nAn operation which when executed performs the save. When writing checkpoints, returns None. The return value is useful in unit tests.   \nRaises  \n---  \nValueError if `checkpoint` is passed into `checkpoint_args`.   \n### `scan`\n```\nscan(\n    initial_state, scan_func, name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that scans a function across an input dataset.\nThis transformation is a stateful relative of [`tf.data.Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map). In addition to mapping `scan_func` across the elements of the input dataset, `scan()` accumulates one or more state tensors, whose initial values are `initial_state`.\n```\ndataset = tf.data.Dataset.range(10)\ninitial_state = tf.constant(0, dtype=tf.int64)\nscan_func = lambda state, i: (state + i, state + i)\ndataset = dataset.scan(initial_state=initial_state, scan_func=scan_func)\nlist(dataset.as_numpy_iterator())\n[0, 1, 3, 6, 10, 15, 21, 28, 36, 45]\n```\n\nArgs  \n---  \n`initial_state` |  A nested structure of tensors, representing the initial state of the accumulator.   \n`scan_func` |  A function that maps `(old_state, input_element)` to `(new_state, output_element)`. It must take two arguments and return a pair of nested structures of tensors. The `new_state` must match the structure of `initial_state`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `shard`\n```\nshard(\n    num_shards, index, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that includes only 1/`num_shards` of this dataset.\n`shard` is deterministic. The Dataset produced by `A.shard(n, i)` will contain all elements of A whose index mod n = i.\n```\nA = tf.data.Dataset.range(10)\nB = A.shard(num_shards=3, index=0)\nlist(B.as_numpy_iterator())\n[0, 3, 6, 9]\nC = A.shard(num_shards=3, index=1)\nlist(C.as_numpy_iterator())\n[1, 4, 7]\nD = A.shard(num_shards=3, index=2)\nlist(D.as_numpy_iterator())\n[2, 5, 8]\n```\n\nThis dataset operator is very useful when running distributed training, as it allows each worker to read a unique subset.\nWhen reading a single input file, you can shard elements as follows:\n```\nd = tf.data.TFRecordDataset(input_file)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n```\n\n#### Important caveats:\n  * Be sure to shard before you use any randomizing operator (such as shuffle).\n  * Generally it is best if the shard operator is used early in the dataset pipeline. For example, when reading from a set of TFRecord files, shard before converting the dataset to input samples. This avoids reading every file on every worker. The following is an example of an efficient sharding strategy within a complete pipeline:\n\n```\nd = Dataset.list_files(pattern, shuffle=False)\nd = d.shard(num_workers, worker_index)\nd = d.repeat(num_epochs)\nd = d.shuffle(shuffle_buffer_size)\nd = d.interleave(tf.data.TFRecordDataset,\n                 cycle_length=num_readers, block_length=1)\nd = d.map(parser_fn, num_parallel_calls=num_map_threads)\n\n```\n\nArgs  \n---  \n`num_shards` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of shards operating in parallel.   \n`index` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the worker index.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`InvalidArgumentError` |  if `num_shards` or `index` are illegal values.  \n### `shuffle`\n```\nshuffle(\n    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\n) -> 'DatasetV2'\n\n```\n\nRandomly shuffles the elements of this dataset.\nThis dataset fills a buffer with `buffer_size` elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required.\nFor instance, if your dataset contains 10,000 elements but `buffer_size` is set to 1,000, then `shuffle` will initially select a random element from only the first 1,000 elements in the buffer. Once an element is selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element, maintaining the 1,000 element buffer.\n`reshuffle_each_iteration` controls whether the shuffle order should be different for each epoch. In TF 1.X, the idiomatic way to create epochs was through the `repeat` transformation:\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\ndataset = dataset.repeat(2)\n# [1, 0, 2, 1, 2, 0]\n\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\ndataset = dataset.repeat(2)\n# [1, 0, 2, 1, 0, 2]\n\n```\n\nIn TF 2.0, [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects are Python iterables which makes it possible to also create epochs through Python iteration:\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=True)\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\nlist(dataset.as_numpy_iterator())\n# [1, 2, 0]\n\n```\n```\ndataset = tf.data.Dataset.range(3)\ndataset = dataset.shuffle(3, reshuffle_each_iteration=False)\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\nlist(dataset.as_numpy_iterator())\n# [1, 0, 2]\n\n```\n\n#### Fully shuffling all the data\nTo shuffle an entire dataset, set `buffer_size=dataset.cardinality()`. This is equivalent to setting the `buffer_size` equal to the number of elements in the dataset, resulting in uniform shuffle.\n```\ndataset = tf.data.Dataset.range(20)\ndataset = dataset.shuffle(dataset.cardinality())\n# [18, 4, 9, 2, 17, 8, 5, 10, 0, 6, 16, 3, 19, 7, 14, 11, 15, 13, 12, 1]\n\n```\n\nArgs  \n---  \n`buffer_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements from this dataset from which the new dataset will sample. To uniformly shuffle the entire dataset, use `buffer_size=dataset.cardinality()`.   \n`seed` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the random seed that will be used to create the distribution. See [`tf.random.set_seed`](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) for behavior.   \n`reshuffle_each_iteration` |  (Optional.) A boolean, which if true indicates that the dataset should be pseudorandomly reshuffled each time it is iterated over. (Defaults to `True`.)   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `skip`\n```\nskip(\n    count, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` that skips `count` elements from this dataset.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.skip(7)\nlist(dataset.as_numpy_iterator())\n[7, 8, 9]\n```\n\nArgs  \n---  \n`count` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of this dataset that should be skipped to form the new dataset. If `count` is greater than the size of this dataset, the new dataset will contain no elements. If `count` is -1, skips the entire dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `snapshot`\n```\nsnapshot(\n    path,\n    compression='AUTO',\n    reader_func=None,\n    shard_func=None,\n    name=None\n) -> 'DatasetV2'\n\n```\n\nAPI to persist the output of the input dataset.\nThe snapshot API allows users to transparently persist the output of their preprocessing pipeline to disk, and materialize the pre-processed data on a different training run.\nThis API enables repeated preprocessing steps to be consolidated, and allows re-use of already processed data, trading off disk storage and network bandwidth for freeing up more valuable CPU resources and accelerator compute time.\nUsers can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the `reader_func` and `shard_func` parameters.\n`shard_func` is a user specified function that maps input elements to snapshot shards.\nUsers may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential `shard_func` could be written.\n```\ndataset = ...\ndataset = dataset.enumerate()\ndataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n    shard_func=lambda x, y: x % NUM_SHARDS, ...)\ndataset = dataset.map(lambda x, y: y)\n\n```\n\n`reader_func` is a user specified function that accepts a single argument: (1) a Dataset of Datasets, each representing a \"split\" of elements of the original dataset. The cardinality of the input dataset matches the number of the shards specified in the `shard_func` (see above). The function should return a Dataset of elements of the original dataset.\nUsers may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.\nHere is an example of a standard reader function a user can define. This function enables both dataset shuffling and parallel reading of datasets:\n```\ndefuser_reader_func(datasets):\n  # shuffle the datasets splits\n  datasets = datasets.shuffle(NUM_CORES)\n  # read datasets in parallel and interleave their elements\n  return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)\n\ndataset = dataset.snapshot(\"/path/to/snapshot/dir\",\n    reader_func=user_reader_func)\n\n```\n\nBy default, snapshot parallelizes reads by the number of cores available on the system, but will not attempt to shuffle the data.\nArgs  \n---  \n`path` |  Required. A directory to use for storing / loading the snapshot to / from.   \n`compression` |  Optional. The type of compression to apply to the snapshot written to disk. Supported options are `GZIP`, `SNAPPY`, `AUTO` or None. Defaults to `AUTO`, which attempts to pick an appropriate compression algorithm for the dataset.   \n`reader_func` |  Optional. A function to control how to read data from snapshot shards.   \n`shard_func` |  Optional. A function to control how to shard data when writing a snapshot.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `sparse_batch`\n```\nsparse_batch(\n    batch_size, row_shape, name=None\n) -> 'DatasetV2'\n\n```\n\nCombines consecutive elements into [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)s.\nLike [`Dataset.padded_batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch), this transformation combines multiple consecutive elements of the dataset, which might have different shapes, into a single element. The resulting element has three components (`indices`, `values`, and `dense_shape`), which comprise a [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor) that represents the same data. The `row_shape` represents the dense shape of each row in the resulting [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), to which the effective batch size is prepended. For example:\n```\n# NOTE: The following examples use `{ ... }` to represent the\n# contents of a dataset.\na = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }\n\na.apply(tf.data.experimental.dense_to_sparse_batch(\n    batch_size=2, row_shape=[6])) ==\n{\n    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices\n     ['a', 'b', 'c', 'a', 'b'],                 # values\n     [2, 6]),                                   # dense_shape\n    ([[0, 0], [0, 1], [0, 2], [0, 3]],\n     ['a', 'b', 'c', 'd'],\n     [1, 6])\n}\n\n```\n\nArgs  \n---  \n`batch_size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of consecutive elements of this dataset to combine in a single batch.   \n`row_shape` |  A [`tf.TensorShape`](https://www.tensorflow.org/api_docs/python/tf/TensorShape) or [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) vector tensor-like object representing the equivalent dense shape of a row in the resulting [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor). Each element of this dataset must have the same rank as `row_shape`, and must have size less than or equal to `row_shape` in each dimension.   \n`name` |  (Optional.) A string indicating a name for the [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `take`\n```\ntake(\n    count, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` with at most `count` elements from this dataset.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take(3)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2]\n```\n\nArgs  \n---  \n`count` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of this dataset that should be taken to form the new dataset. If `count` is -1, or if `count` is greater than the size of this dataset, the new dataset will contain all elements of this dataset.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `take_while`\n```\ntake_while(\n    predicate, name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that stops dataset iteration based on a `predicate`.\n```\ndataset = tf.data.Dataset.range(10)\ndataset = dataset.take_while(lambda x: x < 5)\nlist(dataset.as_numpy_iterator())\n[0, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`predicate` |  A function that maps a nested structure of tensors (having shapes and types defined by `self.output_shapes` and `self.output_types`) to a scalar [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) tensor.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `unbatch`\n```\nunbatch(\n    name=None\n) -> 'DatasetV2'\n\n```\n\nSplits elements of a dataset into multiple elements.\nFor example, if elements of the dataset are shaped `[B, a0, a1, ...]`, where `B` may vary for each input element, then for each element in the dataset, the unbatched dataset will contain `B` consecutive elements of shape `[a0, a1, ...]`.\n```\nelements = [ [1, 2, 3], [1, 2], [1, 2, 3, 4] ]\ndataset = tf.data.Dataset.from_generator(lambda: elements, tf.int64)\ndataset = dataset.unbatch()\nlist(dataset.as_numpy_iterator())\n[1, 2, 3, 1, 2, 1, 2, 3, 4]\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `unique`\n```\nunique(\n    name=None\n) -> 'DatasetV2'\n\n```\n\nA transformation that discards duplicate elements of a `Dataset`.\nUse this transformation to produce a dataset that contains one instance of each unique element in the input. For example:\n```\ndataset = tf.data.Dataset.from_tensor_slices([1, 37, 2, 37, 2, 1])\ndataset = dataset.unique()\nsorted(list(dataset.as_numpy_iterator()))\n[1, 2, 37]\n```\n\nArgs  \n---  \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `window`\n```\nwindow(\n    size, shift=None, stride=1, drop_remainder=False, name=None\n) -> 'DatasetV2'\n\n```\n\nReturns a dataset of \"windows\".\nEach \"window\" is a dataset that contains a subset of elements of the input dataset. These are finite datasets of size `size` (or possibly fewer if there are not enough input elements to fill the window and `drop_remainder` evaluates to `False`).\n#### For example:\n```\ndataset = tf.data.Dataset.range(7).window(3)\nfor window in dataset:\n  print(window)\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n```\n\nSince windows are datasets, they can be iterated over:\n```\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[3, 4, 5]\n[6]\n```\n\n#### Shift\nThe `shift` argument determines the number of input elements to shift between the start of each window. If windows and elements are both numbered starting at 0, the first element in window `k` will be element `k * shift` of the input dataset. In particular, the first element of the first window will always be the first element of the input dataset.\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1,\n                                          drop_remainder=True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 1, 2]\n[1, 2, 3]\n[2, 3, 4]\n[3, 4, 5]\n[4, 5, 6]\n```\n\n#### Stride\nThe `stride` argument determines the stride between input elements within a window.\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1, stride=2,\n                                          drop_remainder=True)\nfor window in dataset:\n  print(list(window.as_numpy_iterator()))\n[0, 2, 4]\n[1, 3, 5]\n[2, 4, 6]\n```\n\n#### Nested elements\nWhen the `window` transformation is applied to a dataset whos elements are nested structures, it produces a dataset where the elements have the same nested structure but each leaf is replaced by a window. In other words, the nesting is applied outside of the windows as opposed inside of them.\n#### The type signature is:\n```\ndefwindow(\n    self: Dataset[Nest[T]], ...\n) -> Dataset[Nest[Dataset[T]]]\n\n```\n\nApplying `window` to a `Dataset` of tuples gives a tuple of windows:\n```\ndataset = tf.data.Dataset.from_tensor_slices(([1, 2, 3, 4, 5],\n                                              [6, 7, 8, 9, 10]))\ndataset = dataset.window(2)\nwindows = next(iter(dataset))\nwindows\n(<...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>,\n <...Dataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>)\n```\n```\ndefto_numpy(ds):\n  return list(ds.as_numpy_iterator())\nfor windows in dataset:\n  print(to_numpy(windows[0]), to_numpy(windows[1]))\n[1, 2] [6, 7]\n[3, 4] [8, 9]\n[5] [10]\n```\n\nApplying `window` to a `Dataset` of dictionaries gives a dictionary of `Datasets`:\n```\ndataset = tf.data.Dataset.from_tensor_slices({'a': [1, 2, 3],\n                                              'b': [4, 5, 6],\n                                              'c': [7, 8, 9]})\ndataset = dataset.window(2)\ndefto_numpy(ds):\n  return list(ds.as_numpy_iterator())\nfor windows in dataset:\n  print(tf.nest.map_structure(to_numpy, windows))\n{'a': [1, 2], 'b': [4, 5], 'c': [7, 8]}\n{'a': [3], 'b': [6], 'c': [9]}\n```\n\n#### Flatten a dataset of windows\nThe [`Dataset.flat_map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map) and [`Dataset.interleave`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave) methods can be used to flatten a dataset of windows into a single dataset.\nThe argument to `flat_map` is a function that takes an element from the dataset and returns a `Dataset`. `flat_map` chains together the resulting datasets sequentially.\nFor example, to turn each window into a dense tensor:\n```\ndataset = tf.data.Dataset.range(7).window(3, shift=1,\n                                          drop_remainder=True)\nbatched = dataset.flat_map(lambda x:x.batch(3))\nfor batch in batched:\n  print(batch.numpy())\n[0 1 2]\n[1 2 3]\n[2 3 4]\n[3 4 5]\n[4 5 6]\n```\n\nArgs  \n---  \n`size` |  A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of elements of the input dataset to combine into a window. Must be positive.   \n`shift` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the number of input elements by which the window moves in each iteration. Defaults to `size`. Must be positive.   \n`stride` |  (Optional.) A [`tf.int64`](https://www.tensorflow.org/api_docs/python/tf#int64) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing the stride of the input elements in the sliding window. Must be positive. The default value of 1 means \"retain every input element\".   \n`drop_remainder` |  (Optional.) A [`tf.bool`](https://www.tensorflow.org/api_docs/python/tf#bool) scalar [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), representing whether the last windows should be dropped if their size is smaller than `size`.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `with_options`\n```\nwith_options(\n    options, name=None\n) -> 'DatasetV2'\n\n```\n\nReturns a new [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) with the given options set.\nThe options are \"global\" in the sense they apply to the entire dataset. If options are set multiple times, they are merged as long as different options do not use different non-default values.\n```\nds = tf.data.Dataset.range(5)\nds = ds.interleave(lambda x: tf.data.Dataset.range(5),\n                   cycle_length=3,\n                   num_parallel_calls=3)\noptions = tf.data.Options()\n# This will make the interleave order non-deterministic.\noptions.deterministic = False\nds = ds.with_options(options)\n```\n\nArgs  \n---  \n`options` |  A [`tf.data.Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options) that identifies the options the use.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \nRaises  \n---  \n`ValueError` |  when an option is set more than once to a non-default value   \n### `zip`\n```\n@staticmethod\nzip(\n    *args, datasets=None, name=None\n) -> 'DatasetV2'\n\n```\n\nCreates a `Dataset` by zipping together the given datasets.\nThis method has similar semantics to the built-in `zip()` function in Python, with the main difference being that the `datasets` argument can be a (nested) structure of `Dataset` objects. The supported nesting mechanisms are documented [here](https://www.tensorflow.org/guide/data#dataset_structure).\n```\n# The datasets or nested structure of datasets `*args` argument\n# determines the structure of elements in the resulting dataset.\na = tf.data.Dataset.range(1, 4)  # ==> [ 1, 2, 3 ]\nb = tf.data.Dataset.range(4, 7)  # ==> [ 4, 5, 6 ]\nds = tf.data.Dataset.zip(a, b)\nlist(ds.as_numpy_iterator())\n[(1, 4), (2, 5), (3, 6)]\nds = tf.data.Dataset.zip(b, a)\nlist(ds.as_numpy_iterator())\n[(4, 1), (5, 2), (6, 3)]\n# The `datasets` argument may contain an arbitrary number of datasets.\nc = tf.data.Dataset.range(7, 13).batch(2)  # ==> [ [7, 8],\n                                           #       [9, 10],\n                                           #       [11, 12] ]\nds = tf.data.Dataset.zip(a, b, c)\nfor element in ds.as_numpy_iterator():\n  print(element)\n(1, 4, array([7, 8]))\n(2, 5, array([ 9, 10]))\n(3, 6, array([11, 12]))\n# The number of elements in the resulting dataset is the same as\n# the size of the smallest dataset in `datasets`.\nd = tf.data.Dataset.range(13, 15)  # ==> [ 13, 14 ]\nds = tf.data.Dataset.zip(a, d)\nlist(ds.as_numpy_iterator())\n[(1, 13), (2, 14)]\n```\n\nArgs  \n---  \n`*args` |  Datasets or nested structures of datasets to zip together. This can't be set if `datasets` is set.   \n`datasets` |  A (nested) structure of datasets. This can't be set if `*args` is set. Note that this exists only for backwards compatibility and it is preferred to use *args.   \n`name` |  (Optional.) A name for the tf.data operation.   \nReturns  \n---  \nA new `Dataset` with the transformation applied as described above.   \n### `__bool__`\n```\n__bool__()\n\n```\n\n### `__iter__`\n```\n__iter__() -> iterator_ops.OwnedIterator\n\n```\n\nCreates an iterator for elements of this dataset.\nThe returned iterator implements the Python Iterator protocol.\nReturns  \n---  \nAn [`tf.data.Iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) for the elements of this dataset.   \nRaises  \n---  \n`RuntimeError` |  If not inside of tf.function and not executing eagerly.   \n### `__len__`\n```\n__len__()\n\n```\n\nReturns the length of the dataset if it is known and finite.\nThis method requires that you are running in eager mode, and that the length of the dataset is known and non-infinite. When the length may be unknown or infinite, or if you are running in graph mode, use [`tf.data.Dataset.cardinality`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cardinality) instead.\nReturns  \n---  \nAn integer representing the length of the dataset.   \nRaises  \n---  \n`RuntimeError` |  If the dataset length is unknown or infinite, or if eager execution is not enabled.   \n### `__nonzero__`\n```\n__nonzero__()\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute": "Public API for tf._api.v2.distribute namespace\n## Modules\n[`cluster_resolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver) module: Public API for tf._api.v2.distribute.cluster_resolver namespace\n[`coordinator`](https://www.tensorflow.org/api_docs/python/tf/distribute/coordinator) module: Public API for tf._api.v2.distribute.coordinator namespace\n[`experimental`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental) module: Public API for tf._api.v2.distribute.experimental namespace\n## Classes\n[`class CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps): Base class for cross-device reduction and broadcasting algorithms.\n[`class DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset): Represents a dataset distributed among devices and machines.\n[`class DistributedIterator`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedIterator): An iterator over [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).\n[`class DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues): Base class for representing distributed values.\n[`class HierarchicalCopyAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce): Hierarchical copy all-reduce implementation of CrossDeviceOps.\n[`class InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext): A class wrapping information needed by an input function.\n[`class InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions): Run options for `experimental_distribute_dataset(s_from_function)`.\n[`class InputReplicationMode`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputReplicationMode): Replication mode for input function.\n[`class MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy): Synchronous training across multiple replicas on one machine.\n[`class MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy): A distribution strategy for synchronous training on multiple workers.\n[`class NcclAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce): NCCL all-reduce implementation of CrossDeviceOps.\n[`class OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy): A distribution strategy for running on a single device.\n[`class ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy): An multi-worker tf.distribute strategy with parameter servers.\n[`class ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp): Indicates how a set of values should be reduced.\n[`class ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice): A CrossDeviceOps implementation that copies values to one device to reduce.\n[`class ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext): A class with a collection of APIs that can be called in a replica context.\n[`class RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions): Run options for `strategy.run`.\n[`class Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server): An in-process TensorFlow server, for use in distributed training.\n[`class Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy): A state & compute distribution policy on a list of devices.\n[`class StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended): Additional APIs for algorithms that need to be distribution-aware.\n[`class TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy): Synchronous training on TPUs and TPU Pods.\n## Functions\n[`experimental_set_strategy(...)`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental_set_strategy): Set a [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) as current without `with strategy.scope()`.\n[`get_replica_context(...)`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context): Returns the current [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) or `None`.\n[`get_strategy(...)`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy): Returns the current [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object.\n[`has_strategy(...)`](https://www.tensorflow.org/api_docs/python/tf/distribute/has_strategy): Return if there is a current non-default [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy).\n[`in_cross_replica_context(...)`](https://www.tensorflow.org/api_docs/python/tf/distribute/in_cross_replica_context): Returns `True` if in a cross-replica context.\n",
  "https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy": "Represents the type of auto-sharding to use.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy)\nOFF: No sharding will be performed.\nAUTO: Attempts FILE-based sharding, falling back to DATA-based sharding.\nFILE: Shards by input files (i.e. each worker will get a set of files to process). When this option is selected, make sure that there is at least as many files as workers. If there are fewer input files than workers, a runtime error will be raised.\nDATA: Shards by elements produced by the dataset. Each worker will process the whole dataset and discard the portion that is not for itself. Note that for this mode to correctly partitions the dataset elements, the dataset needs to produce elements in a deterministic order.\nHINT: Looks for the presence of `shard(SHARD_HINT, ...)` which is treated as a placeholder to replace with `shard(num_workers, worker_index)`.\n## Class Variables  \n---  \nAUTO  |  `<AutoShardPolicy.AUTO: 0>`  \nDATA  |  `<AutoShardPolicy.DATA: 2>`  \nFILE  |  `<AutoShardPolicy.FILE: 1>`  \nHINT  |  `<AutoShardPolicy.HINT: 3>`  \nOFF  |  `<AutoShardPolicy.OFF: -1>`\n",
  "https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions": "Represents options for distributed data processing.\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions)\n```\ntf.data.experimental.DistributeOptions()\n\n```\n\nYou can set the distribution options of a dataset through the `experimental_distribute` property of [`tf.data.Options`](https://www.tensorflow.org/api_docs/python/tf/data/Options); the property is an instance of [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions).\n```\noptions = tf.data.Options()\noptions.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\ndataset = dataset.with_options(options)\n\n```\n\n## Attributes  \n---  \n`auto_shard_policy` |  The type of sharding to use. See [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) for additional information.   \n`num_devices` |  The number of devices attached to this input pipeline. This will be automatically set by `MultiDeviceIterator`.   \n## Methods\n### `__eq__`\n```\n__eq__(\n    other\n)\n\n```\n\nReturn self==value.\n### `__ne__`\n```\n__ne__(\n    other\n)\n\n```\n\nReturn self!=value.\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps": "Base class for cross-device reduction and broadcasting algorithms.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps)\n```\ntf.distribute.CrossDeviceOps()\n\n```\n\nThe main purpose of this class is to be passed to [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) in order to choose among different cross device communication implementations. Prefer using the methods of [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) instead of the ones of this class.\n#### Implementations:\n  * [`tf.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice)\n  * [`tf.distribute.NcclAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce)\n  * [`tf.distribute.HierarchicalCopyAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce)\n\n\n## Methods\n### `batch_reduce`\n```\nbatch_reduce(\n    reduce_op, value_destination_pairs, options=None\n)\n\n```\n\nReduce values to destinations in batches.\nSee [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See [`tf.distribute.CrossDeviceOps.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps#reduce) for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA list of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), one per pair in `value_destination_pairs`.   \nRaises  \n---  \n`ValueError` |  if `value_destination_pairs` is not an iterable of tuples of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and destinations.   \n### `batch_reduce_implementation`\n```\nbatch_reduce_implementation(\n    reduce_op, value_destination_pairs, options\n)\n\n```\n\nImplementation of `batch_reduce`.\nOverriding this method is useful for subclass implementers.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See `reduce` for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA list of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), one per pair in `value_destination_pairs`.   \nRaises  \n---  \n`ValueError` |  if `value_destination_pairs` is not an iterable of tuples of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and destinations.   \n### `broadcast`\n```\nbroadcast(\n    tensor, destinations\n)\n\n```\n\nBroadcast `tensor` to `destinations`.\nThis can only be called in the cross-replica context.\nArgs  \n---  \n`tensor` |  a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object. The value to broadcast.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to broadcast to. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is broadcasted to the devices of that variable, this method doesn't update the variable.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `broadcast_implementation`\n```\nbroadcast_implementation(\n    tensor, destinations\n)\n\n```\n\nImplementation of `broadcast`.\nArgs  \n---  \n`tensor` |  a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object. The value to broadcast.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to broadcast to. `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is broadcasted to the devices of that variable, this method doesn't update the variable.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `reduce`\n```\nreduce(\n    reduce_op, per_replica_value, destinations, options=None\n)\n\n```\n\nReduce `per_replica_value` to `destinations`.\nSee [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`per_replica_value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, and this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \nRaises  \n---  \n`ValueError` |  if per_replica_value can't be converted to a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or if destinations is not a string, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `reduce_implementation`\n```\nreduce_implementation(\n    reduce_op, per_replica_value, destinations, options\n)\n\n```\n\nImplementation of `reduce`.\nOverriding this method is useful for subclass implementers.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`per_replica_value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \nRaises  \n---  \n`ValueError` |  if per_replica_value can't be converted to a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or if destinations is not a string, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). \n",
  "https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_batched_features_dataset": "Returns a `Dataset` of feature dictionaries from `Example` protos.  \n```\ntf.data.experimental.make_batched_features_dataset(\n    file_pattern,\n    batch_size,\n    features,\n    reader=None,\n    label_key=None,\n    reader_args=None,\n    num_epochs=None,\n    shuffle=True,\n    shuffle_buffer_size=10000,\n    shuffle_seed=None,\n    prefetch_buffer_size=None,\n    reader_num_threads=None,\n    parser_num_threads=None,\n    sloppy_ordering=False,\n    drop_final_batch=False\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n\n  \nIf label_key argument is provided, returns a `Dataset` of tuple comprising of feature dictionaries and label.\n#### Example:\n```\nserialized_examples = [\n  features {\n    feature { key: \"age\" value { int64_list { value: [ 0 ] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n    feature { key: \"kws\" value { bytes_list { value: [ \"code\", \"art\" ] } } }\n  },\n  features {\n    feature { key: \"age\" value { int64_list { value: [] } } }\n    feature { key: \"gender\" value { bytes_list { value: [ \"f\" ] } } }\n    feature { key: \"kws\" value { bytes_list { value: [ \"sports\" ] } } }\n  }\n]\n\n```\n\n#### We can use arguments:\n```\nfeatures: {\n  \"age\": FixedLenFeature([], dtype=tf.int64, default_value=-1),\n  \"gender\": FixedLenFeature([], dtype=tf.string),\n  \"kws\": VarLenFeature(dtype=tf.string),\n}\n\n```\n\nAnd the expected output is:\n```\n{\n  \"age\": [[0], [-1]],\n  \"gender\": [[\"f\"], [\"f\"]],\n  \"kws\": SparseTensor(\n    indices=[[0, 0], [0, 1], [1, 0]],\n    values=[\"code\", \"art\", \"sports\"]\n    dense_shape=[2, 2]),\n}\n\n```\n\n## Args  \n---  \n`file_pattern` |  List of files or patterns of file paths containing `Example` records. See [`tf.io.gfile.glob`](https://www.tensorflow.org/api_docs/python/tf/io/gfile/glob) for pattern rules.   \n`batch_size` |  An int representing the number of records to combine in a single batch.   \n`features` |  A `dict` mapping feature keys to `FixedLenFeature` or `VarLenFeature` values. See [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example).   \n`reader` |  A function or class that can be called with a `filenames` tensor and (optional) `reader_args` and returns a `Dataset` of `Example` tensors. Defaults to [`tf.data.TFRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset).   \n`label_key` |  (Optional) A string corresponding to the key labels are stored in `tf.Examples`. If provided, it must be one of the `features` key, otherwise results in `ValueError`.   \n`reader_args` |  Additional arguments to pass to the reader class.   \n`num_epochs` |  Integer specifying the number of times to read through the dataset. If None, cycles through the dataset forever. Defaults to `None`.   \n`shuffle` |  A boolean, indicates whether the input should be shuffled. Defaults to `True`.   \n`shuffle_buffer_size` |  Buffer size of the ShuffleDataset. A large capacity ensures better shuffling but would increase memory usage and startup time.   \n`shuffle_seed` |  Randomization seed to use for shuffling.   \n`prefetch_buffer_size` |  Number of feature batches to prefetch in order to improve performance. Recommended value is the number of batches consumed per training step. Defaults to auto-tune.   \n`reader_num_threads` |  Number of threads used to read `Example` records. If >1, the results will be interleaved. Defaults to `1`.   \n`parser_num_threads` |  Number of threads to use for parsing `Example` tensors into a dictionary of `Feature` tensors. Defaults to `2`.   \n`sloppy_ordering` |  If `True`, reading performance will be improved at the cost of non-deterministic ordering. If `False`, the order of elements produced is deterministic prior to shuffling (elements are still randomized if `shuffle=True`. Note that if the seed is set, then order of elements after shuffling is deterministic). Defaults to `False`.   \n`drop_final_batch` |  If `True`, and the batch size does not evenly divide the input dataset size, the final smaller batch will be dropped. Defaults to `False`.   \n## Returns  \n---  \nA dataset of `dict` elements, (or a tuple of `dict` elements and label). Each `dict` maps feature keys to `Tensor` or `SparseTensor` objects.   \n## Raises  \n---  \n`TypeError` |  If `reader` is of the wrong type.   \n`ValueError` |  If `label_key` is not one of the `features` keys. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset": "Represents a dataset distributed among devices and machines.  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) could be thought of as a \"distributed\" dataset. When you use [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) API to scale training to multiple devices or machines, you also need to distribute the input data, which leads to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) instance, instead of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance in the non-distributed case. In TF 2.x, [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) objects are Python iterables.\nThere are two APIs to create a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) object: [`tf.distribute.Strategy.experimental_distribute_dataset(dataset)`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset)and [`tf.distribute.Strategy.distribute_datasets_from_function(dataset_fn)`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function). _When to use which?_ When you have a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance, and the regular batch splitting (i.e. re-batch the input [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance with a new batch size that is equal to the global batch size divided by the number of replicas in sync) and autosharding (i.e. the [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) options) work for you, use the former API. Otherwise, if you are _not_ using a canonical [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance, or you would like to customize the batch splitting or sharding, you can wrap these logic in a `dataset_fn` and use the latter API. Both API handles prefetch to device for the user. For more details and examples, follow the links to the APIs.\nThere are two main usages of a `DistributedDataset` object:\n  1. Iterate over it to generate the input for a single device or multiple devices, which is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance. To do this, you can:\n     * use a pythonic for-loop construct:\n```\nglobal_batch_size = 4\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensors(([1.],[1.])).repeat(4).batch(global_batch_size)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndeftrain_step(input):\n  features, labels = input\n  return labels - 0.3 * features\nfor x in dist_dataset:\n  # train_step trains the model using the dataset elements\n  loss = strategy.run(train_step, args=(x,))\n  print(\"Loss is\", loss)\n    Loss is PerReplica:{\n      0: tf.Tensor(\n    [[0.7]\n     [0.7]], shape=(2, 1), dtype=float32),\n      1: tf.Tensor(\n    [[0.7]\n     [0.7]], shape=(2, 1), dtype=float32)\n\n\n```\n\nPlacing the loop inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will give a performance boost. However `break` and `return` are currently not supported if the loop is placed inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). We also don't support placing the loop inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) when using [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.experimental.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy) with multiple workers.\n     * use `__iter__` to create an explicit iterator, which is of type [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedIterator)\n```\nglobal_batch_size = 4\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntrain_dataset = tf.data.Dataset.from_tensors(([1.],[1.])).repeat(50).batch(global_batch_size)\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n@tf.function\ndefdistributed_train_step(dataset_inputs):\n  deftrain_step(input):\n    loss = tf.constant(0.1)\n    return loss\n  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,axis=None)\nEPOCHS = 2\nSTEPS = 3\nfor epoch in range(EPOCHS):\n  total_loss = 0.0\n  num_batches = 0\n  dist_dataset_iterator = iter(train_dist_dataset)\n  for _ in range(STEPS):\n    total_loss += distributed_train_step(next(dist_dataset_iterator))\n    num_batches += 1\n  average_train_loss = total_loss / num_batches\n  template = (\"Epoch {}, Loss: {:.4f}\")\n  print (template.format(epoch+1, average_train_loss))\n    Epoch 1, Loss: 0.2000\n    Epoch 2, Loss: 0.2000\n\n```\n\nTo achieve a performance improvement, you can also wrap the `strategy.run` call with a [`tf.range`](https://www.tensorflow.org/api_docs/python/tf/range) inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). This runs multiple steps in a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Autograph will convert it to a [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop) on the worker. However, it is less flexible comparing with running a single step inside [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). For example, you cannot run things eagerly or arbitrary python code within the steps.\n  2. Inspect the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by `DistributedDataset`.\n[`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) generates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) as input to the devices. If you pass the input to a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) and would like to specify the shape and type of each Tensor argument to the function, you can pass a [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) object to the `input_signature` argument of the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). To get the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the input, you can use the `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) or [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedIterator) object.\nFor example:\n```\nglobal_batch_size = 4\nepochs = 1\nsteps_per_epoch = 1\nmirrored_strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensors(([2.])).repeat(100).batch(global_batch_size)\ndist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n@tf.function(input_signature=[dist_dataset.element_spec])\ndeftrain_step(per_replica_inputs):\n  defstep_fn(inputs):\n    return tf.square(inputs)\n  return mirrored_strategy.run(step_fn, args=(per_replica_inputs,))\nfor _ in range(epochs):\n  iterator = iter(dist_dataset)\n  for _ in range(steps_per_epoch):\n    output = train_step(next(iterator))\n    print(output)\n  PerReplica:{\n    0: tf.Tensor(\n  [[4.]\n   [4.]], shape=(2, 1), dtype=float32),\n    1: tf.Tensor(\n  [[4.]\n   [4.]], shape=(2, 1), dtype=float32)\n\n\n```\n\n\n\nVisit the [tutorial](https://www.tensorflow.org/tutorials/distribute/input) on distributed input for more examples and caveats.\n## Attributes  \n---  \n`element_spec` |  The type specification of an element of this [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).```\nglobal_batch_size = 16\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensors(([1.],[2])).repeat(100).batch(global_batch_size)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\ndist_dataset.element_spec\n(PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.float32, name=None),\n                TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)),\n PerReplicaSpec(TensorSpec(shape=(None, 1), dtype=tf.int32, name=None),\n                TensorSpec(shape=(None, 1), dtype=tf.int32, name=None)))\n```\n  \n## Methods\n### `__iter__`\n```\n__iter__()\n\n```\n\nCreates an iterator for the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).\nThe returned iterator implements the Python Iterator protocol.\n#### Example usage:\n```\nglobal_batch_size = 4\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4]).repeat().batch(global_batch_size)\ndistributed_iterator = iter(strategy.experimental_distribute_dataset(dataset))\nprint(next(distributed_iterator))\nPerReplica:{\n  0: tf.Tensor([1 2], shape=(2,), dtype=int32),\n  1: tf.Tensor([3 4], shape=(2,), dtype=int32)\n\n```\n\nReturns  \n---  \nAn [`tf.distribute.DistributedIterator`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedIterator) instance for the given [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) object to enumerate over the distributed data. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce": "Hierarchical copy all-reduce implementation of CrossDeviceOps.  \nInherits From: [`CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps)\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.HierarchicalCopyAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce)\n```\ntf.distribute.HierarchicalCopyAllReduce(\n    num_packs=1\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide  \n---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n  \nIt reduces to one GPU along edges in some hierarchy and broadcasts back to each GPU along the same path. For the batch API, tensors will be repacked or aggregated for more efficient cross-device transportation.\nThis is a reduction created for Nvidia DGX-1 which assumes GPUs connects like that on DGX-1 machine. If you have different GPU inter-connections, it is likely that it would be slower than [`tf.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice).\nFor reduces that are not all-reduce, it falls back to [`tf.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice).\nHere is how you can use `HierarchicalCopyAllReduce` in [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy):\n```\n  strategy = tf.distribute.MirroredStrategy(\n    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())\n\n```\n\n## Args  \n---  \n`num_packs` |  a non-negative integer. The number of packs to split values into. If zero, no packing will be done.   \n## Raises  \n---  \nValueError if `num_packs` is negative.   \n## Methods\n### `batch_reduce`\n```\nbatch_reduce(\n    reduce_op, value_destination_pairs, options=None\n)\n\n```\n\nReduce values to destinations in batches.\nSee [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See [`tf.distribute.CrossDeviceOps.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps#reduce) for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA list of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), one per pair in `value_destination_pairs`.   \nRaises  \n---  \n`ValueError` |  if `value_destination_pairs` is not an iterable of tuples of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and destinations.   \n### `broadcast`\n```\nbroadcast(\n    tensor, destinations\n)\n\n```\n\nBroadcast `tensor` to `destinations`.\nThis can only be called in the cross-replica context.\nArgs  \n---  \n`tensor` |  a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object. The value to broadcast.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to broadcast to. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is broadcasted to the devices of that variable, this method doesn't update the variable.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `reduce`\n```\nreduce(\n    reduce_op, per_replica_value, destinations, options=None\n)\n\n```\n\nReduce `per_replica_value` to `destinations`.\nSee [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`per_replica_value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, and this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \nRaises  \n---  \n`ValueError` |  if per_replica_value can't be converted to a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or if destinations is not a string, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions": "Run options for `experimental_distribute_dataset(s_from_function)`.  \n```\ntf.distribute.InputOptions(\n    experimental_fetch_to_device=None,\n    experimental_replication_mode=[tf.distribute.InputReplicationMode.PER_WORKER](https://www.tensorflow.org/api_docs/python/tf/distribute/InputReplicationMode#PER_WORKER),\n    experimental_place_dataset_on_device=False,\n    experimental_per_replica_buffer_size=1\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrate from TPU embedding_columns to TPUEmbedding layer](https://www.tensorflow.org/guide/migrate/tpu_embedding)\n\n| \n  * [TensorFlow 2 TPUEmbeddingLayer: Quick Start](https://www.tensorflow.org/recommenders/examples/tpu_embedding_layer)\n\n  \nThis can be used to hold some strategy specific configs.\n```\n# Setup TPUStrategy\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)\n\ndataset = tf.data.Dataset.range(16)\ndistributed_dataset_on_host = (\n    strategy.experimental_distribute_dataset(\n        dataset,\n        tf.distribute.InputOptions(\n            experimental_replication_mode=\n            experimental_replication_mode.PER_WORKER,\n            experimental_place_dataset_on_device=False,\n            experimental_per_replica_buffer_size=1)))\n\n```\n\n## Attributes  \n---  \n`experimental_fetch_to_device` |  Boolean. If True, dataset elements will be prefetched to accelerator device memory. When False, dataset elements are prefetched to host device memory. Must be False when using TPUEmbedding API. experimental_fetch_to_device can only be used with experimental_replication_mode=PER_WORKER. Default behavior is same as setting it to True.   \n`experimental_replication_mode` |  Replication mode for the input function. Currently, the InputReplicationMode.PER_REPLICA is only supported with tf.distribute.MirroredStrategy. experimental_distribute_datasets_from_function. The default value is InputReplicationMode.PER_WORKER.   \n`experimental_place_dataset_on_device` |  Boolean. Default to False. When True, dataset will be placed on the device, otherwise it will remain on the host. experimental_place_dataset_on_device=True can only be used with experimental_replication_mode=PER_REPLICA   \n`experimental_per_replica_buffer_size` |  Integer. Default to 1. Indicates the prefetch buffer size in the replica device memory. Users can set it to 0 to completely disable prefetching behavior, or a number greater than 1 to enable larger buffer size. Note that this option is still valid with `experimental_fetch_to_device=False`. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce": "NCCL all-reduce implementation of CrossDeviceOps.  \nInherits From: [`CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps)\nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.NcclAllReduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/NcclAllReduce)\n```\ntf.distribute.NcclAllReduce(\n    num_packs=1\n)\n\n```\n\nIt uses Nvidia NCCL for all-reduce. For the batch API, tensors will be repacked or aggregated for more efficient cross-device transportation.\nFor reduces that are not all-reduce, it falls back to [`tf.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice).\nHere is how you can use `NcclAllReduce` in [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy):\n```\n  strategy = tf.distribute.MirroredStrategy(\n    cross_device_ops=tf.distribute.NcclAllReduce())\n\n```\n\n## Args  \n---  \n`num_packs` |  a non-negative integer. The number of packs to split values into. If zero, no packing will be done.   \n## Raises  \n---  \n`ValueError` |  if `num_packs` is negative.   \n## Methods\n### `batch_reduce`\n```\nbatch_reduce(\n    reduce_op, value_destination_pairs, options=None\n)\n\n```\n\nReduce values to destinations in batches.\nSee [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See [`tf.distribute.CrossDeviceOps.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps#reduce) for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA list of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), one per pair in `value_destination_pairs`.   \nRaises  \n---  \n`ValueError` |  if `value_destination_pairs` is not an iterable of tuples of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and destinations.   \n### `broadcast`\n```\nbroadcast(\n    tensor, destinations\n)\n\n```\n\nBroadcast `tensor` to `destinations`.\nThis can only be called in the cross-replica context.\nArgs  \n---  \n`tensor` |  a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object. The value to broadcast.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to broadcast to. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is broadcasted to the devices of that variable, this method doesn't update the variable.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `reduce`\n```\nreduce(\n    reduce_op, per_replica_value, destinations, options=None\n)\n\n```\n\nReduce `per_replica_value` to `destinations`.\nSee [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`per_replica_value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, and this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \nRaises  \n---  \n`ValueError` |  if per_replica_value can't be converted to a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or if destinations is not a string, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy": "A distribution strategy for running on a single device.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.OneDeviceStrategy(\n    device\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Save and load a model using a distribution strategy](https://www.tensorflow.org/tutorials/distribute/save_and_load)\n  * [Training with Orbit](https://www.tensorflow.org/tfmodels/orbit/index)\n  * [Image classification with Model Garden](https://www.tensorflow.org/tfmodels/vision/image_classification)\n  * [Instance Segmentation with Model Garden](https://www.tensorflow.org/tfmodels/vision/instance_segmentation)\n  * [Object detection with Model Garden](https://www.tensorflow.org/tfmodels/vision/object_detection)\n\n  \nUsing this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via `strategy.run` will also be placed on the specified device as well.\nTypical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines.\n#### For example:\n```\nstrategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n\nwith strategy.scope():\n  v = tf.Variable(1.0)\n  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0\n\ndefstep_fn(x):\n  return x * 2\n\nresult = 0\nfor i in range(10):\n  result += strategy.run(step_fn, args=(i,))\nprint(result)  # 90\n\n```\n\n## Args  \n---  \n`device` |  Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: \"/cpu:0\", \"/gpu:0\", \"/device:CPU:0\", \"/device:GPU:0\"   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.In general, when using a multi-worker [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), there is a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) must set the relevant attribute, or override this property; otherwise, `None` is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver), and in those cases this property will return `None`. The [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, ```\n\nos.environ['TF_CONFIG'] = json.dumps({\n  'cluster': {\n      'worker': [\"localhost:12345\", \"localhost:23456\"],\n      'ps': [\"localhost:34567\"]\n  },\n  'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. Since we set this\n  # as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. Since we\n  # set this as a worker above, this block will not run on this particular\n  # instance.\n\n```\nFor more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s API docstring.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\n`dataset_fn` will be called once for each worker in the strategy. In this case, we only have one worker and one device so `dataset_fn` is called once.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed:\n```\ndefdataset_fn(input_context):\n  batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)\n  return d.shard(\n      input_context.num_input_pipelines, input_context.input_pipeline_id)\n\ninputs = strategy.distribute_datasets_from_function(dataset_fn)\n\nfor batch in inputs:\n  replica_results = strategy.run(replica_fn, args=(batch,))\n\n```\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA \"distributed `Dataset`\", which the caller can iterate over like regular datasets.   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nDistributes a tf.data.Dataset instance provided via dataset.\nIn this case, there is only one device, so this is only a thin wrapper around the input dataset. It will, however, prefetch the input data to the specified device. The returned distributed dataset can be iterated over similar to how regular datasets can.\n#### Example:\n```\nstrategy = tf.distribute.OneDeviceStrategy()\ndataset = tf.data.Dataset.range(10).batch(2)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\nfor x in dist_dataset:\n  print(x)  # [0, 1], [2, 3],...\n\n```\n\nArgs: dataset: [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to be prefetched to device. options: [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed. Returns: A \"distributed `Dataset`\" that the caller can iterate over.\n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nIn `OneDeviceStrategy`, the `value` is always expected to be a single value, so the result is just the value in a tuple.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run()`, `extended.call_for_each_replica()`, or a variable created in `scope`.   \nReturns  \n---  \nA tuple of values contained in `value`. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas.\nIn `OneDeviceStrategy`, there is only one replica, so if axis=None, value is simply returned. If axis is specified as something other than None, such as axis=0, value is reduced along that axis and returned.\n#### Example:\n```\nt = tf.range(10)\n\nresult = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=None).numpy()\n# result: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nresult = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=0).numpy()\n# result: 45\n\n```\nArgs  \n---  \n`reduce_op` |  A [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined.   \n`value` |  A \"per replica\" value, e.g. returned by `run` to be combined into a single tensor.   \n`axis` |  Specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nRun `fn` on each replica, with the given arguments.\nIn `OneDeviceStrategy`, `fn` is simply called within a device scope for the given device, with the provided arguments.\nArgs  \n---  \n`fn` |  The function to run. The output must be a [`tf.nest`](https://www.tensorflow.org/api_docs/python/tf/nest) of `Tensor`s.   \n`args` |  (Optional) Positional arguments to `fn`.   \n`kwargs` |  (Optional) Keyword arguments to `fn`.   \n`options` |  (Optional) An instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nReturn value from running `fn`.   \n### `scope`\n```\nscope()\n\n```\n\nReturns a context manager selecting this Strategy as current.\nInside a `with strategy.scope():` code block, this thread will use a variable creator set by `strategy`, and will enter its \"cross-replica context\".\nIn `OneDeviceStrategy`, all variables created inside `strategy.scope()` will be on `device` specified at strategy construction time. See example in the docs for this class.\nReturns  \n---  \nA context manager to use for creating variables with this strategy. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext": "A class wrapping information needed by an input function.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext)\n```\ntf.distribute.InputContext(\n    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=1\n)\n\n```\n\nThis is a context class that is passed to the user's input function and contains information about the compute replicas and input pipelines. The number of compute replicas (in sync training) helps compute the local batch size from the desired global batch size for each replica. The input pipeline information can be used to return a different subset of the input in each replica (for e.g. shard the input pipeline, use a different input source etc).\n## Args  \n---  \n`num_input_pipelines` |  the number of input pipelines in a cluster.   \n`input_pipeline_id` |  the current input pipeline id, should be an int in [0,`num_input_pipelines`).   \n`num_replicas_in_sync` |  the number of replicas that are in sync.   \n## Attributes  \n---  \n`input_pipeline_id` |  Returns the input pipeline ID.   \n`num_input_pipelines` |  Returns the number of input pipelines.   \n`num_replicas_in_sync` |  Returns the number of compute replicas in sync.   \n## Methods\n### `get_per_replica_batch_size`\n```\nget_per_replica_batch_size(\n    global_batch_size\n)\n\n```\n\nReturns the per-replica batch size.\nArgs  \n---  \n`global_batch_size` |  the global batch size which should be divisible by `num_replicas_in_sync`.   \nReturns  \n---  \nthe per-replica batch size.   \nRaises  \n---  \n`ValueError` |  if `global_batch_size` not divisible by `num_replicas_in_sync`. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy": "A distribution strategy for synchronous training on multiple workers.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.MultiWorkerMirroredStrategy(\n    cluster_resolver=None, communication_options=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n  * [Migrate multi-worker CPU/GPU training](https://www.tensorflow.org/guide/migrate/multi_worker_cpu_gpu_training)\n\n| \n  * [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)\n  * [Custom training loop with Keras and MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n\n  \nThis strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy), it replicates all variables and computations to each local device. The difference is that it uses a distributed collective implementation (e.g. all-reduce), so that multiple workers can work together.\nYou need to launch your program on each worker and configure `cluster_resolver` correctly. For example, if you are using [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver), each worker needs to have its corresponding `task_type` and `task_id` set in the `TF_CONFIG` environment variable. An example TF_CONFIG on worker-0 of a two worker cluster is:\n```\nTF_CONFIG = '{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }'\n\n```\n\nYour program runs on each worker as-is. Note that collectives require each worker to participate. All [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) and non [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) API may use collectives internally, e.g. checkpointing and saving since reading a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with [`tf.VariableSynchronization.ON_READ`](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization#ON_READ) all-reduces the value. Therefore it's recommended to run exactly the same program on each worker. Dispatching based on `task_type` or `task_id` of the worker is error-prone.\n`cluster_resolver.num_accelerators()` determines the number of GPUs the strategy uses. If it's zero, the strategy uses the CPU. All workers need to use the same number of devices, otherwise the behavior is undefined.\nThis strategy is not intended for TPU. Use [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) instead.\nAfter setting up TF_CONFIG, using this strategy is similar to using [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) and [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy).\n```\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nwith strategy.scope():\n  model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2, input_shape=(5,)),\n  ])\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n\ndefdataset_fn(ctx):\n  x = np.random.random((2, 5)).astype(np.float32)\n  y = np.random.randint(2, size=(2, 1))\n  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  return dataset.repeat().batch(1, drop_remainder=True)\ndist_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n\nmodel.compile()\nmodel.fit(dist_dataset)\n\n```\n\nYou can also write your own training loop:\n```\n@tf.function\ndeftrain_step(iterator):\n\n  defstep_fn(inputs):\n    features, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(features, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits)\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  strategy.run(step_fn, args=(next(iterator),))\n\nfor _ in range(NUM_STEP):\n  train_step(iterator)\n\n```\n\nSee [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) for a detailed tutorial.\n**Saving**\nYou need to save and checkpoint on all workers instead of just one. This is because variables whose synchronization=ON_READ triggers aggregation during saving. It's recommended to save to a different path on each worker to avoid race conditions. Each worker saves the same thing. See [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#model_saving_and_loading) tutorial for examples.\n**Known Issues**\n  * [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) does not return the correct number of accelerators. The strategy uses all available GPUs if `cluster_resolver` is [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) or `None`.\n  * In eager mode, the strategy needs to be created before calling any other Tensorflow API.\n\n\n## Args  \n---  \n`cluster_resolver` |  optional [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver). If `None`, [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) is used.   \n`communication_options` |  optional [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). This configures the default options for cross device communications. It can be overridden by options provided to the communication APIs like [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.As a multi-worker strategy, [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy) provides the associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver). If the user provides one in `__init__`, that instance is returned; if the user does not, a default `TFConfigClusterResolver` is provided.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nInvokes `fn` on each replica, with the given arguments.\nThis method is the primary way to distribute your computation with a tf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` is invoked under a replica context. `fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`. Please see the module-level docstring of tf.distribute for the concept of replica context.\nAll arguments in `args` or `kwargs` can be a nested structure of tensors, e.g. a list of tensors, in which case `args` and `kwargs` will be passed to the `fn` invoked on each replica. Or `args` or `kwargs` can be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors, i.e. [`tf.compat.v1.TensorInfo.CompositeTensor`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor), in which case each `fn` call will get the component of a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) corresponding to its replica. Note that arbitrary Python values that are not of the types above are not supported.\n#### Example usage:\n  1. Constant tensor input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndefreplica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n      1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n\n\n```\n\n  2. DistributedValues input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  defreplica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n    <tf.Tensor: shape=(), dtype=int32, numpy=4>\n\n```\n\n  3. Use [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) to allreduce values.\n```\nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndefrun():\n   defvalue_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   defreplica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\n         \"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n      1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n```\n\n\nArgs  \n---  \n`fn` |  The function to run on each replica.   \n`args` |  Optional positional arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`kwargs` |  Optional keyword arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`options` |  An optional instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues": "Base class for representing distributed values.\nA subclass instance of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) is created when creating variables within a distribution strategy, iterating a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) or through [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run). This base class should never be instantiated directly. [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) contains a value per replica. Depending on the subclass, the values could either be synced on update, synced on demand, or never synced.\nTwo representative types of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) are `tf.types.experimental.PerReplica` and `tf.types.experimental.Mirrored` values.\n`PerReplica` values exist on the worker devices, with a different value for each replica. They are produced by iterating through a distributed dataset returned by [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) (Example 1, below) and [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function). They are also the typical result returned by [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) (Example 2).\n`Mirrored` values are like `PerReplica` values, except we know that the value on all replicas are the same. `Mirrored` values are kept synchronized by the distribution strategy in use, while `PerReplica` values are left unsynchronized. `Mirrored` values typically represent model weights. We can safely read a `Mirrored` value in a cross-replica context by using the value on any replica, while PerReplica values should not be read or manipulated in a cross-replica context.\"\n[`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) can be reduced via `strategy.reduce` to obtain a single value across replicas (Example 4), used as input into [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) (Example 3), or collected to inspect the per-replica values using [`tf.distribute.Strategy.experimental_local_results`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_local_results) (Example 5).\n#### Example usages:\n  1. Created from a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset):\n\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)\ndataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\ndistributed_values = next(dataset_iterator)\ndistributed_values\nPerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>,\n  1: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>\n\n```\n\n  1. Returned by `run`:\n\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  ctx = tf.distribute.get_replica_context()\n  return ctx.replica_id_in_sync_group\ndistributed_values = strategy.run(run)\ndistributed_values\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n```\n\n  1. As input into `run`:\n\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)\ndataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\ndistributed_values = next(dataset_iterator)\n@tf.function\ndefrun(input):\n  return input + 1.0\nupdated_value = strategy.run(run, args=(distributed_values,))\nupdated_value\nPerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>,\n  1: <tf.Tensor: shape=(1,), dtype=float32, numpy=array([7.], dtype=float32)>\n\n```\n\n  1. As input into `reduce`:\n\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)\ndataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\ndistributed_values = next(dataset_iterator)\nreduced_value = strategy.reduce(tf.distribute.ReduceOp.SUM,\n                                distributed_values,\n                                axis = 0)\nreduced_value\n<tf.Tensor: shape=(), dtype=float32, numpy=11.0>\n```\n\n  1. How to inspect per-replica values locally:\n\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndataset = tf.data.Dataset.from_tensor_slices([5., 6., 7., 8.]).batch(2)\ndataset_iterator = iter(strategy.experimental_distribute_dataset(dataset))\nper_replica_values = strategy.experimental_local_results(\n   distributed_values)\nper_replica_values\n(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([6.], dtype=float32)>)\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp": "Indicates how a set of values should be reduced.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp)\n  * `SUM`: Add all the values.\n  * `MEAN`: Take the arithmetic mean (\"average\") of the values.\n\n\n## Class Variables  \n---  \nMEAN  |  `<ReduceOp.MEAN: 'MEAN'>`  \nSUM  |  `<ReduceOp.SUM: 'SUM'>`\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions": "Run options for `strategy.run`.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions)\n```\ntf.distribute.RunOptions(\n    experimental_enable_dynamic_batch_size=True,\n    experimental_bucketizing_dynamic_shape=False,\n    experimental_xla_options=None\n)\n\n```\n\nThis can be used to hold some strategy specific configs.\n## Attributes  \n---  \n`experimental_enable_dynamic_batch_size` |  Boolean. Only applies to TPUStrategy. Default to True. If True, TPUStrategy will enable dynamic padder to support dynamic batch size for the inputs. Otherwise only static shape inputs are allowed.   \n`experimental_bucketizing_dynamic_shape` |  Boolean. Only applies to TPUStrategy. Default to False. If True, TPUStrategy will automatic bucketize inputs passed into `run` if the input shape is dynamic. This is a performance optimization to reduce XLA recompilation, which should not have impact on correctness.   \n`experimental_xla_options` |  A [`tf.tpu.XLAOptions`](https://www.tensorflow.org/api_docs/python/tf/tpu/XLAOptions) instance. Only applies to TPUStrategy. Controls the XLA compiling options on TPUs. Default to None. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy": "A state & compute distribution policy on a list of devices.  \n```\ntf.distribute.Strategy(\n    extended\n)\n\n```\n\nSee [the guide](https://www.tensorflow.org/guide/distributed_training) for overview and examples. See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) and [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) for a glossary of concepts mentioned on this page such as \"per-replica\", _replica_ , and _reduce_.\n#### In short:\n  * To use it with Keras `compile`/`fit`, [please read](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_keras).\n  * Otherwise, use [`tf.distribute.Strategy.scope`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#scope) to specify that a strategy should be used when building an executing your model. (This puts you in the \"cross-replica context\" for this strategy, which means the strategy is put in control of things like variable placement.)\n  * If you are writing a custom training loop, you will need to call a few more methods, [see the guide](https://www.tensorflow.org/guide/distributed_training#using_tfdistributestrategy_with_custom_training_loops):\n    * Start by creating a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) normally.\n    * Use [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) to convert a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to something that produces \"per-replica\" values. If you want to manually specify how the dataset should be partitioned across replicas, use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead.\n    * Use [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) to run a function once per replica, taking values that may be \"per-replica\" (e.g. from a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) object) and returning \"per-replica\" values. This function is executed in \"replica context\", which means each operation is performed separately on each replica.\n    * Finally use a method (such as [`tf.distribute.Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce)) to convert the resulting \"per-replica\" values into ordinary `Tensor`s.\n\n\nA custom training loop can be as simple as:\n```\nwith my_strategy.scope():\n  @tf.function\n  defdistribute_train_epoch(dataset):\n    defreplica_fn(input):\n      # process input and return result\n      return result\n\n    total_result = 0\n    for x in dataset:\n      per_replica_result = my_strategy.run(replica_fn, args=(x,))\n      total_result += my_strategy.reduce(tf.distribute.ReduceOp.SUM,\n                                         per_replica_result, axis=None)\n    return total_result\n\n  dist_dataset = my_strategy.experimental_distribute_dataset(dataset)\n  for _ in range(EPOCHS):\n    train_result = distribute_train_epoch(dist_dataset)\n\n```\n\nThis takes an ordinary `dataset` and `replica_fn` and runs it distributed using a particular [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) named `my_strategy` above. Any variables created in `replica_fn` are created using `my_strategy`'s policy, and library functions called by `replica_fn` can use the `get_replica_context()` API to implement distributed-specific behavior.\nYou can use the `reduce` API to aggregate results across replicas and use this as a return value from one iteration over a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). Or you can use [`tf.keras.metrics`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) (such as loss, accuracy, etc.) to accumulate metrics across steps in a given epoch.\nSee the [custom training loop tutorial](https://www.tensorflow.org/tutorials/distribute/custom_training) for a more detailed example.\n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.In general, when using a multi-worker [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), there is a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) must set the relevant attribute, or override this property; otherwise, `None` is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver), and in those cases this property will return `None`. The [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, ```\n\nos.environ['TF_CONFIG'] = json.dumps({\n  'cluster': {\n      'worker': [\"localhost:12345\", \"localhost:23456\"],\n      'ps': [\"localhost:34567\"]\n  },\n  'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. Since we set this\n  # as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. Since we\n  # set this as a worker above, this block will not run on this particular\n  # instance.\n\n```\nFor more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s API docstring.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nInvokes `fn` on each replica, with the given arguments.\nThis method is the primary way to distribute your computation with a tf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` is invoked under a replica context. `fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`. Please see the module-level docstring of tf.distribute for the concept of replica context.\nAll arguments in `args` or `kwargs` can be a nested structure of tensors, e.g. a list of tensors, in which case `args` and `kwargs` will be passed to the `fn` invoked on each replica. Or `args` or `kwargs` can be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors, i.e. [`tf.compat.v1.TensorInfo.CompositeTensor`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor), in which case each `fn` call will get the component of a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) corresponding to its replica. Note that arbitrary Python values that are not of the types above are not supported.\n#### Example usage:\n  1. Constant tensor input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndefreplica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n      1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n\n\n```\n\n  2. DistributedValues input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  defreplica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n    <tf.Tensor: shape=(), dtype=int32, numpy=4>\n\n```\n\n  3. Use [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) to allreduce values.\n```\nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndefrun():\n   defvalue_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   defreplica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\n         \"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n      1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n```\n\n\nArgs  \n---  \n`fn` |  The function to run on each replica.   \n`args` |  Optional positional arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`kwargs` |  Optional keyword arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`options` |  An optional instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice": "A CrossDeviceOps implementation that copies values to one device to reduce.  \nInherits From: [`CrossDeviceOps`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps)\nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.ReductionToOneDevice`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReductionToOneDevice)\n```\ntf.distribute.ReductionToOneDevice(\n    reduce_to_device=None, accumulation_fn=None\n)\n\n```\n\nThis implementation always copies values to one device to reduce them, then broadcast reduced values to the destinations. It doesn't support efficient batching.\nHere is how you can use `ReductionToOneDevice` in [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy):\n```\n  strategy = tf.distribute.MirroredStrategy(\n    cross_device_ops=tf.distribute.ReductionToOneDevice())\n\n```\n\n## Args  \n---  \n`reduce_to_device` |  the intermediate device to reduce to. If None, reduce to the first device in `destinations` of the `reduce` method.   \n`accumulation_fn` |  a function that does accumulation. If None, [`tf.math.add_n`](https://www.tensorflow.org/api_docs/python/tf/math/add_n) is used.   \n## Methods\n### `batch_reduce`\n```\nbatch_reduce(\n    reduce_op, value_destination_pairs, options=None\n)\n\n```\n\nReduce values to destinations in batches.\nSee [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See [`tf.distribute.CrossDeviceOps.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/CrossDeviceOps#reduce) for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA list of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), one per pair in `value_destination_pairs`.   \nRaises  \n---  \n`ValueError` |  if `value_destination_pairs` is not an iterable of tuples of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and destinations.   \n### `broadcast`\n```\nbroadcast(\n    tensor, destinations\n)\n\n```\n\nBroadcast `tensor` to `destinations`.\nThis can only be called in the cross-replica context.\nArgs  \n---  \n`tensor` |  a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object. The value to broadcast.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to broadcast to. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is broadcasted to the devices of that variable, this method doesn't update the variable.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n### `reduce`\n```\nreduce(\n    reduce_op, per_replica_value, destinations, options=None\n)\n\n```\n\nReduce `per_replica_value` to `destinations`.\nSee [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to). This can only be called in the cross-replica context.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) specifying how values should be combined.   \n`per_replica_value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, and this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details.   \nReturns  \n---  \nA [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \nRaises  \n---  \n`ValueError` |  if per_replica_value can't be converted to a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or if destinations is not a string, [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/Server": "An in-process TensorFlow server, for use in distributed training.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server), [`tf.compat.v1.train.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server)\n```\ntf.distribute.Server(\n    server_or_cluster_def,\n    job_name=None,\n    task_index=None,\n    protocol=None,\n    config=None,\n    start=True\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrate multi-worker CPU/GPU training](https://www.tensorflow.org/guide/migrate/multi_worker_cpu_gpu_training)\n\n| \n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n\n  \nA [`tf.distribute.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server) instance encapsulates a set of devices and a [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) target that can participate in distributed training. A server belongs to a cluster (specified by a [`tf.train.ClusterSpec`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec)), and corresponds to a particular task in a named job. The server can communicate with any other server in the same cluster.\n## Args  \n---  \n`server_or_cluster_def` |  A [`tf.train.ServerDef`](https://www.tensorflow.org/api_docs/python/tf/train/ServerDef) or [`tf.train.ClusterDef`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterDef) protocol buffer, or a [`tf.train.ClusterSpec`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec) object, describing the server to be created and/or the cluster of which it is a member.   \n`job_name` |  (Optional.) Specifies the name of the job of which the server is a member. Defaults to the value in `server_or_cluster_def`, if specified.   \n`task_index` |  (Optional.) Specifies the task index of the server in its job. Defaults to the value in `server_or_cluster_def`, if specified. Otherwise defaults to 0 if the server's job has only one task.   \n`protocol` |  (Optional.) Specifies the protocol to be used by the server. Acceptable values include `\"grpc\", \"grpc+verbs\"`. Defaults to the value in `server_or_cluster_def`, if specified. Otherwise defaults to `\"grpc\"`.   \n`config` |  (Options.) A [`tf.compat.v1.ConfigProto`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto) that specifies default configuration options for all sessions that run on this server.   \n`start` |  (Optional.) Boolean, indicating whether to start the server after creating it. Defaults to `True`.   \n## Raises  \n---  \nOr one of its subclasses if an error occurs while creating the TensorFlow server.   \n## Attributes  \n---  \n`server_def` |  Returns the [`tf.train.ServerDef`](https://www.tensorflow.org/api_docs/python/tf/train/ServerDef) for this server.   \n`target` |  Returns the target for a [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) to connect to this server.To create a [`tf.compat.v1.Session`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/Session) that connects to this server, use the following snippet: ```\nserver = tf.distribute.Server(...)\nwith tf.compat.v1.Session(server.target):\n  # ...\n\n```\n  \n## Methods\n### `create_local_server`\n```\n@staticmethod\ncreate_local_server(\n    config=None, start=True\n)\n\n```\n\nCreates a new single-process cluster running on the local host.\nThis method is a convenience wrapper for creating a [`tf.distribute.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server) with a [`tf.train.ServerDef`](https://www.tensorflow.org/api_docs/python/tf/train/ServerDef) that specifies a single-process cluster containing a single task in a job called `\"local\"`.\nArgs  \n---  \n`config` |  (Options.) A [`tf.compat.v1.ConfigProto`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto) that specifies default configuration options for all sessions that run on this server.   \n`start` |  (Optional.) Boolean, indicating whether to start the server after creating it. Defaults to `True`.   \nReturns  \n---  \nA local [`tf.distribute.Server`](https://www.tensorflow.org/api_docs/python/tf/distribute/Server).   \n### `join`\n```\njoin()\n\n```\n\nBlocks until the server has shut down.\nThis method currently blocks forever.\nRaises  \n---  \nOr one of its subclasses if an error occurs while joining the TensorFlow server.   \n### `start`\n```\nstart()\n\n```\n\nStarts this server.\nRaises  \n---  \nOr one of its subclasses if an error occurs while starting the TensorFlow server. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext": "A class with a collection of APIs that can be called in a replica context.  \n```\ntf.distribute.ReplicaContext(\n    strategy, replica_id_in_sync_group\n)\n\n```\n\nYou can use [`tf.distribute.get_replica_context`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to get an instance of `ReplicaContext`, which can only be called inside the function passed to [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run).\n```\nstrategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])\ndeffunc():\n  replica_context = tf.distribute.get_replica_context()\n  return replica_context.replica_id_in_sync_group\nstrategy.run(func)\nPerReplica:{\n  0: <tf.Tensor: shape=(), dtype=int32, numpy=0>,\n  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n```\n\n## Args  \n---  \n`strategy` |  A [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy).   \n`replica_id_in_sync_group` |  An integer, a `Tensor` or None. Prefer an integer whenever possible to avoid issues with nested [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). It accepts a `Tensor` only to be compatible with `tpu.replicate`.   \n## Attributes  \n---  \n`devices` |  Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated)  \n`num_replicas_in_sync` |  Returns number of replicas that are kept in sync.   \n`replica_id_in_sync_group` |  Returns the id of the replica. This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to [`tf.distribute.ReplicaContext.num_replicas_in_sync`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#num_replicas_in_sync) - 1.  \n`strategy` |  The current [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object.   \n## Methods\n### `all_gather`\n```\nall_gather(\n    value, axis, options=None\n)\n\n```\n\nAll-gathers `value` across all replicas along `axis`.\nFor all strategies except [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), the input `value` on different replicas must have the same rank, and their shapes must be the same in all dimensions except the `axis`-th dimension. In other words, their shapes cannot be different in a dimension `d` where `d` does not equal to the `axis` argument. For example, given a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) with component tensors of shape `(1, 2, 3)` and `(1, 3, 3)` on two replicas, you can call `all_gather(..., axis=1, ...)` on it, but not `all_gather(..., axis=0, ...)` or `all_gather(..., axis=2, ...)`. However, with [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), all tensors must have exactly the same rank and same shape.\nYou can pass in a single tensor to all-gather:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefgather_value():\n  ctx = tf.distribute.get_replica_context()\n  local_value = tf.constant([1, 2, 3])\n  return ctx.all_gather(local_value, axis=0)\nresult = strategy.run(gather_value)\nresult\nPerReplica:{\n  0: <tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>,\n  1: <tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>\n\nstrategy.experimental_local_results(result)\n(<tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],\ndtype=int32)>,\n<tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],\ndtype=int32)>)\n```\n\nYou can also pass in a nested structure of tensors to all-gather, say, a list:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefgather_nest():\n  ctx = tf.distribute.get_replica_context()\n  value_1 = tf.constant([1, 2, 3])\n  value_2 = tf.constant([[1, 2], [3, 4]])\n  # all_gather a nest of `tf.distribute.DistributedValues`\n  return ctx.all_gather([value_1, value_2], axis=0)\nresult = strategy.run(gather_nest)\nresult\n[PerReplica:{\n  0: <tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>,\n  1: <tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [1, 2],\n       [3, 4]], dtype=int32)>,\n  1: <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [1, 2],\n       [3, 4]], dtype=int32)>\n}]\nstrategy.experimental_local_results(result)\n([<tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>,\n<tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [1, 2],\n       [3, 4]], dtype=int32)>],\n       [<tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)>,\n       <tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4],\n       [1, 2],\n       [3, 4]], dtype=int32)>])\n```\n\nWhat if you are all-gathering tensors with different shapes on different replicas? Consider the following example with two replicas, where you have `value` as a nested structure consisting of two items to all-gather, `a` and `b`.\n  * On Replica 0, `value` is `{'a': [0], 'b': [[0, 1]]}`.\n  * On Replica 1, `value` is `{'a': [1], 'b': [[2, 3], [4, 5]]}`.\n  * Result for `all_gather` with `axis=0` (on each of the replicas) is:\n```\n{'a': [1, 2], 'b': [[0, 1], [2, 3], [4, 5]]}\n\n```\n\n\nArgs  \n---  \n`value` |  a nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) which [`tf.nest.flatten`](https://www.tensorflow.org/api_docs/python/tf/nest/flatten) accepts, or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance. The structure of the [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) need to be same on all replicas. The underlying tensor constructs can only be dense tensors with non-zero rank, NOT [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). Options to perform collective operations. This overrides the default options if the [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) takes one in the constructor. See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details of the options.   \nReturns  \n---  \nA nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) with the gathered values. The structure is the same as `value`.   \n### `all_reduce`\n```\nall_reduce(\n    reduce_op, value, options=None\n)\n\n```\n\nAll-reduces `value` across all replicas.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  ctx = tf.distribute.get_replica_context()\n  value = tf.identity(1.)\n  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)\nstrategy.experimental_local_results(strategy.run(step_fn))\n(<tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=2.0>)\n```\n\nIt supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify `options` to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  ctx = tf.distribute.get_replica_context()\n  value1 = tf.identity(1.)\n  value2 = tf.identity(2.)\n  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, [value1, value2])\nstrategy.experimental_local_results(strategy.run(step_fn))\n([<tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n<tf.Tensor: shape=(), dtype=float32, numpy=4.0>],\n[<tf.Tensor: shape=(), dtype=float32, numpy=2.0>,\n<tf.Tensor: shape=(), dtype=float32, numpy=4.0>])\n```\n\nNote that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone.\nKnown limitation: if `value` contains [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices), attempting to compute gradient w.r.t `value` would result in an error.\nThis API currently can only be called in the replica context. Other variants to reduce values across replicas are:\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to): the reduce and all-reduce API in the cross-replica context.\n  * [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): the batched reduce and all-reduce API in the cross-replica context.\n  * [`tf.distribute.Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce): a more convenient method to reduce to the host in cross-replica context.\n\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a potentially nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) or [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices) which [`tf.nest.flatten`](https://www.tensorflow.org/api_docs/python/tf/nest/flatten) accepts. The structure and the shapes of `value` need to be same on all replicas.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). Options to perform collective operations. This overrides the default options if the [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) takes one in the constructor. See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details of the options.   \nReturns  \n---  \nA nested structure of [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) with the reduced values. The structure is the same as `value`.   \n### `merge_call`\n```\nmerge_call(\n    merge_fn, args=(), kwargs=None\n)\n\n```\n\nMerge args across replicas and run `merge_fn` in a cross-replica context.\nThis allows communication and coordination when there are multiple calls to the step_fn triggered by a call to `strategy.run(step_fn, ...)`.\nSee [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) for an explanation.\nIf not inside a distributed scope, this is equivalent to:\n```\nstrategy = tf.distribute.get_strategy()\nwith cross-replica-context(strategy):\n  return merge_fn(strategy, *args, **kwargs)\n\n```\nArgs  \n---  \n`merge_fn` |  Function that joins arguments from threads that are given as PerReplica. It accepts [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object as the first argument.   \n`args` |  List or tuple with positional per-thread arguments for `merge_fn`.   \n`kwargs` |  Dict with keyword per-thread arguments for `merge_fn`.   \nReturns  \n---  \nThe return value of `merge_fn`, except for `PerReplica` values which are unpacked. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended": "Additional APIs for algorithms that need to be distribution-aware.  \n```\ntf.distribute.StrategyExtended(\n    container_strategy\n)\n\n```\n\nSome common use cases of functions on this page:\n  * _Locality_\n\n\n[`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) can have the same _locality_ as a _distributed variable_ , which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to [`tf.distribute.StrategyExtended.update`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#update) to update the value of a variable. You may use [`tf.distribute.StrategyExtended.colocate_vars_with`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#colocate_vars_with) to give a variable the same locality as another variable. You may convert a \"PerReplica\" value to a variable's locality by using [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) or [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to).\n  * _How to update a distributed variable_\n\n\nA distributed variable is variables created on multiple devices. As discussed in the [glossary](https://www.tensorflow.org/api_docs/python/tf/distribute), mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:\n  1. In your function passed to [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run), compute a list of (update, variable) pairs. For example, the update might be a gradient of the loss with respect to the variable.\n  2. Switch to cross-replica mode by calling `tf.distribute.get_replica_context().merge_call()` with the updates and variables as arguments.\n  3. Call [`tf.distribute.StrategyExtended.reduce_to(VariableAggregation.SUM, t, v)`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) (for one variable) or [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to) (for a list of variables) to sum the updates.\n  4. Call [`tf.distribute.StrategyExtended.update(v)`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#update) for each variable to update its value.\n\n\nSteps 2 through 4 are done automatically by class [`tf.keras.optimizers.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Optimizer) if you call its [`tf.keras.optimizers.Optimizer.apply_gradients`](https://www.tensorflow.org/api_docs/python/tf/keras/Optimizer#apply_gradients) method in a replica context.\nIn fact, a higher-level solution to update a distributed variable is by calling `assign` on the variable as you would do to a regular [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable). You can call the method in both _replica context_ and _cross-replica context_. For a _mirrored variable_ , calling `assign` in _replica context_ requires you to specify the `aggregation` type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call `assign` on _mirrored variable_ in _cross-replica context_ , you can only assign a single value or assign values from another mirrored variable or a mirrored [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). For a _SyncOnRead variable_ , in _replica context_ , you can simply call `assign` on it and no aggregation happens under the hood. In _cross-replica context_ , you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the `aggregation` type of the variable is [`tf.VariableAggregation.SUM`](https://www.tensorflow.org/api_docs/python/tf/VariableAggregation#SUM), it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the `aggregation` type is [`tf.VariableAggregation.MEAN`](https://www.tensorflow.org/api_docs/python/tf/VariableAggregation#MEAN), the value is assigned to each replica directly.\n## Attributes  \n---  \n`experimental_require_static_shapes` |  Returns `True` if static shape is required; `False` otherwise.   \n`parameter_devices` |  Returns the tuple of all devices used to place variables.   \n`worker_devices` |  Returns the tuple of all devices used to for compute replica execution.  \n## Methods\n### `batch_reduce_to`\n```\nbatch_reduce_to(\n    reduce_op, value_destination_pairs, options=None\n)\n\n```\n\nCombine multiple `reduce_to` calls into one for faster execution.\nSimilar to `reduce_to`, but accepts a list of (value, destinations) pairs. It's more efficient than reduce each value separately.\nThis API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to): the non-batch version of this API.\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.\n  * [`tf.distribute.Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce): a more convenient method to reduce to the host in cross-replica context.\n\n\nSee `reduce_to` for more information.\n@tf.function def step_fn(var):\ndef merge_fn(strategy, value, var): # All-reduce the value. Note that `value` here is a # [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). reduced = strategy.extended.batch_reduce_to( tf.distribute.ReduceOp.SUM, [(value, var)])[0] strategy.extended.update(var, lambda var, value: var.assign(value), args=(reduced,))\nvalue = tf.identity(1.) tf.distribute.get_replica_context().merge_call(merge_fn, args=(value, var))\ndef run(strategy): with strategy.scope(): v = tf.Variable(0.) strategy.run(step_fn, args=(v,)) return v\nrun(tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])) MirroredVariable:{ 0: , 1:  } run(tf.distribute.experimental.CentralStorageStrategy( compute_devices=[\"GPU:0\", \"GPU:1\"], parameter_device=\"CPU:0\"))  run(tf.distribute.OneDeviceStrategy(\"GPU:0\")) \nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value_destination_pairs` |  a sequence of (value, destinations) pairs. See `tf.distribute.Strategy.reduce_to` for descriptions.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). Options to perform collective operations. This overrides the default options if the [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) takes one in the constructor. See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details of the options.   \nReturns  \n---  \nA list of reduced values, one per pair in `value_destination_pairs`.   \n### `colocate_vars_with`\n```\ncolocate_vars_with(\n    colocate_with_variable\n)\n\n```\n\nScope that controls which devices variables will be created on.\nNo operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope).\nThis may only be used inside `self.scope()`.\n#### Example usage:\n```\nwith strategy.scope():\n  var1 = tf.Variable(...)\n  with strategy.extended.colocate_vars_with(var1):\n    # var2 and var3 will be created on the same device(s) as var1\n    var2 = tf.Variable(...)\n    var3 = tf.Variable(...)\n\n  deffn(v1, v2, v3):\n    # operates on v1 from var1, v2 from var2, and v3 from var3\n\n  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there\n  # too.\n  strategy.extended.update(var1, fn, args=(var2, var3))\n\n```\nArgs  \n---  \n`colocate_with_variable` |  A variable created in this strategy's `scope()`. Variables created while in the returned context manager will be on the same set of devices as `colocate_with_variable`.   \nReturns  \n---  \nA context manager.   \n### `reduce_to`\n```\nreduce_to(\n    reduce_op, value, destinations, options=None\n)\n\n```\n\nCombine (via e.g. sum or mean) values across replicas.\n`reduce_to` aggregates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) and distributed variables. It supports both dense values and [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).\nThis API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:\n  * [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): the batch version of this API.\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.\n  * [`tf.distribute.Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#reduce): a more convenient method to reduce to the host in cross-replica context.\n\n\n`destinations` specifies where to reduce the value to, e.g. \"GPU:0\". You can also pass in a `Tensor`, and the destinations will be the device of that tensor. For all-reduce, pass the same to `value` and `destinations`.\nIt can be used in [`tf.distribute.ReplicaContext.merge_call`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#merge_call) to write code that works for all [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy).\n@tf.function def step_fn(var):\ndef merge_fn(strategy, value, var): # All-reduce the value. Note that `value` here is a # [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues). reduced = strategy.extended.reduce_to(tf.distribute.ReduceOp.SUM, value, destinations=var) strategy.extended.update(var, lambda var, value: var.assign(value), args=(reduced,))\nvalue = tf.identity(1.) tf.distribute.get_replica_context().merge_call(merge_fn, args=(value, var))\ndef run(strategy): with strategy.scope(): v = tf.Variable(0.) strategy.run(step_fn, args=(v,)) return v\nrun(tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])) MirroredVariable:{ 0: , 1:  } run(tf.distribute.experimental.CentralStorageStrategy( compute_devices=[\"GPU:0\", \"GPU:1\"], parameter_device=\"CPU:0\"))  run(tf.distribute.OneDeviceStrategy(\"GPU:0\")) \nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) like object.   \n`destinations` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to `value` and `destinations`. Note that if it's a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), the value is reduced to the devices of that variable, and this method doesn't update the variable.   \n`options` |  a [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions). Options to perform collective operations. This overrides the default options if the [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) takes one in the constructor. See [`tf.distribute.experimental.CommunicationOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationOptions) for details of the options.   \nReturns  \n---  \nA tensor or value reduced to `destinations`.   \n### `update`\n```\nupdate(\n    var, fn, args=(), kwargs=None, group=True\n)\n\n```\n\nRun `fn` to update `var` using inputs mirrored to the same devices.\n[`tf.distribute.StrategyExtended.update`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#update) takes a distributed variable `var` to be updated, an update function `fn`, and `args` and `kwargs` for `fn`. It applies `fn` to each component variable of `var` and passes corresponding values from `args` and `kwargs`. Neither `args` nor `kwargs` may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling `fn`. For example, `fn` can be `assign_add` and `args` can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable `var`. Calling `update` will call `assign_add` on each component variable of `var` with the corresponding tensor value on that device.\n#### Example usage:\n```\nstrategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1']) # With 2\ndevices\nwith strategy.scope():\n  v = tf.Variable(5.0, aggregation=tf.VariableAggregation.SUM)\ndefupdate_fn(v):\n  return v.assign(1.0)\nresult = strategy.extended.update(v, update_fn)\n# result is\n# Mirrored:{\n#  0: tf.Tensor(1.0, shape=(), dtype=float32),\n#  1: tf.Tensor(1.0, shape=(), dtype=float32)\n# }\n\n```\n\nIf `var` is mirrored across multiple devices, then this method implements logic as following:\n```\nresults = {}\nfor device, v in var:\n  with tf.device(device):\n    # args and kwargs will be unwrapped if they are mirrored.\n    results[device] = fn(v, *args, **kwargs)\nreturn merged(results)\n\n```\n\nOtherwise, this method returns `fn(var, *args, **kwargs)` colocated with `var`.\nArgs  \n---  \n`var` |  Variable, possibly mirrored to multiple devices, to operate on.   \n`fn` |  Function to call. Should take the variable as the first argument.   \n`args` |  Tuple or list. Additional positional arguments to pass to `fn()`.   \n`kwargs` |  Dict with keyword arguments to pass to `fn()`.   \n`group` |  Boolean. Defaults to True. If False, the return value will be unwrapped.   \nReturns  \n---  \nBy default, the merged return value of `fn` across all replicas. The merged result has dependencies to make sure that if it is evaluated at all, the side effects (updates) will happen on every replica. If instead \"group=False\" is specified, this function will return a nest of lists where each list has an element per replica, and the caller is responsible for ensuring all elements are executed.   \n### `value_container`\n```\nvalue_container(\n    value\n)\n\n```\n\nReturns the container that this per-replica `value` belongs to.\nArgs  \n---  \n`value` |  A value returned by `run()` or a variable created in `scope()`.   \nReturns  \n---  \nA container that `value` belongs to. If value does not belong to any container (including the case of container having been destroyed), returns the value itself. `value in experimental_local_results(value_container(value))` will always be true.   \n### `variable_created_in_scope`\n```\nvariable_created_in_scope(\n    v\n)\n\n```\n\nTests whether `v` was created while this strategy scope was active.\nVariables created inside the strategy scope are \"owned\" by it:\n```\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n  v = tf.Variable(1.)\nstrategy.extended.variable_created_in_scope(v)\nTrue\n```\n\nVariables created outside the strategy are not owned by it:\n```\nstrategy = tf.distribute.MirroredStrategy()\nv = tf.Variable(1.)\nstrategy.extended.variable_created_in_scope(v)\nFalse\n```\nArgs  \n---  \nA [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) instance.   \nReturns  \n---  \nTrue if `v` was created inside the scope, False if not. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy": "Synchronous training on TPUs and TPU Pods.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.TPUStrategy(\n    tpu_cluster_resolver=None,\n    experimental_device_assignment=None,\n    experimental_spmd_xla_partitioning=False\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrate from TPU embedding_columns to TPUEmbedding layer](https://www.tensorflow.org/guide/migrate/tpu_embedding)\n  * [Migrate from TPUEstimator to TPUStrategy](https://www.tensorflow.org/guide/migrate/tpu_estimator)\n\n| \n  * [Training with Orbit](https://www.tensorflow.org/tfmodels/orbit/index)\n  * [TensorFlow 2 TPUEmbeddingLayer: Quick Start](https://www.tensorflow.org/recommenders/examples/tpu_embedding_layer)\n  * [Solve GLUE tasks using BERT on TPU](https://www.tensorflow.org/text/tutorials/bert_glue)\n\n  \nTo construct a TPUStrategy object, you need to run the initialization code as below:\n```\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)\n```\n\nWhile using distribution strategies, the variables created within the strategy's scope will be replicated across all the replicas and can be kept in sync using all-reduce algorithms.\nTo run TF2 programs on TPUs, you can either use `.compile` and `.fit` APIs in [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) with TPUStrategy, or write your own customized training loop by calling `strategy.run` directly. Note that TPUStrategy doesn't support pure eager execution, so please make sure the function passed into `strategy.run` is a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) or `strategy.run` is called inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) if eager behavior is enabled. See more details in https://www.tensorflow.org/guide/tpu.\n`distribute_datasets_from_function` and `experimental_distribute_dataset` APIs can be used to distribute the dataset across the TPU workers when writing your own training loop. If you are using `fit` and `compile` methods available in [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model), then Keras will handle the distribution for you.\nAn example of writing customized training loop on TPUs:\n```\nwith strategy.scope():\n  model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2, input_shape=(5,)),\n  ])\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n```\n```\ndefdataset_fn(ctx):\n  x = np.random.random((2, 5)).astype(np.float32)\n  y = np.random.randint(2, size=(2, 1))\n  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  return dataset.repeat().batch(1, drop_remainder=True)\ndist_dataset = strategy.distribute_datasets_from_function(\n    dataset_fn)\niterator = iter(dist_dataset)\n```\n```\n@tf.function()\ndeftrain_step(iterator):\n  defstep_fn(inputs):\n    features, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(features, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits)\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n  strategy.run(step_fn, args=(next(iterator),))\n```\n```\ntrain_step(iterator)\n```\n\nFor the advanced use cases like model parallelism, you can set `experimental_device_assignment` argument when creating TPUStrategy to specify number of replicas and number of logical devices. Below is an example to initialize TPU system with 2 logical devices and 1 replica.\n```\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntopology = tf.tpu.experimental.initialize_tpu_system(resolver)\ndevice_assignment = tf.tpu.experimental.DeviceAssignment.build(\n    topology,\n    computation_shape=[1, 1, 1, 2],\n    num_replicas=1)\nstrategy = tf.distribute.TPUStrategy(\n    resolver, experimental_device_assignment=device_assignment)\n```\n\nThen you can run a [`tf.add`](https://www.tensorflow.org/api_docs/python/tf/math/add) operation only on logical device 0.\n```\n@tf.function()\ndefstep_fn(inputs):\n  features, _ = inputs\n  output = tf.add(features, features)\n  # Add operation will be executed on logical device 0.\n  output = strategy.experimental_assign_to_logical_device(output, 0)\n  return output\ndist_dataset = strategy.distribute_datasets_from_function(\n    dataset_fn)\niterator = iter(dist_dataset)\nstrategy.run(step_fn, args=(next(iterator),))\n```\n\n`experimental_spmd_xla_partitioning` enables the experimental XLA SPMD feature for model parallelism. This flag can reduce the compilation time and HBM requirements. When running in this mode, every input tensor must either be partitioned (via `strategy.experimental_split_to_logical_devices`) or fully replicated (via `strategy.experimental_replicate_to_logical_devices`) to all logical devices. And calling `strategy.experimental_assign_to_logical_device` will result in a ValueError in this mode.\n## Args  \n---  \n`tpu_cluster_resolver` |  A [`tf.distribute.cluster_resolver.TPUClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver) instance, which provides information about the TPU cluster. If None, it will assume running on a local TPU worker.   \n`experimental_device_assignment` |  Optional [`tf.tpu.experimental.DeviceAssignment`](https://www.tensorflow.org/api_docs/python/tf/tpu/experimental/DeviceAssignment) to specify the placement of replicas on the TPU cluster.   \n`experimental_spmd_xla_partitioning` |  If True, enable the SPMD (Single Program Multiple Data) mode in XLA compiler. This flag only affects the performance of XLA compilation and the HBM requirement of the compiled TPU program. Ceveat: if this flag is True, calling [`tf.distribute.TPUStrategy.experimental_assign_to_logical_device`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy#experimental_assign_to_logical_device) will result in a ValueError.   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.[`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) provides the associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver). If the user provides one in `__init__`, that instance is returned; if the user does not, a default [`tf.distribute.cluster_resolver.TPUClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver) is provided.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_assign_to_logical_device`\n```\nexperimental_assign_to_logical_device(\n    tensor, logical_device_id\n)\n\n```\n\nAdds annotation that `tensor` will be assigned to a logical device.\nThis adds an annotation to `tensor` specifying that operations on `tensor` will be invoked on logical core device id `logical_device_id`. When model parallelism is used, the default behavior is that all ops are placed on zero-th logical device.\n```\n\n# Initializing TPU system with 2 logical devices and 4 replicas.\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntopology = tf.tpu.experimental.initialize_tpu_system(resolver)\ndevice_assignment = tf.tpu.experimental.DeviceAssignment.build(\n    topology,\n    computation_shape=[1, 1, 1, 2],\n    num_replicas=4)\nstrategy = tf.distribute.TPUStrategy(\n    resolver, experimental_device_assignment=device_assignment)\niterator = iter(inputs)\n\n@tf.function()\ndefstep_fn(inputs):\n  output = tf.add(inputs, inputs)\n\n  # Add operation will be executed on logical device 0.\n  output = strategy.experimental_assign_to_logical_device(output, 0)\n  return output\n\nstrategy.run(step_fn, args=(next(iterator),))\n\n```\nArgs  \n---  \n`tensor` |  Input tensor to annotate.   \n`logical_device_id` |  Id of the logical core to which the tensor will be assigned.   \nRaises  \n---  \n`ValueError` |  The logical device id presented is not consistent with total number of partitions specified by the device assignment or the TPUStrategy is constructed with `experimental_spmd_xla_partitioning=True`.   \nReturns  \n---  \nAnnotated tensor with identical value as `tensor`.   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `experimental_replicate_to_logical_devices`\n```\nexperimental_replicate_to_logical_devices(\n    tensor\n)\n\n```\n\nAdds annotation that `tensor` will be replicated to all logical devices.\nThis adds an annotation to tensor `tensor` specifying that operations on `tensor` will be invoked on all logical devices.\n```\n# Initializing TPU system with 2 logical devices and 4 replicas.\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntopology = tf.tpu.experimental.initialize_tpu_system(resolver)\ndevice_assignment = tf.tpu.experimental.DeviceAssignment.build(\n    topology,\n    computation_shape=[1, 1, 1, 2],\n    num_replicas=4)\nstrategy = tf.distribute.TPUStrategy(\n    resolver, experimental_device_assignment=device_assignment)\n\niterator = iter(inputs)\n\n@tf.function()\ndefstep_fn(inputs):\n  images, labels = inputs\n  images = strategy.experimental_split_to_logical_devices(\n    inputs, [1, 2, 4, 1])\n\n  # model() function will be executed on 8 logical devices with `inputs`\n  # split 2 * 4  ways.\n  output = model(inputs)\n\n  # For loss calculation, all logical devices share the same logits\n  # and labels.\n  labels = strategy.experimental_replicate_to_logical_devices(labels)\n  output = strategy.experimental_replicate_to_logical_devices(output)\n  loss = loss_fn(labels, output)\n\n  return loss\n\nstrategy.run(step_fn, args=(next(iterator),))\n\n```\n\nArgs: tensor: Input tensor to annotate.\nReturns  \n---  \nAnnotated tensor with identical value as `tensor`.   \n### `experimental_split_to_logical_devices`\n```\nexperimental_split_to_logical_devices(\n    tensor, partition_dimensions\n)\n\n```\n\nAdds annotation that `tensor` will be split across logical devices.\nThis adds an annotation to tensor `tensor` specifying that operations on `tensor` will be split among multiple logical devices. Tensor `tensor` will be split across dimensions specified by `partition_dimensions`. The dimensions of `tensor` must be divisible by corresponding value in `partition_dimensions`.\nFor example, for system with 8 logical devices, if `tensor` is an image tensor with shape (batch_size, width, height, channel) and `partition_dimensions` is [1, 2, 4, 1], then `tensor` will be split 2 in width dimension and 4 way in height dimension and the split tensor values will be fed into 8 logical devices.\n```\n# Initializing TPU system with 8 logical devices and 1 replica.\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntopology = tf.tpu.experimental.initialize_tpu_system(resolver)\ndevice_assignment = tf.tpu.experimental.DeviceAssignment.build(\n    topology,\n    computation_shape=[1, 2, 2, 2],\n    num_replicas=1)\n# Construct the TPUStrategy. Since we are going to split the image across\n# logical devices, here we set `experimental_spmd_xla_partitioning=True`\n# so that the partitioning can be compiled in SPMD mode, which usually\n# results in faster compilation and smaller HBM requirement if the size of\n# input and activation tensors are much bigger than that of the model\n# parameters. Note that this flag is suggested but not a hard requirement\n# for `experimental_split_to_logical_devices`.\nstrategy = tf.distribute.TPUStrategy(\n    resolver, experimental_device_assignment=device_assignment,\n    experimental_spmd_xla_partitioning=True)\n\niterator = iter(inputs)\n\n@tf.function()\ndefstep_fn(inputs):\n  inputs = strategy.experimental_split_to_logical_devices(\n    inputs, [1, 2, 4, 1])\n\n  # model() function will be executed on 8 logical devices with `inputs`\n  # split 2 * 4  ways.\n  output = model(inputs)\n  return output\n\nstrategy.run(step_fn, args=(next(iterator),))\n\n```\n\nArgs: tensor: Input tensor to annotate. partition_dimensions: An unnested list of integers with the size equal to rank of `tensor` specifying how `tensor` will be partitioned. The product of all elements in `partition_dimensions` must be equal to the total number of logical devices per replica.\nRaises  \n---  \n`ValueError` | 1) If the size of partition_dimensions does not equal to rank of `tensor` or 2) if product of elements of `partition_dimensions` does not match the number of logical devices per replica defined by the implementing DistributionStrategy's device specification or 3) if a known size of `tensor` is not divisible by corresponding value in `partition_dimensions`.   \nReturns  \n---  \nAnnotated tensor with identical value as `tensor`.   \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nRun the computation defined by `fn` on each TPU replica.\nExecutes ops specified by `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`.\nAll arguments in `args` or `kwargs` should either be nest of tensors or [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors.\n#### Example usage:\n```\nresolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\ntf.config.experimental_connect_to_cluster(resolver)\ntf.tpu.experimental.initialize_tpu_system(resolver)\nstrategy = tf.distribute.TPUStrategy(resolver)\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n      strategy.experimental_distribute_values_from_function(value_fn))\n  defreplica_fn(input):\n    return input * 2\n  return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\n```\nArgs  \n---  \n`fn` |  The function to run. The output must be a [`tf.nest`](https://www.tensorflow.org/api_docs/python/tf/nest) of `Tensor`s.   \n`args` |  (Optional) Positional arguments to `fn`.   \n`kwargs` |  (Optional) Keyword arguments to `fn`.   \n`options` |  (Optional) An instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver": "Abstract class for all implementations of ClusterResolvers.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)\nThis defines the skeleton for all implementations of ClusterResolvers. ClusterResolvers are a way for TensorFlow to communicate with various cluster management systems (e.g. GCE, AWS, etc...) and gives TensorFlow necessary information to set up distributed training.\nBy letting TensorFlow communicate with these systems, we will be able to automatically discover and resolve IP addresses for various TensorFlow workers. This will eventually allow us to automatically recover from underlying machine failures and scale TensorFlow worker clusters up and down.\nNote to Implementors of [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) subclass: In addition to these abstract methods, when task_type, task_id, and rpc_layer attributes are applicable, you should also implement them either as properties with getters or setters, or directly set the attributes `self._task_type`, `self._task_id`, or `self._rpc_layer` so the base class' getters and setters are used. See [`tf.distribute.cluster_resolver.SimpleClusterResolver.__init__`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/SimpleClusterResolver#__init__)for an example.\nIn general, multi-client tf.distribute strategies such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) require task_type and task_id properties to be available in the `ClusterResolver` they are using. On the other hand, these concepts are not applicable in single-client strategies, such as [`tf.distribute.experimental.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy), because the program is only expected to be run on one task, so there should not be a need to have code branches according to task type and task id.\n  * task_type is the name of the server's current named job (e.g. 'worker', 'ps' in a distributed parameterized training job).\n  * task_id is the ordinal index of the server within the task type.\n  * rpc_layer is the protocol used by TensorFlow to communicate with other TensorFlow servers in a distributed environment.\n\n\n## Attributes  \n---  \n`environment` |  Returns the current environment which TensorFlow is running in.There are two possible return values, \"google\" (when TensorFlow is running in a Google-internal environment) or an empty string (when TensorFlow is running elsewhere). If you are implementing a ClusterResolver that works in both the Google environment and the open-source world (for instance, a TPU ClusterResolver or similar), you will have to return the appropriate string depending on the environment, which you will have to detect. Otherwise, if you are implementing a ClusterResolver that will only work in open-source TensorFlow, you do not need to implement this property.   \n`task_id` |  Returns the task id this `ClusterResolver` indicates.In TensorFlow distributed environment, each job may have an applicable task id, which is the index of the instance within its task type. This is useful when user needs to run specific code according to task index. For example, ```\ncluster_spec = tf.train.ClusterSpec({\n    \"ps\": [\"localhost:2222\", \"localhost:2223\"],\n    \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]\n})\n\n# SimpleClusterResolver is used here for illustration; other cluster\n# resolvers may be used for other source of task type/id.\nsimple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\n                                        task_id=0)\n\n...\n\nif cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:\n  # Perform something that's only applicable on 'worker' type, id 0. This\n  # block will run on this particular instance since we've specified this\n  # task to be a 'worker', id 0 in above cluster resolver.\nelse:\n  # Perform something that's only applicable on other ids. This block will\n  # not run on this particular instance.\n\n```\nReturns `None` if such information is not available or is not applicable in the current distributed environment, such as training with [`tf.distribute.cluster_resolver.TPUClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TPUClusterResolver). For more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s class docstring.   \n`task_type` |  Returns the task type this `ClusterResolver` indicates.In TensorFlow distributed environment, each job may have an applicable task type. Valid task types in TensorFlow include 'chief': a worker that is designated with more responsibility, 'worker': a regular worker for training/evaluation, 'ps': a parameter server, or 'evaluator': an evaluator that evaluates the checkpoints for metrics. See [Multi-worker configuration](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#multi-worker_configuration) for more information about 'chief' and 'worker' task type, which are most commonly used. Having access to such information is useful when user needs to run specific code according to task types. For example, ```\ncluster_spec = tf.train.ClusterSpec({\n    \"ps\": [\"localhost:2222\", \"localhost:2223\"],\n    \"worker\": [\"localhost:2224\", \"localhost:2225\", \"localhost:2226\"]\n})\n\n# SimpleClusterResolver is used here for illustration; other cluster\n# resolvers may be used for other source of task type/id.\nsimple_resolver = SimpleClusterResolver(cluster_spec, task_type=\"worker\",\n                                        task_id=1)\n\n...\n\nif cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. This block\n  # will run on this particular instance since we've specified this task to\n  # be a worker in above cluster resolver.\nelif cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. This\n  # block will not run on this particular instance.\n\n```\nReturns `None` if such information is not available or is not applicable in the current distributed environment, such as training with [`tf.distribute.experimental.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/TPUStrategy). For more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s class doc.   \n## Methods\n### `cluster_spec`\n```\n@abc.abstractmethod\ncluster_spec()\n\n```\n\nRetrieve the current state of the cluster and return a [`tf.train.ClusterSpec`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec).\nReturns  \n---  \nA [`tf.train.ClusterSpec`](https://www.tensorflow.org/api_docs/python/tf/train/ClusterSpec) representing the state of the cluster at the moment this function is called.   \nImplementors of this function must take care in ensuring that the ClusterSpec returned is up-to-date at the time of calling this function. This usually means retrieving the information from the underlying cluster management system every time this function is invoked and reconstructing a cluster_spec, rather than attempting to cache anything.\n### `master`\n```\n@abc.abstractmethod\nmaster(\n    task_type=None, task_id=None, rpc_layer=None\n)\n\n```\n\nRetrieves the name or URL of the session master.\nArgs  \n---  \n`task_type` |  (Optional) The type of the TensorFlow task of the master.   \n`task_id` |  (Optional) The index of the TensorFlow task of the master.   \n`rpc_layer` |  (Optional) The RPC protocol for the given cluster.   \nReturns  \n---  \nThe name or URL of the session master.   \nImplementors of this function must take care in ensuring that the master returned is up-to-date at the time to calling this function. This usually means retrieving the master every time this function is invoked.\n### `num_accelerators`\n```\nnum_accelerators(\n    task_type=None, task_id=None, config_proto=None\n)\n\n```\n\nReturns the number of accelerator cores per worker.\nThis returns the number of accelerator cores (such as GPUs and TPUs) available per worker.\nOptionally, we allow callers to specify the task_type, and task_id, for if they want to target a specific TensorFlow task to query the number of accelerators. This is to support heterogenous environments, where the number of accelerators cores per host is different.\nArgs  \n---  \n`task_type` |  (Optional) The type of the TensorFlow task of the machine we want to query.   \n`task_id` |  (Optional) The index of the TensorFlow task of the machine we want to query.   \n`config_proto` |  (Optional) Configuration for starting a new session to query how many accelerator cores it has.   \nReturns  \n---  \nA map of accelerator types to number of cores. \n",
  "https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator": "\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy": "Returns the current [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.get_strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy)\n```\ntf.distribute.get_strategy() -> 'StrategyBase'\n\n```\n\n### Used in the notebooks\nUsed in the guide  \n---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n  \nTypically only used in a cross-replica context:\n```\nif tf.distribute.in_cross_replica_context():\n  strategy = tf.distribute.get_strategy()\n  ...\n\n```\n\n## Returns  \n---  \nA [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object. Inside a `with strategy.scope()` block, it returns `strategy`, otherwise it returns the default (single-replica) [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CentralStorageStrategy": "A one-machine strategy that puts all variables on a single device.  \nInherits From: [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=None, parameter_device=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide  \n---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n  \nVariables are assigned to local CPU or the only GPU. If there is more than one GPU, compute operations (other than variable update operations) will be replicated across all GPUs.\n#### For Example:\n```\nstrategy = tf.distribute.experimental.CentralStorageStrategy()\n# Create a dataset\nds = tf.data.Dataset.range(5).batch(2)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(ds)\n\nwith strategy.scope():\n  @tf.function\n  deftrain_step(val):\n    return val + 1\n\n  # Iterate over the distributed dataset\n  for x in dist_dataset:\n    # process dataset elements\n    strategy.run(train_step, args=(x,))\n\n```\n\n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.In general, when using a multi-worker [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy such as [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), there is a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) associated with the strategy used, and such an instance is returned by this property. Strategies that intend to have an associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) must set the relevant attribute, or override this property; otherwise, `None` is returned by default. Those strategies should also provide information regarding what is returned by this property. Single-worker strategies usually do not have a [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver), and in those cases this property will return `None`. The [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver) may be useful when the user needs to access information such as the cluster spec, task type or task id. For example, ```\n\nos.environ['TF_CONFIG'] = json.dumps({\n  'cluster': {\n      'worker': [\"localhost:12345\", \"localhost:23456\"],\n      'ps': [\"localhost:34567\"]\n  },\n  'task': {'type': 'worker', 'index': 0}\n})\n\n# This implicitly uses TF_CONFIG for the cluster and current task info.\nstrategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n\n...\n\nif strategy.cluster_resolver.task_type == 'worker':\n  # Perform something that's only applicable on workers. Since we set this\n  # as a worker above, this block will run on this particular instance.\nelif strategy.cluster_resolver.task_type == 'ps':\n  # Perform something that's only applicable on parameter servers. Since we\n  # set this as a worker above, this block will not run on this particular\n  # instance.\n\n```\nFor more information, please see [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver)'s API docstring.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nDistributes a tf.data.Dataset instance provided via dataset.\nThe returned dataset is a wrapped strategy dataset which creates a multidevice iterator under the hood. It prefetches the input data to the specified devices on the worker. The returned distributed dataset can be iterated over similar to how regular datasets can.\n#### For Example:\n```\nstrategy = tf.distribute.CentralStorageStrategy()  # with 1 CPU and 1 GPU\ndataset = tf.data.Dataset.range(10).batch(2)\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\nfor x in dist_dataset:\n  print(x)  # Prints PerReplica values [0, 1], [2, 3],...\n\n\n```\n\nArgs: dataset: [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to be prefetched to device. options: [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.\nReturns  \n---  \nA \"distributed `Dataset`\" that the caller can iterate over.   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nIn `CentralStorageStrategy` there is a single worker so the value returned will be all the values on that worker.\nArgs  \n---  \n`value` |  A value returned by `run()`, `extended.call_for_each_replica()`, or a variable created in `scope`.   \nReturns  \n---  \nA tuple of values contained in `value`. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\n\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas.\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements. For example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. By default, `reduce` will just aggregate across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient). More often you will want to aggregate across the global batch, which you can get by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\n#### For Example:\n```\nstrategy = tf.distribute.experimental.CentralStorageStrategy(\n    compute_devices=['CPU:0', 'GPU:0'], parameter_device='CPU:0')\nds = tf.data.Dataset.range(10)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(ds)\n\nwith strategy.scope():\n  @tf.function\n  deftrain_step(val):\n    # pass through\n    return val\n\n  # Iterate over the distributed dataset\n  for x in dist_dataset:\n    result = strategy.run(train_step, args=(x,))\n\nresult = strategy.reduce(tf.distribute.ReduceOp.SUM, result,\n                         axis=None).numpy()\n# result: array([ 4,  6,  8, 10])\n\nresult = strategy.reduce(tf.distribute.ReduceOp.SUM, result, axis=0).numpy()\n# result: 28\n\n```\n\nArgs  \n---  \n`reduce_op` |  A [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined.   \n`value` |  A \"per replica\" value, e.g. returned by `run` to be combined into a single tensor.   \n`axis` |  Specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nRun `fn` on each replica, with the given arguments.\nIn `CentralStorageStrategy`, `fn` is called on each of the compute replicas, with the provided \"per replica\" arguments specific to that device.\nArgs  \n---  \n`fn` |  The function to run. The output must be a [`tf.nest`](https://www.tensorflow.org/api_docs/python/tf/nest) of `Tensor`s.   \n`args` |  (Optional) Positional arguments to `fn`.   \n`kwargs` |  (Optional) Keyword arguments to `fn`.   \n`options` |  (Optional) An instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nReturn value from running `fn`.   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/api_docs/python/tf/function": "Compiles a function into a callable TensorFlow graph. (deprecated arguments) (deprecated arguments) (deprecated arguments)  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.function`](https://www.tensorflow.org/api_docs/python/tf/function)\n```\ntf.function(\n    func=None,\n    input_signature=None,\n    autograph=True,\n    jit_compile=None,\n    reduce_retracing=False,\n    experimental_implements=None,\n    experimental_autograph_options=None,\n    experimental_attributes=None,\n    experimental_relax_shapes=None,\n    experimental_compile=None,\n    experimental_follow_type_hints=None\n) -> [tf.types.experimental.PolymorphicFunction](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n  * [Introduction to graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs)\n  * [Random number generation](https://www.tensorflow.org/guide/random_numbers)\n  * [Import a JAX model using JAX2TF](https://www.tensorflow.org/guide/jax2tf)\n\n| \n  * [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training)\n  * [Distributed training with DTensors](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  * [Using DTensors with Keras](https://www.tensorflow.org/tutorials/distribute/dtensor_keras_tutorial)\n\n  \n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) constructs a [`tf.types.experimental.PolymorphicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction) that executes a TensorFlow graph ([`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)) created by trace-compiling the TensorFlow operations in `func`. More information on the topic can be found in [Introduction to Graphs and tf.function](https://www.tensorflow.org/guide/intro_to_graphs).\nSee [Better Performance with tf.function](https://www.tensorflow.org/guide/function) for tips on performance and known limitations.\n#### Example usage:\n```\n@tf.function\ndeff(x, y):\n  return x ** 2 + y\nx = tf.constant([2, 3])\ny = tf.constant([3, -2])\nf(x, y)\n<tf.Tensor: ... numpy=array([7, 7], ...)>\n```\n\nThe trace-compilation allows non-TensorFlow operations to execute, but under special conditions. In general, only TensorFlow operations are guaranteed to run and create fresh results whenever the `PolymorphicFunction` is called.\n## Features\n`func` may use data-dependent Python control flow statements, including `if`, `for`, `while` `break`, `continue` and `return`:\n```\n@tf.function\ndeff(x):\n  if tf.reduce_sum(x) > 0:\n    return x * x\n  else:\n    return -x // 2\nf(tf.constant(-2))\n<tf.Tensor: ... numpy=1>\n```\n\n`func`'s closure may include [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) and [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) objects:\n```\n@tf.function\ndeff():\n  return x ** 2 + y\nx = tf.constant([-2, -3])\ny = tf.Variable([3, -2])\nf()\n<tf.Tensor: ... numpy=array([7, 7], ...)>\n```\n\n`func` may also use ops with side effects, such as [`tf.print`](https://www.tensorflow.org/api_docs/python/tf/print), [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) and others:\n```\nv = tf.Variable(1)\n@tf.function\ndeff(x):\n  for i in tf.range(x):\n    v.assign_add(i)\nf(3)\n\n<tf.Variable ... numpy=4>\n```\n```\nl = []\n@tf.function\ndeff(x):\n  for i in x:\n    l.append(i + 1)    # Caution! Will only happen once when tracing\nf(tf.constant([1, 2, 3]))\n\n[<tf.Tensor ...>]\n```\n\nInstead, use TensorFlow collections like [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray):\n```\n@tf.function\ndeff(x):\n  ta = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n  for i in range(len(x)):\n    ta = ta.write(i, x[i] + 1)\n  return ta.stack()\nf(tf.constant([1, 2, 3]))\n<tf.Tensor: ..., numpy=array([2, 3, 4], ...)>\n```\n\n##  [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) creates polymorphic callables\nInternally, [`tf.types.experimental.PolymorphicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction) may contain multiple [`tf.types.experimental.ConcreteFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/ConcreteFunction)s, each specialized to arguments with different data types or shapes, since TensorFlow can perform more optimizations on graphs of specific shapes, dtypes and values of constant arguments. [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) treats any pure Python values as opaque objects (best thought of as compile-time constants), and builds a separate [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) for each set of Python arguments that it encounters. For more information, see the [tf.function guide](https://www.tensorflow.org/guide/function#rules_of_tracing)\nExecuting a `PolymorphicFunction` will select and execute the appropriate `ConcreteFunction` based on the argument types and values.\nTo obtain an individual `ConcreteFunction`, use the [`PolymorphicFunction.get_concrete_function`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction#get_concrete_function) method. It can be called with the same arguments as `func` and returns a [`tf.types.experimental.ConcreteFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/ConcreteFunction). `ConcreteFunction`s are backed by a single [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph):\n```\n@tf.function\ndeff(x):\n  return x + 1\nisinstance(f.get_concrete_function(1).graph, tf.Graph)\nTrue\n```\n\n`ConcreteFunction`s can be executed just like `PolymorphicFunction`s, but their input is resticted to the types to which they're specialized.\n## Retracing\n`ConcreteFunctions` are built (traced) on the fly, as the `PolymorphicFunction` is called with new TensorFlow types or shapes, or with new Python values as arguments. When `PolymorphicFunction` builds a new trace, it is said that `func` is retraced. Retracing is a frequent performance concern for [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) as it can be considerably slower than executing a graph that's already been traced. It is ideal to minimize the amount of retracing in your code.\n```\n@tf.function\ndeff(x):\n  return tf.abs(x)\nf1 = f.get_concrete_function(1)\nf2 = f.get_concrete_function(2)  # Slow - compiles new graph\nf1 is f2\nFalse\nf1 = f.get_concrete_function(tf.constant(1))\nf2 = f.get_concrete_function(tf.constant(2))  # Fast - reuses f1\nf1 is f2\nTrue\n```\n\nPython numerical arguments should only be used when they take few distinct values, such as hyperparameters like the number of layers in a neural network.\n## Input signatures\nFor Tensor arguments, `PolymorphicFunction`creates a new `ConcreteFunction` for every unique set of input shapes and datatypes. The example below creates two separate `ConcreteFunction`s, each specialized to a different shape:\n```\n@tf.function\ndeff(x):\n  return x + 1\nvector = tf.constant([1.0, 1.0])\nmatrix = tf.constant([[3.0]])\nf.get_concrete_function(vector) is f.get_concrete_function(matrix)\nFalse\n```\n\nAn \"input signature\" can be optionally provided to [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to control this process. The input signature specifies the shape and type of each Tensor argument to the function using a [`tf.TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec) object. More general shapes can be used. This ensures only one `ConcreteFunction` is created, and restricts the `PolymorphicFunction` to the specified shapes and types. It is an effective way to limit retracing when Tensors have dynamic shapes.\n```\n@tf.function(\n    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\ndeff(x):\n  return x + 1\nvector = tf.constant([1.0, 1.0])\nmatrix = tf.constant([[3.0]])\nf.get_concrete_function(vector) is f.get_concrete_function(matrix)\nTrue\n```\n\n## Variables may only be created once\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) only allows creating new [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) objects when it is called for the first time:\n```\nclassMyModule(tf.Module):\n  def__init__(self):\n    self.v = None\n  @tf.function\n  def__call__(self, x):\n    if self.v is None:\n      self.v = tf.Variable(tf.ones_like(x))\n    return self.v * x\n```\n\nIn general, it is recommended to create [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s outside of [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). In simple cases, persisting state across [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) boundaries may be implemented using a pure functional style in which state is represented by [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)s passed as arguments and returned as return values.\nContrast the two styles below:\n```\nstate = tf.Variable(1)\n@tf.function\ndeff(x):\n  state.assign_add(x)\nf(tf.constant(2))  # Non-pure functional style\nstate\n<tf.Variable ... numpy=3>\n```\n```\nstate = tf.constant(1)\n@tf.function\ndeff(state, x):\n  state += x\n  return state\nstate = f(state, tf.constant(2))  # Pure functional style\nstate\n<tf.Tensor: ... numpy=3>\n```\n\n## Python operations execute only once per trace\n`func` may contain TensorFlow operations mixed with pure Python operations. However, when the function is executed, only the TensorFlow operations will run. The Python operations run only once, at trace time. If TensorFlow operations depend on results from Python operations, those results will be frozen into the graph.\n```\n@tf.function\ndeff(a, b):\n  print('this runs at trace time; a is', a, 'and b is', b)\n  return b\nf(1, tf.constant(1))\nthis runs at trace time; a is 1 and b is Tensor(\"...\", shape=(), dtype=int32)\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n```\nf(1, tf.constant(2))\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\n```\n```\nf(2, tf.constant(1))\nthis runs at trace time; a is 2 and b is Tensor(\"...\", shape=(), dtype=int32)\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n```\nf(2, tf.constant(2))\n<tf.Tensor: shape=(), dtype=int32, numpy=2>\n```\n\n## Args  \n---  \n`func` |  The function to be compiled. If `func` is None, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) returns a decorator that can be invoked with a single argument - `func`. In other words, `tf.function(input_signature=...)(func)` is equivalent to [`tf.function(func, input_signature=...)`](https://www.tensorflow.org/api_docs/python/tf/function). The former can be used as decorator.   \n`input_signature` |  A possibly nested sequence of [`tf.TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec) objects specifying the shapes and dtypes of the Tensors that will be supplied to this function. If `None`, a separate function is instantiated for each inferred input signature. If input_signature is specified, every input to `func` must be a `Tensor`, and `func` cannot accept `**kwargs`.   \n`autograph` |  Whether autograph should be applied on `func` before tracing a graph. Data-dependent Python control flow statements require `autograph=True`. For more information, see the [tf.function and AutoGraph guide](https://www.tensorflow.org/guide/function#autograph_transformations).   \n`jit_compile` |  If `True`, compiles the function using [XLA](https://tensorflow.org/xla). XLA performs compiler optimizations, such as fusion, and attempts to emit more efficient code. This may drastically improve the performance. If set to `True`, the whole function needs to be compilable by XLA, or an [`errors.InvalidArgumentError`](https://www.tensorflow.org/api_docs/python/tf/errors/InvalidArgumentError) is thrown. If `None` (default), compiles the function with XLA when running on TPU and goes through the regular function execution path when running on other devices. If `False`, executes the function without XLA compilation. Set this value to `False` when directly running a multi-device function on TPUs (e.g. two TPU cores, one TPU core and its host CPU). Not all functions are compilable, see a list of [sharp corners](https://tensorflow.org/xla/known_issues).   \n`reduce_retracing` |  When True, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) attempts to reduce the amount of retracing, for example by using more generic shapes. This can be controlled for user objects by customizing their associated [`tf.types.experimental.TraceType`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType).   \n`experimental_implements` |  If provided, contains a name of a \"known\" function this implements. For example \"mycompany.my_recurrent_cell\". This is stored as an attribute in inference function, which can then be detected when processing serialized function. See for details. For an example of utilizing this attribute see this `embedded_matmul` (perhaps more efficiently!) by specifying it using this parameter: `@tf.function(experimental_implements=\"embedded_matmul\")` This can either be specified as just the string name of the function or a NameAttrList corresponding to a list of key-value attributes associated with the function name. The name of the function will be in the 'name' field of the NameAttrList. To define a formal TF op for this function implements, try the experimental   \n`experimental_autograph_options` |  Optional tuple of [`tf.autograph.experimental.Feature`](https://www.tensorflow.org/api_docs/python/tf/autograph/experimental/Feature) values.   \n`experimental_attributes` |  Optional dictionary of attributes to include in the generated FunctionDefs.   \n`experimental_relax_shapes` |  Deprecated. Use `reduce_retracing` instead.   \n`experimental_compile` |  Deprecated alias to 'jit_compile'.   \n`experimental_follow_type_hints` |  Deprecated. Please use input_signature or reduce_retracing instead.   \n## Returns  \n---  \nIf `func` is not None, returns a [`tf.types.experimental.PolymorphicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction). If `func` is None, returns a decorator that, when invoked with a single `func` argument, returns a [`tf.types.experimental.PolymorphicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction).   \n## Raises  \n---  \n`ValueError` when attempting to use `jit_compile=True`, but XLA support is not available. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy": "A distribution strategy for synchronous training on multiple workers.  \nInherits From: [`MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), [`Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\n```\ntf.distribute.experimental.MultiWorkerMirroredStrategy(\n    communication=[tf.distribute.experimental.CollectiveCommunication.AUTO](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation#AUTO),\n    cluster_resolver=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Multi-worker training with Estimator](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator)\n\n  \nThis strategy implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy), it replicates all variables and computations to each local device. The difference is that it uses a distributed collective implementation (e.g. all-reduce), so that multiple workers can work together.\nYou need to launch your program on each worker and configure `cluster_resolver` correctly. For example, if you are using [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver), each worker needs to have its corresponding `task_type` and `task_id` set in the `TF_CONFIG` environment variable. An example TF_CONFIG on worker-0 of a two worker cluster is:\n```\nTF_CONFIG = '{\"cluster\": {\"worker\": [\"localhost:12345\", \"localhost:23456\"]}, \"task\": {\"type\": \"worker\", \"index\": 0} }'\n\n```\n\nYour program runs on each worker as-is. Note that collectives require each worker to participate. All [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) and non [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) API may use collectives internally, e.g. checkpointing and saving since reading a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) with [`tf.VariableSynchronization.ON_READ`](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization#ON_READ) all-reduces the value. Therefore it's recommended to run exactly the same program on each worker. Dispatching based on `task_type` or `task_id` of the worker is error-prone.\n`cluster_resolver.num_accelerators()` determines the number of GPUs the strategy uses. If it's zero, the strategy uses the CPU. All workers need to use the same number of devices, otherwise the behavior is undefined.\nThis strategy is not intended for TPU. Use [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy) instead.\nAfter setting up TF_CONFIG, using this strategy is similar to using [`tf.distribute.MirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy) and [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy).\n```\nstrategy = tf.distribute.MultiWorkerMirroredStrategy()\n\nwith strategy.scope():\n  model = tf.keras.Sequential([\n    tf.keras.layers.Dense(2, input_shape=(5,)),\n  ])\n  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n\ndefdataset_fn(ctx):\n  x = np.random.random((2, 5)).astype(np.float32)\n  y = np.random.randint(2, size=(2, 1))\n  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n  return dataset.repeat().batch(1, drop_remainder=True)\ndist_dataset = strategy.distribute_datasets_from_function(dataset_fn)\n\nmodel.compile()\nmodel.fit(dist_dataset)\n\n```\n\nYou can also write your own training loop:\n```\n@tf.function\ndeftrain_step(iterator):\n\n  defstep_fn(inputs):\n    features, labels = inputs\n    with tf.GradientTape() as tape:\n      logits = model(features, training=True)\n      loss = tf.keras.losses.sparse_categorical_crossentropy(\n          labels, logits)\n\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n  strategy.run(step_fn, args=(next(iterator),))\n\nfor _ in range(NUM_STEP):\n  train_step(iterator)\n\n```\n\nSee [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) for a detailed tutorial.\n**Saving**\nYou need to save and checkpoint on all workers instead of just one. This is because variables whose synchronization=ON_READ triggers aggregation during saving. It's recommended to save to a different path on each worker to avoid race conditions. Each worker saves the same thing. See [Multi-worker training with Keras](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#model_saving_and_loading) tutorial for examples.\n**Known Issues**\n  * [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) does not return the correct number of accelerators. The strategy uses all available GPUs if `cluster_resolver` is [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) or `None`.\n  * In eager mode, the strategy needs to be created before calling any other Tensorflow API.\n\n\n## Args  \n---  \n`communication` |  optional [`tf.distribute.experimental.CommunicationImplementation`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation). This is a hint on the preferred collective communication implementation. Possible values include `AUTO`, `RING`, and `NCCL`.   \n`cluster_resolver` |  optional [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver). If `None`, [`tf.distribute.cluster_resolver.TFConfigClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/TFConfigClusterResolver) is used.   \n## Attributes  \n---  \n`cluster_resolver` |  Returns the cluster resolver associated with this strategy.As a multi-worker strategy, [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy) provides the associated [`tf.distribute.cluster_resolver.ClusterResolver`](https://www.tensorflow.org/api_docs/python/tf/distribute/cluster_resolver/ClusterResolver). If the user provides one in `__init__`, that instance is returned; if the user does not, a default `TFConfigClusterResolver` is provided.   \n`extended` |  [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) with additional methods.   \n`num_replicas_in_sync` |  Returns number of replicas over which gradients are aggregated.   \n## Methods\n### `distribute_datasets_from_function`\n```\ndistribute_datasets_from_function(\n    dataset_fn, options=None\n)\n\n```\n\nDistributes [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instances created by calls to `dataset_fn`.\nThe argument `dataset_fn` that users pass in is an input function that has a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) argument and returns a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. It is expected that the returned dataset from `dataset_fn` is already batched by per-replica batch size (i.e. global batch size divided by the number of replicas in sync) and sharded. [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) does not batch or shard the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance returned from the input function. `dataset_fn` will be called on the CPU device of each of the workers and each generates a dataset where every replica on that worker will dequeue one batch of inputs (i.e. if a worker has two replicas, two batches will be dequeued from the `Dataset` every step).\nThis method can be used for several purposes. First, it allows you to specify your own batching and sharding logic. (In contrast, `tf.distribute.experimental_distribute_dataset` does batching and sharding for you.) For example, where `experimental_distribute_dataset` is unable to shard the input files, this method might be used to manually shard the dataset (avoiding the slow fallback behavior in `experimental_distribute_dataset`). In cases where the dataset is infinite, this sharding can be done by creating dataset replicas that differ only in their random seed.\nThe `dataset_fn` should take an [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance where information about batching and input replication can be accessed.\nYou can use `element_spec` property of the [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) returned by this API to query the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the elements returned by the iterator. This can be used to set the `input_signature` property of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Follow [`tf.distribute.DistributedDataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset#element_spec) to see an example.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_datasets_from_function)). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset_fn` |  A function taking a [`tf.distribute.InputContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputContext) instance and returning a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_dataset`\n```\nexperimental_distribute_dataset(\n    dataset, options=None\n)\n\n```\n\nCreates [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe returned [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) can be iterated over similar to regular datasets. NOTE: The user cannot add any more transformations to a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset). You can only create an iterator or examine the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) of the data generated by it. See API docs of [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) to learn more.\nThe following is an example:\n```\nglobal_batch_size = 2\n# Passing the devices is optional.\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\n# Create a dataset\ndataset = tf.data.Dataset.range(4).batch(global_batch_size)\n# Distribute that dataset\ndist_dataset = strategy.experimental_distribute_dataset(dataset)\n@tf.function\ndefreplica_fn(input):\n  return input*2\nresult = []\n# Iterate over the `tf.distribute.DistributedDataset`\nfor x in dist_dataset:\n  # process dataset elements\n  result.append(strategy.run(replica_fn, args=(x,)))\nprint(result)\n[PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([2])>\n}, PerReplica:{\n  0: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([4])>,\n  1: <tf.Tensor: shape=(1,), dtype=int64, numpy=array([6])>\n}]\n```\n\nThree key actions happening under the hood of this method are batching, sharding, and prefetching.\nIn the code snippet above, `dataset` is batched by `global_batch_size`, and calling `experimental_distribute_dataset` on it rebatches `dataset` to a new batch size that is equal to the global batch size divided by the number of replicas in sync. We iterate through it using a Pythonic for loop. `x` is a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing data for all replicas, and each replica gets data of the new batch size. [`tf.distribute.Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#run) will take care of feeding the right per-replica data in `x` to the right `replica_fn` executed on each replica.\nSharding contains autosharding across multiple workers and within every worker. First, in multi-worker distributed training (i.e. when you use [`tf.distribute.experimental.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy) or [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy)), autosharding a dataset over a set of workers means that each worker is assigned a subset of the entire dataset (if the right [`tf.data.experimental.AutoShardPolicy`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy) is set). This is to ensure that at each step, a global batch size of non-overlapping dataset elements will be processed by each worker. Autosharding has a couple of different options that can be specified using [`tf.data.experimental.DistributeOptions`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions). Then, sharding within each worker means the method will split the data among all the worker devices (if more than one a present). This will happen regardless of multi-worker autosharding.\nBy default, this method adds a prefetch transformation at the end of the user provided [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) instance. The argument to the prefetch transformation which is `buffer_size` is equal to the number of replicas in sync.\nIf the above batch splitting and dataset sharding logic is undesirable, please use [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function) instead, which does not do any automatic batching or sharding for you.\nFor a tutorial on more usage and properties of this method, refer to the [tutorial on distributed input](https://www.tensorflow.org/tutorials/distribute/input#tfdistributestrategyexperimental_distribute_dataset). If you are interested in last partial batch handling, read [this section](https://www.tensorflow.org/tutorials/distribute/input#partial_batches).\nArgs  \n---  \n`dataset` |  [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that will be sharded across all replicas using the rules stated above.   \n`options` |  [`tf.distribute.InputOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/InputOptions) used to control options on how this dataset is distributed.   \nReturns  \n---  \nA [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset).   \n### `experimental_distribute_values_from_function`\n```\nexperimental_distribute_values_from_function(\n    value_fn\n)\n\n```\n\nGenerates [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) from `value_fn`.\nThis function is to generate [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) to pass into `run`, `reduce`, or other methods that take distributed values when not using datasets.\nArgs  \n---  \n`value_fn` |  The function to run to generate values. It is called for each replica with `tf.distribute.ValueContext` as the sole argument. It must return a Tensor or a type that can be converted to a Tensor.   \nReturns  \n---  \nA [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing a value for each replica.   \n#### Example usage:\n  1. Return constant value per replica:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return tf.constant(1.)\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n       value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,\n    <tf.Tensor: shape=(), dtype=float32, numpy=1.0>)\n\n```\n\n  2. Distribute values in array based on replica_id:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\narray_value = np.array([3., 2., 1.])\ndefvalue_fn(ctx):\n  return array_value[ctx.replica_id_in_sync_group]\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (3.0, 2.0)\n\n```\n\n  3. Specify values using num_replicas_in_sync:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefvalue_fn(ctx):\n  return ctx.num_replicas_in_sync\ndistributed_values = (\n    strategy.experimental_distribute_values_from_function(\n        value_fn))\nlocal_result = strategy.experimental_local_results(\n    distributed_values)\nlocal_result\n    (2, 2)\n\n```\n\n  4. Place values on devices and distribute:\n```\nstrategy = tf.distribute.TPUStrategy()\nworker_devices = strategy.extended.worker_devices\nmultiple_values = []\nfor i in range(strategy.num_replicas_in_sync):\n  with tf.device(worker_devices[i]):\n    multiple_values.append(tf.constant(1.0))\n\ndefvalue_fn(ctx):\n  return multiple_values[ctx.replica_id_in_sync_group]\n\ndistributed_values = strategy.\n  experimental_distribute_values_from_function(\n  value_fn)\n\n```\n\n\n\n### `experimental_local_results`\n```\nexperimental_local_results(\n    value\n)\n\n```\n\nReturns the list of all local per-replica values contained in `value`.\nArgs  \n---  \n`value` |  A value returned by `experimental_run()`, `run(), or a variable created in`scope`.   \nReturns  \n---  \nA tuple of values contained in `value` where ith element corresponds to ith replica. If `value` represents a single value, this returns `(value,).`  \n### `gather`\n```\ngather(\n    value, axis\n)\n\n```\n\nGather `value` across replicas along `axis` to the current device.\nGiven a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) or [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)-like object `value`, this API gathers and concatenates `value` across replicas along the `axis`-th dimension. The result is copied to the \"current\" device, which would typically be the CPU of the worker on which the program is running. For [`tf.distribute.TPUStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy), it is the first TPU host. For multi-client [`tf.distribute.MultiWorkerMirroredStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/MultiWorkerMirroredStrategy), this is the CPU of each worker.\nThis API can only be called in the cross-replica context. For a counterpart in the replica context, see [`tf.distribute.ReplicaContext.all_gather`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_gather).\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# A DistributedValues with component tensor of shape (2, 1) on each replica\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))\n@tf.function\ndefrun():\n  return strategy.gather(distributed_values, axis=0)\nrun()\n<tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [1],\n       [2]], dtype=int32)>\n```\n\nConsider the following example for more combinations:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\", \"GPU:2\", \"GPU:3\"])\nsingle_tensor = tf.reshape(tf.range(6), shape=(1,2,3))\ndistributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))\n@tf.function\ndefrun(axis):\n  return strategy.gather(distributed_values, axis=axis)\naxis=0\nrun(axis)\n<tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]],\n       [[0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=1\nrun(axis)\n<tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=\narray([[[0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5],\n        [0, 1, 2],\n        [3, 4, 5]]], dtype=int32)>\naxis=2\nrun(axis)\n<tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=\narray([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],\n        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)>\n```\nArgs  \n---  \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with [`tf.distribute.OneDeviceStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/OneDeviceStrategy) or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a [`tf.IndexedSlices`](https://www.tensorflow.org/api_docs/python/tf/IndexedSlices).   \n`axis` |  0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)).   \nReturns  \n---  \nA `Tensor` that's the concatenation of `value` across replicas along `axis` dimension.   \n### `reduce`\n```\nreduce(\n    reduce_op, value, axis\n)\n\n```\n\nReduce `value` across replicas and return result on current device.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\nper_replica_result = strategy.run(step_fn)\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\ntotal\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n```\n\nTo see how this would look with multiple replicas, consider the same example with MirroredStrategy with 2 GPUs:\n```\nstrategy = tf.distribute.MirroredStrategy(devices=[\"GPU:0\", \"GPU:1\"])\ndefstep_fn():\n  i = tf.distribute.get_replica_context().replica_id_in_sync_group\n  return tf.identity(i)\n\nper_replica_result = strategy.run(step_fn)\n# Check devices on which per replica result is:\nstrategy.experimental_local_results(per_replica_result)[0].device\n# /job:localhost/replica:0/task:0/device:GPU:0\nstrategy.experimental_local_results(per_replica_result)[1].device\n# /job:localhost/replica:0/task:0/device:GPU:1\n\ntotal = strategy.reduce(\"SUM\", per_replica_result, axis=None)\n# Check device on which reduced result is:\ntotal.device\n# /job:localhost/replica:0/task:0/device:CPU:0\n\n\n```\n\nThis API is typically used for aggregating the results returned from different replicas, for reporting etc. For example, loss computed from different replicas can be averaged using this API before printing.\nThere are a number of different tf.distribute APIs for reducing values across replicas:\n  * [`tf.distribute.ReplicaContext.all_reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext#all_reduce): This differs from [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) in that it is for replica context and does not copy the results to the host device. `all_reduce` should be typically used for reductions inside the training step such as gradients.\n  * [`tf.distribute.StrategyExtended.reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#reduce_to) and [`tf.distribute.StrategyExtended.batch_reduce_to`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended#batch_reduce_to): These APIs are more advanced versions of [`Strategy.reduce`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#reduce) as they allow customizing the destination of the result. They are also called in cross replica context.\n\n\n_What should axis be?_\nGiven a per-replica value returned by `run`, say a per-example loss, the batch will be divided across all the replicas. This function allows you to aggregate across replicas and optionally also across batch elements by specifying the axis parameter accordingly.\nFor example, if you have a global batch size of 8 and 2 replicas, values for examples `[0, 1, 2, 3]` will be on replica 0 and `[4, 5, 6, 7]` will be on replica 1. With `axis=None`, `reduce` will aggregate only across replicas, returning `[0+4, 1+5, 2+6, 3+7]`. This is useful when each replica is computing a scalar or some other value that doesn't have a \"batch\" dimension (like a gradient or loss).\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=None)\n\n```\n\nSometimes, you will want to aggregate across both the global batch _and_ all replicas. You can get this behavior by specifying the batch dimension as the `axis`, typically `axis=0`. In this case it would return a scalar `0+1+2+3+4+5+6+7`.\n```\nstrategy.reduce(\"sum\", per_replica_result, axis=0)\n\n```\n\nIf there is a last partial batch, you will need to specify an axis so that the resulting shape is consistent across replicas. So if the last batch has size 6 and it is divided into [0, 1, 2, 3] and [4, 5], you would get a shape mismatch unless you specify `axis=0`. If you specify [`tf.distribute.ReduceOp.MEAN`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp#MEAN), using `axis=0` will use the correct denominator of 6. Contrast this with computing `reduce_mean` to get a scalar value on each replica and this function to average those means, which will weigh some values `1/8` and others `1/4`.\nArgs  \n---  \n`reduce_op` |  a [`tf.distribute.ReduceOp`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReduceOp) value specifying how values should be combined. Allows using string representation of the enum such as \"SUM\", \"MEAN\".   \n`value` |  a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) instance, e.g. returned by [`Strategy.run`](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy#run), to be combined into a single tensor. It can also be a regular tensor when used with `OneDeviceStrategy` or default strategy.   \n`axis` |  specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or `None` to only reduce across replicas (e.g. if the tensor has no batch dimension).   \nReturns  \n---  \nA `Tensor`.   \n### `run`\n```\nrun(\n    fn, args=(), kwargs=None, options=None\n)\n\n```\n\nInvokes `fn` on each replica, with the given arguments.\nThis method is the primary way to distribute your computation with a tf.distribute object. It invokes `fn` on each replica. If `args` or `kwargs` have [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), such as those produced by a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) from [`tf.distribute.Strategy.experimental_distribute_dataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#experimental_distribute_dataset) or [`tf.distribute.Strategy.distribute_datasets_from_function`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy#distribute_datasets_from_function), when `fn` is executed on a particular replica, it will be executed with the component of [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) that correspond to that replica.\n`fn` is invoked under a replica context. `fn` may call [`tf.distribute.get_replica_context()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context) to access members such as `all_reduce`. Please see the module-level docstring of tf.distribute for the concept of replica context.\nAll arguments in `args` or `kwargs` can be a nested structure of tensors, e.g. a list of tensors, in which case `args` and `kwargs` will be passed to the `fn` invoked on each replica. Or `args` or `kwargs` can be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) containing tensors or composite tensors, i.e. [`tf.compat.v1.TensorInfo.CompositeTensor`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/TensorInfo/CompositeTensor), in which case each `fn` call will get the component of a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues) corresponding to its replica. Note that arbitrary Python values that are not of the types above are not supported.\n#### Example usage:\n  1. Constant tensor input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\ntensor_input = tf.constant(3.0)\n@tf.function\ndefreplica_fn(input):\n  return input*2.0\nresult = strategy.run(replica_fn, args=(tensor_input,))\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>,\n      1: <tf.Tensor: shape=(), dtype=float32, numpy=6.0>\n\n\n```\n\n  2. DistributedValues input.\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n@tf.function\ndefrun():\n  defvalue_fn(value_context):\n    return value_context.num_replicas_in_sync\n  distributed_values = (\n    strategy.experimental_distribute_values_from_function(\n      value_fn))\n  defreplica_fn2(input):\n    return input*2\n  return strategy.run(replica_fn2, args=(distributed_values,))\nresult = run()\nresult\n    <tf.Tensor: shape=(), dtype=int32, numpy=4>\n\n```\n\n  3. Use [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) to allreduce values.\n```\nstrategy = tf.distribute.MirroredStrategy([\"gpu:0\", \"gpu:1\"])\n@tf.function\ndefrun():\n   defvalue_fn(value_context):\n     return tf.constant(value_context.replica_id_in_sync_group)\n   distributed_values = (\n       strategy.experimental_distribute_values_from_function(\n           value_fn))\n   defreplica_fn(input):\n     return tf.distribute.get_replica_context().all_reduce(\n         \"sum\", input)\n   return strategy.run(replica_fn, args=(distributed_values,))\nresult = run()\nresult\n    PerReplica:{\n      0: <tf.Tensor: shape=(), dtype=int32, numpy=1>,\n      1: <tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n\n```\n\n\nArgs  \n---  \n`fn` |  The function to run on each replica.   \n`args` |  Optional positional arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`kwargs` |  Optional keyword arguments to `fn`. Its element can be a tensor, a nested structure of tensors or a [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues).   \n`options` |  An optional instance of [`tf.distribute.RunOptions`](https://www.tensorflow.org/api_docs/python/tf/distribute/RunOptions) specifying the options to run `fn`.   \nReturns  \n---  \nMerged return value of `fn` across replicas. The structure of the return value is the same as the return value from `fn`. Each element in the structure can either be [`tf.distribute.DistributedValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedValues), `Tensor` objects, or `Tensor`s (for example, if running on a single replica).   \n### `scope`\n```\nscope()\n\n```\n\nContext manager to make the strategy current and distribute variables.\nThis method returns a context manager, and is used as follows:\n```\nstrategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n# Variable created inside scope:\nwith strategy.scope():\n  mirrored_variable = tf.Variable(1.)\nmirrored_variable\nMirroredVariable:{\n  0: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>,\n  1: <tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=1.0>\n\n# Variable created outside scope:\nregular_variable = tf.Variable(1.)\nregular_variable\n<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0>\n```\n\n_What happens when Strategy.scope is entered?_\n  * `strategy` is installed in the global context as the \"current\" strategy. Inside this scope, [`tf.distribute.get_strategy()`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_strategy) will now return this strategy. Outside this scope, it returns the default no-op strategy.\n  * Entering the scope also enters the \"cross-replica context\". See [`tf.distribute.StrategyExtended`](https://www.tensorflow.org/api_docs/python/tf/distribute/StrategyExtended) for an explanation on cross-replica and replica contexts.\n  * Variable creation inside `scope` is intercepted by the strategy. Each strategy defines how it wants to affect the variable creation. Sync strategies like `MirroredStrategy`, `TPUStrategy` and `MultiWorkerMiroredStrategy` create variables replicated on each replica, whereas `ParameterServerStrategy` creates variables on the parameter servers. This is done using a custom [`tf.variable_creator_scope`](https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope).\n  * In some strategies, a default device scope may also be entered: in `MultiWorkerMiroredStrategy`, a default device scope of \"/CPU:0\" is entered on each worker.\n\n\n_What should be in scope and what should be outside?_\nThere are a number of requirements on what needs to happen inside the scope. However, in places where we have information about which strategy is in use, we often enter the scope for the user, so they don't have to do it explicitly (i.e. calling those either inside or outside the scope is OK).\n  * Anything that creates variables that should be distributed variables must be called in a `strategy.scope`. This can be accomplished either by directly calling the variable creating function within the scope context, or by relying on another API like `strategy.run` or [`keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) to automatically enter it for you. Any variable that is created outside scope will not be distributed and may have performance implications. Some common objects that create variables in TF are Models, Optimizers, Metrics. Such objects should always be initialized in the scope, and any functions that may lazily create variables (e.g., [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), etc.) should similarly be called within scope. Another source of variable creation can be a checkpoint restore - when variables are created lazily. Note that any variable created inside a strategy captures the strategy information. So reading and writing to these variables outside the `strategy.scope` can also work seamlessly, without the user having to enter the scope.\n  * Some strategy APIs (such as `strategy.run` and `strategy.reduce`) which require to be in a strategy's scope, enter the scope automatically, which means when using those APIs you don't need to explicitly enter the scope yourself.\n  * When a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) is created inside a `strategy.scope`, the Model object captures the scope information. When high level training framework methods such as `model.compile`, `model.fit`, etc. are then called, the captured scope will be automatically entered, and the associated strategy will be used to distribute the training etc. See a detailed example in [distributed keras tutorial](https://www.tensorflow.org/tutorials/distribute/keras). WARNING: Simply calling `model(..)` does not automatically enter the captured scope -- only high level training framework APIs support this behavior: `model.compile`, `model.fit`, `model.evaluate`, `model.predict` and `model.save` can all be called inside or outside the scope.\n  * The following can be either inside or outside the scope: \n    * Creating the input datasets\n    * Defining [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s that represent your training step\n    * Saving APIs such as [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save). Loading creates variables, so that should go inside the scope if you want to train the model in a distributed way.\n    * Checkpoint saving. As mentioned above - `checkpoint.restore` may sometimes need to be inside scope if it creates variables.\n\nReturns  \n---  \nA context manager. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator": "An object to schedule and coordinate remote function execution.  \n#### View aliases\n**Main aliases**\n[`tf.distribute.coordinator.ClusterCoordinator`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator)\n```\ntf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n\n  \nThis class is used to create fault-tolerant resources and dispatch functions to remote TensorFlow servers.\nCurrently, this class is not supported to be used in a standalone manner. It should be used in conjunction with a [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) strategy that is designed to work with it. The `ClusterCoordinator` class currently only works [`tf.distribute.experimental.ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy).\n**The`schedule` /`join` APIs**\nThe most important APIs provided by this class is the `schedule`/`join` pair. The `schedule` API is non-blocking in that it queues a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) and returns a `RemoteValue` immediately. The queued functions will be dispatched to remote workers in background threads and their `RemoteValue`s will be filled asynchronously. Since `schedule` doesn’t require worker assignment, the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) passed in can be executed on any available worker. If the worker it is executed on becomes unavailable before its completion, it will be migrated to another worker. Because of this fact and function execution is not atomic, a function may be executed more than once.\n**Handling Task Failure**\nThis class when used with [`tf.distribute.experimental.ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy), comes with built-in fault tolerance for worker failures. That is, when some workers are not available for any reason to be reached from the coordinator, the training progress continues to be made with the remaining workers. Upon recovery of a failed worker, it will be added for function execution after datasets created by `create_per_worker_dataset` are re-built on it.\nWhen a parameter server fails, a [`tf.errors.UnavailableError`](https://www.tensorflow.org/api_docs/python/tf/errors/UnavailableError) is raised by `schedule`, `join` or `done`. In this case, in addition to bringing back the failed parameter server, users should restart the coordinator so that it reconnects to workers and parameter servers, re-creates the variables, and loads checkpoints. If the coordinator fails, after the user brings it back, the program will automatically connect to workers and parameter servers, and continue the progress from a checkpoint.\nIt is thus essential that in user's program, a checkpoint file is periodically saved, and restored at the start of the program. If an [`tf.keras.optimizers.Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Optimizer) is checkpointed, after restoring from a checkpoiont, its `iterations` property roughly indicates the number of steps that have been made. This can be used to decide how many epochs and steps are needed before the training completion.\nSee [`tf.distribute.experimental.ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy) docstring for an example usage of this API.\nThis is currently under development, and the API as well as implementation are subject to changes.\n## Args  \n---  \n`strategy` |  a supported [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) object. Currently, only [`tf.distribute.experimental.ParameterServerStrategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/ParameterServerStrategy) is supported.   \n## Raises  \n---  \n`ValueError` |  if the strategy being used is not supported.   \n## Attributes  \n---  \n`strategy` |  Returns the `Strategy` associated with the `ClusterCoordinator`.   \n## Methods\n### `create_per_worker_dataset`\n```\ncreate_per_worker_dataset(\n    dataset_fn\n)\n\n```\n\nCreate dataset on each worker.\nThis creates dataset on workers from the input which can be either a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), a [`tf.distribute.DistributedDataset`](https://www.tensorflow.org/api_docs/python/tf/distribute/DistributedDataset) or a function which returns a dataset, and returns an object that represents the collection of those individual datasets. Calling `iter` on such collection of datasets returns a [`tf.distribute.experimental.coordinator.PerWorkerValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/PerWorkerValues), which is a collection of iterators, where the iterators have been placed on respective workers.\nCalling `next` on a `PerWorkerValues` of iterator is unsupported. The iterator is meant to be passed as an argument into [`tf.distribute.experimental.coordinator.ClusterCoordinator.schedule`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator#schedule). When the scheduled function is about to be executed by a worker, the function will receive the individual iterator that corresponds to the worker. The `next` method can be called on an iterator inside a scheduled function when the iterator is an input of the function.\nCurrently the `schedule` method assumes workers are all the same and thus assumes the datasets on different workers are the same, except they may be shuffled differently if they contain a `dataset.shuffle` operation and a random seed is not set. Because of this, we also recommend the datasets to be repeated indefinitely and schedule a finite number of steps instead of relying on the `OutOfRangeError` from a dataset.\n#### Example:\n```\nstrategy = tf.distribute.experimental.ParameterServerStrategy(\n    cluster_resolver=...)\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy=strategy)\n\n@tf.function\ndefworker_fn(iterator):\n  return next(iterator)\n\ndefper_worker_dataset_fn():\n  return strategy.distribute_datasets_from_function(\n      lambda x: tf.data.Dataset.from_tensor_slices([3] * 3))\n\nper_worker_dataset = coordinator.create_per_worker_dataset(\n    per_worker_dataset_fn)\nper_worker_iter = iter(per_worker_dataset)\nremote_value = coordinator.schedule(worker_fn, args=(per_worker_iter,))\nassert remote_value.fetch() == 3\n\n```\nArgs  \n---  \n`dataset_fn` |  The dataset function that returns a dataset. This is to be executed on the workers.   \nReturns  \n---  \nAn object that represents the collection of those individual datasets. `iter` is expected to be called on this object that returns a [`tf.distribute.experimental.coordinator.PerWorkerValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/PerWorkerValues) of the iterators (that are on the workers).   \n### `done`\n```\ndone()\n\n```\n\nReturns whether all the scheduled functions have finished execution.\nIf any previously scheduled function raises an error, `done` will fail by raising any one of those errors.\nWhen `done` returns True or raises, it guarantees that there is no function that is still being executed.\nReturns  \n---  \nWhether all the scheduled functions have finished execution.   \nRaises  \n---  \n`Exception` |  one of the exceptions caught by the coordinator by any previously scheduled function since the last time an error was thrown or since the beginning of the program.   \n### `fetch`\n```\nfetch(\n    val\n)\n\n```\n\nBlocking call to fetch results from the remote values.\nThis is a wrapper around [`tf.distribute.experimental.coordinator.RemoteValue.fetch`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue#fetch) for a `RemoteValue` structure; it returns the execution results of `RemoteValue`s. If not ready, wait for them while blocking the caller.\n#### Example:\n```\nstrategy = ...\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n    strategy)\n\ndefdataset_fn():\n  return tf.data.Dataset.from_tensor_slices([1, 1, 1])\n\nwith strategy.scope():\n  v = tf.Variable(initial_value=0)\n\n@tf.function\ndefworker_fn(iterator):\n  defreplica_fn(x):\n    v.assign_add(x)\n    return v.read_value()\n  return strategy.run(replica_fn, args=(next(iterator),))\n\ndistributed_dataset = coordinator.create_per_worker_dataset(dataset_fn)\ndistributed_iterator = iter(distributed_dataset)\nresult = coordinator.schedule(worker_fn, args=(distributed_iterator,))\nassert coordinator.fetch(result) == 1\n\n```\nArgs  \n---  \n`val` |  The value to fetch the results from. If this is structure of [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue), `fetch()` will be called on the individual [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) to get the result.   \nReturns  \n---  \nIf `val` is a [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) or a structure of [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue)s, return the fetched [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) values immediately if they are available, or block the call until they are available, and return the fetched [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) values with the same structure. If `val` is other types, return it as-is.   \n### `join`\n```\njoin()\n\n```\n\nBlocks until all the scheduled functions have finished execution.\nIf any previously scheduled function raises an error, `join` will fail by raising any one of those errors, and clear the errors collected so far. If this happens, some of the previously scheduled functions may have not been executed. Users can call `fetch` on the returned [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) to inspect if they have executed, failed, or cancelled. If some that have been cancelled need to be rescheduled, users should call `schedule` with the function again.\nWhen `join` returns or raises, it guarantees that there is no function that is still being executed.\nRaises  \n---  \n`Exception` |  one of the exceptions caught by the coordinator by any previously scheduled function since the last time an error was thrown or since the beginning of the program.   \n### `schedule`\n```\nschedule(\n    fn, args=None, kwargs=None\n)\n\n```\n\nSchedules `fn` to be dispatched to a worker for asynchronous execution.\nThis method is non-blocking in that it queues the `fn` which will be executed later and returns a [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) object immediately. `fetch` can be called on it to wait for the function execution to finish and retrieve its output from a remote worker. On the other hand, call [`tf.distribute.experimental.coordinator.ClusterCoordinator.join`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator#join) to wait for all scheduled functions to finish.\n`schedule` guarantees that `fn` will be executed on a worker at least once; it could be more than once if its corresponding worker fails in the middle of its execution. Note that since worker can fail at any point when executing the function, it is possible that the function is partially executed, but [`tf.distribute.experimental.coordinator.ClusterCoordinator`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/ClusterCoordinator) guarantees that in those events, the function will eventually be executed on any worker that is available.\nIf any previously scheduled function raises an error, `schedule` will raise any one of those errors, and clear the errors collected so far. What happens here, some of the previously scheduled functions may have not been executed. User can call `fetch` on the returned [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) to inspect if they have executed, failed, or cancelled, and reschedule the corresponding function if needed.\nWhen `schedule` raises, it guarantees that there is no function that is still being executed.\nAt this time, there is no support of worker assignment for function execution, or priority of the workers.\n`args` and `kwargs` are the arguments passed into `fn`, when `fn` is executed on a worker. They can be [`tf.distribute.experimental.coordinator.PerWorkerValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/PerWorkerValues) and in this case, the argument will be substituted with the corresponding component on the target worker. Arguments that are not [`tf.distribute.experimental.coordinator.PerWorkerValues`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/PerWorkerValues) will be passed into `fn` as-is. Currently, [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) is not supported to be input `args` or `kwargs`.\nArgs  \n---  \n`fn` |  A [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function); the function to be dispatched to a worker for execution asynchronously. Regular python function is not supported to be scheduled.   \n`args` |  Positional arguments for `fn`.   \n`kwargs` |  Keyword arguments for `fn`.   \nReturns  \n---  \nA [`tf.distribute.experimental.coordinator.RemoteValue`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/coordinator/RemoteValue) object that represents the output of the function scheduled.   \nRaises  \n---  \n`Exception` |  one of the exceptions caught by the coordinator from any previously scheduled function, since the last time an error was thrown or since the beginning of the program. \n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation": "Cross device communication implementation.  \nView aliases\n**Main aliases**\n[`tf.distribute.experimental.CollectiveCommunication`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.experimental.CollectiveCommunication`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation), [`tf.compat.v1.distribute.experimental.CommunicationImplementation`](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation)\n  * `AUTO`: Automatically chosen by Tensorflow.\n  * `RING`: TensorFlow's ring algorithms for all-reduce and all-gather.\n  * `NCCL`: NVIDIA®'s NCCL library. This is now only used for all-reduce on GPUs; all-reduce on CPU, all-gather and broadcast fallbacks to RING.\n\n\n## Class Variables  \n---  \nAUTO  |  `<CommunicationImplementation.AUTO: 'AUTO'>`  \nNCCL  |  `<CommunicationImplementation.NCCL: 'NCCL'>`  \nRING  |  `<CommunicationImplementation.RING: 'RING'>`\n",
  "https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context": "Returns the current [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) or `None`.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.distribute.get_replica_context`](https://www.tensorflow.org/api_docs/python/tf/distribute/get_replica_context)\n```\ntf.distribute.get_replica_context()\n\n```\n\nReturns `None` if in a cross-replica context.\n#### Note that execution:\n  1. starts in the default (single-replica) replica context (this function will return the default `ReplicaContext` object);\n  2. switches to cross-replica context (in which case this will return `None`) when entering a `with tf.distribute.Strategy.scope():` block;\n  3. switches to a (non-default) replica context inside `strategy.run(fn, ...)`;\n  4. if `fn` calls `get_replica_context().merge_call(merge_fn, ...)`, then inside `merge_fn` you are back in the cross-replica context (and again this function will return `None`).\n\n\nMost [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) methods may only be executed in a cross-replica context, in a replica context you should use the API of the [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) object returned by this method instead.\n```\nassert tf.distribute.get_replica_context() is not None  # default\nwith strategy.scope():\n  assert tf.distribute.get_replica_context() is None\n\n  deff():\n    replica_context = tf.distribute.get_replica_context()  # for strategy\n    assert replica_context is not None\n    tf.print(\"Replica id: \", replica_context.replica_id_in_sync_group,\n             \" of \", replica_context.num_replicas_in_sync)\n\n  strategy.run(f)\n\n```\n\n## Returns  \n---  \nThe current [`tf.distribute.ReplicaContext`](https://www.tensorflow.org/api_docs/python/tf/distribute/ReplicaContext) object when in a replica context scope, else `None`.Within a particular block, exactly one of these two things will be true:\n  * `get_replica_context()` returns non-`None`, or\n  * `tf.distribute.is_cross_replica_context()` returns True. \n\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/init_scope": "A context manager that lifts ops out of control-flow scopes and function-building graphs.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.init_scope`](https://www.tensorflow.org/api_docs/python/tf/init_scope)\n```\n@tf_contextlib.contextmanager\ntf.init_scope() -> Iterator[None]\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Better performance with tf.function](https://www.tensorflow.org/guide/function)\n\n| \n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n\n  \nThere is often a need to lift variable initialization ops out of control-flow scopes, function-building graphs, and gradient tapes. Entering an `init_scope` is a mechanism for satisfying these desiderata. In particular, entering an `init_scope` has three effects:\n(1) All control dependencies are cleared the moment the scope is entered; this is equivalent to entering the context manager returned from `control_dependencies(None)`, which has the side-effect of exiting control-flow scopes like [`tf.cond`](https://www.tensorflow.org/api_docs/python/tf/cond) and [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop).\n(2) All operations that are created while the scope is active are lifted into the lowest context on the `context_stack` that is not building a graph function. Here, a context is defined as either a graph or an eager context. Every context switch, i.e., every installation of a graph as the default graph and every switch into eager mode, is logged in a thread-local stack called `context_switches`; the log entry for a context switch is popped from the stack when the context is exited. Entering an `init_scope` is equivalent to crawling up `context_switches`, finding the first context that is not building a graph function, and entering it. A caveat is that if graph mode is enabled but the default graph stack is empty, then entering an `init_scope` will simply install a fresh graph as the default one.\n(3) The gradient tape is paused while the scope is active.\nWhen eager execution is enabled, code inside an init_scope block runs with eager execution enabled even when tracing a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). For example:\n```\ntf.compat.v1.enable_eager_execution()\n\n@tf.function\ndeffunc():\n  # A function constructs TensorFlow graphs,\n  # it does not execute eagerly.\n  assert not tf.executing_eagerly()\n  with tf.init_scope():\n    # Initialization runs with eager execution enabled\n    assert tf.executing_eagerly()\n\n```\n\n## Raises  \n---  \n`RuntimeError` |  if graph state is incompatible with this initialization. \n",
  "https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature": "Configuration for parsing a variable-length input feature.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.VarLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature), [`tf.compat.v1.io.VarLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature)\n```\ntf.io.VarLenFeature(\n    dtype\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [Introduction to Fairness Indicators](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_Example_Colab)\n  * [Fairness Indicators TensorBoard Plugin Example Colab](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_TensorBoard_Plugin_Example_Colab)\n  * [Fairness Indicators on TF-Hub Text Embeddings](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Fairness_Indicators_on_TF_Hub_Text_Embeddings)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n\n  \n## Fields  \n---  \n`dtype` |  Data type of input.   \n## Attributes  \n---  \n`dtype` |  A `namedtuple` alias for field number 0 \n",
  "https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature": "Configuration for passing a RaggedTensor input feature.\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.io.RaggedFeature`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature)\n```\ntf.io.RaggedFeature(\n    dtype,\n    value_key=None,\n    partitions=(),\n    row_splits_dtype=[tf.dtypes.int32](https://www.tensorflow.org/api_docs/python/tf/dtypes#int32),\n    validate=False\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n\n| \n  * [TensorFlow Ranking Keras pipeline for distributed training](https://www.tensorflow.org/ranking/tutorials/ranking_dnn_distributed)\n\n  \n`value_key` specifies the feature key for a variable-length list of values; and `partitions` specifies zero or more feature keys for partitioning those values into higher dimensions. Each element of `partitions` must be one of the following:\n  * `tf.io.RaggedFeature.RowSplits(key: string)`\n  * `tf.io.RaggedFeature.RowLengths(key: string)`\n  * `tf.io.RaggedFeature.RowStarts(key: string)`\n  * `tf.io.RaggedFeature.RowLimits(key: string)`\n  * `tf.io.RaggedFeature.ValueRowIds(key: string)`\n  * `tf.io.RaggedFeature.UniformRowLength(length: int)`.\n\n\nWhere `key` is a feature key whose values are used to partition the values. Partitions are listed from outermost to innermost.\n  * If `len(partitions) == 0` (the default), then:\n    * A feature from a single `tf.Example` is parsed into a 1D [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor).\n    * A feature from a batch of `tf.Example`s is parsed into a 2D [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), where the outer dimension is the batch dimension, and the inner (ragged) dimension is the feature length in each example.\n  * If `len(partitions) == 1`, then:\n    * A feature from a single `tf.Example` is parsed into a 2D [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), where the values taken from the `value_key` are separated into rows using the partition key.\n    * A feature from a batch of `tf.Example`s is parsed into a 3D [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), where the outer dimension is the batch dimension, the two inner dimensions are formed by separating the `value_key` values from each example into rows using that example's partition key.\n  * If `len(partitions) > 1`, then:\n    * A feature from a single `tf.Example` is parsed into a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) whose rank is `len(partitions)+1`, and whose ragged_rank is `len(partitions)`.\n    * A feature from a batch of `tf.Example`s is parsed into a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor) whose rank is `len(partitions)+2` and whose ragged_rank is `len(partitions)+1`, where the outer dimension is the batch dimension.\n\n\nThere is one exception: if the final (i.e., innermost) element(s) of `partitions` are `UniformRowLength`s, then the values are simply reshaped (as a higher-dimensional [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor)), rather than being wrapped in a [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor).\n#### Examples\n```\nimportgoogle.protobuf.text_formataspbtext\nexample_batch = [\n  pbtext.Merge(r'''\n    features {\n      feature {key: \"v\" value {int64_list {value: [3, 1, 4, 1, 5, 9]} } }\n      feature {key: \"s1\" value {int64_list {value: [0, 2, 3, 3, 6]} } }\n      feature {key: \"s2\" value {int64_list {value: [0, 2, 3, 4]} } }\n    }''', tf.train.Example()).SerializeToString(),\n  pbtext.Merge(r'''\n    features {\n      feature {key: \"v\" value {int64_list {value: [2, 7, 1, 8, 2, 8, 1]} } }\n      feature {key: \"s1\" value {int64_list {value: [0, 3, 4, 5, 7]} } }\n      feature {key: \"s2\" value {int64_list {value: [0, 1, 1, 4]} } }\n    }''', tf.train.Example()).SerializeToString()]\n```\n```\nfeatures = {\n    # Zero partitions: returns 1D tf.Tensor for each Example.\n    'f1': tf.io.RaggedFeature(value_key=\"v\", dtype=tf.int64),\n    # One partition: returns 2D tf.RaggedTensor for each Example.\n    'f2': tf.io.RaggedFeature(value_key=\"v\", dtype=tf.int64, partitions=[\n        tf.io.RaggedFeature.RowSplits(\"s1\")]),\n    # Two partitions: returns 3D tf.RaggedTensor for each Example.\n    'f3': tf.io.RaggedFeature(value_key=\"v\", dtype=tf.int64, partitions=[\n        tf.io.RaggedFeature.RowSplits(\"s2\"),\n        tf.io.RaggedFeature.RowSplits(\"s1\")])\n\n```\n```\nfeature_dict = tf.io.parse_single_example(example_batch[0], features)\nfor (name, val) in sorted(feature_dict.items()):\n  print('%s: %s' % (name, val))\nf1: tf.Tensor([3 1 4 1 5 9], shape=(6,), dtype=int64)\nf2: <tf.RaggedTensor [[3, 1], [4], [], [1, 5, 9]]>\nf3: <tf.RaggedTensor [[[3, 1], [4]], [[]], [[1, 5, 9]]]>\n```\n```\nfeature_dict = tf.io.parse_example(example_batch, features)\nfor (name, val) in sorted(feature_dict.items()):\n  print('%s: %s' % (name, val))\nf1: <tf.RaggedTensor [[3, 1, 4, 1, 5, 9],\n                      [2, 7, 1, 8, 2, 8, 1]]>\nf2: <tf.RaggedTensor [[[3, 1], [4], [], [1, 5, 9]],\n                      [[2, 7, 1], [8], [2], [8, 1]]]>\nf3: <tf.RaggedTensor [[[[3, 1], [4]], [[]], [[1, 5, 9]]],\n                      [[[2, 7, 1]], [], [[8], [2], [8, 1]]]]>\n```\n\n## Fields  \n---  \n`dtype` |  Data type of the `RaggedTensor`. Must be one of: [`tf.dtypes.int64`](https://www.tensorflow.org/api_docs/python/tf/dtypes#int64), [`tf.dtypes.float32`](https://www.tensorflow.org/api_docs/python/tf/dtypes#float32), [`tf.dtypes.string`](https://www.tensorflow.org/api_docs/python/tf/dtypes#string).   \n`value_key` |  (Optional.) Key for a `Feature` in the input `Example`, whose parsed `Tensor` will be the resulting [`RaggedTensor.flat_values`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor#flat_values). If not specified, then it defaults to the key for this `RaggedFeature`.   \n`partitions` |  (Optional.) A list of objects specifying the row-partitioning tensors (from outermost to innermost). Each entry in this list must be one of:\n  * `tf.io.RaggedFeature.RowSplits(key: string)`\n  * `tf.io.RaggedFeature.RowLengths(key: string)`\n  * `tf.io.RaggedFeature.RowStarts(key: string)`\n  * `tf.io.RaggedFeature.RowLimits(key: string)`\n  * `tf.io.RaggedFeature.ValueRowIds(key: string)`\n  * `tf.io.RaggedFeature.UniformRowLength(length: int)`. Where `key` is a key for a `Feature` in the input `Example`, whose parsed `Tensor` will be the resulting row-partitioning tensor. \n\n  \n`row_splits_dtype` |  (Optional.) Data type for the row-partitioning tensor(s). One of `int32` or `int64`. Defaults to `int32`.   \n`validate` |  (Optional.) Boolean indicating whether or not to validate that the input values form a valid RaggedTensor. Defaults to `False`.   \n## Attributes  \n---  \n`dtype` |  A `namedtuple` alias for field number 0   \n`value_key` |  A `namedtuple` alias for field number 1   \n`partitions` |  A `namedtuple` alias for field number 2   \n`row_splits_dtype` |  A `namedtuple` alias for field number 3   \n`validate` |  A `namedtuple` alias for field number 4   \n## Child Classes\n[`class RowLengths`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/RowLengths)\n[`class RowLimits`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/RowLimits)\n[`class RowSplits`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/RowSplits)\n[`class RowStarts`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/RowStarts)\n[`class UniformRowLength`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/UniformRowLength)\n[`class ValueRowIds`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature/ValueRowIds)\n",
  "https://www.tensorflow.org/api_docs/python/tf/io": "Public API for tf._api.v2.io namespace\n## Modules\n[`gfile`](https://www.tensorflow.org/api_docs/python/tf/io/gfile) module: Public API for tf._api.v2.io.gfile namespace\n## Classes\n[`class FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature): Configuration for parsing a fixed-length input feature.\n[`class FixedLenSequenceFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenSequenceFeature): Configuration for parsing a variable-length input feature into a `Tensor`.\n[`class RaggedFeature`](https://www.tensorflow.org/api_docs/python/tf/io/RaggedFeature): Configuration for passing a RaggedTensor input feature.\n[`class SparseFeature`](https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature): Configuration for parsing a sparse input feature from an `Example`.\n[`class TFRecordOptions`](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordOptions): Options used for manipulating TFRecord files.\n[`class TFRecordWriter`](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter): A class to write records to a TFRecords file.\n[`class VarLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/VarLenFeature): Configuration for parsing a variable-length input feature.\n## Functions\n[`decode_and_crop_jpeg(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_and_crop_jpeg): Decode and Crop a JPEG-encoded image to a uint8 tensor.\n[`decode_base64(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_base64): Decode web-safe base64-encoded strings.\n[`decode_bmp(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_bmp): Decode the first frame of a BMP-encoded image to a uint8 tensor.\n[`decode_compressed(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_compressed): Decompress strings.\n[`decode_csv(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_csv): Convert CSV records to tensors. Each column maps to one tensor.\n[`decode_gif(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_gif): Decode the frame(s) of a GIF-encoded image to a uint8 tensor.\n[`decode_image(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_image): Function for `decode_bmp`, `decode_gif`, `decode_jpeg`, and `decode_png`.\n[`decode_jpeg(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_jpeg): Decode a JPEG-encoded image to a uint8 tensor.\n[`decode_json_example(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_json_example): Convert JSON-encoded Example records to binary protocol buffer strings.\n[`decode_png(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_png): Decode a PNG-encoded image to a uint8 or uint16 tensor.\n[`decode_proto(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_proto): The op extracts fields from a serialized protocol buffers message into tensors.\n[`decode_raw(...)`](https://www.tensorflow.org/api_docs/python/tf/io/decode_raw): Convert raw bytes from input tensor into numeric tensors.\n[`deserialize_many_sparse(...)`](https://www.tensorflow.org/api_docs/python/tf/io/deserialize_many_sparse): Deserialize and concatenate `SparseTensors` from a serialized minibatch.\n[`encode_base64(...)`](https://www.tensorflow.org/api_docs/python/tf/io/encode_base64): Encode strings into web-safe base64 format.\n[`encode_jpeg(...)`](https://www.tensorflow.org/api_docs/python/tf/io/encode_jpeg): JPEG-encode an image.\n[`encode_png(...)`](https://www.tensorflow.org/api_docs/python/tf/io/encode_png): PNG-encode an image.\n[`encode_proto(...)`](https://www.tensorflow.org/api_docs/python/tf/io/encode_proto): The op serializes protobuf messages provided in the input tensors.\n[`extract_jpeg_shape(...)`](https://www.tensorflow.org/api_docs/python/tf/io/extract_jpeg_shape): Extract the shape information of a JPEG-encoded image.\n[`is_jpeg(...)`](https://www.tensorflow.org/api_docs/python/tf/io/is_jpeg): Convenience function to check if the 'contents' encodes a JPEG image.\n[`match_filenames_once(...)`](https://www.tensorflow.org/api_docs/python/tf/io/match_filenames_once): Save the list of files matching pattern, so it is only computed once.\n[`matching_files(...)`](https://www.tensorflow.org/api_docs/python/tf/io/matching_files): Returns the set of files matching one or more glob patterns.\n[`parse_example(...)`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example): Parses `Example` protos into a `dict` of tensors.\n[`parse_sequence_example(...)`](https://www.tensorflow.org/api_docs/python/tf/io/parse_sequence_example): Parses a batch of `SequenceExample` protos.\n[`parse_single_example(...)`](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_example): Parses a single `Example` proto.\n[`parse_single_sequence_example(...)`](https://www.tensorflow.org/api_docs/python/tf/io/parse_single_sequence_example): Parses a single `SequenceExample` proto.\n[`parse_tensor(...)`](https://www.tensorflow.org/api_docs/python/tf/io/parse_tensor): Transforms a serialized tensorflow.TensorProto proto into a Tensor.\n[`read_file(...)`](https://www.tensorflow.org/api_docs/python/tf/io/read_file): Reads the contents of file.\n[`serialize_many_sparse(...)`](https://www.tensorflow.org/api_docs/python/tf/io/serialize_many_sparse): Serialize `N`-minibatch `SparseTensor` into an `[N, 3]` `Tensor`.\n[`serialize_sparse(...)`](https://www.tensorflow.org/api_docs/python/tf/io/serialize_sparse): Serialize a `SparseTensor` into a 3-vector (1-D `Tensor`) object.\n[`serialize_tensor(...)`](https://www.tensorflow.org/api_docs/python/tf/io/serialize_tensor): Transforms a Tensor into a serialized TensorProto proto.\n[`write_file(...)`](https://www.tensorflow.org/api_docs/python/tf/io/write_file): Writes `contents` to the file at input `filename`.\n[`write_graph(...)`](https://www.tensorflow.org/api_docs/python/tf/io/write_graph): Writes a graph proto to a file.\n",
  "https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature": "Configuration for parsing a fixed-length input feature.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature), [`tf.compat.v1.io.FixedLenFeature`](https://www.tensorflow.org/api_docs/python/tf/io/FixedLenFeature)\n```\ntf.io.FixedLenFeature(\n    shape, dtype, default_value=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n\n| \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Recommending movies: retrieval using a sequential model](https://www.tensorflow.org/recommenders/examples/sequential_retrieval)\n  * [Graph regularization for document classification using natural graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora)\n  * [Passage Ranking using TFR-BERT](https://www.tensorflow.org/ranking/tutorials/tfr_bert)\n\n  \nTo treat sparse input as dense, provide a `default_value`; otherwise, the parse functions will fail on any examples missing this feature.\n## Fields  \n---  \n`shape` |  Shape of input data.   \n`dtype` |  Data type of input.   \n`default_value` |  Value to be used if an example is missing this feature. It must be compatible with `dtype` and of the specified `shape`.   \n## Attributes  \n---  \n`shape` |  A `namedtuple` alias for field number 0   \n`dtype` |  A `namedtuple` alias for field number 1   \n`default_value` |  A `namedtuple` alias for field number 2 \n",
  "https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature": "Configuration for parsing a sparse input feature from an `Example`.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.SparseFeature`](https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature), [`tf.compat.v1.io.SparseFeature`](https://www.tensorflow.org/api_docs/python/tf/io/SparseFeature)\n```\ntf.io.SparseFeature(\n    index_key, value_key, dtype, size, already_sorted=False\n)\n\n```\n\nNote, preferably use `VarLenFeature` (possibly in combination with a `SequenceExample`) in order to parse out `SparseTensor`s instead of `SparseFeature` due to its simplicity.\nClosely mimicking the `SparseTensor` that will be obtained by parsing an `Example` with a `SparseFeature` config, a `SparseFeature` contains a\n  * `value_key`: The name of key for a `Feature` in the `Example` whose parsed `Tensor` will be the resulting [`SparseTensor.values`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#values).\n  * `index_key`: A list of names - one for each dimension in the resulting `SparseTensor` whose `indices[i][dim]` indicating the position of the `i`-th value in the `dim` dimension will be equal to the `i`-th value in the Feature with key named `index_key[dim]` in the `Example`.\n  * `size`: A list of ints for the resulting [`SparseTensor.dense_shape`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#dense_shape).\n\n\nFor example, we can represent the following 2D `SparseTensor`\n```\nSparseTensor(indices=[[3, 1], [20, 0]],\n             values=[0.5, -1.0]\n             dense_shape=[100, 3])\n\n```\n\nwith an `Example` input proto\n```\nfeatures {\n  feature { key: \"val\" value { float_list { value: [ 0.5, -1.0 ] } } }\n  feature { key: \"ix0\" value { int64_list { value: [ 3, 20 ] } } }\n  feature { key: \"ix1\" value { int64_list { value: [ 1, 0 ] } } }\n}\n\n```\n\nand `SparseFeature` config with 2 `index_key`s\n```\nSparseFeature(index_key=[\"ix0\", \"ix1\"],\n              value_key=\"val\",\n              dtype=tf.float32,\n              size=[100, 3])\n\n```\n\n## Fields  \n---  \n`index_key` |  A single string name or a list of string names of index features. For each key the underlying feature's type must be `int64` and its length must always match that of the `value_key` feature. To represent `SparseTensor`s with a `dense_shape` of `rank` higher than 1 a list of length `rank` should be used.   \n`value_key` |  Name of value feature. The underlying feature's type must be `dtype` and its length must always match that of all the `index_key`s' features.   \n`dtype` |  Data type of the `value_key` feature.   \n`size` |  A Python int or list thereof specifying the dense shape. Should be a list if and only if `index_key` is a list. In that case the list must be equal to the length of `index_key`. Each for each entry `i` all values in the `index_key`[i] feature must be in `[0, size[i])`.   \n`already_sorted` |  A Python boolean to specify whether the values in `value_key` are already sorted by their index position. If so skip sorting. False by default (optional).   \n## Attributes  \n---  \n`index_key` |  A `namedtuple` alias for field number 0   \n`value_key` |  A `namedtuple` alias for field number 1   \n`dtype` |  A `namedtuple` alias for field number 2   \n`size` |  A `namedtuple` alias for field number 3   \n`already_sorted` |  A `namedtuple` alias for field number 4 \n",
  "https://www.tensorflow.org/api_docs/python/tf/keras": "DO NOT EDIT.  \nThis file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.\n## Modules\n[`activations`](https://www.tensorflow.org/api_docs/python/tf/keras/activations) module: DO NOT EDIT.\n[`applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications) module: DO NOT EDIT.\n[`backend`](https://www.tensorflow.org/api_docs/python/tf/keras/backend) module: DO NOT EDIT.\n[`callbacks`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks) module: DO NOT EDIT.\n[`config`](https://www.tensorflow.org/api_docs/python/tf/keras/config) module: DO NOT EDIT.\n[`constraints`](https://www.tensorflow.org/api_docs/python/tf/keras/constraints) module: DO NOT EDIT.\n[`datasets`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) module: DO NOT EDIT.\n[`distribution`](https://www.tensorflow.org/api_docs/python/tf/keras/distribution) module: DO NOT EDIT.\n[`dtype_policies`](https://www.tensorflow.org/api_docs/python/tf/keras/dtype_policies) module: DO NOT EDIT.\n[`export`](https://www.tensorflow.org/api_docs/python/tf/keras/export) module: DO NOT EDIT.\n[`initializers`](https://www.tensorflow.org/api_docs/python/tf/keras/initializers) module: DO NOT EDIT.\n[`layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) module: DO NOT EDIT.\n[`legacy`](https://www.tensorflow.org/api_docs/python/tf/keras/legacy) module: DO NOT EDIT.\n[`losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses) module: DO NOT EDIT.\n[`metrics`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) module: DO NOT EDIT.\n[`mixed_precision`](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision) module: DO NOT EDIT.\n[`models`](https://www.tensorflow.org/api_docs/python/tf/keras/models) module: DO NOT EDIT.\n[`ops`](https://www.tensorflow.org/api_docs/python/tf/keras/ops) module: DO NOT EDIT.\n[`optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) module: DO NOT EDIT.\n[`preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing) module: DO NOT EDIT.\n[`quantizers`](https://www.tensorflow.org/api_docs/python/tf/keras/quantizers) module: DO NOT EDIT.\n[`random`](https://www.tensorflow.org/api_docs/python/tf/keras/random) module: DO NOT EDIT.\n[`regularizers`](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers) module: DO NOT EDIT.\n[`tree`](https://www.tensorflow.org/api_docs/python/tf/keras/tree) module: DO NOT EDIT.\n[`utils`](https://www.tensorflow.org/api_docs/python/tf/keras/utils) module: DO NOT EDIT.\n## Classes\n[`class DTypePolicy`](https://www.tensorflow.org/api_docs/python/tf/keras/DTypePolicy): A dtype policy for a Keras layer.\n[`class FloatDTypePolicy`](https://www.tensorflow.org/api_docs/python/tf/keras/FloatDTypePolicy): A dtype policy for a Keras layer.\n[`class Function`](https://www.tensorflow.org/api_docs/python/tf/keras/Function): Class that encapsulates a computation graph of Keras operations.\n[`class Initializer`](https://www.tensorflow.org/api_docs/python/tf/keras/Initializer): Initializer base class: all Keras initializers inherit from this class.\n[`class InputSpec`](https://www.tensorflow.org/api_docs/python/tf/keras/InputSpec): Specifies the rank, dtype and shape of every input to a layer.\n[`class KerasTensor`](https://www.tensorflow.org/api_docs/python/tf/keras/KerasTensor): Symbolic tensor -- encapsulates a shape and a dtype.\n[`class Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer): This is the class from which all layers inherit.\n[`class Loss`](https://www.tensorflow.org/api_docs/python/tf/keras/Loss): Loss base class.\n[`class Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric): Encapsulates metric logic and state.\n[`class Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model): A model grouping layers into an object with training/inference features.\n[`class Operation`](https://www.tensorflow.org/api_docs/python/tf/keras/Operation)\n[`class Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Optimizer): A class for Tensorflow specific optimizer logic.\n[`class Quantizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Quantizer)\n[`class Regularizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Regularizer): Regularizer base class.\n[`class Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential): `Sequential` groups a linear stack of layers into a `Model`.\n[`class StatelessScope`](https://www.tensorflow.org/api_docs/python/tf/keras/StatelessScope): Scope to prevent any update to Keras Variables.\n[`class Variable`](https://www.tensorflow.org/api_docs/python/tf/keras/Variable): Represents a backend-agnostic variable in Keras.\n[`class name_scope`](https://www.tensorflow.org/api_docs/python/tf/keras/name_scope): Creates a sub-namespace for variable paths.\n## Functions\n[`Input(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/Input): Used to instantiate a Keras tensor.\n[`device(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/device)\n[`version(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/version)\n## Other Members  \n---  \n**version** |  `'3.3.3'`\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/Model": "A model grouping layers into an object with training/inference features.\nInherits From: [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer), [`Operation`](https://www.tensorflow.org/api_docs/python/tf/keras/Operation)\nView aliases\n**Main aliases**\n[`tf.keras.models.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n```\ntf.keras.Model(\n    *args, **kwargs\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Use TF1.x models in TF2 workflows](https://www.tensorflow.org/guide/migrate/model_mapping)\n  * [Migrate `tf.feature_column`s to Keras preprocessing layers](https://www.tensorflow.org/guide/migrate/migrating_feature_columns)\n  * [Debug a TensorFlow 2 migrated training pipeline](https://www.tensorflow.org/guide/migrate/migration_debugging)\n  * [Introduction to modules, layers, and models](https://www.tensorflow.org/guide/intro_to_modules)\n  * [Basic training loops](https://www.tensorflow.org/guide/basic_training_loops)\n\n| \n  * [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series)\n  * [Load a pandas DataFrame](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  * [Intro to Autoencoders](https://www.tensorflow.org/tutorials/generative/autoencoder)\n  * [Learned data compression](https://www.tensorflow.org/tutorials/generative/data_compression)\n\n  \nThere are three ways to instantiate a `Model`:\n## With the \"Functional API\"\nYou start from `Input`, you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:\n```\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n```\n\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n#### Example:\n```\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\n\n```\n\nNote that the `backbone` and `activations` models are not created with [`keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects, but with the tensors that originate from [`keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects. Under the hood, the layers and weights will be shared across these models, so that user can train the `full_model`, and use `backbone` or `activations` to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.\n## By subclassing the `Model` class\nIn that case, you should define your layers in `__init__()` and you should implement the model's forward pass in `call()`.\n```\nclassMyModel(keras.Model):\n    def__init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    defcall(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\n\n```\n\nIf you subclass `Model`, you can optionally have a `training` argument (boolean) in `call()`, which you can use to specify a different behavior in training and inference:\n```\nclassMyModel(keras.Model):\n    def__init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    defcall(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\n\n```\n\nOnce the model is created, you can config the model with losses and metrics with `model.compile()`, train the model with `model.fit()`, or use the model to do prediction with `model.predict()`.\n## With the `Sequential` class\nIn addition, [`keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) is a special case of model where the model is purely a stack of single-input, single-output layers.\n```\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\n```\n\n## Attributes  \n---  \n`compiled_metrics`  \n`distribute_reduction_method`  \n`distribute_strategy`  \n`input` |  Retrieves the input tensor(s) of a symbolic operation.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`jit_compile`  \n`layers`  \n`metrics_names`  \n`output` |  Retrieves the output tensor(s) of a layer.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`run_eagerly`  \n## Methods\n### `compile`\n```\ncompile(\n    optimizer='rmsprop',\n    loss=None,\n    loss_weights=None,\n    metrics=None,\n    weighted_metrics=None,\n    run_eagerly=False,\n    steps_per_execution=1,\n    jit_compile='auto',\n    auto_scale_loss=True\n)\n\n```\n\nConfigures the model for training.\n#### Example:\n```\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss=keras.losses.BinaryCrossentropy(),\n    metrics=[\n        keras.metrics.BinaryAccuracy(),\n        keras.metrics.FalseNegatives(),\n    ],\n)\n\n```\n\nArgs  \n---  \n`optimizer` |  String (name of optimizer) or optimizer instance. See [`keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).   \n`loss` |  Loss function. May be a string (name of loss function), or a [`keras.losses.Loss`](https://www.tensorflow.org/api_docs/python/tf/keras/Loss) instance. See [`keras.losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses). A loss function is any callable with the signature `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth values, and `y_pred` are the model's predictions. `y_true` should have shape `(batch_size, d0, .. dN)` (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape `(batch_size, d0, .. dN-1)`). `y_pred` should have shape `(batch_size, d0, .. dN)`. The loss function should return a float tensor.   \n`loss_weights` |  Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the _weighted sum_ of all individual losses, weighted by the `loss_weights` coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients.   \n`metrics` |  List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a [`keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric) instance. See [`keras.metrics`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics). Typically you will use `metrics=['accuracy']`. A function is any callable with the signature `result = fn(y_true, _pred)`. To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`. You can also pass a list to specify a metric or a list of metrics for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the strings 'accuracy' or 'acc', we convert this to one of [`keras.metrics.BinaryAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy), [`keras.metrics.CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy), [`keras.metrics.SparseCategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy) based on the shapes of the targets and of the model output. A similar conversion is done for the strings `\"crossentropy\"` and `\"ce\"` as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the `weighted_metrics` argument instead.   \n`weighted_metrics` |  List of metrics to be evaluated and weighted by `sample_weight` or `class_weight` during training and testing.   \n`run_eagerly` |  Bool. If `True`, this model's forward pass will never be compiled. It is recommended to leave this as `False` when training (for best performance), and to set it to `True` when debugging.   \n`steps_per_execution` |  Int. The number of batches to run during each a single compiled function call. Running multiple batches inside a single compiled function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if `steps_per_execution` is set to `N`, [`Callback.on_batch_begin`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_batch_begin) and [`Callback.on_batch_end`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_batch_end) methods will only be called every `N` batches (i.e. before/after each compiled function execution). Not supported with the PyTorch backend.   \n`jit_compile` |  Bool or `\"auto\"`. Whether to use XLA compilation when compiling a model. For `jax` and `tensorflow` backends, `jit_compile=\"auto\"` enables XLA compilation if the model supports it, and disabled otherwise. For `torch` backend, `\"auto\"` will default to eager execution and `jit_compile=True` will run with `torch.compile` with the `\"inductor\"` backend.   \n`auto_scale_loss` |  Bool. If `True` and the model dtype policy is `\"mixed_float16\"`, the passed optimizer will be automatically wrapped in a `LossScaleOptimizer`, which will dynamically scale the loss to prevent underflow.   \n### `compile_from_config`\n```\ncompile_from_config(\n    config\n)\n\n```\n\nCompiles the model with the information given in config.\nThis method uses the information in the config (optimizer, loss, metrics, etc.) to compile the model.\nArgs  \n---  \n`config` |  Dict containing information for compiling the model.   \n### `compiled_loss`\n```\ncompiled_loss(\n    y, y_pred, sample_weight=None, regularization_losses=None\n)\n\n```\n\n### `compute_loss`\n```\ncompute_loss(\n    x=None, y=None, y_pred=None, sample_weight=None\n)\n\n```\n\nCompute the total loss, validate it, and return it.\nSubclasses can optionally override this method to provide custom loss computation logic.\n#### Example:\n```\nclassMyModel(Model):\n    def__init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_tracker = metrics.Mean(name='loss')\n\n    defcompute_loss(self, x, y, y_pred, sample_weight):\n        loss = ops.means((y_pred - y) ** 2)\n        loss += ops.sum(self.losses)\n        self.loss_tracker.update_state(loss)\n        return loss\n\n    defreset_metrics(self):\n        self.loss_tracker.reset_state()\n\n    @property\n    defmetrics(self):\n        return [self.loss_tracker]\n\ninputs = layers.Input(shape=(10,), name='my_input')\noutputs = layers.Dense(10)(inputs)\nmodel = MyModel(inputs, outputs)\nmodel.add_loss(ops.sum(outputs))\n\noptimizer = SGD()\nmodel.compile(optimizer, loss='mse', steps_per_execution=10)\ndataset = ...\nmodel.fit(dataset, epochs=2, steps_per_epoch=10)\nprint(f\"Custom loss: {model.loss_tracker.result()}\")\n\n```\n\nArgs  \n---  \nInput data.   \nTarget data.   \n`y_pred` |  Predictions returned by the model (output of `model(x)`)   \n`sample_weight` |  Sample weights for weighting the loss function.   \nReturns  \n---  \nThe total loss as a scalar tensor, or `None` if no loss results (which is the case when called by [`Model.test_step`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#test_step)).   \n### `compute_metrics`\n```\ncompute_metrics(\n    x, y, y_pred, sample_weight=None\n)\n\n```\n\nUpdate metric states and collect all metrics to be returned.\nSubclasses can optionally override this method to provide custom metric updating and collection logic.\n#### Example:\n```\nclassMyModel(Sequential):\n    defcompute_metrics(self, x, y, y_pred, sample_weight):\n        # This super call updates `self.compiled_metrics` and returns\n        # results for all metrics listed in `self.metrics`.\n        metric_results = super().compute_metrics(\n            x, y, y_pred, sample_weight)\n\n        # Note that `self.custom_metric` is not listed\n        # in `self.metrics`.\n        self.custom_metric.update_state(x, y, y_pred, sample_weight)\n        metric_results['metric_name'] = self.custom_metric.result()\n        return metric_results\n\n```\n\nArgs  \n---  \nInput data.   \nTarget data.   \n`y_pred` |  Predictions returned by the model output of `model.call(x)`.   \n`sample_weight` |  Sample weights for weighting the loss function.   \nReturns  \n---  \nA `dict` containing values that will be passed to [`keras.callbacks.CallbackList.on_train_batch_end()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CallbackList#on_train_batch_end). Typically, the values of the metrics listed in `self.metrics` are returned.   \n`Example` |  `{'loss': 0.2, 'accuracy': 0.7}`.   \n### `evaluate`\n```\nevaluate(\n    x=None,\n    y=None,\n    batch_size=None,\n    verbose='auto',\n    sample_weight=None,\n    steps=None,\n    callbacks=None,\n    return_dict=False,\n    **kwargs\n)\n\n```\n\nReturns the loss value & metrics values for the model in test mode.\nComputation is done in batches (see the `batch_size` arg.)\nArgs  \n---  \nInput data. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n  * A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n  * A generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. \n\n  \nTarget data. Like the input data `x`, it could be either NumPy array(s) or backend-native tensor(s). If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, `y` should not be specified (since targets will be obtained from the iterator/dataset).   \n`batch_size` |  Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\"auto\"` becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). Defaults to `\"auto\"`.   \n`sample_weight` |  Optional NumPy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) NumPy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`.   \n`steps` |  Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) and `steps` is `None`, evaluation will run until the dataset is exhausted.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during evaluation.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nScalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs.   \n### `export`\n```\nexport(\n    filepath, format='tf_saved_model'\n)\n\n```\n\nCreate a TF SavedModel artifact for inference.\nThis method lets you export a model to a lightweight SavedModel artifact that contains the model's forward pass only (its `call()` method) and can be served via e.g. TF-Serving. The forward pass is registered under the name `serve()` (see example below).\nThe original code of the model (including any custom layers you may have used) is _no longer_ necessary to reload the artifact -- it is entirely standalone.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the artifact.   \n#### Example:\n```\n# Create the artifact\nmodel.export(\"path/to/location\")\n\n# Later, in a different process / environment...\nreloaded_artifact = tf.saved_model.load(\"path/to/location\")\npredictions = reloaded_artifact.serve(input_data)\n\n```\n\nIf you would like to customize your serving endpoints, you can use the lower-level [`keras.export.ExportArchive`](https://www.tensorflow.org/api_docs/python/tf/keras/export/ExportArchive) class. The `export()` method relies on `ExportArchive` internally.\n### `fit`\n```\nfit(\n    x=None,\n    y=None,\n    batch_size=None,\n    epochs=1,\n    verbose='auto',\n    callbacks=None,\n    validation_split=0.0,\n    validation_data=None,\n    shuffle=True,\n    class_weight=None,\n    sample_weight=None,\n    initial_epoch=0,\n    steps_per_epoch=None,\n    validation_steps=None,\n    validation_batch_size=None,\n    validation_freq=1\n)\n\n```\n\nTrains the model for a fixed number of epochs (dataset iterations).\nArgs  \n---  \nInput data. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n  * A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n  * A [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. \n\n  \nTarget data. Like the input data `x`, it could be either NumPy array(s) or backend-native tensor(s). If `x` is a dataset, generator, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, `y` should not be specified (since targets will be obtained from `x`).   \n`batch_size` |  Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`epochs` |  Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. \"auto\" becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g., in a production environment). Defaults to `\"auto\"`.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during training. See [`keras.callbacks`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). Note [`keras.callbacks.ProgbarLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ProgbarLogger) and [`keras.callbacks.History`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History) callbacks are created automatically and need not be passed to `model.fit()`. [`keras.callbacks.ProgbarLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ProgbarLogger) is created or not based on the `verbose` argument in `model.fit()`.   \n`validation_split` |  Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`.   \n`validation_data` |  Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. It could be: \n* A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n* A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n* A Python generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`.   \n`shuffle` |  Boolean, whether to shuffle the training data before each epoch. This argument is ignored when `x` is a generator or a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`class_weight` |  Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. When `class_weight` is specified and targets have a rank of 2 or greater, either `y` must be one-hot encoded, or an explicit final dimension of `1` must be included for sparse class labels.   \n`sample_weight` |  Optional NumPy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) NumPy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead.   \n`initial_epoch` |  Integer. Epoch at which to start training (useful for resuming a previous training run).   \n`steps_per_epoch` |  Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as backend-native tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), and `steps_per_epoch` is `None`, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset.   \n`validation_steps` |  Only relevant if `validation_data` is provided. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If `validation_steps` is `None`, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If `validation_steps` is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time.   \n`validation_batch_size` |  Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`validation_freq` |  Only relevant if validation data is provided. Specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs.   \nUnpacking behavior for iterator-like inputs: A common pattern is to pass an iterator like object such as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) or a [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) to `fit()`, which will in fact yield not only features (`x`) but optionally targets (`y`) and sample weights (`sample_weight`). Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for `y` and `sample_weight` respectively. Any other type provided will be wrapped in a length-one tuple, effectively treating everything as `x`. When yielding dicts, they should still adhere to the top-level tuple structure, e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the `namedtuple`. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into `x`, `y`, and `sample_weight` or passed through as a single element to `x`.\nReturns  \n---  \nA `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).   \n### `from_config`\n```\n@classmethod\nfrom_config(\n    config, custom_objects=None\n)\n\n```\n\nCreates a layer from its config.\nThis method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`).\nArgs  \n---  \n`config` |  A Python dictionary, typically the output of get_config.   \nReturns  \n---  \nA layer instance.   \n### `get_compile_config`\n```\nget_compile_config()\n\n```\n\nReturns a serialized config with information for compiling the model.\nThis method returns a config dictionary containing all the information (optimizer, loss, metrics, etc.) with which the model was compiled.\nReturns  \n---  \nA dict containing information for compiling the model.   \n### `get_layer`\n```\nget_layer(\n    name=None, index=None\n)\n\n```\n\nRetrieves a layer based on either its name (unique) or index.\nIf `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\nArgs  \n---  \n`name` |  String, name of layer.   \n`index` |  Integer, index of layer.   \nReturns  \n---  \nA layer instance.   \n### `get_metrics_result`\n```\nget_metrics_result()\n\n```\n\nReturns the model's metrics values as a dict.\nIf any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.\nReturns  \n---  \nA `dict` containing values of the metrics listed in `self.metrics`.   \n`Example` |  `{'loss': 0.2, 'accuracy': 0.7}`.   \n### `load_weights`\n```\nload_weights(\n    filepath, skip_mismatch=False, **kwargs\n)\n\n```\n\nLoad weights from a file saved via `save_weights()`.\nWeights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n**Partial weight loading**\nIf you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting `skip_mismatch=True`. In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.\nArgs  \n---  \n`filepath` |  String, path to the weights file to load. It can either be a `.weights.h5` file or a legacy `.h5` weights file.   \n`skip_mismatch` |  Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weights.   \n### `loss`\n```\nloss(\n    y, y_pred, sample_weight=None\n)\n\n```\n\n### `make_predict_function`\n```\nmake_predict_function(\n    force=False\n)\n\n```\n\n### `make_test_function`\n```\nmake_test_function(\n    force=False\n)\n\n```\n\n### `make_train_function`\n```\nmake_train_function(\n    force=False\n)\n\n```\n\n### `predict`\n```\npredict(\n    x, batch_size=None, verbose='auto', steps=None, callbacks=None\n)\n\n```\n\nGenerates output predictions for the input samples.\nComputation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.\nFor small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `BatchNormalization` that behave differently during inference.\nArgs  \n---  \nInput samples. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance. \n\n  \n`batch_size` |  Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\"auto\"` becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). Defaults to `\"auto\"`.   \n`steps` |  Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) and `steps` is `None`, `predict()` will run until the input dataset is exhausted.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during prediction.   \nReturns  \n---  \nNumPy array(s) of predictions.   \n### `predict_on_batch`\n```\npredict_on_batch(\n    x\n)\n\n```\n\nReturns predictions for a single batch of samples.\nArgs  \n---  \nInput data. It must be array-like.   \nReturns  \n---  \nNumPy array(s) of predictions.   \n### `predict_step`\n```\npredict_step(\n    data\n)\n\n```\n\n### `reset_metrics`\n```\nreset_metrics()\n\n```\n\n### `save`\n```\nsave(\n    filepath, overwrite=True, **kwargs\n)\n\n```\n\nSaves a model as a `.keras` file.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the model. Must end in `.keras`.   \n`overwrite` |  Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.   \n`save_format` |  The `save_format` argument is deprecated in Keras 3. Format to use, as a string. Only the `\"keras\"` format is supported at this time.   \n#### Example:\n```\nmodel = keras.Sequential(\n    [\n        keras.layers.Dense(5, input_shape=(3,)),\n        keras.layers.Softmax(),\n    ],\n)\nmodel.save(\"model.keras\")\nloaded_model = keras.saving.load_model(\"model.keras\")\nx = keras.random.uniform((10, 3))\nassert np.allclose(model.predict(x), loaded_model.predict(x))\n\n```\n\nNote that `model.save()` is an alias for `keras.saving.save_model()`.\nThe saved `.keras` file contains:\n  * The model's configuration (architecture)\n  * The model's weights\n  * The model's optimizer's state (if any)\n\n\nThus models can be reinstantiated in the exact same state.\n### `save_weights`\n```\nsave_weights(\n    filepath, overwrite=True\n)\n\n```\n\nSaves all layer weights to a `.weights.h5` file.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the model. Must end in `.weights.h5`.   \n`overwrite` |  Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.   \n### `stateless_compute_loss`\n```\nstateless_compute_loss(\n    trainable_variables,\n    non_trainable_variables,\n    metrics_variables,\n    x=None,\n    y=None,\n    y_pred=None,\n    sample_weight=None\n)\n\n```\n\n### `summary`\n```\nsummary(\n    line_length=None,\n    positions=None,\n    print_fn=None,\n    expand_nested=False,\n    show_trainable=False,\n    layer_range=None\n)\n\n```\n\nPrints a string summary of the network.\nArgs  \n---  \n`line_length` |  Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).   \n`positions` |  Relative or absolute positions of log elements in each line. If not provided, becomes `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.   \n`print_fn` |  Print function to use. By default, prints to `stdout`. If `stdout` doesn't work in your environment, change to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.   \n`expand_nested` |  Whether to expand the nested models. Defaults to `False`.   \n`show_trainable` |  Whether to show if a layer is trainable. Defaults to `False`.   \n`layer_range` |  a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model.   \nRaises  \n---  \n`ValueError` |  if `summary()` is called before the model is built.   \n### `symbolic_call`\n```\nsymbolic_call(\n    *args, **kwargs\n)\n\n```\n\n### `test_on_batch`\n```\ntest_on_batch(\n    x, y=None, sample_weight=None, return_dict=False\n)\n\n```\n\nTest the model on a single batch of samples.\nArgs  \n---  \nInput data. Must be array-like.   \nTarget data. Must be array-like.   \n`sample_weight` |  Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nA scalar loss value (when no metrics and `return_dict=False`), a list of loss and metric values (if there are metrics and `return_dict=False`), or a dict of metric and loss values (if `return_dict=True`).   \n### `test_step`\n```\ntest_step(\n    data\n)\n\n```\n\n### `to_json`\n```\nto_json(\n    **kwargs\n)\n\n```\n\nReturns a JSON string containing the network configuration.\nTo load a network from a JSON save file, use [`keras.models.model_from_json(json_string, custom_objects={...})`](https://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_json).\nArgs  \n---  \n`**kwargs` |  Additional keyword arguments to be passed to `json.dumps()`.   \nReturns  \n---  \nA JSON string.   \n### `train_on_batch`\n```\ntrain_on_batch(\n    x, y=None, sample_weight=None, class_weight=None, return_dict=False\n)\n\n```\n\nRuns a single gradient update on a single batch of data.\nArgs  \n---  \nInput data. Must be array-like.   \nTarget data. Must be array-like.   \n`sample_weight` |  Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample.   \n`class_weight` |  Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. When `class_weight` is specified and targets have a rank of 2 or greater, either `y` must be one-hot encoded, or an explicit final dimension of 1 must be included for sparse class labels.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nA scalar loss value (when no metrics and `return_dict=False`), a list of loss and metric values (if there are metrics and `return_dict=False`), or a dict of metric and loss values (if `return_dict=True`).   \n### `train_step`\n```\ntrain_step(\n    data\n)\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks": "DO NOT EDIT.\nThis file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.\n## Classes\n[`class BackupAndRestore`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/BackupAndRestore): Callback to back up and restore the training state.\n[`class CSVLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger): Callback that streams epoch results to a CSV file.\n[`class Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback): Base class used to build new callbacks.\n[`class CallbackList`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CallbackList): Container abstracting a list of callbacks.\n[`class EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping): Stop training when a monitored metric has stopped improving.\n[`class History`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History): Callback that records events into a `History` object.\n[`class LambdaCallback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback): Callback for creating simple, custom callbacks on-the-fly.\n[`class LearningRateScheduler`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler): Learning rate scheduler.\n[`class ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint): Callback to save the Keras model or model weights at some frequency.\n[`class ProgbarLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ProgbarLogger): Callback that prints metrics to stdout.\n[`class ReduceLROnPlateau`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau): Reduce learning rate when a metric has stopped improving.\n[`class RemoteMonitor`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/RemoteMonitor): Callback used to stream events to a server.\n[`class SwapEMAWeights`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/SwapEMAWeights): Swaps model weights and EMA weights before and after evaluation.\n[`class TensorBoard`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard): Enable visualizations for TensorBoard.\n[`class TerminateOnNaN`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TerminateOnNaN): Callback that terminates training when a NaN loss is encountered.\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/Layer": "This is the class from which all layers inherit.\nInherits From: [`Operation`](https://www.tensorflow.org/api_docs/python/tf/keras/Operation)\nView aliases\n**Main aliases**\n[`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.keras.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer)\n```\ntf.keras.Layer(\n    *,\n    activity_regularizer=None,\n    trainable=True,\n    dtype=None,\n    autocast=True,\n    name=None,\n    **kwargs\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Use TF1.x models in TF2 workflows](https://www.tensorflow.org/guide/migrate/model_mapping)\n  * [Validating correctness & numerical equivalence](https://www.tensorflow.org/guide/migrate/validate_correctness)\n  * [Introduction to modules, layers, and models](https://www.tensorflow.org/guide/intro_to_modules)\n  * [Debug a TensorFlow 2 migrated training pipeline](https://www.tensorflow.org/guide/migrate/migration_debugging)\n\n| \n  * [Scalable model compression](https://www.tensorflow.org/tutorials/optimization/compression)\n  * [Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n\n  \nA layer is a callable object that takes as input one or more tensors and that outputs one or more tensors. It involves _computation_ , defined in the `call()` method, and a _state_ (weight variables). State can be created:\n  * in `__init__()`, for instance via `self.add_weight()`;\n  * in the optional `build()` method, which is invoked by the first `__call__()` to the layer, and supplies the shape(s) of the input(s), which may not have been known at initialization time.\n\n\nLayers are recursively composable: If you assign a Layer instance as an attribute of another Layer, the outer layer will start tracking the weights created by the inner layer. Nested layers should be instantiated in the `__init__()` method or `build()` method.\nUsers will just instantiate a layer and then treat it as a callable.\n## Args  \n---  \n`trainable` |  Boolean, whether the layer's variables should be trainable.   \n`name` |  String name of the layer.   \n`dtype` |  The dtype of the layer's computations and weights. Can also be a [`keras.DTypePolicy`](https://www.tensorflow.org/api_docs/python/tf/keras/DTypePolicy), which allows the computation and weight dtype to differ. Defaults to `None`. `None` means to use [`keras.config.dtype_policy()`](https://www.tensorflow.org/api_docs/python/tf/keras/config/dtype_policy), which is a `float32` policy unless set to different value (via [`keras.config.set_dtype_policy()`](https://www.tensorflow.org/api_docs/python/tf/keras/config/set_dtype_policy)).   \nWe recommend that descendants of `Layer` implement the following methods:\n  * `__init__()`: Defines custom layer attributes, and creates layer weights that do not depend on input shapes, using `add_weight()`, or other state.\n  * `build(self, input_shape)`: This method can be used to create weights that depend on the shape(s) of the input(s), using `add_weight()`, or other state. `__call__()` will automatically build the layer (if it has not been built yet) by calling `build()`.\n  * `call(self, *args, **kwargs)`: Called in `__call__` after making sure `build()` has been called. `call()` performs the logic of applying the layer to the input arguments. Two reserved keyword arguments you can optionally use in `call()` are: 1. `training` (boolean, whether the call is in inference mode or training mode). 2. `mask` (boolean tensor encoding masked timesteps in the input, used e.g. in RNN layers). A typical signature for this method is `call(self, inputs)`, and user could optionally add `training` and `mask` if the layer need them.\n  * `get_config(self)`: Returns a dictionary containing the configuration used to initialize this layer. If the keys differ from the arguments in `__init__()`, then override `from_config(self)` as well. This method is used when saving the layer or a model that contains this layer.\n\n\n#### Examples:\nHere's a basic example: a layer with two variables, `w` and `b`, that returns `y = w . x + b`. It shows how to implement `build()` and `call()`. Variables set as attributes of a layer are tracked as weights of the layers (in `layer.weights`).\n```\nclassSimpleDense(Layer):\n    def__init__(self, units=32):\n        super().__init__()\n        self.units = units\n\n    # Create the state of the layer (weights)\n    defbuild(self, input_shape):\n        self.kernel = self.add_weight(\n            shape=(input_shape[-1], self.units),\n            initializer=\"glorot_uniform\",\n            trainable=True,\n            name=\"kernel\",\n        )\n        self.bias = self.add_weight(\n            shape=(self.units,),\n            initializer=\"zeros\",\n            trainable=True,\n            name=\"bias\",\n        )\n\n    # Defines the computation\n    defcall(self, inputs):\n        return ops.matmul(inputs, self.kernel) + self.bias\n\n# Instantiates the layer.\nlinear_layer = SimpleDense(4)\n\n# This will also call `build(input_shape)` and create the weights.\ny = linear_layer(ops.ones((2, 2)))\nassert len(linear_layer.weights) == 2\n\n# These weights are trainable, so they're listed in `trainable_weights`:\nassert len(linear_layer.trainable_weights) == 2\n\n```\n\nBesides trainable weights, updated via backpropagation during training, layers can also have non-trainable weights. These weights are meant to be updated manually during `call()`. Here's a example layer that computes the running sum of its inputs:\n```\nclassComputeSum(Layer):\n\n  def__init__(self, input_dim):\n      super(ComputeSum, self).__init__()\n      # Create a non-trainable weight.\n      self.total = self.add_weight(\n        shape=(),\n        initializer=\"zeros\",\n        trainable=False,\n        name=\"total\",\n      )\n\n  defcall(self, inputs):\n      self.total.assign(self.total + ops.sum(inputs))\n      return self.total\n\nmy_sum = ComputeSum(2)\nx = ops.ones((2, 2))\ny = my_sum(x)\n\nassert my_sum.weights == [my_sum.total]\nassert my_sum.non_trainable_weights == [my_sum.total]\nassert my_sum.trainable_weights == []\n\n```\n\n## Attributes  \n---  \n`name` |  The name of the layer (string).   \n`dtype` |  Dtype of the layer's weights. Alias of `layer.variable_dtype`.   \n`variable_dtype` |  Dtype of the layer's weights.   \n`compute_dtype` |  The dtype of the layer's computations. Layers automatically cast inputs to this dtype, which causes the computations and output to also be in this dtype. When mixed precision is used with a [`keras.DTypePolicy`](https://www.tensorflow.org/api_docs/python/tf/keras/DTypePolicy), this will be different than `variable_dtype`.   \n`trainable_weights` |  List of variables to be included in backprop.   \n`non_trainable_weights` |  List of variables that should not be included in backprop.   \n`weights` |  The concatenation of the lists trainable_weights and non_trainable_weights (in this order).   \n`trainable` |  Whether the layer should be trained (boolean), i.e. whether its potentially-trainable weights should be returned as part of `layer.trainable_weights`.   \n`input_spec` |  Optional (list of) `InputSpec` object(s) specifying the constraints on inputs that can be accepted by the layer.   \n`dtype_policy`  \n`input` |  Retrieves the input tensor(s) of a symbolic operation.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`input_dtype` |  The dtype layer inputs should be converted to.   \n`losses` |  List of scalar losses from `add_loss`, regularizers and sublayers.   \n`metrics` |  List of all metrics.   \n`metrics_variables` |  List of all metric variables.   \n`non_trainable_variables` |  List of all non-trainable layer state.This extends `layer.non_trainable_weights` to include all state used by the layer including state for metrics and `SeedGenerator`s.   \n`output` |  Retrieves the output tensor(s) of a layer.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`supports_masking` |  Whether this layer supports computing a mask using `compute_mask`.   \n`trainable_variables` |  List of all trainable layer state.This is equivalent to `layer.trainable_weights`.   \n`variables` |  List of all layer state, including random seeds.This extends `layer.weights` to include all state used by the layer including `SeedGenerator`s. Note that metrics variables are not included here, use `metrics_variables` to visit all the metric variables.   \n## Methods\n### `add_loss`\n```\nadd_loss(\n    loss\n)\n\n```\n\nCan be called inside of the `call()` method to add a scalar loss.\n#### Example:\n```\nclassMyLayer(Layer):\n    ...\n    defcall(self, x):\n        self.add_loss(ops.sum(x))\n        return x\n\n```\n\n### `add_metric`\n```\nadd_metric()\n\n```\n\n### `add_variable`\n```\nadd_variable(\n    shape,\n    initializer,\n    dtype=None,\n    trainable=True,\n    autocast=True,\n    regularizer=None,\n    constraint=None,\n    name=None\n)\n\n```\n\nAdd a weight variable to the layer.\nAlias of `add_weight()`.\n### `add_weight`\n```\nadd_weight(\n    shape=None,\n    initializer=None,\n    dtype=None,\n    trainable=True,\n    autocast=True,\n    regularizer=None,\n    constraint=None,\n    aggregation='mean',\n    name=None\n)\n\n```\n\nAdd a weight variable to the layer.\nArgs  \n---  \n`shape` |  Shape tuple for the variable. Must be fully-defined (no `None` entries). Defaults to `()` (scalar) if unspecified.   \n`initializer` |  Initializer object to use to populate the initial variable value, or string name of a built-in initializer (e.g. `\"random_normal\"`). If unspecified, defaults to `\"glorot_uniform\"` for floating-point variables and to `\"zeros\"` for all other types (e.g. int, bool).   \n`dtype` |  Dtype of the variable to create, e.g. `\"float32\"`. If unspecified, defaults to the layer's variable dtype (which itself defaults to `\"float32\"` if unspecified).   \n`trainable` |  Boolean, whether the variable should be trainable via backprop or whether its updates are managed manually. Defaults to `True`.   \n`autocast` |  Boolean, whether to autocast layers variables when accessing them. Defaults to `True`.   \n`regularizer` |  Regularizer object to call to apply penalty on the weight. These penalties are summed into the loss function during optimization. Defaults to `None`.   \n`constraint` |  Contrainst object to call on the variable after any optimizer update, or string name of a built-in constraint. Defaults to `None`.   \n`aggregation` |  String, one of `'mean'`, `'sum'`, `'only_first_replica'`. Annotates the variable with the type of multi-replica aggregation to be used for this variable when writing custom data parallel training loops.   \n`name` |  String name of the variable. Useful for debugging purposes.   \n### `build`\n```\nbuild(\n    input_shape\n)\n\n```\n\n### `build_from_config`\n```\nbuild_from_config(\n    config\n)\n\n```\n\nBuilds the layer's states with the supplied config dict.\nBy default, this method calls the `build(config[\"input_shape\"])` method, which creates weights based on the layer's input shape in the supplied config. If your config contains other information needed to load the layer's state, you should override this method.\nArgs  \n---  \n`config` |  Dict containing the input shape associated with this layer.   \n### `call`\n```\ncall(\n    *args, **kwargs\n)\n\n```\n\n### `compute_mask`\n```\ncompute_mask(\n    inputs, previous_mask\n)\n\n```\n\n### `compute_output_shape`\n```\ncompute_output_shape(\n    *args, **kwargs\n)\n\n```\n\n### `compute_output_spec`\n```\ncompute_output_spec(\n    *args, **kwargs\n)\n\n```\n\n### `count_params`\n```\ncount_params()\n\n```\n\nCount the total number of scalars composing the weights.\nReturns  \n---  \nAn integer count.   \n### `from_config`\n```\n@classmethod\nfrom_config(\n    config\n)\n\n```\n\nCreates a layer from its config.\nThis method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`).\nArgs  \n---  \n`config` |  A Python dictionary, typically the output of get_config.   \nReturns  \n---  \nA layer instance.   \n### `get_build_config`\n```\nget_build_config()\n\n```\n\nReturns a dictionary with the layer's input shape.\nThis method returns a config dict that can be used by `build_from_config(config)` to create all states (e.g. Variables and Lookup tables) needed by the layer.\nBy default, the config only contains the input shape that the layer was built with. If you're writing a custom layer that creates state in an unusual way, you should override this method to make sure this state is already created when Keras attempts to load its value upon model loading.\nReturns  \n---  \nA dict containing the input shape associated with the layer.   \n### `get_config`\n```\nget_config()\n\n```\n\nReturns the config of the object.\nAn object config is a Python dictionary (serializable) containing the information needed to re-instantiate it.\n### `get_weights`\n```\nget_weights()\n\n```\n\nReturn the values of `layer.weights` as a list of NumPy arrays.\n### `load_own_variables`\n```\nload_own_variables(\n    store\n)\n\n```\n\nLoads the state of the layer.\nYou can override this method to take full control of how the state of the layer is loaded upon calling [`keras.models.load_model()`](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model).\nArgs  \n---  \n`store` |  Dict from which the state of the model will be loaded.   \n### `quantize`\n```\nquantize(\n    mode\n)\n\n```\n\n### `quantized_call`\n```\nquantized_call(\n    *args, **kwargs\n)\n\n```\n\n### `save_own_variables`\n```\nsave_own_variables(\n    store\n)\n\n```\n\nSaves the state of the layer.\nYou can override this method to take full control of how the state of the layer is saved upon calling `model.save()`.\nArgs  \n---  \n`store` |  Dict where the state of the model will be saved.   \n### `set_weights`\n```\nset_weights(\n    weights\n)\n\n```\n\nSets the values of `layer.weights` from a list of NumPy arrays.\n### `stateless_call`\n```\nstateless_call(\n    trainable_variables,\n    non_trainable_variables,\n    *args,\n    return_losses=False,\n    **kwargs\n)\n\n```\n\nCall the layer without any side effects.\nArgs  \n---  \n`trainable_variables` |  List of trainable variables of the model.   \n`non_trainable_variables` |  List of non-trainable variables of the model.   \n`*args` |  Positional arguments to be passed to `call()`.   \n`return_losses` |  If `True`, `stateless_call()` will return the list of losses created during `call()` as part of its return values.   \n`**kwargs` |  Keyword arguments to be passed to `call()`.   \nReturns  \n---  \nA tuple. By default, returns `(outputs, non_trainable_variables)`. If `return_losses = True`, then returns `(outputs, non_trainable_variables, losses)`.   \n#### Example:\n```\nmodel = ...\ndata = ...\ntrainable_variables = model.trainable_variables\nnon_trainable_variables = model.non_trainable_variables\n# Call the model with zero side effects\noutputs, non_trainable_variables = model.stateless_call(\n    trainable_variables,\n    non_trainable_variables,\n    data,\n)\n# Attach the updated state to the model\n# (until you do this, the model is still in its pre-call state).\nfor ref_var, value in zip(\n    model.non_trainable_variables, non_trainable_variables\n):\n    ref_var.assign(value)\n\n```\n\n### `symbolic_call`\n```\nsymbolic_call(\n    *args, **kwargs\n)\n\n```\n\n### `__call__`\n```\n__call__(\n    *args, **kwargs\n)\n\n```\n\nCall self as a function.\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard": "Enable visualizations for TensorBoard.\nInherits From: [`Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback)\n```\ntf.keras.callbacks.TensorBoard(\n    log_dir='logs',\n    histogram_freq=0,\n    write_graph=True,\n    write_images=False,\n    write_steps_per_second=False,\n    update_freq='epoch',\n    profile_batch=0,\n    embeddings_freq=0,\n    embeddings_metadata=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Migrate TensorBoard: TensorFlow's visualization toolkit](https://www.tensorflow.org/guide/migrate/tensorboard)\n\n| \n  * [Distributed training with Keras](https://www.tensorflow.org/tutorials/distribute/keras)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  * [Transfer learning with TensorFlow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n  * [Overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit)\n  * [TensorBoard Scalars: Logging training metrics in Keras](https://www.tensorflow.org/tensorboard/scalars_and_keras)\n\n  \nTensorBoard is a visualization tool provided with TensorFlow. A TensorFlow installation is required to use this callback.\nThis callback logs events for TensorBoard, including:\n  * Metrics summary plots\n  * Training graph visualization\n  * Weight histograms\n  * Sampled profiling\n\n\nWhen used in `model.evaluate()` or regular validation in addition to epoch summaries, there will be a summary that records evaluation metrics vs `model.optimizer.iterations` written. The metric names will be prepended with `evaluation`, with `model.optimizer.iterations` being the step in the visualized TensorBoard.\nIf you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n```\ntensorboard --logdir=path_to_your_logs\n\n```\n\nYou can find more information about TensorBoard [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n## Args  \n---  \n`log_dir` |  the path of the directory where to save the log files to be parsed by TensorBoard. e.g., `log_dir = os.path.join(working_dir, 'logs')`. This directory should not be reused by any other callbacks.   \n`histogram_freq` |  frequency (in epochs) at which to compute weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.   \n`write_graph` |  (Not supported at this time) Whether to visualize the graph in TensorBoard. Note that the log file can become quite large when `write_graph` is set to `True`.   \n`write_images` |  whether to write model weights to visualize as image in TensorBoard.   \n`write_steps_per_second` |  whether to log the training steps per second into TensorBoard. This supports both epoch and batch frequency logging.   \n`update_freq` |  `\"batch\"` or `\"epoch\"` or integer. When using `\"epoch\"`, writes the losses and metrics to TensorBoard after every epoch. If using an integer, let's say `1000`, all metrics and losses (including custom ones added by [`Model.compile`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile)) will be logged to TensorBoard every 1000 batches. `\"batch\"` is a synonym for 1, meaning that they will be written every batch. Note however that writing too frequently to TensorBoard can slow down your training, especially when used with distribution strategies as it will incur additional synchronization overhead. Batch-level summary writing is also available via `train_step` override. Please see [TensorBoard Scalars tutorial](https://www.tensorflow.org/tensorboard/scalars_and_keras#batch-level_logging) # noqa: E501 for more details.   \n`profile_batch` |  (Not supported at this time) Profile the batch(es) to sample compute characteristics. profile_batch must be a non-negative integer or a tuple of integers. A pair of positive integers signify a range of batches to profile. By default, profiling is disabled.   \n`embeddings_freq` |  frequency (in epochs) at which embedding layers will be visualized. If set to 0, embeddings won't be visualized.   \n`embeddings_metadata` |  Dictionary which maps embedding layer names to the filename of a file in which to save metadata for the embedding layer. In case the same metadata file is to be used for all embedding layers, a single filename can be passed.   \n#### Examples:\n```\ntensorboard_callback = keras.callbacks.TensorBoard(log_dir=\"./logs\")\nmodel.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n# Then run the tensorboard command to view the visualizations.\n\n```\n\nCustom batch-level summaries in a subclassed Model:\n```\nclassMyModel(keras.Model):\n\n    defbuild(self, _):\n        self.dense = keras.layers.Dense(10)\n\n    defcall(self, x):\n        outputs = self.dense(x)\n        tf.summary.histogram('outputs', outputs)\n        return outputs\n\nmodel = MyModel()\nmodel.compile('sgd', 'mse')\n\n# Make sure to set `update_freq=N` to log a batch-level summary every N\n# batches.  In addition to any `tf.summary` contained in `model.call()`,\n# metrics added in `Model.compile` will be logged every N batches.\ntb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)\nmodel.fit(x_train, y_train, callbacks=[tb_callback])\n\n```\n\nCustom batch-level summaries in a Functional API Model:\n```\ndefmy_summary(x):\n    tf.summary.histogram('x', x)\n    return x\n\ninputs = keras.Input(10)\nx = keras.layers.Dense(10)(inputs)\noutputs = keras.layers.Lambda(my_summary)(x)\nmodel = keras.Model(inputs, outputs)\nmodel.compile('sgd', 'mse')\n\n# Make sure to set `update_freq=N` to log a batch-level summary every N\n# batches. In addition to any `tf.summary` contained in `Model.call`,\n# metrics added in `Model.compile` will be logged every N batches.\ntb_callback = keras.callbacks.TensorBoard('./logs', update_freq=1)\nmodel.fit(x_train, y_train, callbacks=[tb_callback])\n\n```\n\n#### Profiling:\n```\n# Profile a single batch, e.g. the 5th batch.\ntensorboard_callback = keras.callbacks.TensorBoard(\n    log_dir='./logs', profile_batch=5)\nmodel.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n\n# Profile a range of batches, e.g. from 10 to 20.\ntensorboard_callback = keras.callbacks.TensorBoard(\n    log_dir='./logs', profile_batch=(10,20))\nmodel.fit(x_train, y_train, epochs=2, callbacks=[tensorboard_callback])\n\n```\n\n## Attributes  \n---  \n`model`  \n`summary`  \n## Methods\n### `on_batch_begin`\n```\non_batch_begin(\n    batch, logs=None\n)\n\n```\n\nA backwards compatibility alias for `on_train_batch_begin`.\n### `on_batch_end`\n```\non_batch_end(\n    batch, logs=None\n)\n\n```\n\nA backwards compatibility alias for `on_train_batch_end`.\n### `on_epoch_begin`\n```\non_epoch_begin(\n    epoch, logs=None\n)\n\n```\n\nCalled at the start of an epoch.\nSubclasses should override for any actions to run. This function should only be called during TRAIN mode.\nArgs  \n---  \n`epoch` |  Integer, index of epoch.   \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_epoch_end`\n```\non_epoch_end(\n    epoch, logs=None\n)\n\n```\n\nRuns metrics and histogram summaries at epoch end.\n### `on_predict_batch_begin`\n```\non_predict_batch_begin(\n    batch, logs=None\n)\n\n```\n\nCalled at the beginning of a batch in `predict` methods.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_predict_batch_end`\n```\non_predict_batch_end(\n    batch, logs=None\n)\n\n```\n\nCalled at the end of a batch in `predict` methods.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Aggregated metric results up until this batch.   \n### `on_predict_begin`\n```\non_predict_begin(\n    logs=None\n)\n\n```\n\nCalled at the beginning of prediction.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_predict_end`\n```\non_predict_end(\n    logs=None\n)\n\n```\n\nCalled at the end of prediction.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_test_batch_begin`\n```\non_test_batch_begin(\n    batch, logs=None\n)\n\n```\n\nCalled at the beginning of a batch in `evaluate` methods.\nAlso called at the beginning of a validation batch in the `fit` methods, if validation data is provided.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_test_batch_end`\n```\non_test_batch_end(\n    batch, logs=None\n)\n\n```\n\nCalled at the end of a batch in `evaluate` methods.\nAlso called at the end of a validation batch in the `fit` methods, if validation data is provided.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Aggregated metric results up until this batch.   \n### `on_test_begin`\n```\non_test_begin(\n    logs=None\n)\n\n```\n\nCalled at the beginning of evaluation or validation.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_test_end`\n```\non_test_end(\n    logs=None\n)\n\n```\n\nCalled at the end of evaluation or validation.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently the output of the last call to `on_test_batch_end()` is passed to this argument for this method but that may change in the future.   \n### `on_train_batch_begin`\n```\non_train_batch_begin(\n    batch, logs=None\n)\n\n```\n\nCalled at the beginning of a training batch in `fit` methods.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_train_batch_end`\n```\non_train_batch_end(\n    batch, logs=None\n)\n\n```\n\nCalled at the end of a training batch in `fit` methods.\nSubclasses should override for any actions to run.\nNote that if the `steps_per_execution` argument to `compile` in `Model` is set to `N`, this method will only be called every `N` batches.\nArgs  \n---  \n`batch` |  Integer, index of batch within the current epoch.   \n`logs` |  Dict. Aggregated metric results up until this batch.   \n### `on_train_begin`\n```\non_train_begin(\n    logs=None\n)\n\n```\n\nCalled at the beginning of training.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently no data is passed to this argument for this method but that may change in the future.   \n### `on_train_end`\n```\non_train_end(\n    logs=None\n)\n\n```\n\nCalled at the end of training.\nSubclasses should override for any actions to run.\nArgs  \n---  \n`logs` |  Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future.   \n### `set_model`\n```\nset_model(\n    model\n)\n\n```\n\nSets Keras model and writes graph if specified.\n### `set_params`\n```\nset_params(\n    params\n)\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/Metric": "Encapsulates metric logic and state.\nView aliases\n**Main aliases**\n[`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.keras.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric)\n```\ntf.keras.Metric(\n    dtype=None, name=None\n)\n\n```\n\n## Args  \n---  \n`name` |  (Optional) string name of the metric instance.   \n`dtype` |  (Optional) data type of the metric result.   \n#### Example:\n```\nm = SomeMetric(...)\nfor input in ...:\n    m.update_state(input)\nprint('Final result: ', m.result())\n\n```\n\nUsage with `compile()` API:\n```\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(0.01),\n              loss=keras.losses.CategoricalCrossentropy(),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n\ndata = np.random.random((1000, 32))\nlabels = np.random.random((1000, 10))\n\nmodel.fit(data, labels, epochs=10)\n\n```\n\nTo be implemented by subclasses:\n  * `__init__()`: All state variables should be created in this method by calling `self.add_variable()` like: `self.var = self.add_variable(...)`\n  * `update_state()`: Has all updates to the state variables like: `self.var.assign(...)`.\n  * `result()`: Computes and returns a scalar value or a dict of scalar values for the metric from the state variables.\n\n\nExample subclass implementation:\n```\nclassBinaryTruePositives(Metric):\n\n    def__init__(self, name='binary_true_positives', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.true_positives = self.add_variable(\n            shape=(),\n            initializer='zeros',\n            name='true_positives'\n        )\n\n    defupdate_state(self, y_true, y_pred, sample_weight=None):\n        y_true = ops.cast(y_true, \"bool\")\n        y_pred = ops.cast(y_pred, \"bool\")\n\n        values = ops.logical_and(\n            ops.equal(y_true, True), ops.equal(y_pred, True))\n        values = ops.cast(values, self.dtype)\n        if sample_weight is not None:\n            sample_weight = ops.cast(sample_weight, self.dtype)\n            sample_weight = ops.broadcast_to(\n                sample_weight, ops.shape(values)\n            )\n            values = ops.multiply(values, sample_weight)\n        self.true_positives.assign(self.true_positives + ops.sum(values))\n\n    defresult(self):\n        return self.true_positives\n\n```\n\n## Attributes  \n---  \n`dtype`  \n`variables`  \n## Methods\n### `add_variable`\n```\nadd_variable(\n    shape, initializer, dtype=None, aggregation='sum', name=None\n)\n\n```\n\n### `add_weight`\n```\nadd_weight(\n    shape=(), initializer=None, dtype=None, name=None\n)\n\n```\n\n### `from_config`\n```\n@classmethod\nfrom_config(\n    config\n)\n\n```\n\n### `get_config`\n```\nget_config()\n\n```\n\nReturn the serializable config of the metric.\n### `reset_state`\n```\nreset_state()\n\n```\n\nReset all of the metric state variables.\nThis function is called between epochs/steps, when a metric is evaluated during training.\n### `result`\n```\nresult()\n\n```\n\nCompute the current metric value.\nReturns  \n---  \nA scalar tensor, or a dictionary of scalar tensors.   \n### `stateless_reset_state`\n```\nstateless_reset_state()\n\n```\n\n### `stateless_result`\n```\nstateless_result(\n    metric_variables\n)\n\n```\n\n### `stateless_update_state`\n```\nstateless_update_state(\n    metric_variables, *args, **kwargs\n)\n\n```\n\n### `update_state`\n```\nupdate_state(\n    *args, **kwargs\n)\n\n```\n\nAccumulate statistics for the metric.\n### `__call__`\n```\n__call__(\n    *args, **kwargs\n)\n\n```\n\nCall self as a function.\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/metrics": "DO NOT EDIT.\nThis file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.\n## Classes\n[`class AUC`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC): Approximates the AUC (Area under the curve) of the ROC or PR curves.\n[`class Accuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy): Calculates how often predictions equal labels.\n[`class BinaryAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy): Calculates how often predictions match binary labels.\n[`class BinaryCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy): Computes the crossentropy metric between the labels and predictions.\n[`class BinaryIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryIoU): Computes the Intersection-Over-Union metric for class 0 and/or 1.\n[`class CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy): Calculates how often predictions match one-hot labels.\n[`class CategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalCrossentropy): Computes the crossentropy metric between the labels and predictions.\n[`class CategoricalHinge`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalHinge): Computes the categorical hinge metric between `y_true` and `y_pred`.\n[`class CosineSimilarity`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CosineSimilarity): Computes the cosine similarity between the labels and predictions.\n[`class F1Score`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/F1Score): Computes F-1 Score.\n[`class FBetaScore`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FBetaScore): Computes F-Beta score.\n[`class FalseNegatives`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalseNegatives): Calculates the number of false negatives.\n[`class FalsePositives`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/FalsePositives): Calculates the number of false positives.\n[`class Hinge`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Hinge): Computes the hinge metric between `y_true` and `y_pred`.\n[`class IoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU): Computes the Intersection-Over-Union metric for specific target classes.\n[`class KLDivergence`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/KLDivergence): Computes Kullback-Leibler divergence metric between `y_true` and `y_pred`.\n[`class LogCoshError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/LogCoshError): Computes the logarithm of the hyperbolic cosine of the prediction error.\n[`class Mean`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean): Compute the (weighted) mean of the given values.\n[`class MeanAbsoluteError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsoluteError): Computes the mean absolute error between the labels and predictions.\n[`class MeanAbsolutePercentageError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanAbsolutePercentageError): Computes mean absolute percentage error between `y_true` and `y_pred`.\n[`class MeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU): Computes the mean Intersection-Over-Union metric.\n[`class MeanMetricWrapper`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanMetricWrapper): Wrap a stateless metric function with the `Mean` metric.\n[`class MeanSquaredError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredError): Computes the mean squared error between `y_true` and `y_pred`.\n[`class MeanSquaredLogarithmicError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanSquaredLogarithmicError): Computes mean squared logarithmic error between `y_true` and `y_pred`.\n[`class Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric): Encapsulates metric logic and state.\n[`class OneHotIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/OneHotIoU): Computes the Intersection-Over-Union metric for one-hot encoded labels.\n[`class OneHotMeanIoU`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/OneHotMeanIoU): Computes mean Intersection-Over-Union metric for one-hot encoded labels.\n[`class Poisson`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Poisson): Computes the Poisson metric between `y_true` and `y_pred`.\n[`class Precision`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision): Computes the precision of the predictions with respect to the labels.\n[`class PrecisionAtRecall`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/PrecisionAtRecall): Computes best precision where recall is >= specified value.\n[`class R2Score`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/R2Score): Computes R2 score.\n[`class Recall`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall): Computes the recall of the predictions with respect to the labels.\n[`class RecallAtPrecision`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RecallAtPrecision): Computes best recall where precision is >= specified value.\n[`class RootMeanSquaredError`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError): Computes root mean squared error metric between `y_true` and `y_pred`.\n[`class SensitivityAtSpecificity`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SensitivityAtSpecificity): Computes best sensitivity where specificity is >= specified value.\n[`class SparseCategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy): Calculates how often predictions match integer labels.\n[`class SparseCategoricalCrossentropy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy): Computes the crossentropy metric between the labels and predictions.\n[`class SparseTopKCategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseTopKCategoricalAccuracy): Computes how often integer targets are in the top `K` predictions.\n[`class SpecificityAtSensitivity`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SpecificityAtSensitivity): Computes best specificity where sensitivity is >= specified value.\n[`class SquaredHinge`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SquaredHinge): Computes the hinge metric between `y_true` and `y_pred`.\n[`class Sum`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Sum): Compute the (weighted) sum of the given values.\n[`class TopKCategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TopKCategoricalAccuracy): Computes how often targets are in the top `K` predictions.\n[`class TrueNegatives`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TrueNegatives): Calculates the number of true negatives.\n[`class TruePositives`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/TruePositives): Calculates the number of true positives.\n## Functions\n[`KLD(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLD): Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n[`MAE(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAE): Computes the mean absolute error between labels and predictions.\n[`MAPE(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE): Computes the mean absolute percentage error between `y_true` & `y_pred`.\n[`MSE(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE): Computes the mean squared error between labels and predictions.\n[`MSLE(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSLE): Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n[`binary_accuracy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/binary_accuracy)\n[`binary_crossentropy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_crossentropy): Computes the binary crossentropy loss.\n[`binary_focal_crossentropy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/binary_focal_crossentropy): Computes the binary focal crossentropy loss.\n[`categorical_accuracy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/categorical_accuracy)\n[`categorical_crossentropy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_crossentropy): Computes the categorical crossentropy loss.\n[`categorical_focal_crossentropy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_focal_crossentropy): Computes the categorical focal crossentropy loss.\n[`categorical_hinge(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/categorical_hinge): Computes the categorical hinge loss between `y_true` & `y_pred`.\n[`deserialize(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/deserialize): Deserializes a serialized metric class/function instance.\n[`get(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/get): Retrieves a Keras metric as a `function`/`Metric` class instance.\n[`hinge(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/hinge): Computes the hinge loss between `y_true` & `y_pred`.\n[`huber(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/huber): Computes Huber loss value.\n[`kld(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLD): Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n[`kullback_leibler_divergence(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/KLD): Computes Kullback-Leibler divergence loss between `y_true` & `y_pred`.\n[`logcosh(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/logcosh): Logarithm of the hyperbolic cosine of the prediction error.\n[`mae(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAE): Computes the mean absolute error between labels and predictions.\n[`mape(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MAPE): Computes the mean absolute percentage error between `y_true` & `y_pred`.\n[`mse(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSE): Computes the mean squared error between labels and predictions.\n[`msle(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MSLE): Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n[`poisson(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/poisson): Computes the Poisson loss between y_true and y_pred.\n[`serialize(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/serialize): Serializes metric function or `Metric` instance.\n[`sparse_categorical_accuracy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_categorical_accuracy)\n[`sparse_categorical_crossentropy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy): Computes the sparse categorical crossentropy loss.\n[`sparse_top_k_categorical_accuracy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/sparse_top_k_categorical_accuracy)\n[`squared_hinge(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/squared_hinge): Computes the squared hinge loss between `y_true` & `y_pred`.\n[`top_k_categorical_accuracy(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/top_k_categorical_accuracy)\n",
  "https://www.tensorflow.org/api_docs/python/tf/parse_example": "\n",
  "https://www.tensorflow.org/api_docs/python/tf/nn/scale_regularization_loss": "Scales the sum of the given regularization losses by number of replicas.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.nn.scale_regularization_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/scale_regularization_loss)\n```\ntf.nn.scale_regularization_loss(\n    regularization_loss\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n| \n  * [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training)\n  * [Custom training loop with Keras and MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n\n  \nUsage with distribution strategy and custom training loop:\n```\nwith strategy.scope():\n  defcompute_loss(self, label, predictions):\n    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(\n        labels, predictions)\n\n    # Compute loss that is scaled by sample_weight and by global batch size.\n    loss = tf.nn.compute_average_loss(\n        per_example_loss,\n        sample_weight=sample_weight,\n        global_batch_size=GLOBAL_BATCH_SIZE)\n\n    # Add scaled regularization losses.\n    loss += tf.nn.scale_regularization_loss(tf.nn.l2_loss(weights))\n    return loss\n\n```\n\n## Args  \n---  \n`regularization_loss` |  Regularization loss.   \n## Returns  \n---  \nScalar loss value. \n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model": "Public API for tf._api.v2.saved_model namespace  \n## Modules\n[`experimental`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental) module: Public API for tf._api.v2.saved_model.experimental namespace\n## Classes\n[`class Asset`](https://www.tensorflow.org/api_docs/python/tf/saved_model/Asset): Represents a file asset to hermetically include in a SavedModel.\n[`class LoadOptions`](https://www.tensorflow.org/api_docs/python/tf/saved_model/LoadOptions): Options for loading a SavedModel.\n[`class SaveOptions`](https://www.tensorflow.org/api_docs/python/tf/saved_model/SaveOptions): Options for saving to SavedModel.\n## Functions\n[`contains_saved_model(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/contains_saved_model): Checks whether the provided export directory could contain a SavedModel.\n[`load(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load): Load a SavedModel from `export_dir`.\n[`save(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save): Exports a [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module) (and subclasses) `obj` to [SavedModel format](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk).\n## Other Members  \n---  \nASSETS_DIRECTORY  |  `'assets'`  \nASSETS_KEY  |  `'saved_model_assets'`  \nCLASSIFY_INPUTS  |  `'inputs'`  \nCLASSIFY_METHOD_NAME  |  `'tensorflow/serving/classify'`  \nCLASSIFY_OUTPUT_CLASSES  |  `'classes'`  \nCLASSIFY_OUTPUT_SCORES  |  `'scores'`  \nDEBUG_DIRECTORY  |  `'debug'`  \nDEBUG_INFO_FILENAME_PB  |  `'saved_model_debug_info.pb'`  \nDEFAULT_SERVING_SIGNATURE_DEF_KEY  |  `'serving_default'`  \nGPU  |  `'gpu'`  \nPREDICT_INPUTS  |  `'inputs'`  \nPREDICT_METHOD_NAME  |  `'tensorflow/serving/predict'`  \nPREDICT_OUTPUTS  |  `'outputs'`  \nREGRESS_INPUTS  |  `'inputs'`  \nREGRESS_METHOD_NAME  |  `'tensorflow/serving/regress'`  \nREGRESS_OUTPUTS  |  `'outputs'`  \nSAVED_MODEL_FILENAME_PB  |  `'saved_model.pb'`  \nSAVED_MODEL_FILENAME_PBTXT  |  `'saved_model.pbtxt'`  \nSAVED_MODEL_SCHEMA_VERSION   \nSERVING  |  `'serve'`  \nTPU  |  `'tpu'`  \nTRAINING  |  `'train'`  \nVARIABLES_DIRECTORY  |  `'variables'`  \nVARIABLES_FILENAME  |  `'variables'`\n",
  "https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss": "Scales per-example losses with sample_weights and computes their average.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.nn.compute_average_loss`](https://www.tensorflow.org/api_docs/python/tf/nn/compute_average_loss)\n```\ntf.nn.compute_average_loss(\n    per_example_loss, sample_weight=None, global_batch_size=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Distributed training with TensorFlow](https://www.tensorflow.org/guide/distributed_training)\n\n| \n  * [Custom training with tf.distribute.Strategy](https://www.tensorflow.org/tutorials/distribute/custom_training)\n  * [Custom training loop with Keras and MultiWorkerMirroredStrategy](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_ctl)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n\n  \nUsage with distribution strategy and custom training loop:\n```\nwith strategy.scope():\n  defcompute_loss(labels, predictions, sample_weight=None):\n\n    # If you are using a `Loss` class instead, set reduction to `NONE` so that\n    # we can do the reduction afterwards and divide by global batch size.\n    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(\n        labels, predictions)\n\n    # Compute loss that is scaled by sample_weight and by global batch size.\n    return tf.nn.compute_average_loss(\n        per_example_loss,\n        sample_weight=sample_weight,\n        global_batch_size=GLOBAL_BATCH_SIZE)\n\n```\n\n## Args  \n---  \n`per_example_loss` |  Per-example loss.   \n`sample_weight` |  Optional weighting for each example.   \n`global_batch_size` |  Optional global batch size value. Defaults to (size of first dimension of `losses`) * (number of replicas).   \n## Returns  \n---  \nScalar loss value, obtained by summing the `per_example_loss` and dividing by `global_batch_size`. If `global_batch_size` is zero, the result is zero. \n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/optimizers": "DO NOT EDIT.\nThis file was autogenerated. Do not edit it by hand, since your modifications would be overwritten.\n## Modules\n[`legacy`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/legacy) module: DO NOT EDIT.\n[`schedules`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules) module: DO NOT EDIT.\n## Classes\n[`class Adadelta`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adadelta): Optimizer that implements the Adadelta algorithm.\n[`class Adafactor`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adafactor): Optimizer that implements the Adafactor algorithm.\n[`class Adagrad`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adagrad): Optimizer that implements the Adagrad algorithm.\n[`class Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam): Optimizer that implements the Adam algorithm.\n[`class AdamW`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/AdamW): Optimizer that implements the AdamW algorithm.\n[`class Adamax`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adamax): Optimizer that implements the Adamax algorithm.\n[`class Ftrl`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Ftrl): Optimizer that implements the FTRL algorithm.\n[`class Lion`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Lion): Optimizer that implements the Lion algorithm.\n[`class LossScaleOptimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/mixed_precision/LossScaleOptimizer): An optimizer that dynamically scales the loss to prevent underflow.\n[`class Nadam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Nadam): Optimizer that implements the Nadam algorithm.\n[`class Optimizer`](https://www.tensorflow.org/api_docs/python/tf/keras/Optimizer): A class for Tensorflow specific optimizer logic.\n[`class RMSprop`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/RMSprop): Optimizer that implements the RMSprop algorithm.\n[`class SGD`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/SGD): Gradient descent (with momentum) optimizer.\n## Functions\n[`deserialize(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/deserialize): Returns a Keras optimizer object via its configuration.\n[`get(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/get): Retrieves a Keras Optimizer instance.\n[`serialize(...)`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/serialize): Returns the optimizer configuration as a Python dict.\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric": "Encapsulates metric logic and state.\nView aliases\n**Main aliases**\n[`tf.keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.keras.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric)\n```\ntf.keras.Metric(\n    dtype=None, name=None\n)\n\n```\n\n## Args  \n---  \n`name` |  (Optional) string name of the metric instance.   \n`dtype` |  (Optional) data type of the metric result.   \n#### Example:\n```\nm = SomeMetric(...)\nfor input in ...:\n    m.update_state(input)\nprint('Final result: ', m.result())\n\n```\n\nUsage with `compile()` API:\n```\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.Dense(10, activation='softmax'))\n\nmodel.compile(optimizer=keras.optimizers.RMSprop(0.01),\n              loss=keras.losses.CategoricalCrossentropy(),\n              metrics=[keras.metrics.CategoricalAccuracy()])\n\ndata = np.random.random((1000, 32))\nlabels = np.random.random((1000, 10))\n\nmodel.fit(data, labels, epochs=10)\n\n```\n\nTo be implemented by subclasses:\n  * `__init__()`: All state variables should be created in this method by calling `self.add_variable()` like: `self.var = self.add_variable(...)`\n  * `update_state()`: Has all updates to the state variables like: `self.var.assign(...)`.\n  * `result()`: Computes and returns a scalar value or a dict of scalar values for the metric from the state variables.\n\n\nExample subclass implementation:\n```\nclassBinaryTruePositives(Metric):\n\n    def__init__(self, name='binary_true_positives', **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.true_positives = self.add_variable(\n            shape=(),\n            initializer='zeros',\n            name='true_positives'\n        )\n\n    defupdate_state(self, y_true, y_pred, sample_weight=None):\n        y_true = ops.cast(y_true, \"bool\")\n        y_pred = ops.cast(y_pred, \"bool\")\n\n        values = ops.logical_and(\n            ops.equal(y_true, True), ops.equal(y_pred, True))\n        values = ops.cast(values, self.dtype)\n        if sample_weight is not None:\n            sample_weight = ops.cast(sample_weight, self.dtype)\n            sample_weight = ops.broadcast_to(\n                sample_weight, ops.shape(values)\n            )\n            values = ops.multiply(values, sample_weight)\n        self.true_positives.assign(self.true_positives + ops.sum(values))\n\n    defresult(self):\n        return self.true_positives\n\n```\n\n## Attributes  \n---  \n`dtype`  \n`variables`  \n## Methods\n### `add_variable`\n```\nadd_variable(\n    shape, initializer, dtype=None, aggregation='sum', name=None\n)\n\n```\n\n### `add_weight`\n```\nadd_weight(\n    shape=(), initializer=None, dtype=None, name=None\n)\n\n```\n\n### `from_config`\n```\n@classmethod\nfrom_config(\n    config\n)\n\n```\n\n### `get_config`\n```\nget_config()\n\n```\n\nReturn the serializable config of the metric.\n### `reset_state`\n```\nreset_state()\n\n```\n\nReset all of the metric state variables.\nThis function is called between epochs/steps, when a metric is evaluated during training.\n### `result`\n```\nresult()\n\n```\n\nCompute the current metric value.\nReturns  \n---  \nA scalar tensor, or a dictionary of scalar tensors.   \n### `stateless_reset_state`\n```\nstateless_reset_state()\n\n```\n\n### `stateless_result`\n```\nstateless_result(\n    metric_variables\n)\n\n```\n\n### `stateless_update_state`\n```\nstateless_update_state(\n    metric_variables, *args, **kwargs\n)\n\n```\n\n### `update_state`\n```\nupdate_state(\n    *args, **kwargs\n)\n\n```\n\nAccumulate statistics for the metric.\n### `__call__`\n```\n__call__(\n    *args, **kwargs\n)\n\n```\n\nCall self as a function.\n",
  "https://www.tensorflow.org/api_docs/python/tf/keras/models/Model": "A model grouping layers into an object with training/inference features.\nInherits From: [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer), [`Operation`](https://www.tensorflow.org/api_docs/python/tf/keras/Operation)\nView aliases\n**Main aliases**\n[`tf.keras.models.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n```\ntf.keras.Model(\n    *args, **kwargs\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Use TF1.x models in TF2 workflows](https://www.tensorflow.org/guide/migrate/model_mapping)\n  * [Migrate `tf.feature_column`s to Keras preprocessing layers](https://www.tensorflow.org/guide/migrate/migrating_feature_columns)\n  * [Debug a TensorFlow 2 migrated training pipeline](https://www.tensorflow.org/guide/migrate/migration_debugging)\n  * [Introduction to modules, layers, and models](https://www.tensorflow.org/guide/intro_to_modules)\n  * [Basic training loops](https://www.tensorflow.org/guide/basic_training_loops)\n\n| \n  * [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series)\n  * [Load a pandas DataFrame](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe)\n  * [Parameter server training with ParameterServerStrategy](https://www.tensorflow.org/tutorials/distribute/parameter_server_training)\n  * [Intro to Autoencoders](https://www.tensorflow.org/tutorials/generative/autoencoder)\n  * [Learned data compression](https://www.tensorflow.org/tutorials/generative/data_compression)\n\n  \nThere are three ways to instantiate a `Model`:\n## With the \"Functional API\"\nYou start from `Input`, you chain layer calls to specify the model's forward pass, and finally you create your model from inputs and outputs:\n```\ninputs = keras.Input(shape=(37,))\nx = keras.layers.Dense(32, activation=\"relu\")(inputs)\noutputs = keras.layers.Dense(5, activation=\"softmax\")(x)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n```\n\nA new Functional API model can also be created by using the intermediate tensors. This enables you to quickly extract sub-components of the model.\n#### Example:\n```\ninputs = keras.Input(shape=(None, None, 3))\nprocessed = keras.layers.RandomCrop(width=128, height=128)(inputs)\nconv = keras.layers.Conv2D(filters=32, kernel_size=3)(processed)\npooling = keras.layers.GlobalAveragePooling2D()(conv)\nfeature = keras.layers.Dense(10)(pooling)\n\nfull_model = keras.Model(inputs, feature)\nbackbone = keras.Model(processed, conv)\nactivations = keras.Model(conv, feature)\n\n```\n\nNote that the `backbone` and `activations` models are not created with [`keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects, but with the tensors that originate from [`keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input) objects. Under the hood, the layers and weights will be shared across these models, so that user can train the `full_model`, and use `backbone` or `activations` to do feature extraction. The inputs and outputs of the model can be nested structures of tensors as well, and the created models are standard Functional API models that support all the existing APIs.\n## By subclassing the `Model` class\nIn that case, you should define your layers in `__init__()` and you should implement the model's forward pass in `call()`.\n```\nclassMyModel(keras.Model):\n    def__init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n\n    defcall(self, inputs):\n        x = self.dense1(inputs)\n        return self.dense2(x)\n\nmodel = MyModel()\n\n```\n\nIf you subclass `Model`, you can optionally have a `training` argument (boolean) in `call()`, which you can use to specify a different behavior in training and inference:\n```\nclassMyModel(keras.Model):\n    def__init__(self):\n        super().__init__()\n        self.dense1 = keras.layers.Dense(32, activation=\"relu\")\n        self.dense2 = keras.layers.Dense(5, activation=\"softmax\")\n        self.dropout = keras.layers.Dropout(0.5)\n\n    defcall(self, inputs, training=False):\n        x = self.dense1(inputs)\n        x = self.dropout(x, training=training)\n        return self.dense2(x)\n\nmodel = MyModel()\n\n```\n\nOnce the model is created, you can config the model with losses and metrics with `model.compile()`, train the model with `model.fit()`, or use the model to do prediction with `model.predict()`.\n## With the `Sequential` class\nIn addition, [`keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) is a special case of model where the model is purely a stack of single-input, single-output layers.\n```\nmodel = keras.Sequential([\n    keras.Input(shape=(None, None, 3)),\n    keras.layers.Conv2D(filters=32, kernel_size=3),\n])\n\n```\n\n## Attributes  \n---  \n`compiled_metrics`  \n`distribute_reduction_method`  \n`distribute_strategy`  \n`input` |  Retrieves the input tensor(s) of a symbolic operation.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`jit_compile`  \n`layers`  \n`metrics_names`  \n`output` |  Retrieves the output tensor(s) of a layer.Only returns the tensor(s) corresponding to the _first time_ the operation was called.   \n`run_eagerly`  \n## Methods\n### `compile`\n```\ncompile(\n    optimizer='rmsprop',\n    loss=None,\n    loss_weights=None,\n    metrics=None,\n    weighted_metrics=None,\n    run_eagerly=False,\n    steps_per_execution=1,\n    jit_compile='auto',\n    auto_scale_loss=True\n)\n\n```\n\nConfigures the model for training.\n#### Example:\n```\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss=keras.losses.BinaryCrossentropy(),\n    metrics=[\n        keras.metrics.BinaryAccuracy(),\n        keras.metrics.FalseNegatives(),\n    ],\n)\n\n```\n\nArgs  \n---  \n`optimizer` |  String (name of optimizer) or optimizer instance. See [`keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).   \n`loss` |  Loss function. May be a string (name of loss function), or a [`keras.losses.Loss`](https://www.tensorflow.org/api_docs/python/tf/keras/Loss) instance. See [`keras.losses`](https://www.tensorflow.org/api_docs/python/tf/keras/losses). A loss function is any callable with the signature `loss = fn(y_true, y_pred)`, where `y_true` are the ground truth values, and `y_pred` are the model's predictions. `y_true` should have shape `(batch_size, d0, .. dN)` (except in the case of sparse loss functions such as sparse categorical crossentropy which expects integer arrays of shape `(batch_size, d0, .. dN-1)`). `y_pred` should have shape `(batch_size, d0, .. dN)`. The loss function should return a float tensor.   \n`loss_weights` |  Optional list or dictionary specifying scalar coefficients (Python floats) to weight the loss contributions of different model outputs. The loss value that will be minimized by the model will then be the _weighted sum_ of all individual losses, weighted by the `loss_weights` coefficients. If a list, it is expected to have a 1:1 mapping to the model's outputs. If a dict, it is expected to map output names (strings) to scalar coefficients.   \n`metrics` |  List of metrics to be evaluated by the model during training and testing. Each of this can be a string (name of a built-in function), function or a [`keras.metrics.Metric`](https://www.tensorflow.org/api_docs/python/tf/keras/Metric) instance. See [`keras.metrics`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics). Typically you will use `metrics=['accuracy']`. A function is any callable with the signature `result = fn(y_true, _pred)`. To specify different metrics for different outputs of a multi-output model, you could also pass a dictionary, such as `metrics={'a':'accuracy', 'b':['accuracy', 'mse']}`. You can also pass a list to specify a metric or a list of metrics for each output, such as `metrics=[['accuracy'], ['accuracy', 'mse']]` or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the strings 'accuracy' or 'acc', we convert this to one of [`keras.metrics.BinaryAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy), [`keras.metrics.CategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/CategoricalAccuracy), [`keras.metrics.SparseCategoricalAccuracy`](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalAccuracy) based on the shapes of the targets and of the model output. A similar conversion is done for the strings `\"crossentropy\"` and `\"ce\"` as well. The metrics passed here are evaluated without sample weighting; if you would like sample weighting to apply, you can specify your metrics via the `weighted_metrics` argument instead.   \n`weighted_metrics` |  List of metrics to be evaluated and weighted by `sample_weight` or `class_weight` during training and testing.   \n`run_eagerly` |  Bool. If `True`, this model's forward pass will never be compiled. It is recommended to leave this as `False` when training (for best performance), and to set it to `True` when debugging.   \n`steps_per_execution` |  Int. The number of batches to run during each a single compiled function call. Running multiple batches inside a single compiled function call can greatly improve performance on TPUs or small models with a large Python overhead. At most, one full epoch will be run each execution. If a number larger than the size of the epoch is passed, the execution will be truncated to the size of the epoch. Note that if `steps_per_execution` is set to `N`, [`Callback.on_batch_begin`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_batch_begin) and [`Callback.on_batch_end`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_batch_end) methods will only be called every `N` batches (i.e. before/after each compiled function execution). Not supported with the PyTorch backend.   \n`jit_compile` |  Bool or `\"auto\"`. Whether to use XLA compilation when compiling a model. For `jax` and `tensorflow` backends, `jit_compile=\"auto\"` enables XLA compilation if the model supports it, and disabled otherwise. For `torch` backend, `\"auto\"` will default to eager execution and `jit_compile=True` will run with `torch.compile` with the `\"inductor\"` backend.   \n`auto_scale_loss` |  Bool. If `True` and the model dtype policy is `\"mixed_float16\"`, the passed optimizer will be automatically wrapped in a `LossScaleOptimizer`, which will dynamically scale the loss to prevent underflow.   \n### `compile_from_config`\n```\ncompile_from_config(\n    config\n)\n\n```\n\nCompiles the model with the information given in config.\nThis method uses the information in the config (optimizer, loss, metrics, etc.) to compile the model.\nArgs  \n---  \n`config` |  Dict containing information for compiling the model.   \n### `compiled_loss`\n```\ncompiled_loss(\n    y, y_pred, sample_weight=None, regularization_losses=None\n)\n\n```\n\n### `compute_loss`\n```\ncompute_loss(\n    x=None, y=None, y_pred=None, sample_weight=None\n)\n\n```\n\nCompute the total loss, validate it, and return it.\nSubclasses can optionally override this method to provide custom loss computation logic.\n#### Example:\n```\nclassMyModel(Model):\n    def__init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_tracker = metrics.Mean(name='loss')\n\n    defcompute_loss(self, x, y, y_pred, sample_weight):\n        loss = ops.means((y_pred - y) ** 2)\n        loss += ops.sum(self.losses)\n        self.loss_tracker.update_state(loss)\n        return loss\n\n    defreset_metrics(self):\n        self.loss_tracker.reset_state()\n\n    @property\n    defmetrics(self):\n        return [self.loss_tracker]\n\ninputs = layers.Input(shape=(10,), name='my_input')\noutputs = layers.Dense(10)(inputs)\nmodel = MyModel(inputs, outputs)\nmodel.add_loss(ops.sum(outputs))\n\noptimizer = SGD()\nmodel.compile(optimizer, loss='mse', steps_per_execution=10)\ndataset = ...\nmodel.fit(dataset, epochs=2, steps_per_epoch=10)\nprint(f\"Custom loss: {model.loss_tracker.result()}\")\n\n```\n\nArgs  \n---  \nInput data.   \nTarget data.   \n`y_pred` |  Predictions returned by the model (output of `model(x)`)   \n`sample_weight` |  Sample weights for weighting the loss function.   \nReturns  \n---  \nThe total loss as a scalar tensor, or `None` if no loss results (which is the case when called by [`Model.test_step`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#test_step)).   \n### `compute_metrics`\n```\ncompute_metrics(\n    x, y, y_pred, sample_weight=None\n)\n\n```\n\nUpdate metric states and collect all metrics to be returned.\nSubclasses can optionally override this method to provide custom metric updating and collection logic.\n#### Example:\n```\nclassMyModel(Sequential):\n    defcompute_metrics(self, x, y, y_pred, sample_weight):\n        # This super call updates `self.compiled_metrics` and returns\n        # results for all metrics listed in `self.metrics`.\n        metric_results = super().compute_metrics(\n            x, y, y_pred, sample_weight)\n\n        # Note that `self.custom_metric` is not listed\n        # in `self.metrics`.\n        self.custom_metric.update_state(x, y, y_pred, sample_weight)\n        metric_results['metric_name'] = self.custom_metric.result()\n        return metric_results\n\n```\n\nArgs  \n---  \nInput data.   \nTarget data.   \n`y_pred` |  Predictions returned by the model output of `model.call(x)`.   \n`sample_weight` |  Sample weights for weighting the loss function.   \nReturns  \n---  \nA `dict` containing values that will be passed to [`keras.callbacks.CallbackList.on_train_batch_end()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CallbackList#on_train_batch_end). Typically, the values of the metrics listed in `self.metrics` are returned.   \n`Example` |  `{'loss': 0.2, 'accuracy': 0.7}`.   \n### `evaluate`\n```\nevaluate(\n    x=None,\n    y=None,\n    batch_size=None,\n    verbose='auto',\n    sample_weight=None,\n    steps=None,\n    callbacks=None,\n    return_dict=False,\n    **kwargs\n)\n\n```\n\nReturns the loss value & metrics values for the model in test mode.\nComputation is done in batches (see the `batch_size` arg.)\nArgs  \n---  \nInput data. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n  * A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n  * A generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. \n\n  \nTarget data. Like the input data `x`, it could be either NumPy array(s) or backend-native tensor(s). If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, `y` should not be specified (since targets will be obtained from the iterator/dataset).   \n`batch_size` |  Integer or `None`. Number of samples per batch of computation. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of a dataset, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\"auto\"` becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). Defaults to `\"auto\"`.   \n`sample_weight` |  Optional NumPy array of weights for the test samples, used for weighting the loss function. You can either pass a flat (1D) NumPy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, instead pass sample weights as the third element of `x`.   \n`steps` |  Integer or `None`. Total number of steps (batches of samples) before declaring the evaluation round finished. Ignored with the default value of `None`. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) and `steps` is `None`, evaluation will run until the dataset is exhausted.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during evaluation.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nScalar test loss (if the model has a single output and no metrics) or list of scalars (if the model has multiple outputs and/or metrics). The attribute `model.metrics_names` will give you the display labels for the scalar outputs.   \n### `export`\n```\nexport(\n    filepath, format='tf_saved_model'\n)\n\n```\n\nCreate a TF SavedModel artifact for inference.\nThis method lets you export a model to a lightweight SavedModel artifact that contains the model's forward pass only (its `call()` method) and can be served via e.g. TF-Serving. The forward pass is registered under the name `serve()` (see example below).\nThe original code of the model (including any custom layers you may have used) is _no longer_ necessary to reload the artifact -- it is entirely standalone.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the artifact.   \n#### Example:\n```\n# Create the artifact\nmodel.export(\"path/to/location\")\n\n# Later, in a different process / environment...\nreloaded_artifact = tf.saved_model.load(\"path/to/location\")\npredictions = reloaded_artifact.serve(input_data)\n\n```\n\nIf you would like to customize your serving endpoints, you can use the lower-level [`keras.export.ExportArchive`](https://www.tensorflow.org/api_docs/python/tf/keras/export/ExportArchive) class. The `export()` method relies on `ExportArchive` internally.\n### `fit`\n```\nfit(\n    x=None,\n    y=None,\n    batch_size=None,\n    epochs=1,\n    verbose='auto',\n    callbacks=None,\n    validation_split=0.0,\n    validation_data=None,\n    shuffle=True,\n    class_weight=None,\n    sample_weight=None,\n    initial_epoch=0,\n    steps_per_epoch=None,\n    validation_steps=None,\n    validation_batch_size=None,\n    validation_freq=1\n)\n\n```\n\nTrains the model for a fixed number of epochs (dataset iterations).\nArgs  \n---  \nInput data. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A dict mapping input names to the corresponding array/tensors, if the model has named inputs.\n  * A [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Should return a tuple of either `(inputs, targets)` or `(inputs, targets, sample_weights)`.\n  * A [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`. \n\n  \nTarget data. Like the input data `x`, it could be either NumPy array(s) or backend-native tensor(s). If `x` is a dataset, generator, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, `y` should not be specified (since targets will be obtained from `x`).   \n`batch_size` |  Integer or `None`. Number of samples per gradient update. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`epochs` |  Integer. Number of epochs to train the model. An epoch is an iteration over the entire `x` and `y` data provided (unless the `steps_per_epoch` flag is set to something other than None). Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\". The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index `epochs` is reached.   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. \"auto\" becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g., in a production environment). Defaults to `\"auto\"`.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during training. See [`keras.callbacks`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks). Note [`keras.callbacks.ProgbarLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ProgbarLogger) and [`keras.callbacks.History`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History) callbacks are created automatically and need not be passed to `model.fit()`. [`keras.callbacks.ProgbarLogger`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ProgbarLogger) is created or not based on the `verbose` argument in `model.fit()`.   \n`validation_split` |  Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the `x` and `y` data provided, before shuffling. This argument is not supported when `x` is a dataset, generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance. If both `validation_data` and `validation_split` are provided, `validation_data` will override `validation_split`.   \n`validation_data` |  Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. Thus, note the fact that the validation loss of data provided using `validation_split` or `validation_data` is not affected by regularization layers like noise and dropout. `validation_data` will override `validation_split`. It could be: \n* A tuple `(x_val, y_val)` of NumPy arrays or tensors.\n* A tuple `(x_val, y_val, val_sample_weights)` of NumPy arrays.\n* A Python generator or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) returning `(inputs, targets)` or `(inputs, targets, sample_weights)`.   \n`shuffle` |  Boolean, whether to shuffle the training data before each epoch. This argument is ignored when `x` is a generator or a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).   \n`class_weight` |  Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. When `class_weight` is specified and targets have a rank of 2 or greater, either `y` must be one-hot encoded, or an explicit final dimension of `1` must be included for sparse class labels.   \n`sample_weight` |  Optional NumPy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) NumPy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample. This argument is not supported when `x` is a dataset, generator, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance, instead provide the sample_weights as the third element of `x`. Note that sample weighting does not apply to metrics specified via the `metrics` argument in `compile()`. To apply sample weighting to your metrics, you can specify them via the `weighted_metrics` in `compile()` instead.   \n`initial_epoch` |  Integer. Epoch at which to start training (useful for resuming a previous training run).   \n`steps_per_epoch` |  Integer or `None`. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as backend-native tensors, the default `None` is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), and `steps_per_epoch` is `None`, the epoch will run until the input dataset is exhausted. When passing an infinitely repeating dataset, you must specify the `steps_per_epoch` argument. If `steps_per_epoch=-1` the training will run indefinitely with an infinitely repeating dataset.   \n`validation_steps` |  Only relevant if `validation_data` is provided. Total number of steps (batches of samples) to draw before stopping when performing validation at the end of every epoch. If `validation_steps` is `None`, validation will run until the `validation_data` dataset is exhausted. In the case of an infinitely repeated dataset, it will run into an infinite loop. If `validation_steps` is specified and only part of the dataset will be consumed, the evaluation will start from the beginning of the dataset at each epoch. This ensures that the same validation samples are used every time.   \n`validation_batch_size` |  Integer or `None`. Number of samples per validation batch. If unspecified, will default to `batch_size`. Do not specify the `validation_batch_size` if your data is in the form of datasets or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`validation_freq` |  Only relevant if validation data is provided. Specifies how many training epochs to run before a new validation run is performed, e.g. `validation_freq=2` runs validation every 2 epochs.   \nUnpacking behavior for iterator-like inputs: A common pattern is to pass an iterator like object such as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) or a [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) to `fit()`, which will in fact yield not only features (`x`) but optionally targets (`y`) and sample weights (`sample_weight`). Keras requires that the output of such iterator-likes be unambiguous. The iterator should return a tuple of length 1, 2, or 3, where the optional second and third elements will be used for `y` and `sample_weight` respectively. Any other type provided will be wrapped in a length-one tuple, effectively treating everything as `x`. When yielding dicts, they should still adhere to the top-level tuple structure, e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate features, targets, and weights from the keys of a single dict. A notable unsupported data type is the `namedtuple`. The reason is that it behaves like both an ordered datatype (tuple) and a mapping datatype (dict). So given a namedtuple of the form: `namedtuple(\"example_tuple\", [\"y\", \"x\"])` it is ambiguous whether to reverse the order of the elements when interpreting the value. Even worse is a tuple of the form: `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])` where it is unclear if the tuple was intended to be unpacked into `x`, `y`, and `sample_weight` or passed through as a single element to `x`.\nReturns  \n---  \nA `History` object. Its `History.history` attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).   \n### `from_config`\n```\n@classmethod\nfrom_config(\n    config, custom_objects=None\n)\n\n```\n\nCreates a layer from its config.\nThis method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`).\nArgs  \n---  \n`config` |  A Python dictionary, typically the output of get_config.   \nReturns  \n---  \nA layer instance.   \n### `get_compile_config`\n```\nget_compile_config()\n\n```\n\nReturns a serialized config with information for compiling the model.\nThis method returns a config dictionary containing all the information (optimizer, loss, metrics, etc.) with which the model was compiled.\nReturns  \n---  \nA dict containing information for compiling the model.   \n### `get_layer`\n```\nget_layer(\n    name=None, index=None\n)\n\n```\n\nRetrieves a layer based on either its name (unique) or index.\nIf `name` and `index` are both provided, `index` will take precedence. Indices are based on order of horizontal graph traversal (bottom-up).\nArgs  \n---  \n`name` |  String, name of layer.   \n`index` |  Integer, index of layer.   \nReturns  \n---  \nA layer instance.   \n### `get_metrics_result`\n```\nget_metrics_result()\n\n```\n\nReturns the model's metrics values as a dict.\nIf any of the metric result is a dict (containing multiple metrics), each of them gets added to the top level returned dict of this method.\nReturns  \n---  \nA `dict` containing values of the metrics listed in `self.metrics`.   \n`Example` |  `{'loss': 0.2, 'accuracy': 0.7}`.   \n### `load_weights`\n```\nload_weights(\n    filepath, skip_mismatch=False, **kwargs\n)\n\n```\n\nLoad weights from a file saved via `save_weights()`.\nWeights are loaded based on the network's topology. This means the architecture should be the same as when the weights were saved. Note that layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n**Partial weight loading**\nIf you have modified your model, for instance by adding a new layer (with weights) or by changing the shape of the weights of a layer, you can choose to ignore errors and continue loading by setting `skip_mismatch=True`. In this case any layer with mismatching weights will be skipped. A warning will be displayed for each skipped layer.\nArgs  \n---  \n`filepath` |  String, path to the weights file to load. It can either be a `.weights.h5` file or a legacy `.h5` weights file.   \n`skip_mismatch` |  Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weights.   \n### `loss`\n```\nloss(\n    y, y_pred, sample_weight=None\n)\n\n```\n\n### `make_predict_function`\n```\nmake_predict_function(\n    force=False\n)\n\n```\n\n### `make_test_function`\n```\nmake_test_function(\n    force=False\n)\n\n```\n\n### `make_train_function`\n```\nmake_train_function(\n    force=False\n)\n\n```\n\n### `predict`\n```\npredict(\n    x, batch_size=None, verbose='auto', steps=None, callbacks=None\n)\n\n```\n\nGenerates output predictions for the input samples.\nComputation is done in batches. This method is designed for batch processing of large numbers of inputs. It is not intended for use inside of loops that iterate over your data and process small numbers of inputs at a time.\nFor small numbers of inputs that fit in one batch, directly use `__call__()` for faster execution, e.g., `model(x)`, or `model(x, training=False)` if you have layers such as `BatchNormalization` that behave differently during inference.\nArgs  \n---  \nInput samples. It could be:\n  * A NumPy array (or array-like), or a list of arrays (in case the model has multiple inputs).\n  * A tensor, or a list of tensors (in case the model has multiple inputs).\n  * A [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instance. \n\n  \n`batch_size` |  Integer or `None`. Number of samples per batch. If unspecified, `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of dataset, generators, or [`keras.utils.PyDataset`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/PyDataset) instances (since they generate batches).   \n`verbose` |  `\"auto\"`, 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = single line. `\"auto\"` becomes 1 for most cases. Note that the progress bar is not particularly useful when logged to a file, so `verbose=2` is recommended when not running interactively (e.g. in a production environment). Defaults to `\"auto\"`.   \n`steps` |  Total number of steps (batches of samples) before declaring the prediction round finished. Ignored with the default value of `None`. If `x` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) and `steps` is `None`, `predict()` will run until the input dataset is exhausted.   \n`callbacks` |  List of [`keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) instances. List of callbacks to apply during prediction.   \nReturns  \n---  \nNumPy array(s) of predictions.   \n### `predict_on_batch`\n```\npredict_on_batch(\n    x\n)\n\n```\n\nReturns predictions for a single batch of samples.\nArgs  \n---  \nInput data. It must be array-like.   \nReturns  \n---  \nNumPy array(s) of predictions.   \n### `predict_step`\n```\npredict_step(\n    data\n)\n\n```\n\n### `reset_metrics`\n```\nreset_metrics()\n\n```\n\n### `save`\n```\nsave(\n    filepath, overwrite=True, **kwargs\n)\n\n```\n\nSaves a model as a `.keras` file.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the model. Must end in `.keras`.   \n`overwrite` |  Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.   \n`save_format` |  The `save_format` argument is deprecated in Keras 3. Format to use, as a string. Only the `\"keras\"` format is supported at this time.   \n#### Example:\n```\nmodel = keras.Sequential(\n    [\n        keras.layers.Dense(5, input_shape=(3,)),\n        keras.layers.Softmax(),\n    ],\n)\nmodel.save(\"model.keras\")\nloaded_model = keras.saving.load_model(\"model.keras\")\nx = keras.random.uniform((10, 3))\nassert np.allclose(model.predict(x), loaded_model.predict(x))\n\n```\n\nNote that `model.save()` is an alias for `keras.saving.save_model()`.\nThe saved `.keras` file contains:\n  * The model's configuration (architecture)\n  * The model's weights\n  * The model's optimizer's state (if any)\n\n\nThus models can be reinstantiated in the exact same state.\n### `save_weights`\n```\nsave_weights(\n    filepath, overwrite=True\n)\n\n```\n\nSaves all layer weights to a `.weights.h5` file.\nArgs  \n---  \n`filepath` |  `str` or `pathlib.Path` object. Path where to save the model. Must end in `.weights.h5`.   \n`overwrite` |  Whether we should overwrite any existing model at the target location, or instead ask the user via an interactive prompt.   \n### `stateless_compute_loss`\n```\nstateless_compute_loss(\n    trainable_variables,\n    non_trainable_variables,\n    metrics_variables,\n    x=None,\n    y=None,\n    y_pred=None,\n    sample_weight=None\n)\n\n```\n\n### `summary`\n```\nsummary(\n    line_length=None,\n    positions=None,\n    print_fn=None,\n    expand_nested=False,\n    show_trainable=False,\n    layer_range=None\n)\n\n```\n\nPrints a string summary of the network.\nArgs  \n---  \n`line_length` |  Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).   \n`positions` |  Relative or absolute positions of log elements in each line. If not provided, becomes `[0.3, 0.6, 0.70, 1.]`. Defaults to `None`.   \n`print_fn` |  Print function to use. By default, prints to `stdout`. If `stdout` doesn't work in your environment, change to `print`. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.   \n`expand_nested` |  Whether to expand the nested models. Defaults to `False`.   \n`show_trainable` |  Whether to show if a layer is trainable. Defaults to `False`.   \n`layer_range` |  a list or tuple of 2 strings, which is the starting layer name and ending layer name (both inclusive) indicating the range of layers to be printed in summary. It also accepts regex patterns instead of exact name. In such case, start predicate will be the first element it matches to `layer_range[0]` and the end predicate will be the last element it matches to `layer_range[1]`. By default `None` which considers all layers of model.   \nRaises  \n---  \n`ValueError` |  if `summary()` is called before the model is built.   \n### `symbolic_call`\n```\nsymbolic_call(\n    *args, **kwargs\n)\n\n```\n\n### `test_on_batch`\n```\ntest_on_batch(\n    x, y=None, sample_weight=None, return_dict=False\n)\n\n```\n\nTest the model on a single batch of samples.\nArgs  \n---  \nInput data. Must be array-like.   \nTarget data. Must be array-like.   \n`sample_weight` |  Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nA scalar loss value (when no metrics and `return_dict=False`), a list of loss and metric values (if there are metrics and `return_dict=False`), or a dict of metric and loss values (if `return_dict=True`).   \n### `test_step`\n```\ntest_step(\n    data\n)\n\n```\n\n### `to_json`\n```\nto_json(\n    **kwargs\n)\n\n```\n\nReturns a JSON string containing the network configuration.\nTo load a network from a JSON save file, use [`keras.models.model_from_json(json_string, custom_objects={...})`](https://www.tensorflow.org/api_docs/python/tf/keras/models/model_from_json).\nArgs  \n---  \n`**kwargs` |  Additional keyword arguments to be passed to `json.dumps()`.   \nReturns  \n---  \nA JSON string.   \n### `train_on_batch`\n```\ntrain_on_batch(\n    x, y=None, sample_weight=None, class_weight=None, return_dict=False\n)\n\n```\n\nRuns a single gradient update on a single batch of data.\nArgs  \n---  \nInput data. Must be array-like.   \nTarget data. Must be array-like.   \n`sample_weight` |  Optional array of the same length as x, containing weights to apply to the model's loss for each sample. In the case of temporal data, you can pass a 2D array with shape `(samples, sequence_length)`, to apply a different weight to every timestep of every sample.   \n`class_weight` |  Optional dictionary mapping class indices (integers) to a weight (float) to apply to the model's loss for the samples from this class during training. This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class. When `class_weight` is specified and targets have a rank of 2 or greater, either `y` must be one-hot encoded, or an explicit final dimension of 1 must be included for sparse class labels.   \n`return_dict` |  If `True`, loss and metric results are returned as a dict, with each key being the name of the metric. If `False`, they are returned as a list.   \nReturns  \n---  \nA scalar loss value (when no metrics and `return_dict=False`), a list of loss and metric values (if there are metrics and `return_dict=False`), or a dict of metric and loss values (if `return_dict=True`).   \n### `train_step`\n```\ntrain_step(\n    data\n)\n\n```\n\n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/Fingerprint": "The SavedModel fingerprint.\n```\ntf.saved_model.experimental.Fingerprint(\n    saved_model_checksum: int = None,\n    graph_def_program_hash: int = None,\n    signature_def_hash: int = None,\n    saved_object_graph_hash: int = None,\n    checkpoint_hash: int = None,\n    version: int = None\n)\n\n```\n\nEach attribute of this class is named after a field name in the FingerprintDef proto and contains the value of the respective field in the protobuf.\n## Args  \n---  \n`saved_model_checksum` |  Value of the`saved_model_checksum`.   \n`graph_def_program_hash` |  Value of the `graph_def_program_hash`.   \n`signature_def_hash` |  Value of the `signature_def_hash`.   \n`saved_object_graph_hash` |  Value of the `saved_object_graph_hash`.   \n`checkpoint_hash` |  Value of the `checkpoint_hash`.   \n`version` |  Value of the producer field of the VersionDef.   \n## Attributes  \n---  \n`saved_model_checksum` |  A uint64 containing the `saved_model_checksum`.   \n`graph_def_program_hash` |  A uint64 containing `graph_def_program_hash`.   \n`signature_def_hash` |  A uint64 containing the `signature_def_hash`.   \n`saved_object_graph_hash` |  A uint64 containing the `saved_object_graph_hash`.   \n`checkpoint_hash` |  A uint64 containing the`checkpoint_hash`.   \n`version` |  An int32 containing the producer field of the VersionDef.   \n## Methods\n### `from_proto`\n```\n@classmethod\nfrom_proto(\n    proto: fingerprint_pb2.FingerprintDef\n) -> 'Fingerprint'\n\n```\n\nConstructs Fingerprint object from protocol buffer message.\n### `singleprint`\n```\nsingleprint() -> fingerprinting_pywrap.Singleprint\n\n```\n\nCanonical fingerprinting ID for a SavedModel.\nUniquely identifies a SavedModel based on the regularized fingerprint attributes. (saved_model_checksum is sensitive to immaterial changes and thus non-deterministic.)\nReturns  \n---  \nThe string concatenation of `graph_def_program_hash`, `signature_def_hash`, `saved_object_graph_hash`, and `checkpoint_hash` fingerprint attributes (separated by '/').   \nRaises  \n---  \n`ValueError` |  If the fingerprint fields cannot be used to construct the singleprint.   \n### `__eq__`\n```\n__eq__(\n    other: Any\n) -> bool\n\n```\n\nReturn self==value.\n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/TrackableResource": "Holds a Tensor which a tf.function can capture.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.saved_model.experimental.TrackableResource`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/TrackableResource)\n```\ntf.saved_model.experimental.TrackableResource(\n    device=''\n)\n\n```\n\nA TrackableResource is most useful for stateful Tensors that require initialization, such as [`tf.lookup.StaticHashTable`](https://www.tensorflow.org/api_docs/python/tf/lookup/StaticHashTable). `TrackableResource`s are discovered by traversing the graph of object attributes, e.g. during [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save).\nA TrackableResource has three methods to override:\n  * `_create_resource` should create the resource tensor handle.\n  * `_initialize` should initialize the resource held at `self.resource_handle`.\n  * `_destroy_resource` is called upon a `TrackableResource`'s destruction and should decrement the resource's ref count. For most resources, this should be done with a call to [`tf.raw_ops.DestroyResourceOp`](https://www.tensorflow.org/api_docs/python/tf/raw_ops/DestroyResourceOp).\n\n\n#### Example usage:\n```\nclassDemoResource(tf.saved_model.experimental.TrackableResource):\n  def__init__(self):\n    super().__init__()\n    self._initialize()\n  def_create_resource(self):\n    return tf.raw_ops.VarHandleOp(dtype=tf.float32, shape=[2])\n  def_initialize(self):\n    tf.raw_ops.AssignVariableOp(\n        resource=self.resource_handle, value=tf.ones([2]))\n  def_destroy_resource(self):\n    tf.raw_ops.DestroyResourceOp(resource=self.resource_handle)\nclassDemoModule(tf.Module):\n  def__init__(self):\n    self.resource = DemoResource()\n  defincrement(self, tensor):\n    return tensor + tf.raw_ops.ReadVariableOp(\n        resource=self.resource.resource_handle, dtype=tf.float32)\ndemo = DemoModule()\ndemo.increment([5, 1])\n<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>\n```\n\n## Args  \n---  \n`device` |  A string indicating a required placement for this resource, e.g. \"CPU\" if this resource must be created on a CPU device. A blank device allows the user to place resource creation, so generally this should be blank unless the resource only makes sense on one device.   \n## Attributes  \n---  \n`resource_handle` |  Returns the resource handle associated with this Resource. \n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/read_fingerprint": "Reads the fingerprint of a SavedModel in `export_dir`.  \n```\ntf.saved_model.experimental.read_fingerprint(\n    export_dir: str\n) -> [tf.saved_model.experimental.Fingerprint](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/Fingerprint)\n\n```\n\nReturns a [`tf.saved_model.experimental.Fingerprint`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/Fingerprint) object that contains the values of the SavedModel fingerprint, which is persisted on disk in the `fingerprint.pb` file in the `export_dir`.\nRead more about fingerprints in the SavedModel guide at <https://www.tensorflow.org/guide/saved_model>\n## Args  \n---  \n`export_dir` |  The directory that contains the SavedModel.   \n## Returns  \n---  \nA [`tf.saved_model.experimental.Fingerprint`](https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/Fingerprint).   \n## Raises  \n---  \n`FileNotFoundError` |  If no or an invalid fingerprint is found. \n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model/load": "Load a SavedModel from `export_dir`.  \n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.saved_model.load_v2`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load)\n```\ntf.saved_model.load(\n    export_dir, tags=None, options=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Using the SavedModel format](https://www.tensorflow.org/guide/saved_model)\n  * [Import a JAX model using JAX2TF](https://www.tensorflow.org/guide/jax2tf)\n  * [Migrate the SavedModel workflow](https://www.tensorflow.org/guide/migrate/saved_model)\n\n| \n  * [Save and load a model using a distribution strategy](https://www.tensorflow.org/tutorials/distribute/save_and_load)\n  * [Simple audio recognition: Recognizing keywords](https://www.tensorflow.org/tutorials/audio/simple_audio)\n  * [Transfer learning with YAMNet for environmental sound classification](https://www.tensorflow.org/tutorials/audio/transfer_learning_audio)\n  * [Distributed training with DTensors](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)\n\n  \nSignatures associated with the SavedModel are available as functions:\n```\nimported = tf.saved_model.load(path)\nf = imported.signatures[\"serving_default\"]\nprint(f(x=tf.constant([[1.]])))\n\n```\n\nObjects exported with [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) additionally have trackable objects and functions assigned to attributes:\n```\nexported = tf.train.Checkpoint(v=tf.Variable(3.))\nexported.f = tf.function(\n    lambda x: exported.v * x,\n    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])\ntf.saved_model.save(exported, path)\nimported = tf.saved_model.load(path)\nassert 3. == imported.v.numpy()\nassert 6. == imported.f(x=tf.constant(2.)).numpy()\n\n```\n\n_Loading Keras models_\nKeras models are trackable, so they can be saved to SavedModel. The object returned by [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) is not a Keras object (i.e. doesn't have `.fit`, `.predict`, etc. methods). A few attributes and functions are still available: `.variables`, `.trainable_variables` and `.__call__`.\n```\nmodel = tf.keras.Model(...)\ntf.saved_model.save(model, path)\nimported = tf.saved_model.load(path)\noutputs = imported(inputs)\n\n```\n\nUse [`tf.keras.models.load_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model) to restore the Keras model.\n_Importing SavedModels from TensorFlow 1.x_\n1.x SavedModels APIs have a flat graph instead of [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) objects. These SavedModels will be loaded with the following attributes:\n  * `.signatures`: A dictionary mapping signature names to functions.\n  * `.prune(feeds, fetches)`: A method which allows you to extract functions for new subgraphs. This is equivalent to importing the SavedModel and naming feeds and fetches in a Session from TensorFlow 1.x.\n```\nimported = tf.saved_model.load(path_to_v1_saved_model)\npruned = imported.prune(\"x:0\", \"out:0\")\npruned(tf.ones([]))\n\n```\n\nSee [`tf.compat.v1.wrap_function`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/wrap_function) for details.\n  * `.variables`: A list of imported variables.\n  * `.graph`: The whole imported graph.\n  * `.restore(save_path)`: A function that restores variables from a checkpoint saved from `tf.compat.v1.Saver`.\n\n\n_Consuming SavedModels asynchronously_\nWhen consuming SavedModels asynchronously (the producer is a separate process), the SavedModel directory will appear before all files have been written, and [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) will fail if pointed at an incomplete SavedModel. Rather than checking for the directory, check for \"saved_model_dir/saved_model.pb\". This file is written atomically as the last [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) file operation.\n## Args  \n---  \n`export_dir` |  The SavedModel directory to load from.   \n`tags` |  A tag or sequence of tags identifying the MetaGraph to load. Optional if the SavedModel contains a single MetaGraph, as for those exported from [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save).   \n`options` |  [`tf.saved_model.LoadOptions`](https://www.tensorflow.org/api_docs/python/tf/saved_model/LoadOptions) object that specifies options for loading.   \n## Returns  \n---  \nA trackable object with a `signatures` attribute mapping from signature keys to functions. If the SavedModel was exported by [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save), it also points to trackable objects, functions, debug info which it has been saved.   \n## Raises  \n---  \n`ValueError` |  If `tags` don't match a MetaGraph in the SavedModel. \n",
  "https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor": "Represents a sparse tensor.\nView aliases\n**Main aliases**\n[`tf.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), [`tf.compat.v1.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor)\n```\ntf.sparse.SparseTensor(\n    indices, values, dense_shape\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Working with sparse tensors](https://www.tensorflow.org/guide/sparse_tensor)\n  * [Migrate from TPU embedding_columns to TPUEmbedding layer](https://www.tensorflow.org/guide/migrate/tpu_embedding)\n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n  * [Introduction to Tensors](https://www.tensorflow.org/guide/tensor)\n\n| \n  * [Client-efficient large-model federated learning via `federated_select` and sparse aggregation](https://www.tensorflow.org/federated/tutorials/sparse_federated_learning)\n  * [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation)\n  * [TFX Estimator Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components)\n  * [TFX Keras Component Tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n\n  \nTensorFlow represents a sparse tensor as three separate dense tensors: `indices`, `values`, and `dense_shape`. In Python, the three tensors are collected into a `SparseTensor` class for ease of use. If you have separate `indices`, `values`, and `dense_shape` tensors, wrap them in a `SparseTensor` object before passing to the ops below.\nConcretely, the sparse tensor `SparseTensor(indices, values, dense_shape)` comprises the following components, where `N` and `ndims` are the number of values and number of dimensions in the `SparseTensor`, respectively:\n  * `indices`: A 2-D int64 tensor of shape `[N, ndims]`, which specifies the indices of the elements in the sparse tensor that contain nonzero values (elements are zero-indexed). For example, `indices=[[1,3], [2,4]]` specifies that the elements with indexes of [1,3] and [2,4] have nonzero values.\n  * `values`: A 1-D tensor of any type and shape `[N]`, which supplies the values for each element in `indices`. For example, given `indices=[[1,3], [2,4]]`, the parameter `values=[18, 3.6]` specifies that element [1,3] of the sparse tensor has a value of 18, and element [2,4] of the tensor has a value of 3.6.\n  * `dense_shape`: A 1-D int64 tensor of shape `[ndims]`, which specifies the dense_shape of the sparse tensor. Takes a list indicating the number of elements in each dimension. For example, `dense_shape=[3,6]` specifies a two-dimensional 3x6 tensor, `dense_shape=[2,3,4]` specifies a three-dimensional 2x3x4 tensor, and `dense_shape=[9]` specifies a one-dimensional tensor with 9 elements.\n\n\nThe corresponding dense tensor satisfies:\n```\ndense.shape = dense_shape\ndense[tuple(indices[i])] = values[i]\n\n```\n\nBy convention, `indices` should be sorted in row-major order (or equivalently lexicographic order on the tuples `indices[i]`). This is not enforced when `SparseTensor` objects are constructed, but most ops assume correct ordering. If the ordering of sparse tensor `st` is wrong, a fixed version can be obtained by calling [`tf.sparse.reorder(st)`](https://www.tensorflow.org/api_docs/python/tf/sparse/reorder).\nExample: The sparse tensor\n```\nSparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\n\n```\n\nrepresents the dense tensor\n```\n[[1, 0, 0, 0]\n [0, 0, 2, 0]\n [0, 0, 0, 0]]\n\n```\n\n## Args  \n---  \n`indices` |  A 2-D int64 tensor of shape `[N, ndims]`.   \n`values` |  A 1-D tensor of any type and shape `[N]`.   \n`dense_shape` |  A 1-D int64 tensor of shape `[ndims]`.   \n## Raises  \n---  \n`ValueError` |  When building an eager SparseTensor if `dense_shape` is unknown or contains unknown elements (None or -1).   \n## Attributes  \n---  \n`dense_shape` |  A 1-D Tensor of int64 representing the shape of the dense tensor.   \n`dtype` |  The `DType` of elements in this tensor.   \n`graph` |  The `Graph` that contains the index, value, and dense_shape tensors.   \n`indices` |  The indices of non-zero values in the represented dense tensor.   \n`op` |  The `Operation` that produces `values` as an output.   \n`shape` |  Get the `TensorShape` representing the shape of the dense tensor.   \n`values` |  The non-zero values in the represented dense tensor.   \n## Methods\n### `consumers`\n```\nconsumers()\n\n```\n\n### `eval`\n```\neval(\n    feed_dict=None, session=None\n)\n\n```\n\nEvaluates this sparse tensor in a `Session`.\nCalling this method will execute all preceding operations that produce the inputs needed for the operation that produces this tensor.\nArgs  \n---  \n`feed_dict` |  A dictionary that maps `Tensor` objects to feed values. See `tf.Session.run` for a description of the valid feed values.   \n`session` |  (Optional.) The `Session` to be used to evaluate this sparse tensor. If none, the default session will be used.   \nReturns  \n---  \nA `SparseTensorValue` object.   \n### `from_value`\n```\n@classmethod\nfrom_value(\n    sparse_tensor_value\n)\n\n```\n\n### `get_shape`\n```\nget_shape() -> [tf.TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape)\n\n```\n\nGet the `TensorShape` representing the shape of the dense tensor.\nReturns  \n---  \nA `TensorShape` object.   \n### `set_shape`\n```\nset_shape(\n    shape\n)\n\n```\n\nUpdates the `TensorShape` representing the shape of the dense tensor.\nWith eager execution this operates as a shape assertion. Here the shapes match:\n```\nst = tf.SparseTensor(\n  indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4])\nst.set_shape([3, 4])\n```\n\nPassing a `None` in the new shape allows any value for that axis:\n```\nst.set_shape([3, None])\n```\n\nAn error is raised if an incompatible shape is passed.\n```\nst.set_shape([1, 4])\nTraceback (most recent call last):\nValueError: Tensor's shape (3, 4) is not compatible with supplied\nshape [1, 4]\n```\n\nWhen executing in a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), or building a model using [`tf.keras.Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input), [`SparseTensor.set_shape`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor#set_shape) will _merge_ the given `shape` with the current shape of this tensor, and set the tensor's shape to the merged value (see [`tf.TensorShape.merge_with`](https://www.tensorflow.org/api_docs/python/tf/TensorShape#merge_with) for details):\n```\nst = tf.keras.Input(shape=[None, None, 3], sparse=True)\nprint(st.shape)\n(None, None, None, 3)\n```\n\nDimensions set to `None` are not updated:\n```\nst.set_shape([None, 224, 224, None])\nprint(st.shape)\n(None, 224, 224, 3)\n```\n\nThe main use case for this is to provide additional shape information that cannot be inferred from the graph alone.\nArgs  \n---  \n`shape` |  A `TensorShape` representing the shape of this tensor, a `TensorShapeProto`, a list, a tuple, or None.   \nRaises  \n---  \n`ValueError` |  If `shape` is not compatible with the current shape of this tensor.   \n### `with_values`\n```\nwith_values(\n    new_values\n)\n\n```\n\nReturns a copy of `self` with `values` replaced by `new_values`.\nThis method produces a new `SparseTensor` that has the same nonzero `indices` and same `dense_shape`, but updated values.\nArgs  \n---  \n`new_values` |  The values of the new `SparseTensor`. Needs to have the same shape as the current `.values` `Tensor`. May have a different type than the current `values`.   \nReturns  \n---  \nA `SparseTensor` with identical indices and shape but updated values.   \n#### Example usage:\n```\nst = tf.sparse.from_dense([[1, 0, 2, 0], [3, 0, 0, 4]])\ntf.sparse.to_dense(st.with_values([10, 20, 30, 40]))  # 4 nonzero values\n<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[10,  0, 20,  0],\n       [30,  0,  0, 40]], dtype=int32)>\n```\n\n### `__div__`\n```\n__div__(\n    y\n)\n\n```\n\nComponent-wise divides a SparseTensor by a dense Tensor.\n_Limitation_ : this Op only broadcasts the dense side to the sparse side, but not the other direction.\nArgs  \n---  \n`sp_indices` |  A `Tensor` of type `int64`. 2-D. `N x R` matrix with the indices of non-empty values in a SparseTensor, possibly not in canonical ordering.   \n`sp_values` |  A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`. 1-D. `N` non-empty values corresponding to `sp_indices`.   \n`sp_shape` |  A `Tensor` of type `int64`. 1-D. Shape of the input SparseTensor.   \n`dense` |  A `Tensor`. Must have the same type as `sp_values`. `R`-D. The dense Tensor operand.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `sp_values`.   \n### `__mul__`\n```\n__mul__(\n    y\n)\n\n```\n\nComponent-wise multiplies a SparseTensor by a dense Tensor.\nThe output locations corresponding to the implicitly zero elements in the sparse tensor will be zero (i.e., will not take up storage space), regardless of the contents of the dense tensor (even if it's +/-INF and that INF*0 == NaN).\n_Limitation_ : this Op only broadcasts the dense side to the sparse side, but not the other direction.\nArgs  \n---  \n`sp_indices` |  A `Tensor` of type `int64`. 2-D. `N x R` matrix with the indices of non-empty values in a SparseTensor, possibly not in canonical ordering.   \n`sp_values` |  A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `uint8`, `int16`, `int8`, `complex64`, `int64`, `qint8`, `quint8`, `qint32`, `bfloat16`, `qint16`, `quint16`, `uint16`, `complex128`, `half`, `uint32`, `uint64`. 1-D. `N` non-empty values corresponding to `sp_indices`.   \n`sp_shape` |  A `Tensor` of type `int64`. 1-D. Shape of the input SparseTensor.   \n`dense` |  A `Tensor`. Must have the same type as `sp_values`. `R`-D. The dense Tensor operand.   \n`name` |  A name for the operation (optional).   \nReturns  \n---  \nA `Tensor`. Has the same type as `sp_values`.   \n### `__truediv__`\n```\n__truediv__(\n    y\n)\n\n```\n\nInternal helper function for 'sp_t / dense_t'.\n",
  "https://www.tensorflow.org/api_docs/python/tf/saved_model/save": "Exports a [tf.Module](https://www.tensorflow.org/api_docs/python/tf/Module) (and subclasses) `obj` to [SavedModel format](https://www.tensorflow.org/guide/saved_model#the_savedmodel_format_on_disk).\n#### View aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.saved_model.experimental.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save), [`tf.compat.v1.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)\n```\ntf.saved_model.save(\n    obj,\n    export_dir: str,\n    signatures=None,\n    options: [tf.saved_model.SaveOptions](https://www.tensorflow.org/api_docs/python/tf/saved_model/SaveOptions) = None\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [Using the SavedModel format](https://www.tensorflow.org/guide/saved_model)\n  * [Import a JAX model using JAX2TF](https://www.tensorflow.org/guide/jax2tf)\n  * [Advanced automatic differentiation](https://www.tensorflow.org/guide/advanced_autodiff)\n\n| \n  * [Save and load a model using a distribution strategy](https://www.tensorflow.org/tutorials/distribute/save_and_load)\n  * [Simple audio recognition: Recognizing keywords](https://www.tensorflow.org/tutorials/audio/simple_audio)\n  * [Distributed training with DTensors](https://www.tensorflow.org/tutorials/distribute/dtensor_ml_tutorial)\n  * [Recommending movies: retrieval](https://www.tensorflow.org/recommenders/examples/basic_retrieval)\n\n  \nThe `obj` must inherit from the \n#### Example usage:\n```\nclassAdder(tf.Module):\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])\n  defadd(self, x):\n    return x + x\n```\n```\nmodel = Adder()\ntf.saved_model.save(model, '/tmp/adder')\n```\n\nThe resulting SavedModel is then servable with an input named \"x\", a scalar with dtype float32.\n_Signatures_\nSignatures define the input and output types for a computation. The optional save `signatures` argument controls which methods in `obj` will be available to programs which consume `SavedModel`s, for example, serving APIs. Python functions may be decorated with [`@tf.function(input_signature=...)`](https://www.tensorflow.org/api_docs/python/tf/function) and passed as signatures directly, or lazily with a call to `get_concrete_function` on the method decorated with [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\n#### Example:\n```\nclassAdder(tf.Module):\n  @tf.function\n  defadd(self, x):\n    return x + x\n```\n```\nmodel = Adder()\ntf.saved_model.save(\n  model, '/tmp/adder',signatures=model.add.get_concrete_function(\n    tf.TensorSpec([], tf.float32)))\n```\n\nIf a [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) does not have an input signature and `get_concrete_function` is not called on that method, the function will not be directly callable in the restored SavedModel.\n#### Example:\n```\nclassAdder(tf.Module):\n  @tf.function\n  defadd(self, x):\n    return x + x\n```\n```\nmodel = Adder()\ntf.saved_model.save(model, '/tmp/adder')\nrestored = tf.saved_model.load('/tmp/adder')\nrestored.add(1.)\nTraceback (most recent call last):\nValueError: Found zero restored functions for caller function.\n```\n\nIf the `signatures` argument is omitted, `obj` will be searched for [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)-decorated methods. If exactly one traced [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is found, that method will be used as the default signature for the SavedModel. Else, any [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) attached to `obj` or its dependencies will be exported for use with [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load).\nWhen invoking a signature in an exported SavedModel, `Tensor` arguments are identified by name. These names will come from the Python function's argument names by default. They may be overridden by specifying a `name=...` argument in the corresponding [`tf.TensorSpec`](https://www.tensorflow.org/api_docs/python/tf/TensorSpec) object. Explicit naming is required if multiple `Tensor`s are passed through a single argument to the Python function.\nThe outputs of functions used as `signatures` must either be flat lists, in which case outputs will be numbered, or a dictionary mapping string keys to `Tensor`, in which case the keys will be used to name outputs.\nSignatures are available in objects returned by [`tf.saved_model.load`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) as a `.signatures` attribute. This is a reserved attribute: [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) on an object with a custom `.signatures` attribute will raise an exception.\n_Using [`tf.saved_model.save_`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)_with Keras models_\nWhile Keras has its own [saving and loading API](https://www.tensorflow.org/guide/keras/save_and_serialize), this function can be used to export Keras models. For example, exporting with a signature specified:\n```\nclassAdder(tf.keras.Model):\n  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n  defconcat(self, x):\n     return x + x\n```\n```\nmodel = Adder()\ntf.saved_model.save(model, '/tmp/adder')\n```\n\nExporting from a function without a fixed signature:\n```\nclassAdder(tf.keras.Model):\n  @tf.function\n  defconcat(self, x):\n     return x + x\n```\n```\nmodel = Adder()\ntf.saved_model.save(\n  model, '/tmp/adder',\n  signatures=model.concat.get_concrete_function(\n    tf.TensorSpec(shape=[], dtype=tf.string, name=\"string_input\")))\n```\n\n[`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) instances constructed from inputs and outputs already have a signature and so do not require a [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator or a `signatures` argument. If neither are specified, the model's forward pass is exported.\n```\nx = tf.keras.layers.Input((4,), name=\"x\")\ny = tf.keras.layers.Dense(5, name=\"out\")(x)\nmodel = tf.keras.Model(x, y)\ntf.saved_model.save(model, '/tmp/saved_model/')\n```\n\nThe exported SavedModel takes \"x\" with shape [None, 4] and returns \"out\" with shape [None, 5]\n_Variables and Checkpoints_\nVariables must be tracked by assigning them to an attribute of a tracked object or to an attribute of `obj` directly. TensorFlow objects (e.g. layers from [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers), optimizers from [`tf.train`](https://www.tensorflow.org/api_docs/python/tf/train)) track their variables automatically. This is the same tracking scheme that [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) uses, and an exported `Checkpoint` object may be restored as a training checkpoint by pointing [`tf.train.Checkpoint.restore`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restore) to the SavedModel's \"variables/\" subdirectory.\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) does not hard-code device annotations from outside the function body, instead of using the calling context's device. This means for example that exporting a model that runs on a GPU and serving it on a CPU will generally work, with some exceptions:\n  * [`tf.device`](https://www.tensorflow.org/api_docs/python/tf/device) annotations inside the body of the function will be hard-coded in the exported model; this type of annotation is discouraged.\n  * Device-specific operations, e.g. with \"cuDNN\" in the name or with device-specific layouts, may cause issues.\n  * For `ConcreteFunctions`, active distribution strategies will cause device placements to be hard-coded in the function.\n\n\nSavedModels exported with [`tf.saved_model.save`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)\n## Args  \n---  \n`obj` |  A trackable object (e.g. tf.Module or tf.train.Checkpoint) to export.   \n`export_dir` |  A directory in which to write the SavedModel.   \n`signatures` |  Optional, one of three types:\n  * A [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) with an input signature specified, which will use the default serving signature key.\n  * The result of `f.get_concrete_function` on a [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)-decorated function `f`, in which case `f` will be used to generate a signature for the SavedModel under the default serving signature key.\n  * A dictionary, which maps signature keys to either [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) instances with input signatures or concrete functions. Keys of such a dictionary may be arbitrary strings, but will typically be from the `tf.saved_model.signature_constants` module. \n\n  \n`options` |  [`tf.saved_model.SaveOptions`](https://www.tensorflow.org/api_docs/python/tf/saved_model/SaveOptions) object for configuring save options.   \n## Raises  \n---  \n`ValueError` |  If `obj` is not trackable.   \n## eager compatibility\nNot well supported when graph building. From TensorFlow 1.x, [`tf.compat.v1.enable_eager_execution()`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/enable_eager_execution) should run first. Calling tf.saved_model.save in a loop when graph building from TensorFlow 1.x will add new save operations to the default graph each iteration.\nMay not be called from within a function body.\n",
  "https://www.tensorflow.org/api_docs/python/tf/summary": "Public API for tf._api.v2.summary namespace\n## Modules\n[`experimental`](https://www.tensorflow.org/api_docs/python/tf/summary/experimental) module: Public API for tf._api.v2.summary.experimental namespace\n## Classes\n[`class SummaryWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/SummaryWriter): Interface representing a stateful summary writer object.\n## Functions\n[`audio(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/audio): Write an audio summary.\n[`create_file_writer(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/create_file_writer): Creates a summary file writer for the given log directory.\n[`create_noop_writer(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/create_noop_writer): Returns a summary writer that does nothing.\n[`flush(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/flush): Forces summary writer to send any buffered data to storage.\n[`graph(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/graph): Writes a TensorFlow graph summary.\n[`histogram(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram): Write a histogram summary.\n[`image(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/image): Write an image summary.\n[`record_if(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/record_if): Sets summary recording on or off per the provided boolean value.\n[`scalar(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar): Write a scalar summary.\n[`should_record_summaries(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/should_record_summaries): Returns boolean Tensor which is True if summaries will be recorded.\n[`text(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/text): Write a text summary.\n[`trace_export(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/trace_export): Stops and exports the active trace as a Summary and/or profile file.\n[`trace_off(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/trace_off): Stops the current trace and discards any collected information.\n[`trace_on(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/trace_on): Starts a trace to record computation graphs and profiling information.\n[`write(...)`](https://www.tensorflow.org/api_docs/python/tf/summary/write): Writes a generic summary to the default SummaryWriter if one exists.\n",
  "https://www.tensorflow.org/api_docs/python/tf/train/Example": "An `Example` is a standard proto storing data for training and inference.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)\n### Used in the notebooks\nUsed in the guide | Used in the tutorials  \n---|---  \n  * [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)\n\n| \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [TensorFlow Ranking Keras pipeline for distributed training](https://www.tensorflow.org/ranking/tutorials/ranking_dnn_distributed)\n  * [FaceSSD Fairness Indicators Example Colab](https://www.tensorflow.org/responsible_ai/fairness_indicators/tutorials/Facessd_Fairness_Indicators_Example_Colab)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n\n  \nAn `Example` proto is a representation of the following python type:\n```\nDict[str,\n     Union[List[bytes],\n           List[int64],\n           List[float]]]\n\n```\n\nIt contains a key-value store [`Example.features`](https://www.tensorflow.org/api_docs/python/tf/train/Example#features) where each key (string) maps to a [`tf.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature) message which contains a fixed-type list. This flexible and compact format allows the storage of large amounts of typed data, but requires that the data shape and use be determined by the configuration files and parsers that are used to read and write this format (refer to [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) for details).\n```\nfromgoogle.protobufimport text_format\nexample = text_format.Parse('''\n  features {\n    feature {key: \"my_feature\"\n             value {int64_list {value: [1, 2, 3, 4]} } }\n  }''',\n  tf.train.Example())\n```\n\nUse [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) to extract tensors from a serialized `Example` proto:\n```\ntf.io.parse_example(\n    example.SerializeToString(),\n    features = {'my_feature': tf.io.RaggedFeature(dtype=tf.int64)})\n{'my_feature': <tf.Tensor: shape=(4,), dtype=float32,\n                           numpy=array([1, 2, 3, 4], dtype=int64)>}\n```\n\nWhile the list of keys, and the contents of each key _could_ be different for every `Example`, TensorFlow expects a fixed list of keys, each with a fixed `tf.dtype`. A conformant `Example` dataset obeys the following conventions:\n  * If a Feature `K` exists in one example with data type `T`, it must be of type `T` in all other examples when present. It may be omitted.\n  * The number of instances of Feature `K` list data may vary across examples, depending on the requirements of the model.\n  * If a Feature `K` doesn't exist in an example, a `K`-specific default will be used, if configured.\n  * If a Feature `K` exists in an example but contains no items, the intent is considered to be an empty tensor and no default will be used.\n\n\n## Attributes  \n---  \n`features` |  `Features features`\n",
  "https://www.tensorflow.org/api_docs/python/tf/summary/scalar": "Write a scalar summary.  \n```\ntf.summary.scalar(\n    name, data, step=None, description=None\n)\n\n```\n\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [pix2pix: Image-to-image translation with a conditional GAN](https://www.tensorflow.org/tutorials/generative/pix2pix)\n  * [TensorBoard Scalars: Logging training metrics in Keras](https://www.tensorflow.org/tensorboard/scalars_and_keras)\n  * [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started)\n  * [Migrating tf.summary usage to TF 2.x](https://www.tensorflow.org/tensorboard/migrate)\n\n  \nSee also [`tf.summary.image`](https://www.tensorflow.org/api_docs/python/tf/summary/image), [`tf.summary.histogram`](https://www.tensorflow.org/api_docs/python/tf/summary/histogram), [`tf.summary.SummaryWriter`](https://www.tensorflow.org/api_docs/python/tf/summary/SummaryWriter).\nWrites simple numeric values for later analysis in TensorBoard. Writes go to the current default summary writer. Each summary point is associated with an integral `step` value. This enables the incremental logging of time series data. A common usage of this API is to log loss during training to produce a loss curve.\n#### For example:\n```\ntest_summary_writer = tf.summary.create_file_writer('test/logdir')\nwith test_summary_writer.as_default():\n    tf.summary.scalar('loss', 0.345, step=1)\n    tf.summary.scalar('loss', 0.234, step=2)\n    tf.summary.scalar('loss', 0.123, step=3)\n\n```\n\nMultiple independent time series may be logged by giving each series a unique `name` value.\nSee [Get started with TensorBoard](https://www.tensorflow.org/tensorboard/get_started) for more examples of effective usage of [`tf.summary.scalar`](https://www.tensorflow.org/api_docs/python/tf/summary/scalar).\nIn general, this API expects that data points are logged with a monotonically increasing step value. Duplicate points for a single step or points logged out of order by step are not guaranteed to display as desired in TensorBoard.\n## Arguments  \n---  \n`name` |  A name for this summary. The summary tag used for TensorBoard will be this name prefixed by any active name scopes.   \n`data` |  A real numeric scalar value, convertible to a `float32` Tensor.   \n`step` |  Explicit `int64`-castable monotonic step value for this summary. If omitted, this defaults to [`tf.summary.experimental.get_step()`](https://www.tensorflow.org/api_docs/python/tf/summary/experimental/get_step), which must not be None.   \n`description` |  Optional long-form description for this summary, as a constant `str`. Markdown is supported. Defaults to empty.   \n## Returns  \n---  \nTrue on success, or false if no summary was written because no default summary writer was available.   \n## Raises  \n---  \n`ValueError` |  if a default writer exists, but no step was provided and [`tf.summary.experimental.get_step()`](https://www.tensorflow.org/api_docs/python/tf/summary/experimental/get_step) is None. \n",
  "https://www.tensorflow.org/api_docs/python/tf/train/FloatList": "Used in [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos. Holds a list of floats.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.train.FloatList`](https://www.tensorflow.org/api_docs/python/tf/train/FloatList)\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n\n  \nAn `Example` proto is a representation of the following python type:\n```\nDict[str,\n     Union[List[bytes],\n           List[int64],\n           List[float]]]\n\n```\n\nThis proto implements the `List[float]` portion.\n```\nfromgoogle.protobufimport text_format\nexample = text_format.Parse('''\n  features {\n    feature {key: \"my_feature\"\n             value {float_list {value: [1., 2., 3., 4. ]} } }\n  }''',\n  tf.train.Example())\nexample.features.feature['my_feature'].float_list.value\n[1.0, 2.0, 3.0, 4.0]\n```\n\nUse [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) to extract tensors from a serialized `Example` proto:\n```\ntf.io.parse_example(\n    example.SerializeToString(),\n    features = {'my_feature': tf.io.RaggedFeature(dtype=tf.float32)})\n{'my_feature': <tf.Tensor: shape=(4,), dtype=float32,\n                           numpy=array([1., 2., 3., 4.], dtype=float32)>}\n```\n\nSee the [`tf.train.Example`](https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample) guide for usage details.\n## Attributes  \n---  \n`value` |  `repeated float value`\n",
  "https://www.tensorflow.org/api_docs/python/tf/train/Feature": "Used in [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos. Contains a list of values.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.train.Feature`](https://www.tensorflow.org/api_docs/python/tf/train/Feature)\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n\n  \nAn `Example` proto is a representation of the following python type:\n```\nDict[str,\n     Union[List[bytes],\n           List[int64],\n           List[float]]]\n\n```\n\nThis proto implements the `Union`.\nThe contained list can be one of three types:\n\n```\nint_feature = tf.train.Feature(\n    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))\nfloat_feature = tf.train.Feature(\n    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))\nbytes_feature = tf.train.Feature(\n    bytes_list=tf.train.BytesList(value=[b\"abc\", b\"1234\"]))\nexample = tf.train.Example(\n    features=tf.train.Features(feature={\n        'my_ints': int_feature,\n        'my_floats': float_feature,\n        'my_bytes': bytes_feature,\n    }))\n```\n\nUse [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) to extract tensors from a serialized `Example` proto:\n```\ntf.io.parse_example(\n    example.SerializeToString(),\n    features = {\n        'my_ints': tf.io.RaggedFeature(dtype=tf.int64),\n        'my_floats': tf.io.RaggedFeature(dtype=tf.float32),\n        'my_bytes': tf.io.RaggedFeature(dtype=tf.string)})\n{'my_bytes': <tf.Tensor: shape=(2,), dtype=string,\n                         numpy=array([b'abc', b'1234'], dtype=object)>,\n 'my_floats': <tf.Tensor: shape=(4,), dtype=float32,\n                          numpy=array([1., 2., 3., 4.], dtype=float32)>,\n 'my_ints': <tf.Tensor: shape=(4,), dtype=int64,\n                        numpy=array([1, 2, 3, 4])>}\n```\n\n## Attributes  \n---  \n`bytes_list` |  `BytesList bytes_list`  \n`float_list` |  `FloatList float_list`  \n`int64_list` |  `Int64List int64_list`\n",
  "https://www.tensorflow.org/api_docs/python/tf/train/BytesList": "Used in [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos. Holds a list of byte-strings.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.train.BytesList`](https://www.tensorflow.org/api_docs/python/tf/train/BytesList)\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n  * [Preprocessing data with TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/transform/census)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n\n  \nAn `Example` proto is a representation of the following python type:\n```\nDict[str,\n     Union[List[bytes],\n           List[int64],\n           List[float]]]\n\n```\n\nThis proto implements the `List[bytes]` portion.\n```\nfromgoogle.protobufimport text_format\nexample = text_format.Parse('''\n  features {\n    feature {key: \"my_feature\"\n             value {bytes_list {value: ['abc', '12345' ]} } }\n  }''',\n  tf.train.Example())\nexample.features.feature['my_feature'].bytes_list.value\n[\"abc\", \"12345\"]\n```\n\nUse [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) to extract tensors from a serialized `Example` proto:\n```\ntf.io.parse_example(\n    example.SerializeToString(),\n    features = {'my_feature': tf.io.RaggedFeature(dtype=tf.string)})\n{'my_feature': <tf.Tensor: shape=(2,), dtype=string,\n                           numpy=array([b'abc', b'12345'], dtype=object)>}\n```\n\nSee the [`tf.train.Example`](https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample) guide for usage details.\n## Attributes  \n---  \n`value` |  `repeated bytes value`\n",
  "https://www.tensorflow.org/api_docs/python/tf/train/Int64List": "Used in [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos. Holds a list of Int64s.  \nView aliases\n**Compat aliases for migration**\nSee [Migration guide](https://www.tensorflow.org/guide/migrate) for more details.\n[`tf.compat.v1.train.Int64List`](https://www.tensorflow.org/api_docs/python/tf/train/Int64List)\n### Used in the notebooks\nUsed in the tutorials  \n---  \n  * [TFRecord and tf.train.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n  * [Feature Engineering using TFX Pipeline and TensorFlow Transform](https://www.tensorflow.org/tfx/tutorials/tfx/penguin_tft)\n  * [Graph regularization for sentiment classification using synthesized graphs](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_lstm_imdb)\n  * [Graph-based Neural Structured Learning in TFX](https://www.tensorflow.org/tfx/tutorials/tfx/neural_structured_learning)\n\n  \nAn `Example` proto is a representation of the following python type:\n```\nDict[str,\n     Union[List[bytes],\n           List[int64],\n           List[float]]]\n\n```\n\nThis proto implements the `List[int64]` portion.\n```\nfromgoogle.protobufimport text_format\nexample = text_format.Parse('''\n  features {\n    feature {key: \"my_feature\"\n             value {int64_list {value: [1, 2, 3, 4]} } }\n  }''',\n  tf.train.Example())\nexample.features.feature['my_feature'].int64_list.value\n[1, 2, 3, 4]\n```\n\nUse [`tf.io.parse_example`](https://www.tensorflow.org/api_docs/python/tf/io/parse_example) to extract tensors from a serialized `Example` proto:\n```\ntf.io.parse_example(\n    example.SerializeToString(),\n    features = {'my_feature': tf.io.RaggedFeature(dtype=tf.int64)})\n{'my_feature': <tf.Tensor: shape=(4,), dtype=float32,\n                           numpy=array([1, 2, 3, 4], dtype=int64)>}\n```\n\nSee the [`tf.train.Example`](https://www.tensorflow.org/tutorials/load_data/tfrecord#tftrainexample) guide for usage details.\n## Attributes  \n---  \n`value` |  `repeated int64 value`\n",
  "https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation": "[View on TensorFlow.org](https://www.tensorflow.org/federated/tutorials/federated_learning_for_text_generation)  \n---  \nThis tutorial builds on the concepts in the [Federated Learning for Image Classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) tutorial, and demonstrates several other useful approaches for federated learning.\nIn particular, we load a previously trained Keras model, and refine it using federated training on a (simulated) decentralized dataset. This is practically important for several reasons . The ability to use serialized models makes it easy to mix federated learning with other ML approaches. Further, this allows use of an increasing range of pre-trained models --- for example, training language models from scratch is rarely necessary, as numerous pre-trained models are now widely available (see, e.g., [TF Hub](https://www.tensorflow.org/hub)). Instead, it makes more sense to start from a pre-trained model, and refine it using Federated Learning, adapting to the particular characteristics of the decentralized data for a particular application.\nFor this tutorial, we start with a RNN that generates ASCII characters, and refine it via federated learning. We also show how the final weights can be fed back to the original Keras model, allowing easy evaluation and text generation using standard tools.\n```\n# @test {\"skip\": true}\npip\n```\n```\nimportcollections\nimportfunctools\nimportos\nimporttime\n\nimportnumpyasnp\nimporttensorflowastf\nimporttensorflow_federatedastff\n\nnp.random.seed(0)\n\n# Test that TFF is working:\ntff.tensorflow_computation(lambda: 'Hello, World!')()\n\n```\n```\nb'Hello, World!'\n\n```\n\n## Load a pre-trained model\nWe load a model that was pre-trained following the TensorFlow tutorial [Text generation using a RNN with eager execution](https://www.tensorflow.org/tutorials/sequences/text_generation). However, rather than training on \nOther than expanding the vocabulary, we didn't modify the original tutorial, so this initial model isn't state-of-the-art, but it produces reasonable predictions and is sufficient for our tutorial purposes. The final model was saved with [`tf.keras.models.save_model(include_optimizer=False)`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).\nWe will use federated learning to fine-tune this model for Shakespeare in this tutorial, using a federated version of the data provided by TFF.\n### Generate the vocab lookup tables\n```\n# A fixed vocabularly of ASCII chars that occur in the works of Shakespeare and Dickens:\nvocab=list(\n'dhlptx@DHLPTX'\n' $(,048cgkoswCGKOSW[_#\\'/37;?bfjnrvzBFJNRVZ\"&*.26:\\naeimquyAEIMQUY]!%)-159\\r'\n)\n\n# Creating a mapping from unique characters to indices\nchar2idx={u:ifori,uinenumerate(vocab)}\nidx2char=np.array(vocab)\n\n```\n\n### Load the pre-trained model and generate some text\n```\ndefload_model(batch_size):\nurls={\n1:'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel',\n8:'https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel',\n}\nassertbatch_sizeinurls,'batch_size must be in '+str(urls.keys())\nurl=urls[batch_size]\nlocal_file=tf.keras.utils.get_file(os.path.basename(url),origin=url)\nreturntf.keras.models.load_model(local_file,compile=False)\n\n```\n```\ndefgenerate_text(model,start_string):\n#Fromhttps://www.tensorflow.org/tutorials/sequences/text_generation\nnum_generate=200\ninput_eval=[char2idx[s]forsinstart_string]\ninput_eval=tf.expand_dims(input_eval,0)\ntext_generated=[]\ntemperature=1.0\n\nmodel.reset_states()\nforiinrange(num_generate):\npredictions=model(input_eval)\npredictions=tf.squeeze(predictions,0)\npredictions=predictions/temperature\npredicted_id=tf.random.categorical(predictions,num_samples=1)[\n        -1, 0\n].numpy()\ninput_eval=tf.expand_dims([predicted_id],0)\ntext_generated.append(idx2char[predicted_id])\n\nreturnstart_string+''.join(text_generated)\n\n```\n```\n# Text generation requires a batch_size=1 model.\nkeras_model_batch1=load_model(batch_size=1)\nprint(\ngenerate_text(keras_model_batch1,'What of TensorFlow Federated, you ask? ')\n)\n\n```\n```\nDownloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch1.kerasmodel\n16193984/16193984 [==============================] - 0s 0us/step\nWhat of TensorFlow Federated, you ask? Same yee you? Have I so,\noften games in a man who rode one knee over his friend, with the\nstone faces of the dread prisoners, dud a tender mastery. They\nare not alive is infirmed us--to ever resume\n\n```\n\n## Load and Preprocess the Federated Shakespeare Data\nThe [`tff.simulation.datasets`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets) package provides a variety of datasets that are split into \"clients\", where each client corresponds to a dataset on a particular device that might participate in federated learning.\nThese datasets provide realistic non-IID data distributions that replicate in simulation the challenges of training on real decentralized data. Some of the pre-processing of this data was done using tools from the \n```\ntrain_data,test_data=tff.simulation.datasets.shakespeare.load_data()\n\n```\n\nThe datasets provided by [`shakespeare.load_data()`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/shakespeare/load_data) consist of a sequence of string `Tensors`, one for each line spoken by a particular character in a Shakespeare play. The client keys consist of the name of the play joined with the name of the character, so for example `MUCH_ADO_ABOUT_NOTHING_OTHELLO` corresponds to the lines for the character Othello in the play _Much Ado About Nothing_. Note that in a real federated learning scenario clients are never identified or tracked by ids, but for simulation it is useful to work with keyed datasets.\nHere, for example, we can look at some data from King Lear:\n```\n# Here the play is \"The Tragedy of King Lear\" and the character is \"King\".\nraw_example_dataset = train_data.create_tf_dataset_for_client(\n    'THE_TRAGEDY_OF_KING_LEAR_KING'\n)\n# To allow for future extensions, each entry x\n# is an OrderedDict with a single key 'snippets' which contains the text.\nfor x in raw_example_dataset.take(2):\n  print(x['snippets'])\n\n```\n```\ntf.Tensor(b'', shape=(), dtype=string)\ntf.Tensor(b'What?', shape=(), dtype=string)\n\n```\n\nWe now use [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) transformations to prepare this data for training the char RNN loaded above. \n```\n# Input pre-processing parameters\nSEQ_LENGTH = 100\nBATCH_SIZE = 8\nBUFFER_SIZE = 100  # For dataset shuffling\n\n```\n```\n# Construct a lookup table to map string chars to indexes,\n# using the vocab loaded above:\ntable=tf.lookup.StaticHashTable(\ntf.lookup.KeyValueTensorInitializer(\nkeys=vocab,values=tf.constant(list(range(len(vocab))),dtype=tf.int64)\n),\ndefault_value=0,\n)\n\n\ndefto_ids(x):\ns=tf.reshape(x['snippets'],shape=[1])\nchars=tf.strings.bytes_split(s).values\nids=table.lookup(chars)\nreturnids\n\n\ndefsplit_input_target(chunk):\ninput_text=tf.map_fn(lambdax:x[:-1],chunk)\ntarget_text=tf.map_fn(lambdax:x[1:],chunk)\nreturn(input_text,target_text)\n\n\ndefpreprocess(dataset):\nreturn(\n# Map ASCII chars to int64 indexes using the vocab\ndataset.map(to_ids)\n# Split into individual chars\n.unbatch()\n# Form example sequences of SEQ_LENGTH +1\n.batch(SEQ_LENGTH+1,drop_remainder=True)\n# Shuffle and form minibatches\n.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True)\n# And finally split into (input, target) tuples,\n# each of length SEQ_LENGTH.\n.map(split_input_target)\n)\n\n```\n\nNote that in the formation of the original sequences and in the formation of batches above, we use `drop_remainder=True` for simplicity. This means that any characters (clients) that don't have at least `(SEQ_LENGTH + 1) * BATCH_SIZE` chars of text will have empty datasets. A typical approach to address this would be to pad the batches with a special token, and then mask the loss to not take the padding tokens into account.\nThis would complicate the example somewhat, so for this tutorial we only use full batches, as in the [standard tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation). However, in the federated setting this issue is more significant, because many users might have small datasets.\nNow we can preprocess our `raw_example_dataset`, and check the types:\n```\nexample_dataset = preprocess(raw_example_dataset)\nprint(example_dataset.element_spec)\n\n```\n```\n(TensorSpec(shape=(8, 100), dtype=tf.int64, name=None), TensorSpec(shape=(8, 100), dtype=tf.int64, name=None))\n\n```\n\n## Compile the model and test on the preprocessed data\nWe loaded an uncompiled keras model, but in order to run `keras_model.evaluate`, we need to compile it with a loss and metrics. We will also compile in an optimizer, which will be used as the on-device optimizer in Federated Learning.\nThe original tutorial didn't have char-level accuracy (the fraction of predictions where the highest probability was put on the correct next char). This is a useful metric, so we add it. However, we need to define a new metric class for this because our predictions have rank 3 (a vector of logits for each of the `BATCH_SIZE * SEQ_LENGTH` predictions), and `SparseCategoricalAccuracy` expects only rank 2 predictions.\n```\nclassFlattenedCategoricalAccuracy(tf.keras.metrics.SparseCategoricalAccuracy):\n\ndef__init__(self,name='accuracy',dtype=tf.float32):\nsuper().__init__(name,dtype=dtype)\n\ndefupdate_state(self,y_true,y_pred,sample_weight=None):\ny_true=tf.reshape(y_true,[-1,1])\ny_pred=tf.reshape(y_pred,[-1,len(vocab),1])\nreturnsuper().update_state(y_true,y_pred,sample_weight)\n\n```\n\nNow we can compile a model, and evaluate it on our `example_dataset`.\n```\nBATCH_SIZE=(\n8# The training and eval batch size for the rest of this tutorial.\n)\nkeras_model=load_model(batch_size=BATCH_SIZE)\nkeras_model.compile(\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\nmetrics=[FlattenedCategoricalAccuracy()],\n)\n\n# Confirm that loss is much lower on Shakespeare than on random data\nloss,accuracy=keras_model.evaluate(example_dataset.take(5),verbose=0)\nprint(\n'Evaluating on an example Shakespeare character: {a:3f}'.format(a=accuracy)\n)\n\n# As a sanity check, we can construct some completely random data, where we expect\n# the accuracy to be essentially random:\nrandom_guessed_accuracy=1.0/len(vocab)\nprint(\n'Expected accuracy for random guessing: {a:.3f}'.format(\na=random_guessed_accuracy\n)\n)\nrandom_indexes=np.random.randint(\nlow=0,high=len(vocab),size=1*BATCH_SIZE*(SEQ_LENGTH+1)\n)\ndata=collections.OrderedDict(\nsnippets=tf.constant(''.join(np.array(vocab)[random_indexes]),shape=[1,1])\n)\nrandom_dataset=preprocess(tf.data.Dataset.from_tensor_slices(data))\nloss,accuracy=keras_model.evaluate(random_dataset,steps=10,verbose=0)\nprint('Evaluating on completely random data: {a:.3f}'.format(a=accuracy))\n\n```\n```\nDownloading data from https://storage.googleapis.com/tff-models-public/dickens_rnn.batch8.kerasmodel\n16193984/16193984 [==============================] - 0s 0us/step\nEvaluating on an example Shakespeare character: 0.45.000\nExpected accuracy for random guessing: 0.012\nEvaluating on completely random data: 0.011\n\n```\n\n## Fine-tune the model with Federated Learning\nTFF serializes all TensorFlow computations so they can potentially be run in a non-Python environment (even though at the moment, only a simulation runtime implemented in Python is available). Even though we are running in eager mode, (TF 2.0), currently TFF serializes TensorFlow computations by constructing the necessary ops inside the context of a \"`with tf.Graph.as_default()`\" statement. Thus, we need to provide a function that TFF can use to introduce our model into a graph it controls. We do this as follows:\n```\n# Clone the keras_model inside `create_tff_model()`, which TFF will\n# call to produce a new copy of the model inside the graph that it will\n# serialize. Note: we want to construct all the necessary objects we'll need\n# _inside_ this method.\ndefcreate_tff_model():\n# TFF uses an `input_spec` so it knows the types and shapes\n# that your model expects.\ninput_spec=example_dataset.element_spec\nkeras_model_clone=tf.keras.models.clone_model(keras_model)\nreturntff.learning.models.from_keras_model(\nkeras_model_clone,\ninput_spec=input_spec,\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\nmetrics=[FlattenedCategoricalAccuracy()],\n)\n\n```\n\nNow we are ready to construct a Federated Averaging iterative process, which we will use to improve the model (for details on the Federated Averaging algorithm, see the paper \nWe use a compiled Keras model to perform standard (non-federated) evaluation after each round of federated training. This is useful for research purposes when doing simulated federated learning and there is a standard test dataset. \nIn a realistic production setting this same technique might be used to take models trained with federated learning and evaluate them on a centralized benchmark dataset for testing or quality assurance purposes.\n```\n# This command builds all the TensorFlow graphs and serializes them:\nfed_avg = tff.learning.algorithms.build_weighted_fed_avg(\n    model_fn=create_tff_model,\n    client_optimizer_fn=tff.learning.optimizers.build_sgdm(learning_rate=0.5),\n)\n\n```\n\nHere is the simplest possible loop, where we run federated averaging for one round on a single client on a single batch:\n```\nstate = fed_avg.initialize()\nresult = fed_avg.next(state, [example_dataset.take(5)])\nstate = result.state\ntrain_metrics = result.metrics['client_work']['train']\nprint(\n    'loss={l:.3f}, accuracy={a:.3f}'.format(\n        l=train_metrics['loss'], a=train_metrics['accuracy']\n    )\n)\n\n```\n```\nloss=4.399, accuracy=0.139\n\n```\n\nNow let's write a slightly more interesting training and evaluation loop.\nSo that this simulation still runs relatively quickly, we train on the same three clients each round, only considering two minibatches for each.\n```\ndef data(client, source=train_data):\n  return preprocess(source.create_tf_dataset_for_client(client)).take(5)\n\n\nclients = [\n    'ALL_S_WELL_THAT_ENDS_WELL_CELIA',\n    'MUCH_ADO_ABOUT_NOTHING_OTHELLO',\n]\n\ntrain_datasets = [data(client) for client in clients]\n\n# We concatenate the test datasets for evaluation with Keras by creating a\n# Dataset of Datasets, and then identity flat mapping across all the examples.\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n    [data(client, test_data) for client in clients]\n).flat_map(lambda x: x)\n\n```\n\nThe initial state of the model produced by `fed_avg.initialize()` is based on the random initializers for the Keras model, not the weights that were loaded, since `clone_model()` does not clone the weights. To start training from a pre-trained model, we set the model weights in the server state directly from the loaded model.\n```\nNUM_ROUNDS=5\n\n# The state of the FL server, containing the model and optimization state.\nstate=fed_avg.initialize()\n\n# Load our pre-trained Keras model weights into the global model state.\npre_trained_weights=tff.learning.models.ModelWeights(\ntrainable=[v.numpy()forvinkeras_model.trainable_weights],\nnon_trainable=[v.numpy()forvinkeras_model.non_trainable_weights],\n)\nstate=fed_avg.set_model_weights(state,pre_trained_weights)\n\n\ndefkeras_evaluate(state,round_num):\n# Take our global model weights and push them back into a Keras model to\n# use its standard `.evaluate()` method.\nkeras_model=load_model(batch_size=BATCH_SIZE)\nkeras_model.compile(\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\nmetrics=[FlattenedCategoricalAccuracy()],\n)\nmodel_weights=fed_avg.get_model_weights(state)\nmodel_weights.assign_weights_to(keras_model)\nloss,accuracy=keras_model.evaluate(example_dataset,steps=2,verbose=0)\nprint('\\tEval: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss,a=accuracy))\n\n\nforround_numinrange(NUM_ROUNDS):\nprint('Round {r}'.format(r=round_num))\nkeras_evaluate(state,round_num)\nresult=fed_avg.next(state,train_datasets)\nstate=result.state\ntrain_metrics=result.metrics['client_work']['train']\nprint(\n'\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(\nl=train_metrics['loss'],a=train_metrics['accuracy']\n)\n)\n\nprint('Final evaluation')\nkeras_evaluate(state,NUM_ROUNDS+1)\n\n```\n```\nRound 0\n    Eval: loss=3.171, accuracy=0.428\n    Train: loss=4.309, accuracy=0.098\nRound 1\n    Eval: loss=4.188, accuracy=0.185\n    Train: loss=4.037, accuracy=0.223\nRound 2\n    Eval: loss=3.948, accuracy=0.200\n    Train: loss=3.797, accuracy=0.228\nRound 3\n    Eval: loss=3.826, accuracy=0.179\n    Train: loss=3.662, accuracy=0.219\nRound 4\n    Eval: loss=3.723, accuracy=0.171\n    Train: loss=3.440, accuracy=0.245\nFinal evaluation\n    Eval: loss=3.599, accuracy=0.181\n\n```\n\nWith the default changes, we haven't done enough training to make a big difference, but if you train longer on more Shakespeare data, you should see a difference in the style of the text generated with the updated model:\n```\n# Set our newly trained weights back in the originally created model.\nkeras_model_batch1.set_weights([v.numpy() for v in keras_model.weights])\n# Text generation requires batch_size=1\nprint(\n    generate_text(keras_model_batch1, 'What of TensorFlow Federated, you ask? ')\n)\n\n```\n```\nWhat of TensorFlow Federated, you ask? She will be\nheard of; or whether they recovered her faltering place, that a great mark of\nbeing so final dark and distrustner the dearer to the chin, all\nstaftly towards him, or trot's in foot thro\n\n```\n\n## Suggested extensions\nThis tutorial is just the first step! Here are some ideas for how you might try extending this notebook:\n  * Write a more realistic training loop where you sample clients to train on randomly.\n  * Use \"`.repeat(NUM_EPOCHS)`\" on the client datasets to try multiple epochs of local training (e.g., as in [Federated Learning for Image Classification](https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification) which does this.\n  * Change the `compile()` command to experiment with using different optimization algorithms on the client.\n  * Try the `server_optimizer` argument to `build_weighted_fed_avg` to try different algorithms for applying the model updates on the server.\n\n\n",
  "https://www.tensorflow.org/federated/tutorials/sparse_federated_learning": "[View on TensorFlow.org](https://www.tensorflow.org/federated/tutorials/sparse_federated_learning)  \n---  \nThis tutorial shows how TFF can be used to train a very large model where each client device only downloads and updates a small part of the model, using [`tff.federated_select`](https://www.tensorflow.org/federated/api_docs/python/tff/federated_select) and sparse aggregation. While this tutorial is fairly self-contained, the [`tff.federated_select` tutorial](https://www.tensorflow.org/federated/tutorials/federated_select) and [custom FL algorithms tutorial](https://www.tensorflow.org/federated/tutorials/building_your_own_federated_learning_algorithm) provide good introductions to some of the techniques used here.\nConcretely, in this tutorial we consider logistic regression for multi-label classification, predicting which \"tags\" are associated with a text string based on a bag-of-words feature representation. Importantly, communication and client-side computation costs are controlled by a fixed constant (`MAX_TOKENS_SELECTED_PER_CLIENT`), and _do not_ scale with the overall vocabulary size, which could be extremely large in practical settings.\n```\npip\n```\n```\nimportcollections\nfromcollections.abcimport Callable\nimportitertools\n\nimportnumpyasnp\nimporttensorflowastf\nimporttensorflow_federatedastff\n\n```\n\nEach client will `federated_select` the rows of the model weights for at most this many unique tokens. This upper-bounds the size of the client's local model and the amount of server -> client (`federated_select`) and client - > server `(federated_aggregate`) communication performed.\nThis tutorial should still run correctly even if you set this as small as 1 (ensuring not all tokens from each client are selected) or to a large value, though model convergence may be effected.\n```\nMAX_TOKENS_SELECTED_PER_CLIENT = 6\n\n```\n\nWe also define a few constants for various types. For this colab, a **token** is an integer identifier for a particular word after parsing the dataset. \n```\n#Therearesomeconstraintsontypes\n#herethatwillrequiresomeexplicittypeconversions:\n#-`tff.federated_select`requiresint32\n#-`tf.SparseTensor`requiresint64indices.\nTOKEN_DTYPE=np.int64\nSELECT_KEY_DTYPE=np.int32\n\n#Typeforcountsoftokenoccurences.\nTOKEN_COUNT_DTYPE=np.int32\n\n#Asparsefeaturevectorcanbethoughtofasamap\n#fromTOKEN_DTYPEtoFEATURE_DTYPE.#Ourfeaturesare{0,1}indicators,sowecouldpotentially\n#usenp.int8asanoptimization.\nFEATURE_DTYPE=np.int32\n\n```\n\n# Setting up the problem: Dataset and Model\nWe construct a tiny toy dataset for easy experimentation in this tutorial. However, the format of the dataset is compatible with [Federated StackOverflow](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data), and the \n## Dataset parsing and pre-processing\n```\nNUM_OOV_BUCKETS=1\n\nBatchType=collections.namedtuple('BatchType',['tokens', 'tags'])\n\ndefbuild_to_ids_fn(word_vocab:list[str],\ntag_vocab:list[str])->Callable[[tf.Tensor],tf.Tensor]:\n\"\"\"Constructs a function mapping examples to sequences of token indices.\"\"\"\nword_table_values=np.arange(len(word_vocab),dtype=np.int64)\nword_table=tf.lookup.StaticVocabularyTable(\ntf.lookup.KeyValueTensorInitializer(word_vocab,word_table_values),\nnum_oov_buckets=NUM_OOV_BUCKETS)\n\ntag_table_values=np.arange(len(tag_vocab),dtype=np.int64)\ntag_table=tf.lookup.StaticVocabularyTable(\ntf.lookup.KeyValueTensorInitializer(tag_vocab,tag_table_values),\nnum_oov_buckets=NUM_OOV_BUCKETS)\n\ndefto_ids(example):\n\"\"\"Converts a Stack Overflow example to a bag-of-words/tags format.\"\"\"\nsentence=tf.strings.join([example['tokens'],example['title']],\nseparator=' ')\n\n#Werepresentthatlabel(outputtags)densely.\nraw_tags=example['tags']\ntags=tf.strings.split(raw_tags,sep='|')\ntags=tag_table.lookup(tags)\ntags,_=tf.unique(tags)\ntags=tf.one_hot(tags,len(tag_vocab)+NUM_OOV_BUCKETS)\ntags=tf.reduce_max(tags,axis=0)\n\n#WerepresentthefeaturesasaSparseTensorof{0,1}s.\nwords=tf.strings.split(sentence)\ntokens=word_table.lookup(words)\ntokens,_=tf.unique(tokens)\n#Note:Wecouldchoosetousethewordcountsasthefeaturevector\n#insteadofjust{0,1}values(seetf.unique_with_counts).\ntokens=tf.reshape(tokens,shape=(tf.size(tokens),1))\ntokens_st=tf.SparseTensor(\ntokens,\ntf.ones(tf.size(tokens),dtype=FEATURE_DTYPE),\ndense_shape=(len(word_vocab)+NUM_OOV_BUCKETS,))\ntokens_st=tf.sparse.reorder(tokens_st)\n\nreturnBatchType(tokens_st,tags)\n\nreturnto_ids\n\n```\n```\ndefbuild_preprocess_fn(word_vocab,tag_vocab):\n\n@tf.function\ndefpreprocess_fn(dataset):\nto_ids=build_to_ids_fn(word_vocab,tag_vocab)\n#We*don't*shuffleinordertomakethiscolabdeterministicfor\n#easiertestingandreproducibility.\n#Butreal-worldtrainingshoulduse`.shuffle()`.\nreturndataset.map(to_ids,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\nreturnpreprocess_fn\n\n```\n\n## A tiny toy dataset\nWe construct a tiny toy dataset with a global vocabulary of 12 words and 3 clients. This tiny example is useful for testing edge cases (for example, we have two clients with less than `MAX_TOKENS_SELECTED_PER_CLIENT = 6` distinct tokens, and one with more) and developing the code.\nHowever, the real-world use cases of this approach would be global vocabularies of 10s of millions or more, with perhaps 1000s of distinct tokens appearing on each client. Because the format of the data is the same, the extension to more realistic testbed problems, e.g. the [`tff.simulation.datasets.stackoverflow.load_data()`](https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets/stackoverflow/load_data) dataset, should be straightforward.\nFirst, we define our word and tag vocabularies.\n```\n# Features\nFRUIT_WORDS = ['apple', 'orange', 'pear', 'kiwi']\nVEGETABLE_WORDS = ['carrot', 'broccoli', 'arugula', 'peas']\nFISH_WORDS = ['trout', 'tuna', 'cod', 'salmon']\nWORD_VOCAB = FRUIT_WORDS + VEGETABLE_WORDS + FISH_WORDS\n\n# Labels\nTAG_VOCAB = ['FRUIT', 'VEGETABLE', 'FISH']\n\n```\n\nNow, we create 3 clients with small local datasets. If you are running this tutorial in colab, it may be useful to use the \"mirror cell in tab\" feature to pin this cell and its output in order to interpret/check the output of the functions developed below.\n```\npreprocess_fn=build_preprocess_fn(WORD_VOCAB,TAG_VOCAB)\n\n\ndefmake_dataset(raw):\nd=tf.data.Dataset.from_tensor_slices(\n# Matches the StackOverflow formatting\ncollections.OrderedDict(\ntokens=tf.constant([t[0]fortinraw]),\ntags=tf.constant([t[1]fortinraw]),\ntitle=[''for_inraw]))\nd=preprocess_fn(d)\nreturnd\n\n\n# 4 distinct tokens\nCLIENT1_DATASET=make_dataset([\n('apple orange apple orange','FRUIT'),\n('carrot trout','VEGETABLE|FISH'),\n('orange apple','FRUIT'),\n('orange','ORANGE|CITRUS')# 2 OOV tag\n])\n\n# 6 distinct tokens\nCLIENT2_DATASET=make_dataset([\n('pear cod','FRUIT|FISH'),\n('arugula peas','VEGETABLE'),\n('kiwi pear','FRUIT'),\n('sturgeon','FISH'),# OOV word\n('sturgeon bass','FISH')# 2 OOV words\n])\n\n# A client with all possible words & tags (13 distinct tokens).\n# With MAX_TOKENS_SELECTED_PER_CLIENT = 6, we won't download the model\n# slices for all tokens that occur on this client.\nCLIENT3_DATASET=make_dataset([\n(' '.join(WORD_VOCAB+['oovword']),'|'.join(TAG_VOCAB)),\n# Mathe the OOV token and 'salmon' occur in the largest number\n# of examples on this client:\n('salmon oovword','FISH|OOVTAG')\n])\n\nprint('Word vocab')\nfori,wordinenumerate(WORD_VOCAB):\nprint(f'{i:2d} {word}')\n\nprint('\\nTag vocab')\nfori,taginenumerate(TAG_VOCAB):\nprint(f'{i:2d} {tag}')\n\n```\n```\nWord vocab\n 0 apple\n 1 orange\n 2 pear\n 3 kiwi\n 4 carrot\n 5 broccoli\n 6 arugula\n 7 peas\n 8 trout\n 9 tuna\n10 cod\n11 salmon\n\nTag vocab\n 0 FRUIT\n 1 VEGETABLE\n 2 FISH\n\n```\n\nDefine constants for the raw numbers of input features (tokens/words) and labels (post tags). Our actual input/output spaces are `NUM_OOV_BUCKETS = 1` larger because we add an OOV token / tag.\n```\nNUM_WORDS = len(WORD_VOCAB) \nNUM_TAGS = len(TAG_VOCAB)\n\nWORD_VOCAB_SIZE = NUM_WORDS + NUM_OOV_BUCKETS\nTAG_VOCAB_SIZE = NUM_TAGS + NUM_OOV_BUCKETS\n\n```\n\nCreate batched versions of the datasets, and individual batches, which will be useful in testing code as we go.\n```\nbatched_dataset1 = CLIENT1_DATASET.batch(2)\nbatched_dataset2 = CLIENT2_DATASET.batch(3)\nbatched_dataset3 = CLIENT3_DATASET.batch(2)\n\nbatch1 = next(iter(batched_dataset1))\nbatch2 = next(iter(batched_dataset2))\nbatch3 = next(iter(batched_dataset3))\n\n```\n\n## Define a model with sparse inputs\nWe use a simple independent logistic regression model for each tag.\n```\ndefcreate_logistic_model(word_vocab_size:int,vocab_tags_size:int):\n\nmodel=tf.keras.models.Sequential([\ntf.keras.layers.InputLayer(input_shape=(word_vocab_size,),sparse=True),\ntf.keras.layers.Dense(\nvocab_tags_size,\nactivation='sigmoid',\nkernel_initializer=tf.keras.initializers.zeros,\n#Forsimplicity,don'tuseabiasvector;thismeansthemodel\n#isasingletensor,andweonlyneedsparseaggregationof\n#theper-tokenslicesofthemodel.Generalizingtoalsohandle\n#othermodelweightsthatarefullyupdated#(non-densebroadcastandaggregate)wouldbeagoodexercise.\nuse_bias=False),\n])\n\nreturnmodel\n\n```\n\nLet's make sure it works, first by making predictions:\n```\nmodel = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\np = model.predict(batch1.tokens)\nprint(p)\n\n```\n```\n[[0.5 0.5 0.5 0.5]\n [0.5 0.5 0.5 0.5]]\n\n```\n\nAnd some simple centralized training:\n```\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.001),\n              loss=tf.keras.losses.BinaryCrossentropy())\nmodel.train_on_batch(batch1.tokens, batch1.tags)\n\n```\n\n# Building blocks for the federated computation\nWe will implement a simple version of the \nWe use `M` as shorthand for `MAX_TOKENS_SELECTED_PER_CLIENT`. At a high level, one round of training involves these steps:\n  1. Each participating client scans over its local dataset, parsing the input strings and mapping them to the correct tokens (int indexes). This requires access to the global (large) dictionary (this could potentially be avoided using `U` unique tokens occur on device, we choose the `num_actual_tokens = min(U, M)` most frequent tokens to train.\n  2. The clients use `federated_select` to retrieve the model coefficients for the `num_actual_tokens` selected tokens from the server. Each model slice is a tensor of shape `(TAG_VOCAB_SIZE, )`, so the total data transmitted to the client is at most of size `TAG_VOCAB_SIZE * M` (see note below).\n  3. The clients construct a mapping `global_token -> local_token` where the local token (int index) is the index of the global token in the list of selected tokens.\n  4. The clients use a \"small\" version of the global model that only has coefficients for at most `M` tokens, from the range `[0, num_actual_tokens)`. The `global -> local` mapping is used to initialize the dense parameters of this model from the selected model slices.\n  5. Clients train their local model using SGD on data preprocessed with the `global -> local` mapping.\n  6. Clients turn the parameters of their local model into `IndexedSlices` updates using the `local -> global` mapping to index the rows. The server aggregates these updates using a sparse sum aggregation.\n  7. The server takes the (dense) result of the above aggregation, divides it by the number of clients participating, and applies the resulting average update to the global model.\n\n\nIn this section we construct the building blocks for these steps, which will then be combined in a final `federated_computation` that captures the full logic of one training round.\n### Count client tokens and decide which model slices to `federated_select`\nEach device needs to decide which \"slices\" of the model are relevant to its local training dataset. For our problem, we do this by (sparsely!) counting how many examples contain each token in the client training data set.\n```\n@tf.function\ndeftoken_count_fn(token_counts,batch):\n\"\"\"Adds counts from `batch` to the running `token_counts` sum.\"\"\"\n# Sum across the batch dimension.\nflat_tokens=tf.sparse.reduce_sum(\nbatch.tokens,axis=0,output_is_sparse=True)\nflat_tokens=tf.cast(flat_tokens,dtype=TOKEN_COUNT_DTYPE)\nreturntf.sparse.add(token_counts,flat_tokens)\n\n```\n```\n#Simpletests\n#Createtheinitialzerotokencountsusingemptytensors.\ninitial_token_counts=tf.SparseTensor(\nindices=tf.zeros(shape=(0,1),dtype=TOKEN_DTYPE),\nvalues=tf.zeros(shape=(0,),dtype=TOKEN_COUNT_DTYPE),\ndense_shape=(WORD_VOCAB_SIZE,))\n\nclient_token_counts=batched_dataset1.reduce(initial_token_counts,\ntoken_count_fn)\ntokens=tf.reshape(client_token_counts.indices,(-1,)).numpy()\nprint('tokens:',tokens)\nnp.testing.assert_array_equal(tokens,[0,1,4,8])\n#Thecountisthenumberof*examples*inwhichthetoken/word\n#occurs,notthetotalnumberofoccurences,sincewestillfeaturize\n#multipleoccurencesinthesameexampleasa\"1\".\ncounts=client_token_counts.values.numpy()\nprint('counts:',counts)\nnp.testing.assert_array_equal(counts,[2,3,1,1])\n\n```\n```\ntokens: [0 1 4 8]\ncounts: [2 3 1 1]\n\n```\n\nWe will select the model parameters corresponding to the `MAX_TOKENS_SELECTED_PER_CLIENT` most frequently occuring tokens on device. If fewer than this many tokens occur on device, we pad the list to enable the use of `federated_select`.\nNote that other strategies are possibly better, for example, randomly selecting tokens (perhaps based on their occurrence probability). This would ensure that all slices of the model (for which the client has data) have some chance of being updated.\n```\n@tf.function\ndefkeys_for_client(client_dataset,max_tokens_per_client):\n\"\"\"Computes a set of max_tokens_per_client keys.\"\"\"\ninitial_token_counts=tf.SparseTensor(\nindices=tf.zeros((0,1),dtype=TOKEN_DTYPE),\nvalues=tf.zeros((0,),dtype=TOKEN_COUNT_DTYPE),\ndense_shape=(WORD_VOCAB_SIZE,))\nclient_token_counts=client_dataset.reduce(initial_token_counts,\ntoken_count_fn)\n# Find the most-frequently occuring tokens\ntokens=tf.reshape(client_token_counts.indices,shape=(-1,))\ncounts=client_token_counts.values\nperm=tf.argsort(counts,direction='DESCENDING')\ntokens=tf.gather(tokens,perm)\ncounts=tf.gather(counts,perm)\nnum_raw_tokens=tf.shape(tokens)[0]\nactual_num_tokens=tf.minimum(max_tokens_per_client,num_raw_tokens)\nselected_tokens=tokens[:actual_num_tokens]\npaddings=[[0,max_tokens_per_client-tf.shape(selected_tokens)[0]]]\npadded_tokens=tf.pad(selected_tokens,paddings=paddings)\n# Make sure the type is statically determined\npadded_tokens=tf.reshape(padded_tokens,shape=(max_tokens_per_client,))\n\n# We will pass these tokens as keys into `federated_select`, which\n# requires SELECT_KEY_DTYPE=np.int32 keys.\npadded_tokens=tf.cast(padded_tokens,dtype=SELECT_KEY_DTYPE)\nreturnpadded_tokens,actual_num_tokens\n\n```\n```\n#Simpletest\n\n#Case1:actual_num_tokensmax_tokens_per_client\nselected_tokens,actual_num_tokens=keys_for_client(batched_dataset1,3)\nasserttf.size(selected_tokens)==3\nassertactual_num_tokens==3\n\n#Case2:actual_num_tokensmax_tokens_per_client\nselected_tokens,actual_num_tokens=keys_for_client(batched_dataset1,10)\nasserttf.size(selected_tokens)==10\nassertactual_num_tokens==4\n\n```\n\n### Map global tokens to local tokens\nThe above selection gives us a dense set of tokens in the range `[0, actual_num_tokens)` which we will use for the on-device model. However, the dataset we read has tokens from the much larger global vocabulary range `[0, WORD_VOCAB_SIZE)`. \nThus, we need to map the global tokens to their corresponding local tokens. The local token ids are simply given by the indexes into the `selected_tokens` tensor computed in the previous step.\n```\n@tf.function\ndefmap_to_local_token_ids(client_data,client_keys):\nglobal_to_local=tf.lookup.StaticHashTable(\n#Noteint32->int64mapsarenotsupported\ntf.lookup.KeyValueTensorInitializer(\nkeys=tf.cast(client_keys,dtype=TOKEN_DTYPE),\n#Noteweneedtousetf.shape,notthestatic#shapeclient_keys.shape[0]\nvalues=tf.range(0,limit=tf.shape(client_keys)[0],\ndtype=TOKEN_DTYPE)),\n#Weuse-1fortokensthatwerenotselected,whichcanoccurforclients\n#withmorethanMAX_TOKENS_SELECTED_PER_CLIENTdistincttokens.\n#Wewillsimplyremovetheseinvalidindicesfromthebatchbelow.\ndefault_value=-1)\n\ndefto_local_ids(sparse_tokens):\nindices_t=tf.transpose(sparse_tokens.indices)\nbatch_indices=indices_t[0]#Firstcolumn\ntokens=indices_t[1]#Secondcolumn\ntokens=tf.map_fn(\nlambdaglobal_token_id:global_to_local.lookup(global_token_id),tokens)\n#Removetokensthataren'tactuallyavailable(lookedupas-1):\navailable_tokens=tokens=0\ntokens=tokens[available_tokens]\nbatch_indices=batch_indices[available_tokens]\n\nupdated_indices=tf.transpose(\ntf.concat([[batch_indices],[tokens]],axis=0))\nst=tf.sparse.SparseTensor(\nupdated_indices,\ntf.ones(tf.size(tokens),dtype=FEATURE_DTYPE),\n#EachclienthasatmostMAX_TOKENS_SELECTED_PER_CLIENTdistincttokens.\ndense_shape=[sparse_tokens.dense_shape[0],MAX_TOKENS_SELECTED_PER_CLIENT])\nst=tf.sparse.reorder(st)\nreturnst\n\nreturnclient_data.map(lambdab:BatchType(to_local_ids(b.tokens),b.tags))\n\n```\n```\n#Simpletest\nclient_keys,actual_num_tokens=keys_for_client(\nbatched_dataset3,MAX_TOKENS_SELECTED_PER_CLIENT)\nclient_keys=client_keys[:actual_num_tokens]\n\nd=map_to_local_token_ids(batched_dataset3,client_keys)\nbatch=next(iter(d))\nall_tokens=tf.gather(batch.tokens.indices,indices=1,axis=1)\n#Confirmwehavelocalindicesintherange[0,MAX):\nasserttf.math.reduce_max(all_tokens)MAX_TOKENS_SELECTED_PER_CLIENT\nasserttf.math.reduce_max(all_tokens)=0\n\n```\n\n### Train the local (sub)model on each client\nNote `federated_select` will return the selected slices as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) in the same order as the selection keys. So, we first define a utility function to take such a Dataset and convert it to a single dense tensor which can be used as the model weights of the client model.\n```\n@tf.function\ndefslices_dataset_to_tensor(slices_dataset):\n\"\"\"Convert a dataset of slices to a tensor.\"\"\"\n#Usebatchingtogatheralloftheslicesintoasingletensor.\nd=slices_dataset.batch(MAX_TOKENS_SELECTED_PER_CLIENT,\ndrop_remainder=False)\niter_d=iter(d)\ntensor=next(iter_d)\n#Makesurewehaveconsumedeverything\nopt=iter_d.get_next_as_optional()\ntf.Assert(tf.logical_not(opt.has_value()),data=[''],name='CHECK_EMPTY')\nreturntensor\n\n```\n```\n#Simpletest\nweights=np.random.random(\nsize=(MAX_TOKENS_SELECTED_PER_CLIENT,TAG_VOCAB_SIZE)).astype(np.float32)\nmodel_slices_as_dataset=tf.data.Dataset.from_tensor_slices(weights)\nweights2=slices_dataset_to_tensor(model_slices_as_dataset)\nnp.testing.assert_array_equal(weights,weights2)\n\n```\n\nWe now have all the components we need to define a simple local training loop which will run on each client.\n```\n@tf.function\ndefclient_train_fn(model,client_optimizer,\nmodel_slices_as_dataset,client_data,\nclient_keys,actual_num_tokens):\n\ninitial_model_weights=slices_dataset_to_tensor(model_slices_as_dataset)\nassertlen(model.trainable_variables)==1\nmodel.trainable_variables[0].assign(initial_model_weights)\n\n# Only keep the \"real\" (unpadded) keys.\nclient_keys=client_keys[:actual_num_tokens]\n\nclient_data=map_to_local_token_ids(client_data,client_keys)\n\nloss_fn=tf.keras.losses.BinaryCrossentropy()\nforfeatures,labelsinclient_data:\nwithtf.GradientTape()astape:\npredictions=model(features)\nloss=loss_fn(labels,predictions)\ngrads=tape.gradient(loss,model.trainable_variables)\nclient_optimizer.apply_gradients(zip(grads,model.trainable_variables))\n\nmodel_weights_delta=model.trainable_weights[0]-initial_model_weights\nmodel_weights_delta=tf.slice(model_weights_delta,begin=[0,0],size=[actual_num_tokens,-1])\nreturnclient_keys,model_weights_delta\n\n```\n```\n#Simpletest\n#Noteifyouexecutethiscellasecondtime,youneedtoalsore-execute\n#thepreceedingcelltoavoid\"tf.function-decorated function tried to \n# create variables on non-first call\"errors.\non_device_model=create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\nTAG_VOCAB_SIZE)\nclient_optimizer=tf.keras.optimizers.SGD(learning_rate=0.001)\nclient_keys,actual_num_tokens=keys_for_client(\nbatched_dataset2,MAX_TOKENS_SELECTED_PER_CLIENT)\n\nmodel_slices_as_dataset=tf.data.Dataset.from_tensor_slices(\nnp.zeros((MAX_TOKENS_SELECTED_PER_CLIENT,TAG_VOCAB_SIZE),\ndtype=np.float32))\n\nkeys,delta=client_train_fn(\non_device_model,\nclient_optimizer,\nmodel_slices_as_dataset,\nclient_data=batched_dataset3,\nclient_keys=client_keys,\nactual_num_tokens=actual_num_tokens)\n\nprint(delta)\n\n```\n\n### Aggregate IndexedSlices\nWe use [`tff.federated_aggregate`](https://www.tensorflow.org/federated/api_docs/python/tff/federated_aggregate) to construct a federated sparse sum for `IndexedSlices`. This simple implementation has the constraint that the `dense_shape` is known statically in advance. Note also that this sum is only _semi-sparse_ , in the sense that the client -> server communication is sparse, but the server maintains a dense representation of the sum in `accumulate` and `merge`, and outputs this dense representation.\n```\ndeffederated_indexed_slices_sum(slice_indices,slice_values,dense_shape):\n\"\"\"\n  Sums IndexedSlices@CLIENTS to a dense @SERVER Tensor.\n\n  Intermediate aggregation is performed by converting to a dense representation,\n  which may not be suitable for all applications.\n\n  Args:\n    slice_indices: An IndexedSlices.indices tensor @CLIENTS.\n    slice_values: An IndexedSlices.values tensor @CLIENTS.\n    dense_shape: A statically known dense shape.\n\n  Returns:\n    A dense tensor placed @SERVER representing the sum of the client's\n    IndexedSclies.\n\"\"\"\nslices_dtype=slice_values.type_signature.member.dtype\nzero=tff.tensorflow.computation(\nlambda:tf.zeros(dense_shape,dtype=slices_dtype))()\n\n@tf.function\ndefaccumulate_slices(dense,client_value):\nindices,slices=client_value\n# There is no built-in way to add `IndexedSlices`, but \n# tf.convert_to_tensor is a quick way to convert to a dense representation\n# so we can add them.\nreturndense+tf.convert_to_tensor(\ntf.IndexedSlices(slices,indices,dense_shape))\n\n\nreturntff.federated_aggregate(\n(slice_indices,slice_values),\nzero=zero,\naccumulate=tff.tensorflow.computation(accumulate_slices),\nmerge=tff.tensorflow.computation(lambdad1,d2:tf.add(d1,d2,name='merge')),\nreport=tff.tensorflow.computation(lambdad:d))\n\n```\n\nConstruct a minimal `federated_computation` as a test\n```\ndense_shape=(6,2)\nindices_type=tff.TensorType(np.int64,(None,))\nvalues_type=tff.TensorType(np.float32,(None,2))\nclient_slice_type=tff.FederatedType(\n(indices_type,values_type),tff.CLIENTS)\n\n@tff.federated_computation(client_slice_type)\ndeftest_sum_indexed_slices(indices_values_at_client):\nindices,values=indices_values_at_client\nreturnfederated_indexed_slices_sum(indices,values,dense_shape)\n\nprint(test_sum_indexed_slices.type_signature)\n\n```\n```\n({<int64[?],float32[?,2]>}@CLIENTS -> float32[6,2]@SERVER)\n\n```\n```\nx=tf.IndexedSlices(\nvalues=np.array([[2.,2.1],[0.,0.1],[1.,1.1],[5.,5.1]],\ndtype=np.float32),\nindices=[2,0,1,5],\ndense_shape=dense_shape)\ny=tf.IndexedSlices(\nvalues=np.array([[0.,0.3],[3.1,3.2]],dtype=np.float32),\nindices=[1,3],\ndense_shape=dense_shape)\n\n#Sumone.\nresult=test_sum_indexed_slices([(x.indices,x.values)])\nnp.testing.assert_array_equal(tf.convert_to_tensor(x),result)\n\n#Sumtwo.\nexpected=[[0.,0.1],[1.,1.4],[2.,2.1],[3.1,3.2],[0.,0.],[5.,5.1]]\nresult=test_sum_indexed_slices([(x.indices,x.values),(y.indices,y.values)])\nnp.testing.assert_array_almost_equal(expected,result)\n\n```\n\n# Putting it all together in a `federated_computation`\nWe now use TFF to bind together the components into a [`tff.federated_computation`](https://www.tensorflow.org/federated/api_docs/python/tff/federated_computation).\n```\nDENSE_MODEL_SHAPE=(WORD_VOCAB_SIZE,TAG_VOCAB_SIZE)\nclient_data_type=tff.SequenceType(batched_dataset1.element_spec)\nmodel_type=tff.TensorType(np.float32,shape=DENSE_MODEL_SHAPE)\n\n```\n\nWe use a basic server training function based on Federated Averaging, applying the update with a server learning rate of 1.0. It is important that we apply an update (delta) to the model, rather than simply averaging client-supplied models, as otherwise if a given slice of the model wasn't trained on by any client on a given round its coefficients could be zeroed out.\n```\n@tff.tensorflow.computation\ndefserver_update(current_model_weights,update_sum,num_clients):\naverage_update=update_sum/num_clients\nreturncurrent_model_weights+average_update\n\n```\n\nWe need a couple more [`tff.tensorflow.computation`](https://www.tensorflow.org/federated/api_docs/python/tff/tensorflow/computation) components:\n```\n# Function to select slices from the model weights in federated_select:\nselect_fn=tff.tensorflow.computation(\nlambdamodel_weights,index:tf.gather(model_weights,index))\n\n\n# We need to wrap `client_train_fn` as a `tff.tensorflow.computation`, making\n# sure we do any operations that might construct `tf.Variable`s outside\n# of the `tf.function` we are wrapping.\n@tff.tensorflow.computation\ndefclient_train_fn_tff(model_slices_as_dataset,client_data,client_keys,\nactual_num_tokens):\n# Note this is amaller than the global model, using\n# MAX_TOKENS_SELECTED_PER_CLIENT which is much smaller than WORD_VOCAB_SIZE.\n# We would like a model of size `actual_num_tokens`, but we\n# can't build the model dynamically, so we will slice off the padded\n# weights at the end.\nclient_model=create_logistic_model(MAX_TOKENS_SELECTED_PER_CLIENT,\nTAG_VOCAB_SIZE)\nclient_optimizer=tf.keras.optimizers.SGD(learning_rate=0.1)\nreturnclient_train_fn(client_model,client_optimizer,\nmodel_slices_as_dataset,client_data,client_keys,\nactual_num_tokens)\n\n@tff.tensorflow.computation\ndefkeys_for_client_tff(client_data):\nreturnkeys_for_client(client_data,MAX_TOKENS_SELECTED_PER_CLIENT)\n\n```\n\nWe're now ready to put all the pieces together!\n```\n@tff.federated_computation(\ntff.FederatedType(model_type,tff.SERVER),tff.FederatedType(client_data_type,tff.CLIENTS))\ndefsparse_model_update(server_model,client_data):\nmax_tokens=tff.federated_value(MAX_TOKENS_SELECTED_PER_CLIENT,tff.SERVER)\nkeys_at_clients,actual_num_tokens=tff.federated_map(\nkeys_for_client_tff,client_data)\n\nmodel_slices=tff.federated_select(keys_at_clients,max_tokens,server_model,\nselect_fn)\n\nupdate_keys,update_slices=tff.federated_map(\nclient_train_fn_tff,\n(model_slices,client_data,keys_at_clients,actual_num_tokens))\n\ndense_update_sum=federated_indexed_slices_sum(update_keys,update_slices,\nDENSE_MODEL_SHAPE)\nnum_clients=tff.federated_sum(tff.federated_value(1.0,tff.CLIENTS))\n\nupdated_server_model=tff.federated_map(\nserver_update,(server_model,dense_update_sum,num_clients))\n\nreturnupdated_server_model\n\n\nprint(sparse_model_update.type_signature)\n\n```\n```\n(<server_model=float32[13,4]@SERVER,client_data={<tokens=<indices=int64[?,2],values=int32[?],dense_shape=int64[2]>,tags=float32[?,4]>*}@CLIENTS> -> float32[13,4]@SERVER)\n\n```\n\n# Let's train a model!\nNow that we have our training function, let's try it out.\n```\nserver_model = create_logistic_model(WORD_VOCAB_SIZE, TAG_VOCAB_SIZE)\nserver_model.compile(  # Compile to make evaluation easy.\n    optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.0),  # Unused\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    metrics=[ \n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.AUC(name='auc'),\n      tf.keras.metrics.Recall(top_k=2, name='recall_at_2'),\n  ])\n\ndef evaluate(model, dataset, name):\n  metrics = model.evaluate(dataset, verbose=0)\n  metrics_str = ', '.join([f'{k}={v:.2f}' for k, v in \n                          (zip(server_model.metrics_names, metrics))])\n  print(f'{name}: {metrics_str}')\n\n```\n```\nprint('Before training')\nevaluate(server_model,batched_dataset1,'Client 1')\nevaluate(server_model,batched_dataset2,'Client 2')\nevaluate(server_model,batched_dataset3,'Client 3')\n\nmodel_weights=server_model.trainable_weights[0]\n\nclient_datasets=[batched_dataset1, batched_dataset2, batched_dataset3]\nfor_inrange(10):#Run10roundsofFedAvg\n#Wetrainon1,2,or3clientsperround,selecting\n#randomly.\ncohort_size=np.random.randint(1,4)\nclients=np.random.choice([0, 1, 2],cohort_size,replace=False)\nprint('Training on clients',clients)\nmodel_weights=sparse_model_update(\nmodel_weights,[client_datasets[i]foriinclients])\nserver_model.set_weights([model_weights])\n\nprint('After training')\nevaluate(server_model,batched_dataset1,'Client 1')\nevaluate(server_model,batched_dataset2,'Client 2')\nevaluate(server_model,batched_dataset3,'Client 3')\n\n```\n```\nBefore training\nClient 1: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.60\nClient 2: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.50\nClient 3: loss=0.69, precision=0.00, auc=0.50, recall_at_2=0.40\nTraining on clients [0 1]\nTraining on clients [0 2 1]\nTraining on clients [2 0]\nTraining on clients [1 0 2]\nTraining on clients [2]\nTraining on clients [2 0]\nTraining on clients [1 2 0]\nTraining on clients [0]\nTraining on clients [2]\nTraining on clients [1 2]\nAfter training\nClient 1: loss=0.67, precision=0.80, auc=0.91, recall_at_2=0.80\nClient 2: loss=0.68, precision=0.67, auc=0.96, recall_at_2=1.00\nClient 3: loss=0.65, precision=1.00, auc=0.93, recall_at_2=0.80\n\n```\n\n",
  "https://www.tensorflow.org/community": "#  Community \nExplore ways to get involved below, and stay up-to-date with the latest announcements and events by subscribing to the TensorFlow newsletter.\n##  Get involved \nBe a part of our global contributor community by writing code, commenting on blogs, or attending meetups.\n[ Forum ](https://discuss.tensorflow.org)\nJoin the community forum to share ideas and best practices, get help with technical questions, and discuss TensorFlow with other developers.\n[ Explore TensorFlow Forum ](https://discuss.tensorflow.org)\n[ Groups ](https://www.tensorflow.org/community/groups)\nExplore our developer communities around the world to attend local events and collaborate on topics of interest.\n[ RFC Process ](https://www.tensorflow.org/community/contribute/rfc_process)\nContribute to the development of TensorFlow through our RFC process, an open collaboration where experts can provide feedback on proposed designs/features, or request changes.\n[ Contribute ](https://www.tensorflow.org/community/contribute)\nWe welcome contributions and collaboration on TensorFlow. For more information and to learn best practices, please read our Contributor Guide.\n[ See Contributor Guide ](https://www.tensorflow.org/community/contribute)\nTo report bugs or make feature requests, file an issue on GitHub. Please choose the appropriate repository for the project.\n##  Stay informed \nJoin the TensorFlow announcement mailing list to learn about the latest release updates, security advisories, and other important information from the TensorFlow team.\nBefore using TensorFlow, please take a look at our security model, lists of recent security advisories and announcements, and ways you can report security issues to us on Github.\n[ Blog ](https://blog.tensorflow.org)\nThe TensorFlow Blog contains regular postings from the TensorFlow team, as well as articles from the community.\nOur YouTube Channel has a great lineup of shows covering all the things you can do with TensorFlow and AI. Subscribe to the channel to be notified about all the latest videos.\nFor news and updates, follow @tensorflow on Twitter.\n###  Sign up for the TensorFlow newsletter \n##  Explore work by the TensorFlow community from all around the world \n",
  "https://www.tensorflow.org/api_docs/python/tf/variable_creator_scope": "Scope which defines a variable creation function to be used by variable().  \n```\n@tf_contextlib.contextmanager\ntf.variable_creator_scope(\n    variable_creator\n)\n\n```\n\n### Used in the notebooks\nUsed in the guide  \n---  \n  * [Validating correctness & numerical equivalence](https://www.tensorflow.org/guide/migrate/validate_correctness)\n\n  \nvariable_creator is expected to be a function with the following signature:\n```\n  defvariable_creator(next_creator, **kwargs)\n\n```\n\nThe creator is supposed to eventually call the next_creator to create a variable if it does want to create a variable and not call Variable or ResourceVariable directly. This helps make creators composable. A creator may choose to create multiple variables, return already existing variables, or simply register that a variable was created and defer to the next creators in line. Creators can also modify the keyword arguments seen by the next creators.\nCustom getters in the variable scope will eventually resolve down to these custom creators when they do create variables.\nThe valid keyword arguments in kwds are:\n  * initial_value: A `Tensor`, or Python object convertible to a `Tensor`, which is the initial value for the Variable. The initial value must have a shape specified unless `validate_shape` is set to False. Can also be a callable with no argument that returns the initial value when called. In that case, `dtype` must be specified. (Note that initializer functions from init_ops.py must first be bound to a shape before being used here.)\n  * trainable: If `True`, the default, GradientTapes automatically watch uses of this Variable.\n  * validate_shape: If `False`, allows the variable to be initialized with a value of unknown shape. If `True`, the default, the shape of `initial_value` must be known.\n  * caching_device: Optional device string describing where the Variable should be cached for reading. Defaults to the Variable's device. If not `None`, caches on another device. Typical use is to cache on the device where the Ops using the Variable reside, to deduplicate copying through `Switch` and other conditional statements.\n  * name: Optional name for the variable. Defaults to `'Variable'` and gets uniquified automatically. dtype: If set, initial_value will be converted to the given type. If `None`, either the datatype will be kept (if `initial_value` is a Tensor), or `convert_to_tensor` will decide.\n  * constraint: A constraint function to be applied to the variable after updates by some algorithms.\n  * synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class [`tf.VariableSynchronization`](https://www.tensorflow.org/api_docs/python/tf/VariableSynchronization). By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize.\n  * aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class [`tf.VariableAggregation`](https://www.tensorflow.org/api_docs/python/tf/VariableAggregation).\n\n\nThis set may grow over time, so it's important the signature of creators is as mentioned above.\n## Args  \n---  \n`variable_creator` |  the passed creator   \n## Yields  \n---  \nA scope in which the creator is active \n",
  "https://www.tensorflow.org/guide/checkpoint": "[View on TensorFlow.org](https://www.tensorflow.org/guide/checkpoint)  \n---  \nThe phrase \"Saving a TensorFlow model\" typically means one of two things:\n  1. Checkpoints, OR \n  2. SavedModel.\n\n\nCheckpoints capture the exact value of all parameters ([`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) objects) used by a model. Checkpoints do not contain any description of the computation defined by the model and thus are typically only useful when source code that will use the saved parameter values is available.\nThe SavedModel format on the other hand includes a serialized description of the computation defined by the model in addition to the parameter values (checkpoint). Models in this format are independent of the source code that created the model. They are thus suitable for deployment via TensorFlow Serving, TensorFlow Lite, TensorFlow.js, or programs in other programming languages (the C, C++, Java, Go, Rust, C# etc. TensorFlow APIs).\nThis guide covers APIs for writing and reading checkpoints.\n## Setup\n```\nimporttensorflowastf\n\n```\n```\nclassNet(tf.keras.Model):\n\"\"\"A simple linear model.\"\"\"\n\ndef__init__(self):\nsuper(Net,self).__init__()\nself.l1=tf.keras.layers.Dense(5)\n\ndefcall(self,x):\nreturnself.l1(x)\n\n```\n```\nnet = Net()\n\n```\n\n## Saving from [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) training APIs\nSee the [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) guide on saving and restoring.\n[`tf.keras.Model.save_weights`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights) saves a TensorFlow checkpoint. \n```\nnet.save_weights('easy_checkpoint')\n\n```\n\n## Writing checkpoints\nThe persistent state of a TensorFlow model is stored in [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) objects. These can be constructed directly, but are often created through high-level APIs like [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) or [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\nThe easiest way to manage variables is by attaching them to Python objects, then referencing those objects. \nSubclasses of [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint), [`tf.keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer), and [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model) automatically track variables assigned to their attributes. The following example constructs a simple linear model, then writes checkpoints which contain values for all of the model's variables.\nYou can easily save a model-checkpoint with [`Model.save_weights`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#save_weights).\n### Manual checkpointing\n#### Setup\nTo help demonstrate all the features of [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint), define a toy dataset and optimization step:\n```\ndef toy_dataset():\n  inputs = tf.range(10.)[:, None]\n  labels = inputs * 5. + tf.range(5.)[None, :]\n  return tf.data.Dataset.from_tensor_slices(\n    dict(x=inputs, y=labels)).repeat().batch(2)\n\n```\n```\ndeftrain_step(net,example,optimizer):\n\"\"\"Trains `net` on `example` using `optimizer`.\"\"\"\nwithtf.GradientTape()astape:\noutput=net(example['x'])\nloss=tf.reduce_mean(tf.abs(output-example['y']))\nvariables=net.trainable_variables\ngradients=tape.gradient(loss,variables)\noptimizer.apply_gradients(zip(gradients,variables))\nreturnloss\n\n```\n\n#### Create the checkpoint objects\nUse a [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) object to manually create a checkpoint, where the objects you want to checkpoint are set as attributes on the object.\nA [`tf.train.CheckpointManager`](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager) can also be helpful for managing multiple checkpoints.\n```\nopt = tf.keras.optimizers.Adam(0.1)\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)\nmanager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n\n```\n\n#### Train and checkpoint the model\nThe following training loop creates an instance of the model and of an optimizer, then gathers them into a [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) object. It calls the training step in a loop on each batch of data, and periodically writes checkpoints to disk.\n```\ndeftrain_and_checkpoint(net,manager):\nckpt.restore(manager.latest_checkpoint)\nifmanager.latest_checkpoint:\nprint(\"Restored from {}\".format(manager.latest_checkpoint))\nelse:\nprint(\"Initializing from scratch.\")\n\nfor_inrange(50):\nexample=next(iterator)\nloss=train_step(net,example,opt)\nckpt.step.assign_add(1)\nifint(ckpt.step)%10==0:\nsave_path=manager.save()\nprint(\"Saved checkpoint for step {}: {}\".format(int(ckpt.step),save_path))\nprint(\"loss {:1.2f}\".format(loss.numpy()))\n\n```\n```\ntrain_and_checkpoint(net, manager)\n\n```\n\n#### Restore and continue training\nAfter the first training cycle you can pass a new model and manager, but pick up training exactly where you left off:\n```\nopt = tf.keras.optimizers.Adam(0.1)\nnet = Net()\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)\nmanager = tf.train.CheckpointManager(ckpt, './tf_ckpts', max_to_keep=3)\n\ntrain_and_checkpoint(net, manager)\n\n```\n\nThe [`tf.train.CheckpointManager`](https://www.tensorflow.org/api_docs/python/tf/train/CheckpointManager) object deletes old checkpoints. Above it's configured to keep only the three most recent checkpoints.\n```\nprint(manager.checkpoints)  # List the three remaining checkpoints\n\n```\n\nThese paths, e.g. `'./tf_ckpts/ckpt-10'`, are not files on disk. Instead they are prefixes for an `index` file and one or more data files which contain the variable values. These prefixes are grouped together in a single `checkpoint` file (`'./tf_ckpts/checkpoint'`) where the `CheckpointManager` saves its state.\n```\nls\n```\n\nLoading mechanics TensorFlow matches variables to checkpointed values by traversing a directed graph with named edges, starting from the object being loaded. Edge names typically come from attribute names in objects, for example the `\"l1\"` in `self.l1 = tf.keras.layers.Dense(5)`. `tf.train.Checkpoint` uses its keyword argument names, as in the `\"step\"` in `tf.train.Checkpoint(step=...)`. The dependency graph from the example above looks like this:\nThe optimizer is in red, regular variables are in blue, and the optimizer slot variables are in orange. The other nodes—for example, representing the [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint)—are in black.\nSlot variables are part of the optimizer's state, but are created for a specific variable. For example, the `'m'` edges above correspond to momentum, which the Adam optimizer tracks for each variable. Slot variables are only saved in a checkpoint if the variable and the optimizer would both be saved, thus the dashed edges.\nCalling `restore` on a [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) object queues the requested restorations, restoring variable values as soon as there's a matching path from the `Checkpoint` object. For example, you can load just the bias from the model you defined above by reconstructing one path to it through the network and the layer.\n```\nto_restore = tf.Variable(tf.zeros([5]))\nprint(to_restore.numpy())  # All zeros\nfake_layer = tf.train.Checkpoint(bias=to_restore)\nfake_net = tf.train.Checkpoint(l1=fake_layer)\nnew_root = tf.train.Checkpoint(net=fake_net)\nstatus = new_root.restore(tf.train.latest_checkpoint('./tf_ckpts/'))\nprint(to_restore.numpy())  # This gets the restored value.\n\n```\n\nThe dependency graph for these new objects is a much smaller subgraph of the larger checkpoint you wrote above. It includes only the bias and a save counter that [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) uses to number checkpoints.\n`restore` returns a status object, which has optional assertions. All of the objects created in the new `Checkpoint` have been restored, so `status.assert_existing_objects_matched` passes.\n```\nstatus.assert_existing_objects_matched()\n\n```\n\nThere are many objects in the checkpoint which haven't matched, including the layer's kernel and the optimizer's variables. `status.assert_consumed` only passes if the checkpoint and the program match exactly, and would throw an exception here.\n### Deferred restorations\n`Layer` objects in TensorFlow may defer the creation of variables to their first call, when input shapes are available. For example, the shape of a `Dense` layer's kernel depends on both the layer's input and output shapes, and so the output shape required as a constructor argument is not enough information to create the variable on its own. Since calling a `Layer` also reads the variable's value, a restore must happen between the variable's creation and its first use.\nTo support this idiom, [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) defers restores which don't yet have a matching variable.\n```\ndeferred_restore = tf.Variable(tf.zeros([1, 5]))\nprint(deferred_restore.numpy())  # Not restored; still zeros\nfake_layer.kernel = deferred_restore\nprint(deferred_restore.numpy())  # Restored\n\n```\n\n### Manually inspecting checkpoints\n[`tf.train.load_checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/load_checkpoint) returns a `CheckpointReader` that gives lower level access to the checkpoint contents. It contains mappings from each variable's key, to the shape and dtype for each variable in the checkpoint. A variable's key is its object path, like in the graphs displayed above.\n```\nreader=tf.train.load_checkpoint('./tf_ckpts/')\nshape_from_key=reader.get_variable_to_shape_map()\ndtype_from_key=reader.get_variable_to_dtype_map()\n\nsorted(shape_from_key.keys())\n\n```\n\nSo if you're interested in the value of `net.l1.kernel` you can get the value with the following code:\n```\nkey='net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE'\n\nprint(\"Shape:\",shape_from_key[key])\nprint(\"Dtype:\",dtype_from_key[key].name)\n\n```\n\nIt also provides a `get_tensor` method allowing you to inspect the value of a variable:\n```\nreader.get_tensor(key)\n\n```\n\n### Object tracking\nCheckpoints save and restore the values of [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) objects by \"tracking\" any variable or trackable object set in one of its attributes. When executing a save, variables are gathered recursively from all of the reachable tracked objects.\nAs with direct attribute assignments like `self.l1 = tf.keras.layers.Dense(5)`, assigning lists and dictionaries to attributes will track their contents.\n```\nsave = tf.train.Checkpoint()\nsave.listed = [tf.Variable(1.)]\nsave.listed.append(tf.Variable(2.))\nsave.mapped = {'one': save.listed[0]}\nsave.mapped['two'] = save.listed[1]\nsave_path = save.save('./tf_list_example')\n\nrestore = tf.train.Checkpoint()\nv2 = tf.Variable(0.)\nassert 0. == v2.numpy()  # Not restored yet\nrestore.mapped = {'two': v2}\nrestore.restore(save_path)\nassert 2. == v2.numpy()\n\n```\n\nYou may notice wrapper objects for lists and dictionaries. These wrappers are checkpointable versions of the underlying data-structures. Just like the attribute based loading, these wrappers restore a variable's value as soon as it's added to the container.\n```\nrestore.listed = []\nprint(restore.listed)  # ListWrapper([])\nv1 = tf.Variable(0.)\nrestore.listed.append(v1)  # Restores v1, from restore() in the previous cell\nassert 1. == v1.numpy()\n\n```\n\nTrackable objects include [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint), [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) and its subclasses (e.g. [`keras.layers.Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Layer) and [`keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)), and recognized Python containers:\n  * `dict` (and `collections.OrderedDict`)\n  * `list`\n  * `tuple` (and `collections.namedtuple`, `typing.NamedTuple`)\n\n\nOther container types are **not supported** , including:\n  * `collections.defaultdict`\n  * `set`\n\n\nAll other Python objects are **ignored** , including:\n  * `int`\n  * `string`\n  * `float`\n\n\n## Summary\nTensorFlow objects provide an easy automatic mechanism for saving and restoring the values of variables they use.\n",
  "https://www.tensorflow.org/guide/core": "The TensorFlow Core APIs provide a set of comprehensive, composable, and extensible low-level APIs for high-performance (distributed and accelerated) computation, primarily aimed at building machine learning (ML) models as well as authoring ML workflow tools and frameworks within the TensorFlow platform. These APIs provide a foundation for creating highly configurable models with fine-grained control and new frameworks from the ground up.\nThe Core APIs can be used as an alternative to high-level machine learning APIs like Keras. These high-level APIs are best suited for general machine learning needs. They offer a variety of modules that abstract away the complexities of ML while also offering functionalities for customization through subclassing. If you are looking for an overview of TensorFlow using Keras, see the Quickstarts and Keras sections in the [tutorials](https://www.tensorflow.org/tutorials).\n## Who should use the Core APIs\nThe TensorFlow Core low-level APIs are designed with the following ML developers in mind:\n  * Researchers building complex models with high levels of configurability\n  * Developers interested in using TensorFlow as a high-performance scientific computing platform\n  * Framework authors building tools on top of the TensorFlow platform\n  * High-level API users interested in: \n    * Adding additional functionalities to their machine learning workflows such as custom layers, losses, models, and optimizers\n    * Learning more about the inner workings of their models\n\n\n## Core API applications\nThe TensorFlow Core APIs provide access to low level functionality within the TensorFlow ecosystem. This API provides more flexibility and control for building ML models, applications, and tools, compared to high-level APIs, such as Keras.\n### Build models and workflows\nThe Core APIs are most commonly used to build highly customizable and optimized machine learning models and workflows. Here are some of the ways that the TensorFlow Core APIs can improve your machine learning models and workflow development:\n  * Building non-traditional models or layers that do not fully fit the structures supported by high-level APIs\n  * Building custom layers, losses, models, and optimizers within Keras\n  * Implementing new optimization techniques to expedite convergence during training\n  * Creating custom metrics for performance evaluation\n  * Designing highly-configurable training loops with support for features like batching, cross-validation, and distribution strategies\n\n\n### Build frameworks and tools\nThe TensorFlow Core APIs can also serve as the building blocks for new high-level frameworks. Here are some examples of tools and frameworks that are created with the low-level APIs: \n  * [TensorFlow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization): a suite of tools to optimize ML models for deployment and execution\n  * [TensorFlow Graphics](https://www.tensorflow.org/graphics): a library for making useful graphics functions widely accessible\n\n\n### Build for scientific computing\nThe TensorFlow Core APIs can also be applied outside the realm of machine learning. Here are a few general-purpose use cases of TensorFlow for scientific computing: \n  * Physics simulations for solid mechanics and \n  * Graphics rendering applications like \n  * Solving \n\n\n## Core API components\nHere are some of the fundamental components that comprise TensorFlow Core’s low- level APIs. Note that this is not an all-encompassing list:\n  * Data structures : [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable), [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray)\n  * Primitive APIs: [`tf.shape`](https://www.tensorflow.org/api_docs/python/tf/shape), [slicing](https://www.tensorflow.org/guide/tensor_slicing), [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat), [`tf.bitwise`](https://www.tensorflow.org/api_docs/python/tf/bitwise)\n  * Numerical: [`tf.math`](https://www.tensorflow.org/api_docs/python/tf/math), [`tf.linalg`](https://www.tensorflow.org/api_docs/python/tf/linalg), [`tf.random`](https://www.tensorflow.org/api_docs/python/tf/random)\n  * Functional components: [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)\n  * Distribution: [DTensor](https://www.tensorflow.org/guide/dtensor_overview)\n  * Export: [`tf.saved_model`](https://www.tensorflow.org/api_docs/python/tf/saved_model)\n\n\n## Next steps\nThe _Build with Core_ documentation provides tutorials of basic machine learning concepts from scratch. The tutorials in this section help you get comfortable with writing low-level code with Core APIs that you can then apply to more complex use cases of your own.\nTo get started using and learning more about the Core APIs, check out the [Quickstart for TensorFlow Core](https://www.tensorflow.org/guide/core/quickstart_core).\n",
  "https://www.tensorflow.org/guide/data": "[View on TensorFlow.org](https://www.tensorflow.org/guide/data)  \n---  \nThe [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training. The pipeline for a text model might involve extracting symbols from raw text data, converting them to embedding identifiers with a lookup table, and batching together sequences of different lengths. The [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API makes it possible to handle large amounts of data, read from different data formats, and perform complex transformations.\nThe [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API introduces a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) abstraction that represents a sequence of elements, in which each element consists of one or more components. For example, in an image pipeline, an element might be a single training example, with a pair of tensor components representing the image and its label.\nThere are two distinct ways to create a dataset:\n  * A data **source** constructs a `Dataset` from data stored in memory or in one or more files.\n  * A data **transformation** constructs a dataset from one or more [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects.\n\n```\nimporttensorflowastf\n\n```\n```\n2024-08-15 01:37:36.963860: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 01:37:36.985171: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 01:37:36.991452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n```\nimportpathlib\nimportos\nimportmatplotlib.pyplotasplt\nimportpandasaspd\nimportnumpyasnp\n\nnp.set_printoptions(precision=4)\n\n```\n\n## Basic mechanics\nTo create an input pipeline, you must start with a data _source_. For example, to construct a `Dataset` from data in memory, you can use [`tf.data.Dataset.from_tensors()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors) or [`tf.data.Dataset.from_tensor_slices()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices). Alternatively, if your input data is stored in a file in the recommended TFRecord format, you can use [`tf.data.TFRecordDataset()`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset).\nOnce you have a `Dataset` object, you can _transform_ it into a new `Dataset` by chaining method calls on the [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) object. For example, you can apply per-element transformations such as [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map), and multi-element transformations such as [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch). Refer to the documentation for [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for a complete list of transformations.\nThe `Dataset` object is a Python iterable. This makes it possible to consume its elements using a for loop:\n```\ndataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\ndataset\n\n```\n```\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723685859.835217   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.839003   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.842691   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.846561   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.858030   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.861635   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.865105   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.868512   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.871403   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.874859   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.878307   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685859.881840   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.098140   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.100277   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.102280   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.104281   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.106309   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.108307   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.110218   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.112117   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.114046   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.116014   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.117904   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.119808   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.158075   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.160123   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.162060   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.163993   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.165963   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.167940   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.169863   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.171778   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.173638   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.176135   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.178420   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723685861.180782   44933 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n<_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>\n\n```\n```\nfor elem in dataset:\n  print(elem.numpy())\n\n```\n```\n8\n3\n0\n8\n2\n1\n\n```\n\nOr by explicitly creating a Python iterator using `iter` and consuming its elements using `next`:\n```\nit = iter(dataset)\n\nprint(next(it).numpy())\n\n```\n```\n8\n\n```\n\nAlternatively, dataset elements can be consumed using the `reduce` transformation, which reduces all elements to produce a single result. The following example illustrates how to use the `reduce` transformation to compute the sum of a dataset of integers.\n```\nprint(dataset.reduce(0, lambda state, value: state + value).numpy())\n\n```\n```\n22\n\n```\n\n### Dataset structure\nA dataset produces a sequence of _elements_ , where each element is the same (nested) structure of _components_. Individual components of the structure can be of any type representable by [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec), including [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor), [`tf.sparse.SparseTensor`](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor), [`tf.RaggedTensor`](https://www.tensorflow.org/api_docs/python/tf/RaggedTensor), [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray), or [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe Python constructs that can be used to express the (nested) structure of elements include `tuple`, `dict`, `NamedTuple`, and `OrderedDict`. In particular, `list` is not a valid construct for expressing the structure of dataset elements. This is because early [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) users felt strongly about `list` inputs (for example, when passed to [`tf.data.Dataset.from_tensors`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensors)) being automatically packed as tensors and `list` outputs (for example, return values of user-defined functions) being coerced into a `tuple`. As a consequence, if you would like a `list` input to be treated as a structure, you need to convert it into `tuple` and if you would like a `list` output to be a single component, then you need to explicitly pack it using [`tf.stack`](https://www.tensorflow.org/api_docs/python/tf/stack).\nThe [`Dataset.element_spec`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#element_spec) property allows you to inspect the type of each element component. The property returns a _nested structure_ of [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) objects, matching the structure of the element, which may be a single component, a tuple of components, or a nested tuple of components. For example:\n```\ndataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n\ndataset1.element_spec\n\n```\n```\nTensorSpec(shape=(10,), dtype=tf.float32, name=None)\n\n```\n```\ndataset2 = tf.data.Dataset.from_tensor_slices(\n   (tf.random.uniform([4]),\n    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n\ndataset2.element_spec\n\n```\n```\n(TensorSpec(shape=(), dtype=tf.float32, name=None),\n TensorSpec(shape=(100,), dtype=tf.int32, name=None))\n\n```\n```\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n\ndataset3.element_spec\n\n```\n```\n(TensorSpec(shape=(10,), dtype=tf.float32, name=None),\n (TensorSpec(shape=(), dtype=tf.float32, name=None),\n  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))\n\n```\n```\n# Dataset containing a sparse tensor.\ndataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1, 2], dense_shape=[3, 4]))\n\ndataset4.element_spec\n\n```\n```\nSparseTensorSpec(TensorShape([3, 4]), tf.int32)\n\n```\n```\n# Use value_type to see the type of value represented by the element spec\ndataset4.element_spec.value_type\n\n```\n```\ntensorflow.python.framework.sparse_tensor.SparseTensor\n\n```\n\nThe `Dataset` transformations support datasets of any structure. When using the [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map), and [`Dataset.filter`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter) transformations, which apply a function to each element, the element structure determines the arguments of the function:\n```\ndataset1 = tf.data.Dataset.from_tensor_slices(\n    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32))\n\ndataset1\n\n```\n```\n<_TensorSliceDataset element_spec=TensorSpec(shape=(10,), dtype=tf.int32, name=None)>\n\n```\n```\nfor z in dataset1:\n  print(z.numpy())\n\n```\n```\n[3 4 1 6 1 8 5 8 9 4]\n[2 7 6 9 2 6 6 4 9 7]\n[8 7 9 6 3 4 5 8 4 4]\n[2 1 1 1 3 9 7 8 6 8]\n\n```\n```\ndataset2 = tf.data.Dataset.from_tensor_slices(\n   (tf.random.uniform([4]),\n    tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)))\n\ndataset2\n\n```\n```\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None))>\n\n```\n```\ndataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n\ndataset3\n\n```\n```\n<_ZipDataset element_spec=(TensorSpec(shape=(10,), dtype=tf.int32, name=None), (TensorSpec(shape=(), dtype=tf.float32, name=None), TensorSpec(shape=(100,), dtype=tf.int32, name=None)))>\n\n```\n```\nfor a, (b,c) in dataset3:\n  print('shapes: {a.shape}, {b.shape}, {c.shape}'.format(a=a, b=b, c=c))\n\n```\n```\nshapes: (10,), (), (100,)\nshapes: (10,), (), (100,)\nshapes: (10,), (), (100,)\nshapes: (10,), (), (100,)\n\n```\n\n## Reading input data\n### Consuming NumPy arrays\nRefer to the [Loading NumPy arrays](https://www.tensorflow.org/tutorials/load_data/numpy) tutorial for more examples.\nIf all of your input data fits in memory, the simplest way to create a `Dataset` from them is to convert them to [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects and use [`Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices).\n```\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\n\n```\n```\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n```\n```\nimages, labels = train\nimages = images/255\n\ndataset = tf.data.Dataset.from_tensor_slices((images, labels))\ndataset\n\n```\n```\n<_TensorSliceDataset element_spec=(TensorSpec(shape=(28, 28), dtype=tf.float64, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))>\n\n```\n\n### Consuming Python generators\nAnother common data source that can easily be ingested as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) is the python generator.\n```\ndefcount(stop):\n  i = 0\n  while i<stop:\n    yield i\n    i += 1\n\n```\n```\nfor n in count(5):\n  print(n)\n\n```\n```\n0\n1\n2\n3\n4\n\n```\n\nThe [`Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) constructor converts the python generator to a fully functional [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nThe constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. It takes an optional `args` argument, which is passed as the callable's arguments.\nThe `output_types` argument is required because [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) builds a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) internally, and graph edges require a `tf.dtype`.\n```\nds_counter = tf.data.Dataset.from_generator(count, args=[25], output_types=tf.int32, output_shapes = (), )\n\n```\n```\nfor count_batch in ds_counter.repeat().batch(10).take(10):\n  print(count_batch.numpy())\n\n```\n```\n[0 1 2 3 4 5 6 7 8 9]\n[10 11 12 13 14 15 16 17 18 19]\n[20 21 22 23 24  0  1  2  3  4]\n[ 5  6  7  8  9 10 11 12 13 14]\n[15 16 17 18 19 20 21 22 23 24]\n[0 1 2 3 4 5 6 7 8 9]\n[10 11 12 13 14 15 16 17 18 19]\n[20 21 22 23 24  0  1  2  3  4]\n[ 5  6  7  8  9 10 11 12 13 14]\n[15 16 17 18 19 20 21 22 23 24]\n\n```\n\nThe `output_shapes` argument is not _required_ but is highly recommended as many TensorFlow operations do not support tensors with an unknown rank. If the length of a particular axis is unknown or variable, set it as `None` in the `output_shapes`.\nIt's also important to note that the `output_shapes` and `output_types` follow the same nesting rules as other dataset methods.\nHere is an example generator that demonstrates both aspects: it returns tuples of arrays, where the second array is a vector with unknown length.\n```\ndefgen_series():\n  i = 0\n  while True:\n    size = np.random.randint(0, 10)\n    yield i, np.random.normal(size=(size,))\n    i += 1\n\n```\n```\nfor i, series in gen_series():\n  print(i, \":\", str(series))\n  if i > 5:\n    break\n\n```\n```\n0 : [1.1274]\n1 : [-0.5822  0.8497 -1.3594  0.2083 -0.3007  1.2171 -0.3551]\n2 : [-1.2016 -0.1085  0.4088  0.0801  1.4901 -2.3102]\n3 : [ 0.5816 -0.6447 -0.9673  0.5282  0.52   -0.2634  0.3001  0.8753]\n4 : [ 0.0888  0.071   1.26   -0.347  -0.2643 -1.0757  0.4192]\n5 : [ 0.4911  0.8377  0.3576 -0.0351  0.9663]\n6 : [-0.1996  0.5808  0.4589  1.8229 -0.5712]\n\n```\n\nThe first output is an `int32` the second is a `float32`.\nThe first item is a scalar, shape `()`, and the second is a vector of unknown length, shape `(None,)`\n```\nds_series = tf.data.Dataset.from_generator(\n    gen_series,\n    output_types=(tf.int32, tf.float32),\n    output_shapes=((), (None,)))\n\nds_series\n\n```\n```\n<_FlatMapDataset element_spec=(TensorSpec(shape=(), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n\n```\n\nNow it can be used like a regular [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Note that when batching a dataset with a variable shape, you need to use [`Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch).\n```\nds_series_batch = ds_series.shuffle(20).padded_batch(10)\n\nids, sequence_batch = next(iter(ds_series_batch))\nprint(ids.numpy())\nprint()\nprint(sequence_batch.numpy())\n\n```\n```\n[ 5 19 20 11  4 10 17  8 27 18]\n\n[[-0.7479  0.867  -0.0558 -1.0825 -0.4113  0.0312  0.    ]\n [-1.0498 -0.3941  0.      0.      0.      0.      0.    ]\n [-0.2709  0.0236  0.0746  0.3704  0.      0.      0.    ]\n [ 1.6525 -0.861   0.5642  0.9961  0.7463  0.      0.    ]\n [ 0.4122 -0.118   1.5491  1.9578  0.      0.      0.    ]\n [-1.6237  1.3636 -0.2079  0.      0.      0.      0.    ]\n [ 0.      0.      0.      0.      0.      0.      0.    ]\n [ 0.      0.      0.      0.      0.      0.      0.    ]\n [-1.3268  0.9881  0.531   0.      0.      0.      0.    ]\n [ 0.0284 -1.4974 -0.545  -1.2795  0.7032  1.4058  0.1412]]\n\n```\n\nFor a more realistic example, try wrapping [`preprocessing.image.ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\nFirst download the data:\n```\nflowers = tf.keras.utils.get_file(\n    'flower_photos',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n    untar=True)\n\n```\n```\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n228813984/228813984 ━━━━━━━━━━━━━━━━━━━━ 2s 0us/step\n\n```\n\nCreate the [`image.ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n```\nimg_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)\n\n```\n```\nimages, labels = next(img_gen.flow_from_directory(flowers))\n\n```\n```\nFound 3670 images belonging to 5 classes.\n\n```\n```\nprint(images.dtype, images.shape)\nprint(labels.dtype, labels.shape)\n\n```\n```\nfloat32 (32, 256, 256, 3)\nfloat32 (32, 5)\n\n```\n```\nds = tf.data.Dataset.from_generator(\n    lambda: img_gen.flow_from_directory(flowers),\n    output_types=(tf.float32, tf.float32),\n    output_shapes=([32,256,256,3], [32,5])\n)\n\nds.element_spec\n\n```\n```\n(TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None),\n TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))\n\n```\n```\nfor images, labels in ds.take(1):\n  print('images.shape: ', images.shape)\n  print('labels.shape: ', labels.shape)\n\n```\n```\nFound 3670 images belonging to 5 classes.\nimages.shape:  (32, 256, 256, 3)\nlabels.shape:  (32, 5)\n\n```\n\n### Consuming TFRecord data\nRefer to the [Loading TFRecords](https://www.tensorflow.org/tutorials/load_data/tfrecord) tutorial for an end-to-end example.\nThe [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API supports a variety of file formats so that you can process large datasets that do not fit in memory. For example, the TFRecord file format is a simple record-oriented binary format that many TensorFlow applications use for training data. The [`tf.data.TFRecordDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset) class enables you to stream over the contents of one or more TFRecord files as part of an input pipeline.\nHere is an example using the test file from the French Street Name Signs (FSNS).\n```\n# Creates a dataset that reads all of the examples from two files.\nfsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n\n```\n```\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\n7904079/7904079 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n```\n\nThe `filenames` argument to the `TFRecordDataset` initializer can either be a string, a list of strings, or a [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) of strings. Therefore if you have two sets of files for training and validation purposes, you can create a factory method that produces the dataset, taking filenames as an input argument:\n```\ndataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\ndataset\n\n```\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n```\n\nMany TensorFlow projects use serialized [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) records in their TFRecord files. These need to be decoded before they can be inspected:\n```\nraw_example = next(iter(dataset))\nparsed = tf.train.Example.FromString(raw_example.numpy())\n\nparsed.features.feature['image/text']\n\n```\n```\nbytes_list {\n  value: \"Rue Perreyon\"\n}\n\n```\n\n### Consuming text data\nRefer to the [Load text](https://www.tensorflow.org/tutorials/load_data/text) tutorial for an end-to-end example.\nMany datasets are distributed as one or more text files. The [`tf.data.TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) provides an easy way to extract lines from one or more text files. Given one or more filenames, a `TextLineDataset` will produce one string-valued element per line of those files.\n```\ndirectory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\nfile_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n\nfile_paths = [\n    tf.keras.utils.get_file(file_name, directory_url + file_name)\n    for file_name in file_names\n]\n\n```\n```\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/cowper.txt\n815980/815980 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/derby.txt\n809730/809730 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/illiad/butler.txt\n807992/807992 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n```\n```\ndataset = tf.data.TextLineDataset(file_paths)\n\n```\n\nHere are the first few lines of the first file:\n```\nfor line in dataset.take(5):\n  print(line.numpy())\n\n```\n```\nb\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\nb'His wrath pernicious, who ten thousand woes'\nb\"Caused to Achaia's host, sent many a soul\"\nb'Illustrious into Ades premature,'\nb'And Heroes gave (so stood the will of Jove)'\n\n```\n\nTo alternate lines between files use [`Dataset.interleave`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#interleave). This makes it easier to shuffle files together. Here are the first, second and third lines from each translation:\n```\nfiles_ds = tf.data.Dataset.from_tensor_slices(file_paths)\nlines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n\nfor i, line in enumerate(lines_ds.take(9)):\n  if i % 3 == 0:\n    print()\n  print(line.numpy())\n\n```\n```\nb\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\nb\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\"\nb'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'\n\nb'His wrath pernicious, who ten thousand woes'\nb'The vengeance, deep and deadly; whence to Greece'\nb'countless ills upon the Achaeans. Many a brave soul did it send'\n\nb\"Caused to Achaia's host, sent many a soul\"\nb'Unnumbered ills arose; which many a soul'\nb'hurrying down to Hades, and many a hero did it yield a prey to dogs and'\n\n```\n\nBy default, a `TextLineDataset` yields _every_ line of each file, which may not be desirable, for example, if the file starts with a header line, or contains comments. These lines can be removed using the [`Dataset.skip()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#skip) or [`Dataset.filter`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter) transformations. Here, you skip the first line, then filter to find only survivors.\n```\ntitanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\ntitanic_lines = tf.data.TextLineDataset(titanic_file)\n\n```\n```\nDownloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n30874/30874 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step\n\n```\n```\nfor line in titanic_lines.take(10):\n  print(line.numpy())\n\n```\n```\nb'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'\nb'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'\nb'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\nb'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\nb'1,female,35.0,1,0,53.1,First,C,Southampton,n'\nb'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'\nb'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'\nb'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\nb'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\nb'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n\n```\n```\ndefsurvived(line):\n  return tf.not_equal(tf.strings.substr(line, 0, 1), \"0\")\n\nsurvivors = titanic_lines.skip(1).filter(survived)\n\n```\n```\nfor line in survivors.take(10):\n  print(line.numpy())\n\n```\n```\nb'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\nb'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\nb'1,female,35.0,1,0,53.1,First,C,Southampton,n'\nb'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\nb'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\nb'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\nb'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'\nb'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'\nb'1,male,28.0,0,0,35.5,First,A,Southampton,y'\nb'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n'\n\n```\n\n### Consuming CSV data\nRefer to the [Loading CSV Files](https://www.tensorflow.org/tutorials/load_data/csv) and [Loading Pandas DataFrames](https://www.tensorflow.org/tutorials/load_data/pandas_dataframe) tutorials for more examples.\nThe CSV file format is a popular format for storing tabular data in plain text.\nFor example:\n```\ntitanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n\n```\n```\ndf = pd.read_csv(titanic_file)\ndf.head()\n\n```\n\nIf your data fits in memory the same [`Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) method works on dictionaries, allowing this data to be easily imported:\n```\ntitanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n\nfor feature_batch in titanic_slices.take(1):\n  for key, value in feature_batch.items():\n    print(\"  {!r:20s}: {}\".format(key, value))\n\n```\n```\n'survived'          : 0\n  'sex'               : b'male'\n  'age'               : 22.0\n  'n_siblings_spouses': 1\n  'parch'             : 0\n  'fare'              : 7.25\n  'class'             : b'Third'\n  'deck'              : b'unknown'\n  'embark_town'       : b'Southampton'\n  'alone'             : b'n'\n\n```\n\nA more scalable approach is to load from disk as necessary.\nThe [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) module provides methods to extract records from one or more CSV files that comply with \nThe [`tf.data.experimental.make_csv_dataset`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset) function is the high-level interface for reading sets of CSV files. It supports column type inference and many other features, like batching and shuffling, to make usage simple.\n```\ntitanic_batches = tf.data.experimental.make_csv_dataset(\n    titanic_file, batch_size=4,\n    label_name=\"survived\")\n\n```\n```\nfor feature_batch, label_batch in titanic_batches.take(1):\n  print(\"'survived': {}\".format(label_batch))\n  print(\"features:\")\n  for key, value in feature_batch.items():\n    print(\"  {!r:20s}: {}\".format(key, value))\n\n```\n```\n'survived': [0 0 0 0]\nfeatures:\n  'sex'               : [b'male' b'male' b'male' b'male']\n  'age'               : [28. 46. 28. 26.]\n  'n_siblings_spouses': [0 1 0 0]\n  'parch'             : [1 0 0 0]\n  'fare'              : [33.     61.175   8.05    7.8875]\n  'class'             : [b'Second' b'First' b'Third' b'Third']\n  'deck'              : [b'unknown' b'E' b'unknown' b'unknown']\n  'embark_town'       : [b'Southampton' b'Southampton' b'Southampton' b'Southampton']\n  'alone'             : [b'n' b'n' b'y' b'y']\n\n```\n\nYou can use the `select_columns` argument if you only need a subset of columns.\n```\ntitanic_batches = tf.data.experimental.make_csv_dataset(\n    titanic_file, batch_size=4,\n    label_name=\"survived\", select_columns=['class', 'fare', 'survived'])\n\n```\n```\nfor feature_batch, label_batch in titanic_batches.take(1):\n  print(\"'survived': {}\".format(label_batch))\n  for key, value in feature_batch.items():\n    print(\"  {!r:20s}: {}\".format(key, value))\n\n```\n```\n'survived': [1 1 0 0]\n  'fare'              : [10.5   35.5   12.875 29.125]\n  'class'             : [b'Second' b'First' b'Second' b'Third']\n\n```\n\nThere is also a lower-level [`experimental.CsvDataset`](https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset) class which provides finer grained control. It does not support column type inference. Instead you must specify the type of each column.\n```\ntitanic_types  = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string, tf.string, tf.string, tf.string]\ndataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types , header=True)\n\nfor line in dataset.take(10):\n  print([item.numpy() for item in line])\n\n```\n```\n[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n[1, b'female', 38.0, 1, 0, 71.2833, b'First', b'C', b'Cherbourg', b'n']\n[1, b'female', 26.0, 0, 0, 7.925, b'Third', b'unknown', b'Southampton', b'y']\n[1, b'female', 35.0, 1, 0, 53.1, b'First', b'C', b'Southampton', b'n']\n[0, b'male', 28.0, 0, 0, 8.4583, b'Third', b'unknown', b'Queenstown', b'y']\n[0, b'male', 2.0, 3, 1, 21.075, b'Third', b'unknown', b'Southampton', b'n']\n[1, b'female', 27.0, 0, 2, 11.1333, b'Third', b'unknown', b'Southampton', b'n']\n[1, b'female', 14.0, 1, 0, 30.0708, b'Second', b'unknown', b'Cherbourg', b'n']\n[1, b'female', 4.0, 1, 1, 16.7, b'Third', b'G', b'Southampton', b'n']\n[0, b'male', 20.0, 0, 0, 8.05, b'Third', b'unknown', b'Southampton', b'y']\n\n```\n\nIf some columns are empty, this low-level interface allows you to provide default values instead of column types.\n```\n%%writefile missing.csv\n1,2,3,4\n,2,3,4\n1,,3,4\n1,2,,4\n1,2,3,\n,,,\n\n```\n```\nWriting missing.csv\n\n```\n```\n# Creates a dataset that reads all of the records from two CSV files, each with\n# four float columns which may have missing values.\n\nrecord_defaults = [999,999,999,999]\ndataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults)\ndataset = dataset.map(lambda *items: tf.stack(items))\ndataset\n\n```\n```\n<_MapDataset element_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None)>\n\n```\n```\nfor line in dataset:\n  print(line.numpy())\n\n```\n```\n[1 2 3 4]\n[999   2   3   4]\n[  1 999   3   4]\n[  1   2 999   4]\n[  1   2   3 999]\n[999 999 999 999]\n\n```\n\nBy default, a `CsvDataset` yields _every_ column of _every_ line of the file, which may not be desirable, for example if the file starts with a header line that should be ignored, or if some columns are not required in the input. These lines and fields can be removed with the `header` and `select_cols` arguments respectively.\n```\n# Creates a dataset that reads all of the records from two CSV files with\n# headers, extracting float data from columns 2 and 4.\nrecord_defaults = [999, 999] # Only provide defaults for the selected columns\ndataset = tf.data.experimental.CsvDataset(\"missing.csv\", record_defaults, select_cols=[1, 3])\ndataset = dataset.map(lambda *items: tf.stack(items))\ndataset\n\n```\n```\n<_MapDataset element_spec=TensorSpec(shape=(2,), dtype=tf.int32, name=None)>\n\n```\n```\nfor line in dataset:\n  print(line.numpy())\n\n```\n```\n[2 4]\n[2 4]\n[999   4]\n[2 4]\n[  2 999]\n[999 999]\n\n```\n\n### Consuming sets of files\nThere are many datasets distributed as a set of files, where each file is an example.\n```\nflowers_root = tf.keras.utils.get_file(\n    'flower_photos',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n    untar=True)\nflowers_root = pathlib.Path(flowers_root)\n\n```\n\nThe root directory contains a directory for each class:\n```\nfor item in flowers_root.glob(\"*\"):\n  print(item.name)\n\n```\n```\ndaisy\ntulips\nsunflowers\nLICENSE.txt\ndandelion\nroses\n\n```\n\nThe files in each class directory are examples:\n```\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n\nfor f in list_ds.take(5):\n  print(f.numpy())\n\n```\n```\nb'/home/kbuilder/.keras/datasets/flower_photos/tulips/4955884820_7e4ce4d7e5_m.jpg'\nb'/home/kbuilder/.keras/datasets/flower_photos/dandelion/6250363717_17732e992e_n.jpg'\nb'/home/kbuilder/.keras/datasets/flower_photos/tulips/14278331403_4c475f9a9b.jpg'\nb'/home/kbuilder/.keras/datasets/flower_photos/dandelion/480621885_4c8b50fa11_m.jpg'\nb'/home/kbuilder/.keras/datasets/flower_photos/tulips/5716293002_a8be6a6dd3_n.jpg'\n\n```\n\nRead the data using the [`tf.io.read_file`](https://www.tensorflow.org/api_docs/python/tf/io/read_file) function and extract the label from the path, returning `(image, label)` pairs:\n```\ndefprocess_path(file_path):\n  label = tf.strings.split(file_path, os.sep)[-2]\n  return tf.io.read_file(file_path), label\n\nlabeled_ds = list_ds.map(process_path)\n\n```\n```\nfor image_raw, label_text in labeled_ds.take(1):\n  print(repr(image_raw.numpy()[:100]))\n  print()\n  print(label_text.numpy())\n\n```\n```\nb'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x03\\x02\\x02\\x03\\x02\\x02\\x03\\x03\\x03\\x03\\x04\\x03\\x03\\x04\\x05\\x08\\x05\\x05\\x04\\x04\\x05\\n\\x07\\x07\\x06\\x08\\x0c\\n\\x0c\\x0c\\x0b\\n\\x0b\\x0b\\r\\x0e\\x12\\x10\\r\\x0e\\x11\\x0e\\x0b\\x0b\\x10\\x16\\x10\\x11\\x13\\x14\\x15\\x15\\x15\\x0c\\x0f\\x17\\x18\\x16\\x14\\x18\\x12\\x14\\x15\\x14\\xff\\xdb\\x00C\\x01\\x03\\x04\\x04\\x05\\x04\\x05'\n\nb'dandelion'\n\n```\n\n## Batching dataset elements\n### Simple batching\nThe simplest form of batching stacks `n` consecutive elements of a dataset into a single element. The [`Dataset.batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) transformation does exactly this, with the same constraints as the [`tf.stack()`](https://www.tensorflow.org/api_docs/python/tf/stack) operator, applied to each component of the elements: i.e., for each component _i_ , all elements must have a tensor of the exact same shape.\n```\ninc_dataset = tf.data.Dataset.range(100)\ndec_dataset = tf.data.Dataset.range(0, -100, -1)\ndataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\nbatched_dataset = dataset.batch(4)\n\nfor batch in batched_dataset.take(4):\n  print([arr.numpy() for arr in batch])\n\n```\n```\n[array([0, 1, 2, 3]), array([ 0, -1, -2, -3])]\n[array([4, 5, 6, 7]), array([-4, -5, -6, -7])]\n[array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11])]\n[array([12, 13, 14, 15]), array([-12, -13, -14, -15])]\n\n```\n\nWhile [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) tries to propagate shape information, the default settings of [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) result in an unknown batch size because the last batch may not be full. Note the `None`s in the shape:\n```\nbatched_dataset\n\n```\n```\n<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>\n\n```\n\nUse the `drop_remainder` argument to ignore that last batch, and get full shape propagation:\n```\nbatched_dataset = dataset.batch(7, drop_remainder=True)\nbatched_dataset\n\n```\n```\n<_BatchDataset element_spec=(TensorSpec(shape=(7,), dtype=tf.int64, name=None), TensorSpec(shape=(7,), dtype=tf.int64, name=None))>\n\n```\n\n### Batching tensors with padding\nThe above recipe works for tensors that all have the same size. However, many models (including sequence models) work with input data that can have varying size (for example, sequences of different lengths). To handle this case, the [`Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) transformation enables you to batch tensors of different shapes by specifying one or more dimensions in which they may be padded.\n```\ndataset = tf.data.Dataset.range(100)\ndataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\ndataset = dataset.padded_batch(4, padded_shapes=(None,))\n\nfor batch in dataset.take(2):\n  print(batch.numpy())\n  print()\n\n```\n```\n[[0 0 0]\n [1 0 0]\n [2 2 0]\n [3 3 3]]\n\n[[4 4 4 4 0 0 0]\n [5 5 5 5 5 0 0]\n [6 6 6 6 6 6 0]\n [7 7 7 7 7 7 7]]\n\n```\n\nThe [`Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch) transformation allows you to set different padding for each dimension of each component, and it may be variable-length (signified by `None` in the example above) or constant-length. It is also possible to override the padding value, which defaults to 0.\n## Training workflows\n### Processing multiple epochs\nThe [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) API offers two main ways to process multiple epochs of the same data.\nThe simplest way to iterate over a dataset in multiple epochs is to use the [`Dataset.repeat()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) transformation. First, create a dataset of titanic data:\n```\ntitanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\ntitanic_lines = tf.data.TextLineDataset(titanic_file)\n\n```\n```\ndefplot_batch_sizes(ds):\n  batch_sizes = [batch.shape[0] for batch in ds]\n  plt.bar(range(len(batch_sizes)), batch_sizes)\n  plt.xlabel('Batch number')\n  plt.ylabel('Batch size')\n\n```\n\nApplying the [`Dataset.repeat()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) transformation with no arguments will repeat the input indefinitely.\nThe [`Dataset.repeat`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) transformation concatenates its arguments without signaling the end of one epoch and the beginning of the next epoch. Because of this a [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) applied after [`Dataset.repeat`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) will yield batches that straddle epoch boundaries:\n```\ntitanic_batches = titanic_lines.repeat(3).batch(128)\nplot_batch_sizes(titanic_batches)\n\n```\n\nIf you need clear epoch separation, put [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) before the repeat:\n```\ntitanic_batches = titanic_lines.batch(128).repeat(3)\n\nplot_batch_sizes(titanic_batches)\n\n```\n\nIf you would like to perform a custom computation (for example, to collect statistics) at the end of each epoch then it's simplest to restart the dataset iteration on each epoch:\n```\nepochs = 3\ndataset = titanic_lines.batch(128)\n\nfor epoch in range(epochs):\n  for batch in dataset:\n    print(batch.shape)\n  print(\"End of epoch: \", epoch)\n\n```\n```\n(128,)\n(128,)\n(128,)\n(128,)\n(116,)\nEnd of epoch:  0\n(128,)\n(128,)\n(128,)\n(128,)\n(116,)\nEnd of epoch:  1\n(128,)\n(128,)\n(128,)\n(128,)\n(116,)\nEnd of epoch:  2\n\n```\n\n### Randomly shuffling input data\nThe [`Dataset.shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer.\nAdd an index to the dataset so you can see the effect:\n```\nlines = tf.data.TextLineDataset(titanic_file)\ncounter = tf.data.experimental.Counter()\n\ndataset = tf.data.Dataset.zip((counter, lines))\ndataset = dataset.shuffle(buffer_size=100)\ndataset = dataset.batch(20)\ndataset\n\n```\n```\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_44933/4092668703.py:2: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Dataset.counter(...)` instead.\n<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>\n\n```\n\nSince the `buffer_size` is 100, and the batch size is 20, the first batch contains no elements with an index over 120.\n```\nn,line_batch = next(iter(dataset))\nprint(n.numpy())\n\n```\n```\n[ 99  18   1  29  66  88  47  30  80  46  68  44  35  40  33  95 108 105\n  38 113]\n\n```\n\nAs with [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the order relative to [`Dataset.repeat`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) matters.\n[`Dataset.shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:\n```\ndataset = tf.data.Dataset.zip((counter, lines))\nshuffled = dataset.shuffle(buffer_size=100).batch(10).repeat(2)\n\nprint(\"Here are the item ID's near the epoch boundary:\\n\")\nfor n, line_batch in shuffled.skip(60).take(5):\n  print(n.numpy())\n\n```\n```\nHere are the item ID's near the epoch boundary:\n\n[469 414 497 584 615 612 625 627 603 621]\n[582 553 343 602 626 567 486 593 616 525]\n[557 576 478 533 591 398 484 431]\n[66 43 51 18 94  3 76 52 90 57]\n[  0 101  71  86  56  17  33  70 110  75]\n\n```\n```\nshuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\nplt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\nplt.ylabel(\"Mean item ID\")\nplt.legend()\n\n```\n```\n<matplotlib.legend.Legend at 0x7f373c471af0>\n\n```\n\nBut a repeat before a shuffle mixes the epoch boundaries together:\n```\ndataset = tf.data.Dataset.zip((counter, lines))\nshuffled = dataset.repeat(2).shuffle(buffer_size=100).batch(10)\n\nprint(\"Here are the item ID's near the epoch boundary:\\n\")\nfor n, line_batch in shuffled.skip(55).take(15):\n  print(n.numpy())\n\n```\n```\nHere are the item ID's near the epoch boundary:\n\n[583 415   1 542 563   9 620 622 551 548]\n[589 592 365 571  33 557 618  31 541  27]\n[537  24 615  43  18 550  11   8  39 369]\n[601  38 485  20 627  46  22  23 322 608]\n[626 590 491  63  29 564  17  19 617  66]\n[508 580  72  45  57  54 556  62  14 511]\n[623  73  75  79 599 372  21  83 547  26]\n[486   4   0 573  74  49   0  53  95  34]\n[ 60 605  15  90  99 549  16  50  91  80]\n[106 108 112 297 561  44  52  82  86  71]\n[581  77 117  28 567  10  30   3  81  89]\n[587  32 102   7 135  51 113 110 114 451]\n[ 59  64  68 116  76 306 367 128 552 136]\n[111 569 522   5  67 616 154 131 512  37]\n[539 103 142  78  85   2  87  12 149 137]\n\n```\n```\nrepeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n\nplt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\nplt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\nplt.ylabel(\"Mean item ID\")\nplt.legend()\n\n```\n```\n<matplotlib.legend.Legend at 0x7f373c462b20>\n\n```\n\n## Preprocessing data\nThe [`Dataset.map(f)`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) transformation produces a new dataset by applying a given function `f` to each element of the input dataset. It is based on the `f` takes the [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects that represent a single element in the input, and returns the [`tf.Tensor`](https://www.tensorflow.org/api_docs/python/tf/Tensor) objects that will represent a single element in the new dataset. Its implementation uses standard TensorFlow operations to transform one element into another.\nThis section covers common examples of how to use [`Dataset.map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map).\n### Decoding image data and resizing it\nWhen training a neural network on real-world image data, it is often necessary to convert images of different sizes to a common size, so that they may be batched into a fixed size.\nRebuild the flower filenames dataset:\n```\nlist_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n\n```\n\nWrite a function that manipulates the dataset elements.\n```\n# Reads an image from a file, decodes it into a dense tensor, and resizes it\n# to a fixed shape.\ndefparse_image(filename):\n  parts = tf.strings.split(filename, os.sep)\n  label = parts[-2]\n\n  image = tf.io.read_file(filename)\n  image = tf.io.decode_jpeg(image)\n  image = tf.image.convert_image_dtype(image, tf.float32)\n  image = tf.image.resize(image, [128, 128])\n  return image, label\n\n```\n\nTest that it works.\n```\nfile_path = next(iter(list_ds))\nimage, label = parse_image(file_path)\n\ndefshow(image, label):\n  plt.figure()\n  plt.imshow(image)\n  plt.title(label.numpy().decode('utf-8'))\n  plt.axis('off')\n\nshow(image, label)\n\n```\n\nMap it over the dataset.\n```\nimages_ds = list_ds.map(parse_image)\n\nfor image, label in images_ds.take(2):\n  show(image, label)\n\n```\n\n### Applying arbitrary Python logic\nFor performance reasons, use TensorFlow operations for preprocessing your data whenever possible. However, it is sometimes useful to call external Python libraries when parsing your input data. You can use the [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) operation in a [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) transformation.\nFor example, if you want to apply a random rotation, the [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) module only has [`tf.image.rot90`](https://www.tensorflow.org/api_docs/python/tf/image/rot90), which is not very useful for image augmentation.\nTo demonstrate [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function), try using the `scipy.ndimage.rotate` function instead:\n```\nimportscipy.ndimageasndimage\n\n@tf.py_function(Tout=tf.float32)\ndefrandom_rotate_image(image):\n  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape=False)\n  return image\n\n```\n```\nimage, label = next(iter(images_ds))\nimage = random_rotate_image(image)\nshow(image, label)\n\n```\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.07214577..1.0803627].\n\n```\n\nTo use this function with [`Dataset.map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) the same caveats apply as with [`Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator), you need to describe the return shapes and types when you apply the function:\n```\ndeftf_random_rotate_image(image, label):\n  im_shape = image.shape\n  image = random_rotate_image(image)\n  image.set_shape(im_shape)\n  return image, label\n\n```\n```\nrot_ds = images_ds.map(tf_random_rotate_image)\n\nfor image, label in rot_ds.take(2):\n  show(image, label)\n\n```\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.014158356..1.0156134].\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.067302234..1.1018459].\n\n```\n\n### Parsing `tf.Example` protocol buffer messages\nMany input pipelines extract [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protocol buffer messages from a TFRecord format. Each [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) record contains one or more \"features\", and the input pipeline typically converts these features into tensors.\n```\nfsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\ndataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\ndataset\n\n```\n```\n<TFRecordDatasetV2 element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)>\n\n```\n\nYou can work with [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example) protos outside of a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to understand the data:\n```\nraw_example = next(iter(dataset))\nparsed = tf.train.Example.FromString(raw_example.numpy())\n\nfeature = parsed.features.feature\nraw_img = feature['image/encoded'].bytes_list.value[0]\nimg = tf.image.decode_png(raw_img)\nplt.imshow(img)\nplt.axis('off')\n_ = plt.title(feature[\"image/text\"].bytes_list.value[0])\n\n```\n\n```\nraw_example = next(iter(dataset))\n\n```\n```\ndeftf_parse(eg):\n  example = tf.io.parse_example(\n      eg[tf.newaxis], {\n          'image/encoded': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n          'image/text': tf.io.FixedLenFeature(shape=(), dtype=tf.string)\n      })\n  return example['image/encoded'][0], example['image/text'][0]\n\n```\n```\nimg, txt = tf_parse(raw_example)\nprint(txt.numpy())\nprint(repr(img.numpy()[:20]), \"...\")\n\n```\n```\nb'Rue Perreyon'\nb'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02X' ...\n\n```\n```\ndecoded = dataset.map(tf_parse)\ndecoded\n\n```\n```\n<_MapDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))>\n\n```\n```\nimage_batch, text_batch = next(iter(decoded.batch(10)))\nimage_batch.shape\n\n```\n```\nTensorShape([10])\n\n```\n\n### Time series windowing\nFor an end-to-end time series example see: [Time series forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series).\nTime series data is often organized with the time axis intact.\nUse a simple [`Dataset.range`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#range) to demonstrate:\n```\nrange_ds = tf.data.Dataset.range(100000)\n\n```\n\nTypically, models based on this sort of data will want a contiguous time slice.\nThe simplest approach would be to batch the data:\n#### Using `batch`\n```\nbatches = range_ds.batch(10, drop_remainder=True)\n\nfor batch in batches.take(5):\n  print(batch.numpy())\n\n```\n```\n[0 1 2 3 4 5 6 7 8 9]\n[10 11 12 13 14 15 16 17 18 19]\n[20 21 22 23 24 25 26 27 28 29]\n[30 31 32 33 34 35 36 37 38 39]\n[40 41 42 43 44 45 46 47 48 49]\n\n```\n\nOr to make dense predictions one step into the future, you might shift the features and labels by one step relative to each other:\n```\ndefdense_1_step(batch):\n  # Shift features and labels one step relative to each other.\n  return batch[:-1], batch[1:]\n\npredict_dense_1_step = batches.map(dense_1_step)\n\nfor features, label in predict_dense_1_step.take(3):\n  print(features.numpy(), \" => \", label.numpy())\n\n```\n```\n[0 1 2 3 4 5 6 7 8]  =>  [1 2 3 4 5 6 7 8 9]\n[10 11 12 13 14 15 16 17 18]  =>  [11 12 13 14 15 16 17 18 19]\n[20 21 22 23 24 25 26 27 28]  =>  [21 22 23 24 25 26 27 28 29]\n\n```\n\nTo predict a whole window instead of a fixed offset you can split the batches into two parts:\n```\nbatches = range_ds.batch(15, drop_remainder=True)\n\ndeflabel_next_5_steps(batch):\n  return (batch[:-5],   # Inputs: All except the last 5 steps\n          batch[-5:])   # Labels: The last 5 steps\n\npredict_5_steps = batches.map(label_next_5_steps)\n\nfor features, label in predict_5_steps.take(3):\n  print(features.numpy(), \" => \", label.numpy())\n\n```\n```\n[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12 13 14]\n[15 16 17 18 19 20 21 22 23 24]  =>  [25 26 27 28 29]\n[30 31 32 33 34 35 36 37 38 39]  =>  [40 41 42 43 44]\n\n```\n\nTo allow some overlap between the features of one batch and the labels of another, use [`Dataset.zip`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#zip):\n```\nfeature_length = 10\nlabel_length = 3\n\nfeatures = range_ds.batch(feature_length, drop_remainder=True)\nlabels = range_ds.batch(feature_length).skip(1).map(lambda labels: labels[:label_length])\n\npredicted_steps = tf.data.Dataset.zip((features, labels))\n\nfor features, label in predicted_steps.take(5):\n  print(features.numpy(), \" => \", label.numpy())\n\n```\n```\n[0 1 2 3 4 5 6 7 8 9]  =>  [10 11 12]\n[10 11 12 13 14 15 16 17 18 19]  =>  [20 21 22]\n[20 21 22 23 24 25 26 27 28 29]  =>  [30 31 32]\n[30 31 32 33 34 35 36 37 38 39]  =>  [40 41 42]\n[40 41 42 43 44 45 46 47 48 49]  =>  [50 51 52]\n\n```\n\n#### Using `window`\nWhile using [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) works, there are situations where you may need finer control. The [`Dataset.window`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) method gives you complete control, but requires some care: it returns a `Dataset` of `Datasets`. Go to the [Dataset structure](https://www.tensorflow.org/guide/data#dataset_structure) section for details.\n```\nwindow_size = 5\n\nwindows = range_ds.window(window_size, shift=1)\nfor sub_ds in windows.take(5):\n  print(sub_ds)\n\n```\n```\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n<_VariantDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n\n```\n\nThe [`Dataset.flat_map`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map) method can take a dataset of datasets and flatten it into a single dataset:\n```\nfor x in windows.flat_map(lambda x: x).take(30):\n   print(x.numpy(), end=' ')\n\n```\n```\n0 1 2 3 4 1 2 3 4 5 2 3 4 5 6 3 4 5 6 7 4 5 6 7 8 5 6 7 8 9\n\n```\n\nIn nearly all cases, you will want to [`Dataset.batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the dataset first:\n```\ndefsub_to_batch(sub):\n  return sub.batch(window_size, drop_remainder=True)\n\nfor example in windows.flat_map(sub_to_batch).take(5):\n  print(example.numpy())\n\n```\n```\n[0 1 2 3 4]\n[1 2 3 4 5]\n[2 3 4 5 6]\n[3 4 5 6 7]\n[4 5 6 7 8]\n\n```\n\nNow, you can see that the `shift` argument controls how much each window moves over.\nPutting this together you might write this function:\n```\ndefmake_window_dataset(ds, window_size=5, shift=1, stride=1):\n  windows = ds.window(window_size, shift=shift, stride=stride)\n\n  defsub_to_batch(sub):\n    return sub.batch(window_size, drop_remainder=True)\n\n  windows = windows.flat_map(sub_to_batch)\n  return windows\n\n```\n```\nds = make_window_dataset(range_ds, window_size=10, shift = 5, stride=3)\n\nfor example in ds.take(10):\n  print(example.numpy())\n\n```\n```\n[ 0  3  6  9 12 15 18 21 24 27]\n[ 5  8 11 14 17 20 23 26 29 32]\n[10 13 16 19 22 25 28 31 34 37]\n[15 18 21 24 27 30 33 36 39 42]\n[20 23 26 29 32 35 38 41 44 47]\n[25 28 31 34 37 40 43 46 49 52]\n[30 33 36 39 42 45 48 51 54 57]\n[35 38 41 44 47 50 53 56 59 62]\n[40 43 46 49 52 55 58 61 64 67]\n[45 48 51 54 57 60 63 66 69 72]\n\n```\n\nThen it's easy to extract labels, as before:\n```\ndense_labels_ds = ds.map(dense_1_step)\n\nfor inputs,labels in dense_labels_ds.take(3):\n  print(inputs.numpy(), \"=>\", labels.numpy())\n\n```\n```\n[ 0  3  6  9 12 15 18 21 24] => [ 3  6  9 12 15 18 21 24 27]\n[ 5  8 11 14 17 20 23 26 29] => [ 8 11 14 17 20 23 26 29 32]\n[10 13 16 19 22 25 28 31 34] => [13 16 19 22 25 28 31 34 37]\n\n```\n\n### Resampling\nWhen working with a dataset that is very class-imbalanced, you may want to resample the dataset. [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) provides two methods to do this. The credit card fraud dataset is a good example of this sort of problem.\n```\nzip_path = tf.keras.utils.get_file(\n    origin='https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n    fname='creditcard.zip',\n    extract=True)\n\ncsv_path = zip_path.replace('.zip', '.csv')\n\n```\n```\nDownloading data from https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip\n69155632/69155632 ━━━━━━━━━━━━━━━━━━━━ 1s 0us/step\n\n```\n```\ncreditcard_ds = tf.data.experimental.make_csv_dataset(\n    csv_path, batch_size=1024, label_name=\"Class\",\n    # Set the column types: 30 floats and an int.\n    column_defaults=[float()]*30+[int()])\n\n```\n\nNow, check the distribution of classes, it is highly skewed:\n```\ndefcount(counts, batch):\n  features, labels = batch\n  class_1 = labels == 1\n  class_1 = tf.cast(class_1, tf.int32)\n\n  class_0 = labels == 0\n  class_0 = tf.cast(class_0, tf.int32)\n\n  counts['class_0'] += tf.reduce_sum(class_0)\n  counts['class_1'] += tf.reduce_sum(class_1)\n\n  return counts\n\n```\n```\ncounts = creditcard_ds.take(10).reduce(\n    initial_state={'class_0': 0, 'class_1': 0},\n    reduce_func = count)\n\ncounts = np.array([counts['class_0'].numpy(),\n                   counts['class_1'].numpy()]).astype(np.float32)\n\nfractions = counts/counts.sum()\nprint(fractions)\n\n```\n```\n[0.996 0.004]\n\n```\n\nA common approach to training with an imbalanced dataset is to balance it. [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) includes a few methods which enable this workflow:\n#### Datasets sampling\nOne approach to resampling a dataset is to use `sample_from_datasets`. This is more applicable when you have a separate [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for each class.\nHere, just use filter to generate them from the credit card fraud data:\n```\nnegative_ds = (\n  creditcard_ds\n    .unbatch()\n    .filter(lambda features, label: label==0)\n    .repeat())\npositive_ds = (\n  creditcard_ds\n    .unbatch()\n    .filter(lambda features, label: label==1)\n    .repeat())\n\n```\n```\nfor features, label in positive_ds.batch(10).take(1):\n  print(label.numpy())\n\n```\n```\n[1 1 1 1 1 1 1 1 1 1]\n\n```\n\nTo use [`tf.data.Dataset.sample_from_datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets) pass the datasets, and the weight for each:\n```\nbalanced_ds = tf.data.Dataset.sample_from_datasets(\n    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)\n\n```\n\nNow the dataset produces examples of each class with a 50/50 probability:\n```\nfor features, labels in balanced_ds.take(10):\n  print(labels.numpy())\n\n```\n```\n[1 0 1 0 0 1 0 1 1 0]\n[1 0 0 0 0 0 0 0 1 1]\n[0 0 1 0 0 1 0 0 1 0]\n[0 1 1 0 1 0 0 1 1 0]\n[0 1 1 0 0 0 1 1 1 1]\n[1 1 1 1 1 1 0 0 0 0]\n[0 1 1 0 1 0 0 1 1 1]\n[1 1 0 0 0 0 0 1 0 1]\n[1 1 0 1 1 1 1 0 0 1]\n[0 1 1 0 0 1 0 0 0 0]\n\n```\n\n#### Rejection resampling\nOne problem with the above [`Dataset.sample_from_datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#sample_from_datasets) approach is that it needs a separate [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) per class. You could use [`Dataset.filter`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#filter) to create those two datasets, but that results in all the data being loaded twice.\nThe [`tf.data.Dataset.rejection_resample`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#rejection_resample) method can be applied to a dataset to rebalance it, while only loading it once. Elements will be dropped or repeated to achieve balance.\nThe `rejection_resample` method takes a `class_func` argument. This `class_func` is applied to each dataset element, and is used to determine which class an example belongs to for the purposes of balancing.\nThe goal here is to balance the label distribution, and the elements of `creditcard_ds` are already `(features, label)` pairs. So the `class_func` just needs to return those labels:\n```\ndefclass_func(features, label):\n  return label\n\n```\n\nThe resampling method deals with individual examples, so in this case you must `unbatch` the dataset before applying that method.\nThe method needs a target distribution, and optionally an initial distribution estimate as inputs.\n```\nresample_ds = (\n    creditcard_ds\n    .unbatch()\n    .rejection_resample(class_func, target_dist=[0.5,0.5],\n                        initial_dist=fractions)\n    .batch(10))\n\n```\n```\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4968: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\nInstructions for updating:\nUse tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n\n```\n\nThe `rejection_resample` method returns `(class, example)` pairs where the `class` is the output of the `class_func`. In this case, the `example` was already a `(feature, label)` pair, so use `map` to drop the extra copy of the labels:\n```\nbalanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)\n\n```\n\nNow the dataset produces examples of each class with a 50/50 probability:\n```\nfor features, labels in balanced_ds.take(10):\n  print(labels.numpy())\n\n```\n```\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\nProportion of examples rejected by sampler is high: [0.995996118][0.995996118 0.00400390616][0 1]\n[1 0 1 0 1 0 1 0 1 1]\n[1 0 1 1 1 1 0 0 1 0]\n[1 0 1 1 0 1 0 0 0 1]\n[0 1 0 0 0 0 1 1 1 1]\n[1 0 0 0 1 1 1 0 1 0]\n[0 0 0 1 0 0 1 0 1 1]\n[0 1 0 0 0 0 1 0 1 0]\n[1 0 0 0 0 1 0 0 0 1]\n[0 0 0 0 1 1 1 1 1 0]\n[1 1 0 1 1 1 1 1 1 0]\n\n```\n\n## Iterator Checkpointing\nTensorflow supports [taking checkpoints](https://www.tensorflow.org/guide/checkpoint) so that when your training process restarts it can restore the latest checkpoint to recover most of its progress. In addition to checkpointing the model variables, you can also checkpoint the progress of the dataset iterator. This could be useful if you have a large dataset and don't want to start the dataset from the beginning on each restart. Note however that iterator checkpoints may be large, since transformations such as [`Dataset.shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle) and [`Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) require buffering elements within the iterator.\nTo include your iterator in a checkpoint, pass the iterator to the [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) constructor.\n```\nrange_ds = tf.data.Dataset.range(20)\n\niterator = iter(range_ds)\nckpt = tf.train.Checkpoint(step=tf.Variable(0), iterator=iterator)\nmanager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep=3)\n\nprint([next(iterator).numpy() for _ in range(5)])\n\nsave_path = manager.save()\n\nprint([next(iterator).numpy() for _ in range(5)])\n\nckpt.restore(manager.latest_checkpoint)\n\nprint([next(iterator).numpy() for _ in range(5)])\n\n```\n```\n[0, 1, 2, 3, 4]\n[5, 6, 7, 8, 9]\n[5, 6, 7, 8, 9]\n\n```\n\n## Using [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data) with [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras)\nThe [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) API simplifies many aspects of creating and executing machine learning models. Its [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and [`Model.evaluate`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate) and [`Model.predict`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) APIs support datasets as inputs. Here is a quick dataset and model setup:\n```\ntrain, test = tf.keras.datasets.fashion_mnist.load_data()\n\nimages, labels = train\nimages = images/255.0\nlabels = labels.astype(np.int32)\n\n```\n```\nfmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\nfmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)\n\nmodel = tf.keras.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(10)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n```\n\nPassing a dataset of `(feature, label)` pairs is all that's needed for [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) and [`Model.evaluate`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate):\n```\nmodel.fit(fmnist_train_ds, epochs=2)\n\n```\n```\nEpoch 1/2\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723685884.693688   45100 service.cc:146] XLA service 0x7f35cc006690 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1723685884.693721   45100 service.cc:154]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1723685884.693725   45100 service.cc:154]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1723685884.693728   45100 service.cc:154]   StreamExecutor device (2): Tesla T4, Compute Capability 7.5\nI0000 00:00:1723685884.693731   45100 service.cc:154]   StreamExecutor device (3): Tesla T4, Compute Capability 7.5\n136/1875 ━━━━━━━━━━━━━━━━━━━━ 1s 1ms/step - accuracy: 0.5241 - loss: 1.4346\nI0000 00:00:1723685885.241810   45100 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 3s 1ms/step - accuracy: 0.7449 - loss: 0.7643\nEpoch 2/2\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8381 - loss: 0.4704\n<keras.src.callbacks.history.History at 0x7f373f583250>\n\n```\n\nIf you pass an infinite dataset, for example by calling [`Dataset.repeat`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat), you just need to also pass the `steps_per_epoch` argument:\n```\nmodel.fit(fmnist_train_ds.repeat(), epochs=2, steps_per_epoch=20)\n\n```\n```\nEpoch 1/2\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8254 - loss: 0.4682  \nEpoch 2/2\n20/20 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.8622 - loss: 0.4263\n<keras.src.callbacks.history.History at 0x7f37443e1190>\n\n```\n\nFor evaluation you can pass the number of evaluation steps:\n```\nloss, accuracy = model.evaluate(fmnist_train_ds)\nprint(\"Loss :\", loss)\nprint(\"Accuracy :\", accuracy)\n\n```\n```\n1875/1875 ━━━━━━━━━━━━━━━━━━━━ 2s 1ms/step - accuracy: 0.8504 - loss: 0.4343\nLoss : 0.4353208839893341\nAccuracy : 0.849216639995575\n\n```\n\nFor long datasets, set the number of steps to evaluate:\n```\nloss, accuracy = model.evaluate(fmnist_train_ds.repeat(), steps=10)\nprint(\"Loss :\", loss)\nprint(\"Accuracy :\", accuracy)\n\n```\n```\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step - accuracy: 0.8411 - loss: 0.5209  \nLoss : 0.46679750084877014\nAccuracy : 0.84375\n\n```\n\nThe labels are not required when calling [`Model.predict`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict).\n```\npredict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)\nresult = model.predict(predict_ds, steps = 10)\nprint(result.shape)\n\n```\n```\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step  \n(320, 10)\n\n```\n\nBut the labels are ignored if you do pass a dataset containing them:\n```\nresult = model.predict(fmnist_train_ds, steps = 10)\nprint(result.shape)\n\n```\n```\n10/10 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \n(320, 10)\n\n```\n\n",
  "https://www.tensorflow.org/guide/estimator": "[View on TensorFlow.org](https://www.tensorflow.org/guide/estimator)  \n---  \nThis document introduces `tf.estimator`—a high-level TensorFlow API. Estimators encapsulate the following actions:\n  * Training\n  * Evaluation\n  * Prediction\n  * Export for serving\n\n\nTensorFlow implements several pre-made Estimators. Custom estimators are still suported, but mainly as a backwards compatibility measure. **Custom estimators should not be used for new code**. All Estimators—pre-made or custom ones—are classes based on the `tf.estimator.Estimator` class.\nFor a quick example, try [Estimator tutorials](https://www.tensorflow.org/tutorials/estimator/linear). For an overview of the API design, check the \n## Setup\n```\npip\n```\n```\nimporttempfile\nimportos\n\nimporttensorflowastf\nimporttensorflow_datasetsastfds\n\n```\n```\n2024-01-24 02:20:44.732508: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-01-24 02:20:44.732557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-01-24 02:20:44.734001: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\n## Advantages\nSimilar to a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model), an `estimator` is a model-level abstraction. The `tf.estimator` provides some capabilities currently still under development for [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras). These are:\n  * Parameter server based training\n  * Full [TFX](http://tensorflow.org/tfx) integration\n\n\n## Estimators Capabilities\nEstimators provide the following benefits:\n  * You can run Estimator-based models on a local host or on a distributed multi-server environment without changing your model. Furthermore, you can run Estimator-based models on CPUs, GPUs, or TPUs without recoding your model.\n  * Estimators provide a safe distributed training loop that controls how and when to: \n    * Load data\n    * Handle exceptions\n    * Create checkpoint files and recover from failures\n    * Save summaries for TensorBoard\n\n\nWhen writing an application with Estimators, you must separate the data input pipeline from the model. This separation simplifies experiments with different datasets.\n## Using pre-made Estimators\nPre-made Estimators enable you to work at a much higher conceptual level than the base TensorFlow APIs. You no longer have to worry about creating the computational graph or sessions since Estimators handle all the \"plumbing\" for you. Furthermore, pre-made Estimators let you experiment with different model architectures by making only minimal code changes. `tf.estimator.DNNClassifier`, for example, is a pre-made Estimator class that trains classification models based on dense, feed-forward neural networks.\nA TensorFlow program relying on a pre-made Estimator typically consists of the following four steps:\n### 1. Write an input functions\nFor example, you might create one function to import the training set and another function to import the test set. Estimators expect their inputs to be formatted as a pair of objects:\n  * A dictionary in which the keys are feature names and the values are Tensors (or SparseTensors) containing the corresponding feature data\n  * A Tensor containing one or more labels\n\n\nThe `input_fn` should return a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) that yields pairs in that format. \nFor example, the following code builds a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Titanic dataset's `train.csv` file:\n```\ndeftrain_input_fn():\n  titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n  titanic = tf.data.experimental.make_csv_dataset(\n      titanic_file, batch_size=32,\n      label_name=\"survived\")\n  titanic_batches = (\n      titanic.cache().repeat().shuffle(500)\n      .prefetch(tf.data.AUTOTUNE))\n  return titanic_batches\n\n```\n\nThe `input_fn` is executed in a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) and can also directly return a `(features_dics, labels)` pair containing graph tensors, but this is error prone outside of simple cases like returning constants.\n### 2. Define the feature columns.\nEach [`tf.feature_column`](https://www.tensorflow.org/api_docs/python/tf/feature_column) identifies a feature name, its type, and any input pre-processing. \nFor example, the following snippet creates three feature columns.\n  * The first uses the `age` feature directly as a floating-point input. \n  * The second uses the `class` feature as a categorical input.\n  * The third uses the `embark_town` as a categorical input, but uses the `hashing trick` to avoid the need to enumerate the options, and to set the number of options.\n\n\nFor further information, check the [feature columns tutorial](https://www.tensorflow.org/tutorials/keras/feature_columns).\n```\nage = tf.feature_column.numeric_column('age')\ncls = tf.feature_column.categorical_column_with_vocabulary_list('class', ['First', 'Second', 'Third']) \nembark = tf.feature_column.categorical_column_with_hash_bucket('embark_town', 32)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/4100412726.py:1: numeric_column (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/4100412726.py:2: categorical_column_with_vocabulary_list (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/4100412726.py:3: categorical_column_with_hash_bucket (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\n\n```\n\n### 3. Instantiate the relevant pre-made Estimator.\nFor example, here's a sample instantiation of a pre-made Estimator named `LinearClassifier`:\n```\nmodel_dir = tempfile.mkdtemp()\nmodel = tf.estimator.LinearClassifier(\n    model_dir=model_dir,\n    feature_columns=[embark, cls, age],\n    n_classes=2\n)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/1904628810.py:2: LinearClassifierV2.__init__ (from tensorflow_estimator.python.estimator.canned.linear) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/head_utils.py:54: BinaryClassHead.__init__ (from tensorflow_estimator.python.estimator.head.binary_class_head) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:944: Estimator.__init__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1844: RunConfig.__init__ (from tensorflow_estimator.python.estimator.run_config) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmpbt9n791j', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n\n```\n\nFor more information, you can go the [linear classifier tutorial](https://www.tensorflow.org/tutorials/estimator/linear).\n### 4. Call a training, evaluation, or inference method.\nAll Estimators provide `train`, `evaluate`, and `predict` methods.\n```\nmodel = model.train(input_fn=train_input_fn, steps=100)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:385: StopAtStepHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nDownloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n30874/30874 [==============================] - 0s 0us/step\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/legacy/ftrl.py:173: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\nInstructions for updating:\nCall initializer instance with the dtype argument instead of passing it to the constructor\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/model_fn.py:250: EstimatorSpec.__new__ (from tensorflow_estimator.python.estimator.model_fn) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Done calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1416: NanTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1419: LoggingTensorHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/basic_session_run_hooks.py:232: SecondOrStepTimer.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1456: CheckpointSaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Create CheckpointSaverHook.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:579: StepCounterHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:586: SummarySaverHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmpbt9n791j/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1455: SessionRunArgs.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1454: SessionRunContext.__init__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/monitored_session.py:1474: SessionRunValues.__new__ (from tensorflow.python.training.session_run_hook) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:loss = 0.6931472, step = 0\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 100...\nINFO:tensorflow:Saving checkpoints for 100 into /tmpfs/tmp/tmpbt9n791j/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 100...\nINFO:tensorflow:Loss for final step: 0.59910536.\n2024-01-24 02:20:52.016569: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n```\n```\nresult = model.evaluate(train_input_fn, steps=10)\n\nfor key, value in result.items():\n  print(key, \":\", value)\n\n```\n```\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2024-01-24T02:20:52\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/training/evaluation.py:260: FinalOpsHook.__init__ (from tensorflow.python.training.basic_session_run_hooks) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpbt9n791j/model.ckpt-100\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [4/10]\nINFO:tensorflow:Evaluation [5/10]\nINFO:tensorflow:Evaluation [6/10]\nINFO:tensorflow:Evaluation [7/10]\nINFO:tensorflow:Evaluation [8/10]\nINFO:tensorflow:Evaluation [9/10]\nINFO:tensorflow:Evaluation [10/10]\nINFO:tensorflow:Inference Time : 3.02373s\nINFO:tensorflow:Finished evaluation at 2024-01-24-02:20:55\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.715625, accuracy_baseline = 0.615625, auc = 0.7548389, auc_precision_recall = 0.66771877, average_loss = 0.56671846, global_step = 100, label/mean = 0.384375, loss = 0.56671846, precision = 0.7, prediction/mean = 0.37144834, recall = 0.45528457\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: /tmpfs/tmp/tmpbt9n791j/model.ckpt-100\naccuracy : 0.715625\naccuracy_baseline : 0.615625\nauc : 0.7548389\nauc_precision_recall : 0.66771877\naverage_loss : 0.56671846\nlabel/mean : 0.384375\nloss : 0.56671846\nprecision : 0.7\nprediction/mean : 0.37144834\nrecall : 0.45528457\nglobal_step : 100\n2024-01-24 02:20:55.868009: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n```\n```\nfor pred in model.predict(train_input_fn):\n  for key, value in pred.items():\n    print(key, \":\", value)\n  break\n\n```\n```\nINFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/base_head.py:786: ClassificationOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:561: RegressionOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/head/binary_class_head.py:563: PredictOutput.__init__ (from tensorflow.python.saved_model.model_utils.export_output) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpbt9n791j/model.ckpt-100\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nlogits : [-0.6829183]\nlogistic : [0.33561027]\nprobabilities : [0.6643897  0.33561027]\nclass_ids : [0]\nclasses : [b'0']\nall_class_ids : [0 1]\nall_classes : [b'0' b'1']\n2024-01-24 02:20:56.898963: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n```\n\n### Benefits of pre-made Estimators\nPre-made Estimators encode best practices, providing the following benefits:\n  * Best practices for determining where different parts of the computational graph should run, implementing strategies on a single machine or on a cluster.\n  * Best practices for event (summary) writing and universally useful summaries.\n\n\nIf you don't use pre-made Estimators, you must implement the preceding features yourself.\n## Custom Estimators\nThe heart of every Estimator—whether pre-made or custom—is its _model function_ , `model_fn`, which is a method that builds graphs for training, evaluation, and prediction. When you are using a pre-made Estimator, someone else has already implemented the model function. When relying on a custom Estimator, you must write the model function yourself.\n## Create an Estimator from a Keras model\nYou can convert existing Keras models to Estimators with `tf.keras.estimator.model_to_estimator`. This is helpful if you want to modernize your model code, but your training pipeline still requires Estimators. \nInstantiate a Keras MobileNet V2 model and compile the model with the optimizer, loss, and metrics to train with:\n```\nkeras_mobilenet_v2 = tf.keras.applications.MobileNetV2(\n    input_shape=(160, 160, 3), include_top=False)\nkeras_mobilenet_v2.trainable = False\n\nestimator_model = tf.keras.Sequential([\n    keras_mobilenet_v2,\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(1)\n])\n\n# Compile the model\nestimator_model.compile(\n    optimizer='adam',\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=['accuracy'])\n\n```\n```\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n9406464/9406464 [==============================] - 0s 0us/step\n\n```\n\nCreate an `Estimator` from the compiled Keras model. The initial model state of the Keras model is preserved in the created `Estimator`:\n```\nest_mobilenet_v2 = tf.keras.estimator.model_to_estimator(keras_model=estimator_model)\n\n```\n```\nINFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmpjcucleiz\nINFO:tensorflow:Using the Keras model provided.\nWARNING:absl:You are using `tf.keras.optimizers.experimental.Optimizer` in TF estimator, which only supports `tf.keras.optimizers.legacy.Optimizer`. Automatically converting your optimizer to `tf.keras.optimizers.legacy.Optimizer`.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n  warnings.warn(\n2024-01-24 02:21:01.520653: W tensorflow/c/c_api.cc:305] Operation '{name:'block_16_project_BN/gamma/Assign' id:2164 op device:{requested: '', assigned: ''} def:{ { {node block_16_project_BN/gamma/Assign} } = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](block_16_project_BN/gamma, block_16_project_BN/gamma/Initializer/ones)} }' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmpjcucleiz', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmpjcucleiz', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:2404: WarmStartSettings.__new__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:2404: WarmStartSettings.__new__ (from tensorflow_estimator.python.estimator.estimator) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\n\n```\n\nTreat the derived `Estimator` as you would with any other `Estimator`.\n```\nIMG_SIZE = 160  # All images will be resized to 160x160\n\ndefpreprocess(image, label):\n  image = tf.cast(image, tf.float32)\n  image = (image/127.5) - 1\n  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n  return image, label\n\n```\n```\ndeftrain_input_fn(batch_size):\n  data = tfds.load('cats_vs_dogs', as_supervised=True)\n  train_data = data['train']\n  train_data = train_data.map(preprocess).shuffle(500).batch(batch_size)\n  return train_data\n\n```\n\nTo train, call Estimator's train function:\n```\nest_mobilenet_v2.train(input_fn=lambda: train_input_fn(32), steps=50)\n\n```\n```\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmpfs/tmp/tmpjcucleiz/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/tmpfs/tmp/tmpjcucleiz/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\nINFO:tensorflow:Warm-starting from: /tmpfs/tmp/tmpjcucleiz/keras/keras_model.ckpt\nINFO:tensorflow:Warm-starting from: /tmpfs/tmp/tmpjcucleiz/keras/keras_model.ckpt\nINFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\nINFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\nINFO:tensorflow:Warm-started 158 variables.\nINFO:tensorflow:Warm-started 158 variables.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmpjcucleiz/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmpjcucleiz/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:loss = 0.6796611, step = 0\nINFO:tensorflow:loss = 0.6796611, step = 0\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50...\nINFO:tensorflow:Saving checkpoints for 50 into /tmpfs/tmp/tmpjcucleiz/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 50 into /tmpfs/tmp/tmpjcucleiz/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50...\nINFO:tensorflow:Loss for final step: 0.69532585.\nINFO:tensorflow:Loss for final step: 0.69532585.\n<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x7f29400d3670>\n\n```\n\nSimilarly, to evaluate, call the Estimator's evaluate function:\n```\nest_mobilenet_v2.evaluate(input_fn=lambda: train_input_fn(32), steps=10)\n\n```\n```\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n  updates = self.state_updates\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2024-01-24T02:21:19\nINFO:tensorflow:Starting evaluation at 2024-01-24T02:21:19\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpjcucleiz/model.ckpt-50\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpjcucleiz/model.ckpt-50\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [4/10]\nINFO:tensorflow:Evaluation [4/10]\nINFO:tensorflow:Evaluation [5/10]\nINFO:tensorflow:Evaluation [5/10]\nINFO:tensorflow:Evaluation [6/10]\nINFO:tensorflow:Evaluation [6/10]\nINFO:tensorflow:Evaluation [7/10]\nINFO:tensorflow:Evaluation [7/10]\nINFO:tensorflow:Evaluation [8/10]\nINFO:tensorflow:Evaluation [8/10]\nINFO:tensorflow:Evaluation [9/10]\nINFO:tensorflow:Evaluation [9/10]\nINFO:tensorflow:Evaluation [10/10]\nINFO:tensorflow:Evaluation [10/10]\nINFO:tensorflow:Inference Time : 2.81526s\nINFO:tensorflow:Inference Time : 2.81526s\nINFO:tensorflow:Finished evaluation at 2024-01-24-02:21:22\nINFO:tensorflow:Finished evaluation at 2024-01-24-02:21:22\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.54375, global_step = 50, loss = 0.6558426\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.54375, global_step = 50, loss = 0.6558426\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 50: /tmpfs/tmp/tmpjcucleiz/model.ckpt-50\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 50: /tmpfs/tmp/tmpjcucleiz/model.ckpt-50\n{'accuracy': 0.54375, 'loss': 0.6558426, 'global_step': 50}\n\n```\n\nFor more details, please refer to the documentation for `tf.keras.estimator.model_to_estimator`.\n## Saving object-based checkpoints with Estimator\nEstimators by default save checkpoints with variable names rather than the object graph described in the [Checkpoint guide](https://www.tensorflow.org/guide/checkpoint). [`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) will read name-based checkpoints, but variable names may change when moving parts of a model outside of the Estimator's `model_fn`. For forwards compatibility saving object-based checkpoints makes it easier to train a model inside an Estimator and then use it outside of one.\n```\nimporttensorflow.compat.v1astf_compat\n\n```\n```\ndeftoy_dataset():\n  inputs = tf.range(10.)[:, None]\n  labels = inputs * 5. + tf.range(5.)[None, :]\n  return tf.data.Dataset.from_tensor_slices(\n    dict(x=inputs, y=labels)).repeat().batch(2)\n\n```\n```\nclassNet(tf.keras.Model):\n\"\"\"A simple linear model.\"\"\"\n\n  def__init__(self):\n    super(Net, self).__init__()\n    self.l1 = tf.keras.layers.Dense(5)\n\n  defcall(self, x):\n    return self.l1(x)\n\n```\n```\ndefmodel_fn(features, labels, mode):\n  net = Net()\n  opt = tf.keras.optimizers.Adam(0.1)\n  ckpt = tf.train.Checkpoint(step=tf_compat.train.get_global_step(),\n                             optimizer=opt, net=net)\n  with tf.GradientTape() as tape:\n    output = net(features['x'])\n    loss = tf.reduce_mean(tf.abs(output - features['y']))\n  variables = net.trainable_variables\n  gradients = tape.gradient(loss, variables)\n  return tf.estimator.EstimatorSpec(\n    mode,\n    loss=loss,\n    train_op=tf.group(opt.apply_gradients(zip(gradients, variables)),\n                      ckpt.step.assign_add(1)),\n    # Tell the Estimator to save \"ckpt\" in an object-based format.\n    scaffold=tf_compat.train.Scaffold(saver=ckpt))\n\ntf.keras.backend.clear_session()\nest = tf.estimator.Estimator(model_fn, './tf_estimator_example/')\nest.train(toy_dataset, steps=10)\n\n```\n```\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_model_dir': './tf_estimator_example/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Using config: {'_model_dir': './tf_estimator_example/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into ./tf_estimator_example/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into ./tf_estimator_example/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:loss = 4.4765882, step = 1\nINFO:tensorflow:loss = 4.4765882, step = 1\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 4 vs previous value: 4. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\nWARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 4 vs previous value: 4. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\nINFO:tensorflow:Saving checkpoints for 10 into ./tf_estimator_example/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 10 into ./tf_estimator_example/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\nINFO:tensorflow:Loss for final step: 44.10201.\nINFO:tensorflow:Loss for final step: 44.10201.\n<tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x7f29407836a0>\n\n```\n\n[`tf.train.Checkpoint`](https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint) can then load the Estimator's checkpoints from its `model_dir`.\n```\nopt = tf.keras.optimizers.Adam(0.1)\nnet = Net()\nckpt = tf.train.Checkpoint(\n  step=tf.Variable(1, dtype=tf.int64), optimizer=opt, net=net)\nckpt.restore(tf.train.latest_checkpoint('./tf_estimator_example/'))\nckpt.step.numpy()  # From est.train(..., steps=10)\n\n```\n```\n10\n\n```\n\n## SavedModels from Estimators\nEstimators export SavedModels through `tf.Estimator.export_saved_model`.\n```\ninput_column = tf.feature_column.numeric_column(\"x\")\n\nestimator = tf.estimator.LinearClassifier(feature_columns=[input_column])\n\ndefinput_fn():\n  return tf.data.Dataset.from_tensor_slices(\n    ({\"x\": [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(16)\nestimator.train(input_fn)\n\n```\n```\nINFO:tensorflow:Using default config.\nINFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmps_vspwiv\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmps_vspwiv\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmps_vspwiv', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmps_vspwiv', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmps_vspwiv/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmps_vspwiv/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:loss = 0.6931472, step = 0\nINFO:tensorflow:loss = 0.6931472, step = 0\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 50...\nINFO:tensorflow:Saving checkpoints for 50 into /tmpfs/tmp/tmps_vspwiv/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 50 into /tmpfs/tmp/tmps_vspwiv/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 50...\nINFO:tensorflow:Loss for final step: 0.42848104.\nINFO:tensorflow:Loss for final step: 0.42848104.\n<tensorflow_estimator.python.estimator.canned.linear.LinearClassifierV2 at 0x7f2940787640>\n\n```\n\nTo save an `Estimator` you need to create a `serving_input_receiver`. This function builds a part of a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) that parses the raw data received by the SavedModel. \nThe `tf.estimator.export` module contains functions to help build these `receivers`.\nThe following code builds a receiver, based on the `feature_columns`, that accepts serialized `tf.Example` protocol buffers, which are often used with [tf-serving](https://tensorflow.org/serving).\n```\ntmpdir = tempfile.mkdtemp()\n\nserving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n  tf.feature_column.make_parse_example_spec([input_column]))\n\nestimator_base_path = os.path.join(tmpdir, 'from_estimator')\nestimator_path = estimator.export_saved_model(estimator_base_path, serving_input_fn)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/121230551.py:4: make_parse_example_spec_v2 (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/121230551.py:4: make_parse_example_spec_v2 (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Keras preprocessing layers instead, either directly or via the `tf.keras.utils.FeatureSpace` utility. Each of `tf.feature_column.*` has a functional equivalent in `tf.keras.layers` for feature preprocessing when training a Keras model.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/121230551.py:3: build_parsing_serving_input_receiver_fn (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/121230551.py:3: build_parsing_serving_input_receiver_fn (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/export/export.py:312: ServingInputReceiver.__new__ (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/export/export.py:312: ServingInputReceiver.__new__ (from tensorflow_estimator.python.estimator.export.export) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:168: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:168: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py:83: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/saved_model/model_utils/export_utils.py:83: get_tensor_from_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\nINFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\nINFO:tensorflow:Signatures INCLUDED in export for Classify: ['serving_default', 'classification']\nINFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\nINFO:tensorflow:Signatures INCLUDED in export for Regress: ['regression']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\nINFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Train: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Signatures INCLUDED in export for Eval: None\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmps_vspwiv/model.ckpt-50\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmps_vspwiv/model.ckpt-50\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:Assets added to graph.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:No assets to write.\nINFO:tensorflow:SavedModel written to: /tmpfs/tmp/tmpvsrxtilj/from_estimator/temp-1706062885/saved_model.pb\nINFO:tensorflow:SavedModel written to: /tmpfs/tmp/tmpvsrxtilj/from_estimator/temp-1706062885/saved_model.pb\n\n```\n\nYou can also load and run that model, from python:\n```\nimported = tf.saved_model.load(estimator_path)\n\ndefpredict(x):\n  example = tf.train.Example()\n  example.features.feature[\"x\"].float_list.value.extend([x])\n  return imported.signatures[\"predict\"](\n    examples=tf.constant([example.SerializeToString()]))\n\n```\n```\nprint(predict(1.5))\nprint(predict(3.5))\n\n```\n```\n{'logistic': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.576523]], dtype=float32)>, 'probabilities': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.42347696, 0.576523  ]], dtype=float32)>, 'class_ids': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[1]])>, 'logits': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.30851614]], dtype=float32)>, 'classes': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'1']], dtype=object)>, 'all_class_ids': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>, 'all_classes': <tf.Tensor: shape=(1, 2), dtype=string, numpy=array([[b'0', b'1']], dtype=object)>}\n{'logistic': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.2338471]], dtype=float32)>, 'probabilities': <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.7661529 , 0.23384713]], dtype=float32)>, 'class_ids': <tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[0]])>, 'logits': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[-1.1867142]], dtype=float32)>, 'classes': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'0']], dtype=object)>, 'all_class_ids': <tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)>, 'all_classes': <tf.Tensor: shape=(1, 2), dtype=string, numpy=array([[b'0', b'1']], dtype=object)>}\n\n```\n\n`tf.estimator.export.build_raw_serving_input_receiver_fn` allows you to create input functions which take raw tensors rather than [`tf.train.Example`](https://www.tensorflow.org/api_docs/python/tf/train/Example)s.\n## Using [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with Estimator (Limited support)\n`tf.estimator` is a distributed training TensorFlow API that originally supported the async parameter server approach. `tf.estimator` now supports [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy). If you're using `tf.estimator`, you can change to distributed training with very few changes to your code. With this, Estimator users can now do synchronous distributed training on multiple GPUs and multiple workers, as well as use TPUs. This support in Estimator is, however, limited. Check out the [What's supported now](https://www.tensorflow.org/guide/estimator#estimator_support) section below for more details.\nUsing [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) with Estimator is slightly different than in the Keras case. Instead of using `strategy.scope`, now you pass the strategy object into the `RunConfig` for the Estimator.\nYou can refer to the [distributed training guide](https://www.tensorflow.org/guide/distributed_training) for more information.\nHere is a snippet of code that shows this with a premade Estimator `LinearRegressor` and `MirroredStrategy`:\n```\nmirrored_strategy = tf.distribute.MirroredStrategy()\nconfig = tf.estimator.RunConfig(\n    train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)\nregressor = tf.estimator.LinearRegressor(\n    feature_columns=[tf.feature_column.numeric_column('feats')],\n    optimizer='SGD',\n    config=config)\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\nINFO:tensorflow:Initializing RunConfig with distribution strategies.\nINFO:tensorflow:Not using Distribute Coordinator.\nINFO:tensorflow:Not using Distribute Coordinator.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/4277059788.py:4: LinearRegressorV2.__init__ (from tensorflow_estimator.python.estimator.canned.linear) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/tmp/ipykernel_9073/4277059788.py:4: LinearRegressorV2.__init__ (from tensorflow_estimator.python.estimator.canned.linear) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1344: RegressionHead.__init__ (from tensorflow_estimator.python.estimator.head.regression_head) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/canned/linear.py:1344: RegressionHead.__init__ (from tensorflow_estimator.python.estimator.head.regression_head) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.keras instead.\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmpjagdxixr\nWARNING:tensorflow:Using temporary folder as model directory: /tmpfs/tmp/tmpjagdxixr\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmpjagdxixr', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f28cc70eaf0>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f28cc70eaf0>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\nINFO:tensorflow:Using config: {'_model_dir': '/tmpfs/tmp/tmpjagdxixr', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f28cc70eaf0>, '_device_fn': None, '_protocol': None, '_eval_distribute': <tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7f28cc70eaf0>, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}\n\n```\n\nHere, you use a premade Estimator, but the same code works with a custom Estimator as well. `train_distribute` determines how training will be distributed, and `eval_distribute` determines how evaluation will be distributed. This is another difference from Keras where you use the same strategy for both training and eval.\nNow you can train and evaluate this Estimator with an input function:\n```\ndefinput_fn():\n  dataset = tf.data.Dataset.from_tensors(({\"feats\":[1.]}, [1.]))\n  return dataset.repeat(1000).batch(10)\nregressor.train(input_fn=input_fn, steps=10)\nregressor.evaluate(input_fn=input_fn, steps=10)\n\n```\n```\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1246: StrategyBase.configure (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\nInstructions for updating:\nuse `update_config_proto` instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/estimator.py:1246: StrategyBase.configure (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\nInstructions for updating:\nuse `update_config_proto` instead.\n/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:462: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n  warnings.warn(\"To make it possible to preserve tf.data options across \"\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Collective all_reduce tensors: 2 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Collective all_reduce tensors: 2 all_reduces, num_devices = 4, group_size = 4, implementation = CommunicationImplementation.NCCL, num_packs = 1\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Create CheckpointSaverHook.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/util.py:95: DistributedIteratorV1.initialize (from tensorflow.python.distribute.v1.input_lib) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the iterator's `initializer` property instead.\nWARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow_estimator/python/estimator/util.py:95: DistributedIteratorV1.initialize (from tensorflow.python.distribute.v1.input_lib) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the iterator's `initializer` property instead.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmpjagdxixr/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 0 into /tmpfs/tmp/tmpjagdxixr/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n2024-01-24 02:21:31.119820: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.121097: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.125859: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.126357: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.137091: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.137578: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.143860: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:31.144344: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\nINFO:tensorflow:loss = 4.0, step = 0\nINFO:tensorflow:loss = 4.0, step = 0\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\nINFO:tensorflow:Saving checkpoints for 10 into /tmpfs/tmp/tmpjagdxixr/model.ckpt.\nINFO:tensorflow:Saving checkpoints for 10 into /tmpfs/tmp/tmpjagdxixr/model.ckpt.\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\nINFO:tensorflow:Loss for final step: 1.1510792e-12.\nINFO:tensorflow:Loss for final step: 1.1510792e-12.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Reduce to /replica:0/task:0/device:CPU:0 then broadcast to ('/replica:0/task:0/device:CPU:0',).\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2024-01-24T02:21:35\nINFO:tensorflow:Starting evaluation at 2024-01-24T02:21:35\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpjagdxixr/model.ckpt-10\nINFO:tensorflow:Restoring parameters from /tmpfs/tmp/tmpjagdxixr/model.ckpt-10\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Done running local_init_op.\n2024-01-24 02:21:36.170934: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.172154: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.176484: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.176957: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.182511: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.182980: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.196737: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorFromStringHandle' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorFromStringHandle} }\n    .  Registered:  device='CPU'\n\n2024-01-24 02:21:36.197227: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'MultiDeviceIteratorGetNextFromShard' OpKernel for GPU devices compatible with node { {node MultiDeviceIteratorGetNextFromShard} }\n    .  Registered:  device='CPU'\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [1/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:Evaluation [2/10]\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [3/10]\nINFO:tensorflow:Evaluation [4/10]\nINFO:tensorflow:Evaluation [4/10]\nINFO:tensorflow:Evaluation [5/10]\nINFO:tensorflow:Evaluation [5/10]\nINFO:tensorflow:Evaluation [6/10]\nINFO:tensorflow:Evaluation [6/10]\nINFO:tensorflow:Evaluation [7/10]\nINFO:tensorflow:Evaluation [7/10]\nINFO:tensorflow:Evaluation [8/10]\nINFO:tensorflow:Evaluation [8/10]\nINFO:tensorflow:Evaluation [9/10]\nINFO:tensorflow:Evaluation [9/10]\nINFO:tensorflow:Evaluation [10/10]\nINFO:tensorflow:Evaluation [10/10]\nINFO:tensorflow:Inference Time : 1.83647s\nINFO:tensorflow:Inference Time : 1.83647s\nINFO:tensorflow:Finished evaluation at 2024-01-24-02:21:37\nINFO:tensorflow:Finished evaluation at 2024-01-24-02:21:37\nINFO:tensorflow:Saving dict for global step 10: average_loss = 1.4210855e-14, global_step = 10, label/mean = 1.0, loss = 1.4210855e-14, prediction/mean = 0.99999994\nINFO:tensorflow:Saving dict for global step 10: average_loss = 1.4210855e-14, global_step = 10, label/mean = 1.0, loss = 1.4210855e-14, prediction/mean = 0.99999994\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /tmpfs/tmp/tmpjagdxixr/model.ckpt-10\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /tmpfs/tmp/tmpjagdxixr/model.ckpt-10\n{'average_loss': 1.4210855e-14,\n 'label/mean': 1.0,\n 'loss': 1.4210855e-14,\n 'prediction/mean': 0.99999994,\n 'global_step': 10}\n\n```\n\nAnother difference to highlight here between Estimator and Keras is the input handling. In Keras, each batch of the dataset is split automatically across the multiple replicas. In Estimator, however, you do not perform automatic batch splitting, nor automatically shard the data across different workers. You have full control over how you want your data to be distributed across workers and devices, and you must provide an `input_fn` to specify how to distribute your data.\nYour `input_fn` is called once per worker, thus giving one dataset per worker. Then one batch from that dataset is fed to one replica on that worker, thereby consuming N batches for N replicas on 1 worker. In other words, the dataset returned by the `input_fn` should provide batches of size `PER_REPLICA_BATCH_SIZE`. And the global batch size for a step can be obtained as `PER_REPLICA_BATCH_SIZE * strategy.num_replicas_in_sync`.\nWhen performing multi-worker training, you should either split your data across the workers, or shuffle with a random seed on each. You can check an example of how to do this in the [Multi-worker training with Estimator](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator) tutorial.\nAnd similarly, you can use multi worker and parameter server strategies as well. The code remains the same, but you need to use `tf.estimator.train_and_evaluate`, and set `TF_CONFIG` environment variables for each binary running in your cluster.\n### What's supported now?\nThere is limited support for training with Estimator using all strategies except `TPUStrategy`. Basic training and evaluation should work, but a number of advanced features such as [`v1.train.Scaffold`](https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/Scaffold) do not. There may also be a number of bugs in this integration and there are no plans to actively improve this support (the focus is on Keras and custom training loop support). If at all possible, you should prefer to use [`tf.distribute`](https://www.tensorflow.org/api_docs/python/tf/distribute) with those APIs instead.\nTraining API | MirroredStrategy | TPUStrategy | MultiWorkerMirroredStrategy | CentralStorageStrategy | ParameterServerStrategy  \n---|---|---|---|---|---  \nEstimator API | Limited support | Not supported | Limited support | Limited support | Limited support  \n### Examples and tutorials\nHere are some end-to-end examples that show how to use various strategies with Estimator:\n  1. The [Multi-worker Training with Estimator tutorial](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_estimator) shows how you can train with multiple workers using `MultiWorkerMirroredStrategy` on the MNIST dataset.\n  2. An end-to-end example of `tensorflow/ecosystem` using Kubernetes templates. It starts with a Keras model and converts it to an Estimator using the `tf.keras.estimator.model_to_estimator` API.\n  3. The official `MirroredStrategy` or `MultiWorkerMirroredStrategy`.\n\n\n",
  "https://www.tensorflow.org/guide/gpu": "[View on TensorFlow.org](https://www.tensorflow.org/guide/gpu)  \n---  \nTensorFlow code, and [`tf.keras`](https://www.tensorflow.org/api_docs/python/tf/keras) models will transparently run on a single GPU with no code changes required.\nThe simplest way to run on multiple GPUs, on one or many machines, is using [Distribution Strategies](https://www.tensorflow.org/guide/distributed_training).\nThis guide is for users who have tried these approaches and found that they need fine-grained control of how TensorFlow uses the GPU. To learn how to debug performance issues for single and multi-GPU scenarios, see the [Optimize TensorFlow GPU Performance](https://www.tensorflow.org/guide/gpu_performance_analysis) guide.\n## Setup\nEnsure you have the latest TensorFlow gpu release installed.\n```\nimporttensorflowastf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n```\n```\n2024-08-15 02:53:40.344028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 02:53:40.365851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 02:53:40.372242: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nNum GPUs Available:  4\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723690422.944962  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.948934  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.952655  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.955880  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.967120  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.970596  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.973980  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.976984  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.979869  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.983344  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.986754  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690422.989690  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n\n```\n\n## Overview\nTensorFlow supports running computations on a variety of types of devices, including CPU and GPU. They are represented with string identifiers for example:\n  * `\"/device:CPU:0\"`: The CPU of your machine.\n  * `\"/GPU:0\"`: Short-hand notation for the first GPU of your machine that is visible to TensorFlow.\n  * `\"/job:localhost/replica:0/task:0/device:GPU:1\"`: Fully qualified name of the second GPU of your machine that is visible to TensorFlow.\n\n\nIf a TensorFlow operation has both CPU and GPU implementations, by default, the GPU device is prioritized when the operation is assigned. For example, [`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) has both CPU and GPU kernels and on a system with devices `CPU:0` and `GPU:0`, the `GPU:0` device is selected to run `tf.matmul` unless you explicitly request to run it on another device.\nIf a TensorFlow operation has no corresponding GPU implementation, then the operation falls back to the CPU device. For example, since [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast) only has a CPU kernel, on a system with devices `CPU:0` and `GPU:0`, the `CPU:0` device is selected to run `tf.cast`, even if requested to run on the `GPU:0` device.\n## Logging device placement\nTo find out which devices your operations and tensors are assigned to, put [`tf.debugging.set_log_device_placement(True)`](https://www.tensorflow.org/api_docs/python/tf/debugging/set_log_device_placement) as the first statement of your program. Enabling device placement logging causes any Tensor allocations or operations to be printed.\n```\ntf.debugging.set_log_device_placement(True)\n\n# Create some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)\n\n```\n```\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nI0000 00:00:1723690424.215487  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.217630  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.219585  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.221664  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.223723  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.225666  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.227528  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.229544  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.231494  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.233433  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.235295  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.237325  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.276919  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.278939  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.280845  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.282884  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.284977  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.286923  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.288779  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.290783  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.292741  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.295170  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.297460  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690424.299854  162671 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\ntf.Tensor(\n[[22. 28.]\n [49. 64.]], shape=(2, 2), dtype=float32)\n\n```\n\nThe above code will print an indication the `MatMul` op was executed on `GPU:0`.\n## Manual device placement\nIf you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use `with tf.device` to create a device context, and all the operations within that context will run on the same designated device.\n```\ntf.debugging.set_log_device_placement(True)\n\n# Place tensors on the CPU\nwith tf.device('/CPU:0'):\n  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Run on the GPU\nc = tf.matmul(a, b)\nprint(c)\n\n```\n```\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\ntf.Tensor(\n[[22. 28.]\n [49. 64.]], shape=(2, 2), dtype=float32)\n\n```\n\nYou will see that now `a` and `b` are assigned to `CPU:0`. Since a device was not explicitly specified for the `MatMul` operation, the TensorFlow runtime will choose one based on the operation and available devices (`GPU:0` in this example) and automatically copy tensors between devices if required.\n## Limiting GPU memory growth\nBy default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to [`tf.config.set_visible_devices`](https://www.tensorflow.org/api_docs/python/tf/config/set_visible_devices) method.\n```\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n\n```\n```\nVisible devices cannot be modified after being initialized\n\n```\n\nIn some cases it is desirable for the process to only allocate a subset of the available memory, or to only grow the memory usage as is needed by the process. TensorFlow provides two methods to control this.\nThe first option is to turn on memory growth by calling [`tf.config.experimental.set_memory_growth`](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth), which attempts to allocate only as much GPU memory as needed for the runtime allocations: it starts out allocating very little memory, and as the program gets run and more GPU memory is needed, the GPU memory region is extended for the TensorFlow process. Memory is not released since it can lead to memory fragmentation. To turn on memory growth for a specific GPU, use the following code prior to allocating any tensors or executing any ops.\n```\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  try:\n    # Currently, memory growth needs to be the same across GPUs\n    for gpu in gpus:\n      tf.config.experimental.set_memory_growth(gpu, True)\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n    print(e)\n\n```\n```\nPhysical devices cannot be modified after being initialized\n\n```\n\nAnother way to enable this option is to set the environmental variable `TF_FORCE_GPU_ALLOW_GROWTH` to `true`. This configuration is platform specific.\nThe second method is to configure a virtual GPU device with [`tf.config.set_logical_device_configuration`](https://www.tensorflow.org/api_docs/python/tf/config/set_logical_device_configuration) and set a hard limit on the total memory to allocate on the GPU.\n```\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n  try:\n    tf.config.set_logical_device_configuration(\n        gpus[0],\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n\n```\n```\nVirtual devices cannot be modified after being initialized\n\n```\n\nThis is useful if you want to truly bound the amount of GPU memory available to the TensorFlow process. This is common practice for local development when the GPU is shared with other applications such as a workstation GUI.\n## Using a single GPU on a multi-GPU system\nIf you have more than one GPU in your system, the GPU with the lowest ID will be selected by default. If you would like to run on a different GPU, you will need to specify the preference explicitly:\n```\ntf.debugging.set_log_device_placement(True)\n\ntry:\n  # Specify an invalid GPU device\n  with tf.device('/device:GPU:2'):\n    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n    c = tf.matmul(a, b)\nexcept RuntimeError as e:\n  print(e)\n\n```\n```\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:2\n\n```\n\nIf the device you have specified does not exist, you will get a `RuntimeError`: `.../device:GPU:2 unknown device`.\nIf you would like TensorFlow to automatically choose an existing and supported device to run the operations in case the specified one doesn't exist, you can call [`tf.config.set_soft_device_placement(True)`](https://www.tensorflow.org/api_docs/python/tf/config/set_soft_device_placement).\n```\ntf.config.set_soft_device_placement(True)\ntf.debugging.set_log_device_placement(True)\n\n# Creates some tensors\na = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\nb = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\nc = tf.matmul(a, b)\n\nprint(c)\n\n```\n```\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\ntf.Tensor(\n[[22. 28.]\n [49. 64.]], shape=(2, 2), dtype=float32)\n\n```\n\n## Using multiple GPUs\nDeveloping for multiple GPUs will allow a model to scale with the additional resources. If developing on a system with a single GPU, you can simulate multiple GPUs with virtual devices. This enables easy testing of multi-GPU setups without requiring additional resources.\n```\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n  # Create 2 virtual GPUs with 1GB memory each\n  try:\n    tf.config.set_logical_device_configuration(\n        gpus[0],\n        [tf.config.LogicalDeviceConfiguration(memory_limit=1024),\n         tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n\n```\n```\nVirtual devices cannot be modified after being initialized\n\n```\n\nOnce there are multiple logical GPUs available to the runtime, you can utilize the multiple GPUs with [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) or with manual placement.\n#### With [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy)\nThe best practice for using multiple GPUs is to use [`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy). Here is a simple example:\n```\ntf.debugging.set_log_device_placement(True)\ngpus = tf.config.list_logical_devices('GPU')\nstrategy = tf.distribute.MirroredStrategy(gpus)\nwith strategy.scope():\n  inputs = tf.keras.layers.Input(shape=(1,))\n  predictions = tf.keras.layers.Dense(1)(inputs)\n  model = tf.keras.models.Model(inputs=inputs, outputs=predictions)\n  model.compile(loss='mse',\n                optimizer=tf.keras.optimizers.SGD(learning_rate=0.2))\n\n```\n```\nINFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op FloorMod in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Cast in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op StatelessRandomGetKeyCounter in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\na: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nb: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\nproduct_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:2\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\na: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nb: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:2\nproduct_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:1\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:2\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:3\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:3\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nx: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\ny: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nFloorMod: (FloorMod): /job:localhost/replica:0/task:0/device:GPU:0\nz_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nx: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nCast: (Cast): /job:localhost/replica:0/task:0/device:GPU:0\ny_RetVal: (_DeviceRetval): /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\n_EagerConst: (_EagerConst): /job:localhost/replica:0/task:0/device:GPU:0\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nseed: (_Arg): /job:localhost/replica:0/task:0/device:CPU:0\nStatelessRandomGetKeyCounter: (StatelessRandomGetKeyCounter): /job:localhost/replica:0/task:0/device:GPU:0\nkey_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\ncounter_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nshape: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\nkey: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\ncounter: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nalg: (_DeviceArg): /job:localhost/replica:0/taExecuting op StatelessRandomUniformV2 in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Sub in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Mul in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AddV2 in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nsk:0/device:CPU:0\nStatelessRandomUniformV2: (StatelessRandomUniformV2): /job:localhost/replica:0/task:0/device:GPU:0\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nx: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\ny: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nSub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0\nz_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nx: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\ny: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nMul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0\nz_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nx: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\ny: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nAddV2: (AddV2): /job:localhost/replica:0/task:0/device:GPU:0\nz_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nvalue_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:2\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:2\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:3\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:3\nNoOp: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0\ndims: (_DeviceArg): /job:localhost/replica:0/task:0/device:CPU:0\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nFill: (Fill): /job:localhost/replica:0/task:0/device:GPU:0\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nvalue_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nVarHandleOp: (VarHandleOp): /job:lExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\nocalhost/replica:0/task:0/device:GPU:1\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nvalue_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:1\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:1\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:1\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:2\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:2\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:2\ninput: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nIdentity: (Identity): /job:localhost/replica:0/task:0/device:GPU:3\noutput_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nvalue: (_Arg): /job:localhost/replica:0/task:0/device:GPU:3\nAssignVariableOp: (AssignVariableOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:0\nresource: (_Arg): /job:localhost/replica:0/task:0/device:GPU:0\nReadVariableOp: (ReadVariableOp): /job:localhost/replica:0/task:0/device:GPU:0\nvalue_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:1\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:1\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:2\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:2\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:3\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/device:GPU:3\nresource_RetVal: (_Retval): /job:localhost/replica:0/task:0/device:GPU:0\nVarHandleOp: (VarHandleOp): /job:localhost/replica:0/task:0/deviExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Fill in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op ReadVariableOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op Identity in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op VarHandleOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AssignVariableOp in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op NoOp in device /job:localhost/replica:0/task:0/device:GPU:0\n\n```\n\nThis program will run a copy of your model on each GPU, splitting the input data between them, also known as \"\nFor more information about distribution strategies, check out the guide [here](https://www.tensorflow.org/guide/distributed_training).\n#### Manual placement\n[`tf.distribute.Strategy`](https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy) works under the hood by replicating computation across devices. You can manually implement replication by constructing your model on each GPU. For example:\n```\ntf.debugging.set_log_device_placement(True)\n\ngpus = tf.config.list_logical_devices('GPU')\nif gpus:\n  # Replicate your computation on multiple GPUs\n  c = []\n  for gpu in gpus:\n    with tf.device(gpu.name):\n      a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n      b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n      c.append(tf.matmul(a, b))\n\n  with tf.device('/CPU:0'):\n    matmul_sum = tf.add_n(c)\n\n  print(matmul_sum)\n\n```\n```\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:1\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:2\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op MatMul in device /job:localhost/replica:0/task:0/device:GPU:3\nExecuting op AddN in device /job:localhost/replica:0/task:0/device:CPU:0\ntf.Tensor(\n[[ 88. 112.]\n [196. 256.]], shape=(2, 2), dtype=float32)\n\n```\n\n",
  "https://www.tensorflow.org/guide/function": "[View on TensorFlow.org](https://www.tensorflow.org/guide/function)  \n---  \nIn TensorFlow 2, [eager execution](https://www.tensorflow.org/guide/basics) is turned on by default. The user interface is intuitive and flexible (running one-off operations is much easier and faster), but this can come at the expense of performance and deployability.\nYou can use [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use `SavedModel`.\nThis guide will help you conceptualize how [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) works under the hood, so you can use it effectively.\nThe main takeaways and recommendations are:\n  * Debug in eager mode, then decorate with [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\n  * Don't rely on Python side effects like object mutation or list appends.\n  * [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) works best with TensorFlow ops; NumPy and Python calls are converted to constants.\n\n\n## Setup\n```\nimporttensorflowastf\n\n```\n```\n2024-08-15 02:57:28.958444: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 02:57:28.979712: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 02:57:28.986177: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\nDefine a helper function to demonstrate the kinds of errors you might encounter:\n```\nimporttraceback\nimportcontextlib\n\n# Some helper code to demonstrate the kinds of errors you might encounter.\n@contextlib.contextmanager\ndefassert_raises(error_class):\n  try:\n    yield\n  except error_class as e:\n    print('Caught expected exception \\n{}:'.format(error_class))\n    traceback.print_exc(limit=2)\n  except Exception as e:\n    raise e\n  else:\n    raise Exception('Expected {} to be raised but no error was raised!'.format(\n        error_class))\n\n```\n\n## Basics\n### Usage\nA [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) that you define (for example by applying the [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator) is just like a core TensorFlow operation: You can execute it eagerly; you can compute gradients; and so on.\n```\n@tf.function  # The decorator converts `add` into a `PolymorphicFunction`.\ndefadd(a, b):\n  return a + b\n\nadd(tf.ones([2, 2]), tf.ones([2, 2]))  #  [[2., 2.], [2., 2.]]\n\n```\n```\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723690651.607368  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.611235  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.614398  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.618234  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.629890  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.633433  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.636337  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.639748  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.643233  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.646588  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.649526  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690651.652949  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.865955  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.868101  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.870112  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.872121  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.874165  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.876153  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.878068  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.879960  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.881883  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.883841  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.885768  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.887660  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.926250  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.928321  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.930298  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.932288  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.934241  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.936253  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.938172  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.940080  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.942041  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.944593  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.946947  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\nI0000 00:00:1723690652.949245  167534 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[2., 2.],\n       [2., 2.]], dtype=float32)>\n\n```\n```\nv = tf.Variable(1.0)\nwith tf.GradientTape() as tape:\n  result = add(v, 1.0)\ntape.gradient(result, v)\n\n```\n```\n<tf.Tensor: shape=(), dtype=float32, numpy=1.0>\n\n```\n\nYou can use [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s inside other [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s.\n```\n@tf.function\ndefdense_layer(x, w, b):\n  return add(tf.matmul(x, w), b)\n\ndense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))\n\n```\n```\n<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[3., 3.],\n       [3., 3.],\n       [3., 3.]], dtype=float32)>\n\n```\n\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s can be faster than eager code, especially for graphs with many small ops. But for graphs with a few expensive ops (like convolutions), you may not see much speedup.\n```\nimporttimeit\nconv_layer = tf.keras.layers.Conv2D(100, 3)\n\n@tf.function\ndefconv_fn(image):\n  return conv_layer(image)\n\nimage = tf.zeros([1, 200, 200, 100])\n# Warm up\nconv_layer(image); conv_fn(image)\nprint(\"Eager conv:\", timeit.timeit(lambda: conv_layer(image), number=10))\nprint(\"Function conv:\", timeit.timeit(lambda: conv_fn(image), number=10))\nprint(\"Note how there's not much difference in performance for convolutions\")\n\n```\n```\nW0000 00:00:1723690654.228267  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.285525  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.290477  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.295072  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.299820  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.304580  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.322737  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.327483  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.332646  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.337747  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.343046  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.347480  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.361780  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.370325  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.381185  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nW0000 00:00:1723690654.405763  167534 gpu_timer.cc:114] Skipping the delay kernel, measurement accuracy will be reduced\nEager conv: 0.011224052000216034\nFunction conv: 0.005400947000453016\nNote how there's not much difference in performance for convolutions\n\n```\n\n### Tracing\nThis section exposes how [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) works under the hood, including implementation details _which may change in the future_. However, once you understand why and when tracing happens, it's much easier to use [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) effectively!\n#### What is \"tracing\"?\nA [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) runs your program in a [TensorFlow Graph](https://www.tensorflow.org/guide/intro_to_graphs#what_are_graphs). However, a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) cannot represent all the things that you'd write in an eager TensorFlow program. For instance, Python supports polymorphism, but [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) requires its inputs to have a specified data type and dimension. Or you may perform side tasks like reading command-line arguments, raising an error, or working with a more complex Python object; none of these things can run in a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph).\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) bridges this gap by separating your code in two stages:\n1) In the first stage, referred to as \"**tracing** \", [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) creates a new [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph). Python code runs normally, but all TensorFlow operations (like adding two Tensors) are _deferred_ : they are captured by the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) and not run.\n2) In the second stage, a [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) which contains everything that was deferred in the first stage is run. This stage is much faster than the tracing stage.\nDepending on its inputs, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will not always run the first stage when it is called. See [\"Rules of tracing\"](https://www.tensorflow.org/guide/function#rules_of_tracing) below to get a better sense of how it makes that determination. Skipping the first stage and only executing the second stage is what gives you TensorFlow's high performance.\nWhen [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) does decide to trace, the tracing stage is immediately followed by the second stage, so calling the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) both creates and runs the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph). Later you will see how you can run only the tracing stage with [`get_concrete_function`](https://www.tensorflow.org/guide/function#obtaining_concrete_functions).\nWhen you pass arguments of different types into a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), both stages are run:\n```\n@tf.function\ndefdouble(a):\n  print(\"Tracing with\", a)\n  return a + a\n\nprint(double(tf.constant(1)))\nprint()\nprint(double(tf.constant(1.1)))\nprint()\nprint(double(tf.constant(\"a\")))\nprint()\n\n```\n```\nTracing with Tensor(\"a:0\", shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\n\nTracing with Tensor(\"a:0\", shape=(), dtype=float32)\ntf.Tensor(2.2, shape=(), dtype=float32)\n\nTracing with Tensor(\"a:0\", shape=(), dtype=string)\ntf.Tensor(b'aa', shape=(), dtype=string)\n\n```\n\nNote that if you repeatedly call a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) with the same argument type, TensorFlow will skip the tracing stage and reuse a previously traced graph, as the generated graph would be identical.\n```\n# This doesn't print 'Tracing with ...'\nprint(double(tf.constant(\"b\")))\n\n```\n```\ntf.Tensor(b'bb', shape=(), dtype=string)\n\n```\n\nYou can use `pretty_printed_concrete_signatures()` to see all of the available traces:\n```\nprint(double.pretty_printed_concrete_signatures())\n\n```\n```\nInput Parameters:\n  a (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.int32, name=None)\nOutput Type:\n  TensorSpec(shape=(), dtype=tf.int32, name=None)\nCaptures:\n  None\n\nInput Parameters:\n  a (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.float32, name=None)\nOutput Type:\n  TensorSpec(shape=(), dtype=tf.float32, name=None)\nCaptures:\n  None\n\nInput Parameters:\n  a (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.string, name=None)\nOutput Type:\n  TensorSpec(shape=(), dtype=tf.string, name=None)\nCaptures:\n  None\n\n```\n\nSo far, you've seen that [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) creates a cached, dynamic dispatch layer over TensorFlow's graph tracing logic. To be more specific about the terminology:\n  * A [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) is the raw, language-agnostic, portable representation of a TensorFlow computation.\n  * Tracing is the process through which new [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)s are generated from Python code.\n  * An instance of [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) is specialized to the specific input types it was traced with. Differing types require retracing.\n  * Each traced [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) has a corresponding `ConcreteFunction`.\n  * A [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) manages a cache of `ConcreteFunction`s and picks the right one for your inputs.\n  * [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) wraps the Python function that will be traced, returning a [`tf.types.experimental.PolymorphicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/PolymorphicFunction) object.\n\n\n#### Rules of tracing\nWhen called, a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) first evaluates the type of each input argument using the [`tf.types.experimental.TraceType`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/TraceType) of each argument. This is used to construct a [`tf.types.experimental.FunctionType`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/FunctionType) describing the signature of the desired `ConcreteFunction`. We compare this `FunctionType` to the `FunctionType`s of existing `ConcreteFunction`s. If a matching `ConcreteFunction` is found, the call is dispatched to it. If no match is found, a new `ConcreteFunction` is traced for the desired `FunctionType`.\nIf multiple matches are found, the most specific signature is chosen. Matching is done by `TensorShape([1, 2])` is a subtype of `TensorShape([None, None])` and so a call to the tf.function with `TensorShape([1, 2])` can be dispatched to the `ConcreteFunction` produced with `TensorShape([None, None])` but if a `ConcreteFunction` with `TensorShape([1, None])` also exists then it will be prioritized since it is more specific.\nThe `TraceType` is determined from input arguments as follows:\n  * For `Tensor`, the type is parameterized by the `Tensor`'s `dtype` and `shape`; ranked shapes are a subtype of unranked shapes; fixed dimensions are a subtype of unknown dimensions\n  * For `Variable`, the type is similar to `Tensor`, but also includes a unique resource ID of the variable, necessary to correctly wire control dependencies\n  * For Python primitive values, the type corresponds to the **value** itself. For example, the `TraceType` of the value `3` is `LiteralTraceType<3>`, not `int`.\n  * For Python ordered containers such as `list` and `tuple`, etc., the type is parameterized by the types of their elements; for example, the type of `[1, 2]` is `ListTraceType<LiteralTraceType<1>, LiteralTraceType<2>>` and the type for `[2, 1]` is `ListTraceType<LiteralTraceType<2>, LiteralTraceType<1>>` which is different.\n  * For Python mappings such as `dict`, the type is also a mapping from the same keys but to the types of values instead of the actual values. For example, the type of `{1: 2, 3: 4}`, is `MappingTraceType<<KeyValue<1, LiteralTraceType<2>>>, <KeyValue<3, LiteralTraceType<4>>>>`. However, unlike ordered containers, `{1: 2, 3: 4}` and `{3: 4, 1: 2}` have equivalent types.\n  * For Python objects which implement the `__tf_tracing_type__` method, the type is whatever that method returns.\n  * For any other Python objects, the type is a generic `TraceType`, and the matching precedure is:\n    * First it checks if the object is the same object used in the previous trace (using Python `id()` or `is`). Note that this will still match if the object has changed, so if you use Python objects as [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) arguments it's best to use _immutable_ ones.\n    * Next it checks if the object is equal to the object used in the previous trace (using Python `==`).\nNote that this procedure only keeps a \n\n\n### Controlling retracing\nRetracing, which is when your [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) creates more than one trace, helps ensure that TensorFlow generates correct graphs for each set of inputs. However, tracing is an expensive operation! If your [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) retraces a new graph for every call, you'll find that your code executes more slowly than if you didn't use [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\nTo control the tracing behavior, you can use the following techniques:\n#### Pass a fixed `input_signature` to [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)\nThis forces [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to constrain itself to only one [`tf.types.experimental.FunctionType`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/FunctionType) composed of the types enumerated by the `input_signature`. Calls that cannot be dispatched to this `FunctionType` will throw an error.\n```\n@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\ndefnext_collatz(x):\n  print(\"Tracing with\", x)\n  return tf.where(x % 2 == 0, x // 2, 3 * x + 1)\n\nprint(next_collatz(tf.constant([1, 2])))\n# You specified a 1-D tensor in the input signature, so this should fail.\nwith assert_raises(TypeError):\n  next_collatz(tf.constant([[1, 2], [3, 4]]))\n\n# You specified an int32 dtype in the input signature, so this should fail.\nwith assert_raises(TypeError):\n  next_collatz(tf.constant([1.0, 2.0]))\n\n```\n```\nTracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\ntf.Tensor([4 1], shape=(2,), dtype=int32)\nCaught expected exception \n  <class 'TypeError'>:\nCaught expected exception \n  <class 'TypeError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/3657259638.py\", line 9, in <module>\n    next_collatz(tf.constant([[1, 2], [3, 4]]))\nTypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(2, 2), dtype=tf.int32, name=None) to TensorSpec(shape=(None,), dtype=tf.int32, name=None)`. Received args: (<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)>,) and kwargs: {} for signature: (x: TensorSpec(shape=(None,), dtype=tf.int32, name=None)).\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/3657259638.py\", line 13, in <module>\n    next_collatz(tf.constant([1.0, 2.0]))\nTypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(2,), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.int32, name=None)`. Received args: (<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>,) and kwargs: {} for signature: (x: TensorSpec(shape=(None,), dtype=tf.int32, name=None)).\n\n```\n\n#### Use unknown dimensions for flexibility\nSince TensorFlow matches tensors based on their shape, using a `None` dimension as a wildcard will allow [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s to reuse traces for variably-sized input. Variably-sized input can occur if you have sequences of different length, or images of different sizes for each batch. You can check out the [Transformer](https://www.tensorflow.org/text/tutorials/transformer) and [Deep Dream](https://www.tensorflow.org/tutorials/generative/deepdream) tutorials for examples.\n```\n@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))\ndefg(x):\n  print('Tracing with', x)\n  return x\n\n# No retrace!\nprint(g(tf.constant([1, 2, 3])))\nprint(g(tf.constant([1, 2, 3, 4, 5])))\n\n```\n```\nTracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\ntf.Tensor([1 2 3], shape=(3,), dtype=int32)\ntf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n\n```\n\n#### Use `reduce_retracing` for automatic flexibility\nWhen `reduce_retracing` is enabled, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) automatically identifies supertypes of the input types it is observing and chooses to trace more generalized graphs automatically. It is less efficient than setting the `input_signature` directly but useful when many types need to be supported.\n```\n@tf.function(reduce_retracing=True)\ndefg(x):\n  print('Tracing with', x)\n  return x\n\n# Traces once.\nprint(g(tf.constant([1, 2, 3])))\n\n# Traces again, but more generalized this time.\nprint(g(tf.constant([1, 2, 3, 4, 5])))\n\n# No more tracing!\nprint(g(tf.constant([1, 2, 3, 4, 5, 6, 7])))\nprint(g(tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9])))\n\n```\n```\nTracing with Tensor(\"x:0\", shape=(3,), dtype=int32)\ntf.Tensor([1 2 3], shape=(3,), dtype=int32)\nTracing with Tensor(\"x:0\", shape=(None,), dtype=int32)\ntf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7 8 9], shape=(9,), dtype=int32)\n\n```\n\n#### Pass tensors instead of python literals\nOften, Python arguments are used to control hyperparameters and graph constructions - for example, `num_layers=10` or `training=True` or `nonlinearity='relu'`. So, if the Python argument changes, it makes sense that you'd have to retrace the graph.\nHowever, it's possible that a Python argument is not being used to control graph construction. In these cases, a change in the Python value can trigger needless retracing. Take, for example, this training loop, which AutoGraph will dynamically unroll. Despite the multiple traces, the generated graph is actually identical, so retracing is unnecessary.\n```\ndeftrain_one_step():\n  pass\n\n@tf.function\ndeftrain(num_steps):\n  print(\"Tracing with num_steps = \", num_steps)\n  tf.print(\"Executing with num_steps = \", num_steps)\n  for _ in tf.range(num_steps):\n    train_one_step()\n\nprint(\"Retracing occurs for different Python arguments.\")\ntrain(num_steps=10)\ntrain(num_steps=20)\n\nprint()\nprint(\"Traces are reused for Tensor arguments.\")\ntrain(num_steps=tf.constant(10))\ntrain(num_steps=tf.constant(20))\n\n```\n```\nRetracing occurs for different Python arguments.\nTracing with num_steps =  10\nExecuting with num_steps =  10\nTracing with num_steps =  20\nExecuting with num_steps =  20\n\nTraces are reused for Tensor arguments.\nTracing with num_steps =  Tensor(\"num_steps:0\", shape=(), dtype=int32)\nExecuting with num_steps =  10\nExecuting with num_steps =  20\n\n```\n\nIf you need to force retracing, create a new [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Separate [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) objects are guaranteed not to share traces.\n```\ndeff():\n  print('Tracing!')\n  tf.print('Executing')\n\ntf.function(f)()\ntf.function(f)()\n\n```\n```\nTracing!\nExecuting\nTracing!\nExecuting\n\n```\n\n#### Use the tracing protocol\nWhere possible, you should prefer converting the Python type into a [`tf.experimental.ExtensionType`](https://www.tensorflow.org/api_docs/python/tf/experimental/ExtensionType) instead. Moreover, the `TraceType` of an `ExtensionType` is the [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) associated with it. Therefore, if needed, you can simply override the default [`tf.TypeSpec`](https://www.tensorflow.org/api_docs/python/tf/TypeSpec) to take control of an `ExtensionType`'s `Tracing Protocol`. Refer to the _Customizing the ExtensionType's TypeSpec_ section in the [Extension types](https://www.tensorflow.org/guide/extension_type) guide for details.\nOtherwise, for direct control over when [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) should retrace in regards to a particular Python type, you can implement the `Tracing Protocol` for it yourself.\n```\n@tf.function\ndefget_mixed_flavor(fruit_a, fruit_b):\n  return fruit_a.flavor + fruit_b.flavor\n\nclassFruit:\n  flavor = tf.constant([0, 0])\n\nclassApple(Fruit):\n  flavor = tf.constant([1, 2])\n\nclassMango(Fruit):\n  flavor = tf.constant([3, 4])\n\n# As described in the above rules, a generic TraceType for `Apple` and `Mango`\n# is generated (and a corresponding ConcreteFunction is traced) but it fails to\n# match the second function call since the first pair of Apple() and Mango()\n# have gone out out of scope by then and deleted.\nget_mixed_flavor(Apple(), Mango()) # Traces a new concrete function\nget_mixed_flavor(Apple(), Mango()) # Traces a new concrete function again\n\n# However, each subclass of the `Fruit` class has a fixed flavor, and you\n# can reuse an existing traced concrete function if it was the same\n# subclass. Avoiding such unnecessary tracing of concrete functions\n# can have significant performance benefits.\n\nclassFruitTraceType(tf.types.experimental.TraceType):\n  def__init__(self, fruit):\n    self.fruit_type = type(fruit)\n    self.fruit_value = fruit\n\n  defis_subtype_of(self, other):\n      # True if self subtypes `other` and `other`'s type matches FruitTraceType.\n      return (type(other) is FruitTraceType and\n              self.fruit_type is other.fruit_type)\n\n  defmost_specific_common_supertype(self, others):\n      # `self` is the specific common supertype if all input types match it.\n      return self if all(self == other for other in others) else None\n\n  defplaceholder_value(self, placeholder_context=None):\n      # Use the fruit itself instead of the type for correct tracing.\n      return self.fruit_value\n\n  def__eq__(self, other):\n    return type(other) is FruitTraceType and self.fruit_type == other.fruit_type\n\n  def__hash__(self):\n    return hash(self.fruit_type)\n\nclassFruitWithTraceType:\n\n  def__tf_tracing_type__(self, context):\n    return FruitTraceType(self)\n\nclassAppleWithTraceType(FruitWithTraceType):\n  flavor = tf.constant([1, 2])\n\nclassMangoWithTraceType(FruitWithTraceType):\n  flavor = tf.constant([3, 4])\n\n# Now if you try calling it again:\nget_mixed_flavor(AppleWithTraceType(), MangoWithTraceType()) # Traces a new concrete function\nget_mixed_flavor(AppleWithTraceType(), MangoWithTraceType()) # Re-uses the traced concrete function\n\n```\n```\n<tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)>\n\n```\n\n### Obtaining concrete functions\nEvery time a function is traced, a new concrete function is created. You can directly obtain a concrete function, by using `get_concrete_function`.\n```\nprint(\"Obtaining concrete trace\")\ndouble_strings = double.get_concrete_function(tf.constant(\"a\"))\nprint(\"Executing traced function\")\nprint(double_strings(tf.constant(\"a\")))\nprint(double_strings(a=tf.constant(\"b\")))\n\n```\n```\nObtaining concrete trace\nExecuting traced function\ntf.Tensor(b'aa', shape=(), dtype=string)\ntf.Tensor(b'bb', shape=(), dtype=string)\n\n```\n```\n# You can also call get_concrete_function on an InputSpec\ndouble_strings_from_inputspec = double.get_concrete_function(tf.TensorSpec(shape=[], dtype=tf.string))\nprint(double_strings_from_inputspec(tf.constant(\"c\")))\n\n```\n```\ntf.Tensor(b'cc', shape=(), dtype=string)\n\n```\n\nPrinting a `ConcreteFunction` displays a summary of its input arguments (with types) and its output type.\n```\nprint(double_strings)\n\n```\n```\nConcreteFunction Input Parameters:\n  a (POSITIONAL_OR_KEYWORD): TensorSpec(shape=(), dtype=tf.string, name=None)\nOutput Type:\n  TensorSpec(shape=(), dtype=tf.string, name=None)\nCaptures:\n  None\n\n```\n\nYou can also directly retrieve a concrete function's signature.\n```\nprint(double_strings.function_type)\n\n```\n```\n(a: TensorSpec(shape=(), dtype=tf.string, name=None)) -> TensorSpec(shape=(), dtype=tf.string, name=None)\n\n```\n\nUsing a concrete trace with incompatible types will throw an error\n```\nwith assert_raises(tf.errors.InvalidArgumentError):\n  double_strings(tf.constant(1))\n\n```\n```\nCaught expected exception \n  <class 'tensorflow.python.framework.errors_impl.InvalidArgumentError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\", line 442, in bind_function_inputs\n    bound_arguments = function_type.bind_with_defaults(\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 277, in bind_with_defaults\n    with_default_args[arg_name] = constraint.cast(\nTypeError: Can not cast TensorSpec(shape=(), dtype=tf.int32, name=None) to TensorSpec(shape=(), dtype=tf.string, name=None)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1179, in _call_impl\n    return self._call_with_structured_signature(args, kwargs)\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1259, in _call_with_structured_signature\n    function_type_utils.canonicalize_function_inputs(\nTypeError: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(), dtype=tf.int32, name=None) to TensorSpec(shape=(), dtype=tf.string, name=None)`. Received args: (<tf.Tensor: shape=(), dtype=int32, numpy=1>,) and kwargs: {} for signature: (a: TensorSpec(shape=(), dtype=tf.string, name=None)) -> TensorSpec(shape=(), dtype=tf.string, name=None).\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/3196284684.py\", line 2, in <module>\n    double_strings(tf.constant(1))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_double_189 as input #0(zero-based) was expected to be a string tensor but is a int32 tensor [Op:__inference_double_189]\n\n```\n\nYou may notice that Python arguments are given special treatment in a concrete function's input signature. Prior to TensorFlow 2.3, Python arguments were simply removed from the concrete function's signature. Starting with TensorFlow 2.3, Python arguments remain in the signature, but are constrained to take the value set during tracing.\n```\n@tf.function\ndefpow(a, b):\n  return a ** b\n\nsquare = pow.get_concrete_function(a=tf.TensorSpec(None, tf.float32), b=2)\nprint(square)\n\n```\n```\nConcreteFunction Input Parameters:\n  a (POSITIONAL_OR_KEYWORD): TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)\n  b (POSITIONAL_OR_KEYWORD): Literal[2]\nOutput Type:\n  TensorSpec(shape=<unknown>, dtype=tf.float32, name=None)\nCaptures:\n  None\n\n```\n```\nassert square(tf.constant(10.0)) == 100\n\nwith assert_raises(TypeError):\n  square(tf.constant(10.0), b=3)\n\n```\n```\nCaught expected exception \n  <class 'TypeError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\", line 442, in bind_function_inputs\n    bound_arguments = function_type.bind_with_defaults(\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/core/function/polymorphism/function_type.py\", line 277, in bind_with_defaults\n    with_default_args[arg_name] = constraint.cast(\nValueError: Can not cast 3 to Literal[2]\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1179, in _call_impl\n    return self._call_with_structured_signature(args, kwargs)\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1259, in _call_with_structured_signature\n    function_type_utils.canonicalize_function_inputs(\nTypeError: Binding inputs to tf.function failed due to `Can not cast 3 to Literal[2]`. Received args: (<tf.Tensor: shape=(), dtype=float32, numpy=10.0>,) and kwargs: {'b': 3} for signature: (a: TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), b: Literal[2]) -> TensorSpec(shape=<unknown>, dtype=tf.float32, name=None).\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1182, in _call_impl\n    return self._call_with_flat_signature(args, kwargs)\n  File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\", line 1233, in _call_with_flat_signature\n    raise TypeError(f\"{self._flat_signature_summary()} got unexpected \"\nTypeError: pow(a) got unexpected keyword arguments: b.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/2310937119.py\", line 4, in <module>\n    square(tf.constant(10.0), b=3)\nTypeError: Binding inputs to tf.function failed due to `Can not cast 3 to Literal[2]`. Received args: (<tf.Tensor: shape=(), dtype=float32, numpy=10.0>,) and kwargs: {'b': 3} for signature: (a: TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), b: Literal[2]) -> TensorSpec(shape=<unknown>, dtype=tf.float32, name=None).\nFallback to flat signature also failed due to: pow(a) got unexpected keyword arguments: b.\n\n```\n\n### Obtaining graphs\nAlthough retrieving the actual [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) object is not something you'll normally need to do, you can obtain it easily from any concrete function.\n```\ngraph = double_strings.graph\nfor node in graph.as_graph_def().node:\n  print(f'{node.input} -> {node.name}')\n\n```\n```\n[] -> a\n['a', 'a'] -> add\n['add'] -> Identity\n\n```\n\nIn reality, [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph)s are not directly callable. We actually use an [`tf.types.experimental.AtomicFunction`](https://www.tensorflow.org/api_docs/python/tf/types/experimental/AtomicFunction) to perform the computations described by the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph). You can access the `AtomicFunction` describing the traced [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) and call it directly instead of the `ConcreteFunction`:\n```\natomic_fn = double_strings.inference_fn\natomic_fn(tf.constant(\"a\"))\n\n```\n```\n<tf.Tensor: shape=(), dtype=string, numpy=b'aa'>\n\n```\n\nThis has the advantage of having lower Python overhead for high-performance scenarios. But it should only be used for forward inference (no gradient support), and captured tensor values (if any) would need to be explicitly supplied.\n### Debugging\nIn general, debugging code is easier in eager mode than inside [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). You should ensure that your code executes error-free in eager mode before decorating with [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). To assist in the debugging process, you can call [`tf.config.run_functions_eagerly(True)`](https://www.tensorflow.org/api_docs/python/tf/config/run_functions_eagerly) to globally disable and reenable [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\nWhen tracking down issues that only appear within [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), here are some tips:\n  * Plain old Python `print` calls only execute during tracing, helping you track down when your function gets (re)traced.\n  * [`tf.print`](https://www.tensorflow.org/api_docs/python/tf/print) calls will execute every time, and can help you track down intermediate values during execution.\n  * [`tf.debugging.enable_check_numerics`](https://www.tensorflow.org/api_docs/python/tf/debugging/enable_check_numerics) is an easy way to track down where NaNs and Inf are created.\n  * `pdb` (the `pdb` will drop you into AutoGraph-transformed source code.)\n\n\n## AutoGraph transformations\nAutoGraph is a library that is on by default in [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), and transforms a subset of Python eager code into graph-compatible TensorFlow ops. This includes control flow like `if`, `for`, `while`.\nTensorFlow ops like [`tf.cond`](https://www.tensorflow.org/api_docs/python/tf/cond) and [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop) continue to work, but control flow is often easier to write and understand when written in Python.\n```\n# A simple loop\n\n@tf.function\ndeff(x):\n  while tf.reduce_sum(x) > 1:\n    tf.print(x)\n    x = tf.tanh(x)\n  return x\n\nf(tf.random.uniform([5]))\n\n```\n```\n[0.722626925 0.640327692 0.725044 0.904435039 0.868018746]\n[0.61853379 0.565122604 0.620023966 0.718450606 0.700366139]\n[0.550106347 0.511768281 0.551144719 0.615948677 0.604600191]\n[0.500599921 0.471321791 0.501377642 0.548301 0.540314913]\n[0.462588847 0.439266682 0.463199914 0.499245733 0.493226349]\n[0.432191819 0.413036436 0.432688653 0.461523771 0.456773371]\n[0.407151431 0.391047835 0.407565802 0.431325316 0.427450746]\n[0.386051297 0.372263193 0.386403859 0.406428277 0.403188676]\n[0.367951065 0.355969697 0.368255854 0.38543576 0.382673979]\n[0.352198243 0.341659099 0.352465183 0.367418766 0.365027398]\n[0.338323593 0.328957736 0.33856 0.351731867 0.349634588]\n[0.325979948 0.317583948 0.326191217 0.337910533 0.336051434]\n[0.314903945 0.307320684 0.315094262 0.325610697 0.323947728]\n[0.304891765 0.297997624 0.30506441 0.314571291 0.313072115]\n[0.295782804 0.289479077 0.29594034 0.304590017 0.303229302]\n[0.287448555 0.281655282 0.287593067 0.295507431 0.294265062]\n[0.279784769 0.274436355 0.279917955 0.287195921 0.286055595]\n[0.272705853 0.267748028 0.272829145 0.279551893 0.278500348]\n[0.266140789 0.261528105 0.266255379 0.272490293 0.271516532]\n[0.26003018 0.255724251 0.260137022 0.265940517 0.265035421]\n[0.254323781 0.250291914 0.254423678 0.259843439 0.258999288]\n[0.248978764 0.245193034 0.249072418 0.25414905 0.253359258]\n[0.243958414 0.240394741 0.244046524 0.248814836 0.248073786]\n[0.239231125 0.235868543 0.239314198 0.243804231 0.24310714]\n[0.234769359 0.231589615 0.234847859 0.239085764 0.238428399]\n[0.230549142 0.227536201 0.230623439 0.234632015 0.234010741]\n[0.226549357 0.223689109 0.22661984 0.23041907 0.229830697]\n[0.222751439 0.220031396 0.222818434 0.226425976 0.225867674]\n[0.21913895 0.216548 0.219202697 0.222634196 0.222103462]\n[0.215697214 0.213225439 0.215757981 0.219027311 0.218521982]\n[0.212413162 0.210051686 0.212471202 0.215590775 0.215108871]\n[0.209275112 0.207015961 0.209330618 0.212311521 0.211851314]\n[0.206272557 0.204108506 0.206325665 0.209177911 0.20873782]\n[0.203395993 0.201320544 0.203446865 0.206179485 0.20575805]\n[0.200636819 0.198644072 0.200685605 0.203306749 0.202902704]\n<tf.Tensor: shape=(5,), dtype=float32, numpy=\narray([0.19798723, 0.19607186, 0.19803411, 0.20055115, 0.20016332],\n      dtype=float32)>\n\n```\n\nIf you're curious you can inspect the code AutoGraph generates.\n```\nprint(tf.autograph.to_code(f.python_function))\n\n```\n```\ndef tf__f(x):\n    with ag__.FunctionScope('f', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n        do_return = False\n        retval_ = ag__.UndefinedReturnValue()\n\n        def get_state():\n            return (x,)\n\n        def set_state(vars_):\n            nonlocal x\n            (x,) = vars_\n\n        def loop_body():\n            nonlocal x\n            ag__.converted_call(ag__.ld(tf).print, (ag__.ld(x),), None, fscope)\n            x = ag__.converted_call(ag__.ld(tf).tanh, (ag__.ld(x),), None, fscope)\n\n        def loop_test():\n            return ag__.converted_call(ag__.ld(tf).reduce_sum, (ag__.ld(x),), None, fscope) > 1\n        ag__.while_stmt(loop_test, loop_body, get_state, set_state, ('x',), {})\n        try:\n            do_return = True\n            retval_ = ag__.ld(x)\n        except:\n            do_return = False\n            raise\n        return fscope.ret(retval_, do_return)\n\n```\n\n### Conditionals\nAutoGraph will convert some `if <condition>` statements into the equivalent `tf.cond` calls. This substitution is made if `<condition>` is a Tensor. Otherwise, the `if` statement is executed as a Python conditional.\nA Python conditional executes during tracing, so exactly one branch of the conditional will be added to the graph. Without AutoGraph, this traced graph would be unable to take the alternate branch if there is data-dependent control flow.\n[`tf.cond`](https://www.tensorflow.org/api_docs/python/tf/cond) traces and adds both branches of the conditional to the graph, dynamically selecting a branch at execution time. Tracing can have unintended side effects; check out \n```\n@tf.function\ndeffizzbuzz(n):\n  for i in tf.range(1, n + 1):\n    print('Tracing for loop')\n    if i % 15 == 0:\n      print('Tracing fizzbuzz branch')\n      tf.print('fizzbuzz')\n    elif i % 3 == 0:\n      print('Tracing fizz branch')\n      tf.print('fizz')\n    elif i % 5 == 0:\n      print('Tracing buzz branch')\n      tf.print('buzz')\n    else:\n      print('Tracing default branch')\n      tf.print(i)\n\nfizzbuzz(tf.constant(5))\nfizzbuzz(tf.constant(20))\n\n```\n```\nTracing for loop\nTracing fizzbuzz branch\nTracing fizz branch\nTracing buzz branch\nTracing default branch\n1\n2\nfizz\n4\nbuzz\n1\n2\nfizz\n4\nbuzz\nfizz\n7\n8\nfizz\nbuzz\n11\nfizz\n13\n14\nfizzbuzz\n16\n17\nfizz\n19\nbuzz\n\n```\n\nSee the \n### Loops\nAutoGraph will convert some `for` and `while` statements into the equivalent TensorFlow looping ops, like [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop). If not converted, the `for` or `while` loop is executed as a Python loop.\nThis substitution is made in the following situations:\n  * `for x in y`: if `y` is a Tensor, convert to [`tf.while_loop`](https://www.tensorflow.org/api_docs/python/tf/while_loop). In the special case where `y` is a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), a combination of [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) ops are generated.\n  * `while <condition>`: if `<condition>` is a Tensor, convert to `tf.while_loop`.\n\n\nA Python loop executes during tracing, adding additional ops to the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) for every iteration of the loop.\nA TensorFlow loop traces the body of the loop, and dynamically selects how many iterations to run at execution time. The loop body only appears once in the generated [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph).\nSee the `for` and `while` statements.\n#### Looping over Python data\nA common pitfall is to loop over Python/NumPy data within a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). This loop will execute during the tracing process, adding a copy of your model to the [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) for each iteration of the loop.\nIf you want to wrap the entire training loop in [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), the safest way to do this is to wrap your data as a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) so that AutoGraph will dynamically unroll the training loop.\n```\ndefmeasure_graph_size(f, *args):\n  g = f.get_concrete_function(*args).graph\n  print(\"{}({}) contains {} nodes in its graph\".format(\n      f.__name__, ', '.join(map(str, args)), len(g.as_graph_def().node)))\n\n@tf.function\ndeftrain(dataset):\n  loss = tf.constant(0)\n  for x, y in dataset:\n    loss += tf.abs(y - x) # Some dummy computation.\n  return loss\n\nsmall_data = [(1, 1)] * 3\nbig_data = [(1, 1)] * 10\nmeasure_graph_size(train, small_data)\nmeasure_graph_size(train, big_data)\n\nmeasure_graph_size(train, tf.data.Dataset.from_generator(\n    lambda: small_data, (tf.int32, tf.int32)))\nmeasure_graph_size(train, tf.data.Dataset.from_generator(\n    lambda: big_data, (tf.int32, tf.int32)))\n\n```\n```\ntrain([(1, 1), (1, 1), (1, 1)]) contains 11 nodes in its graph\ntrain([(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]) contains 32 nodes in its graph\ntrain(<_FlatMapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.int32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>) contains 6 nodes in its graph\ntrain(<_FlatMapDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.int32, name=None), TensorSpec(shape=<unknown>, dtype=tf.int32, name=None))>) contains 6 nodes in its graph\n\n```\n\nWhen wrapping Python/NumPy data in a Dataset, be mindful of [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator) versus `tf.data.Dataset.from_tensor_slices`. The former will keep the data in Python and fetch it via [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) which can have performance implications, whereas the latter will bundle a copy of the data as one large [`tf.constant()`](https://www.tensorflow.org/api_docs/python/tf/constant) node in the graph, which can have memory implications.\nReading data from files via `TFRecordDataset`, `CsvDataset`, etc. is the most effective way to consume data, as then TensorFlow itself can manage the asynchronous loading and prefetching of data, without having to involve Python. To learn more, see the [`tf.data`: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data) guide.\n#### Accumulating values in a loop\nA common pattern is to accumulate intermediate values from a loop. Normally, this is accomplished by appending to a Python list or adding entries to a Python dictionary. However, as these are Python side effects, they will not work as expected in a dynamically unrolled loop. Use [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray) to accumulate results from a dynamically unrolled loop.\n```\nbatch_size = 2\nseq_len = 3\nfeature_size = 4\n\ndefrnn_step(inp, state):\n  return inp + state\n\n@tf.function\ndefdynamic_rnn(rnn_step, input_data, initial_state):\n  # [batch, time, features] -> [time, batch, features]\n  input_data = tf.transpose(input_data, [1, 0, 2])\n  max_seq_len = input_data.shape[0]\n\n  states = tf.TensorArray(tf.float32, size=max_seq_len)\n  state = initial_state\n  for i in tf.range(max_seq_len):\n    state = rnn_step(input_data[i], state)\n    states = states.write(i, state)\n  return tf.transpose(states.stack(), [1, 0, 2])\n\ndynamic_rnn(rnn_step,\n            tf.random.uniform([batch_size, seq_len, feature_size]),\n            tf.zeros([batch_size, feature_size]))\n\n```\n```\n<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\narray([[[0.98815036, 0.8358947 , 0.15233278, 0.58257985],\n        [1.7802314 , 1.215749  , 0.6186948 , 0.9416343 ],\n        [2.1005788 , 1.2919371 , 1.1675987 , 1.4443643 ]],\n\n       [[0.751495  , 0.8949536 , 0.16761959, 0.45424747],\n        [0.9617816 , 1.7412133 , 0.37147725, 0.7925167 ],\n        [1.655664  , 1.9362986 , 1.1732976 , 1.12577   ]]], dtype=float32)>\n\n```\n\n## Limitations\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) has a few limitations by design that you should be aware of when converting a Python function to a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function).\n### Executing Python side effects\nSide effects, like printing, appending to lists, and mutating globals, can behave unexpectedly inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), sometimes executing twice or not all. They only happen the first time you call a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) with a set of inputs. Afterwards, the traced [`tf.Graph`](https://www.tensorflow.org/api_docs/python/tf/Graph) is reexecuted, without executing the Python code.\nThe general rule of thumb is to avoid relying on Python side effects in your logic and only use them to debug your traces. Otherwise, TensorFlow APIs like [`tf.data`](https://www.tensorflow.org/api_docs/python/tf/data), [`tf.print`](https://www.tensorflow.org/api_docs/python/tf/print), [`tf.summary`](https://www.tensorflow.org/api_docs/python/tf/summary), [`tf.Variable.assign`](https://www.tensorflow.org/api_docs/python/tf/Variable#assign), and [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray) are the best way to ensure your code will be executed by the TensorFlow runtime with each call.\n```\n@tf.function\ndeff(x):\n  print(\"Traced with\", x)\n  tf.print(\"Executed with\", x)\n\nf(1)\nf(1)\nf(2)\n\n```\n```\nTraced with 1\nExecuted with 1\nExecuted with 1\nTraced with 2\nExecuted with 2\n\n```\n\nIf you would like to execute Python code during each invocation of a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), `tf. py_function` is an exit hatch. The drawbacks of [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) are that it's not portable or particularly performant, cannot be saved with `SavedModel`, and does not work well in distributed (multi-GPU, TPU) setups. Also, since [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) has to be wired into the graph, it casts all inputs/outputs to tensors.\n```\n@tf.py_function(Tout=tf.float32)\ndefpy_plus(x, y):\n  print('Executing eagerly.')\n  return x + y\n\n@tf.function\ndeftf_wrapper(x, y):\n  print('Tracing.')\n  return py_plus(x, y)\n\n```\n\nThe [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will trace the first time:\n```\ntf_wrapper(tf.constant(1.0), tf.constant(2.0)).numpy()\n\n```\n```\nTracing.\nExecuting eagerly.\n3.0\n\n```\n\nBut the [`tf.py_function`](https://www.tensorflow.org/api_docs/python/tf/py_function) inside executes eagerly every time:\n```\ntf_wrapper(tf.constant(1.0), tf.constant(2.0)).numpy()\n\n```\n```\nExecuting eagerly.\n3.0\n\n```\n\n#### Changing Python global and free variables\nChanging Python global and \n```\nexternal_list = []\n\n@tf.function\ndefside_effect(x):\n  print('Python side effect')\n  external_list.append(x)\n\nside_effect(1)\nside_effect(1)\nside_effect(1)\n# The list append only happened once!\nassert len(external_list) == 1\n\n```\n```\nPython side effect\n\n```\n\nSometimes unexpected behaviors are very hard to notice. In the example below, the `counter` is intended to safeguard the increment of a variable. However because it is a python integer and not a TensorFlow object, it's value is captured during the first trace. When the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is used, the `assign_add` will be recorded unconditionally in the underlying graph. Therefore `v` will increase by 1, every time the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is called. This issue is common among users that try to migrate their Graph-mode Tensorflow code to Tensorflow 2 using [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorators, when python side-effects (the `counter` in the example) are used to determine what ops to run (`assign_add` in the example). Usually, users realize this only after seeing suspicious numerical results, or significantly lower performance than expected (e.g. if the guarded operation is very costly).\n```\nclassModel(tf.Module):\n  def__init__(self):\n    self.v = tf.Variable(0)\n    self.counter = 0\n\n  @tf.function\n  def__call__(self):\n    if self.counter == 0:\n      # A python side-effect\n      self.counter += 1\n      self.v.assign_add(1)\n\n    return self.v\n\nm = Model()\nfor n in range(3):\n  print(m().numpy()) # prints 1, 2, 3\n\n```\n```\n1\n2\n3\n\n```\n\nA workaround to achieve the expected behavior is using [`tf.init_scope`](https://www.tensorflow.org/api_docs/python/tf/init_scope) to lift the operations outside of the function graph. This ensures that the variable increment is only done once during tracing time. It should be noted `init_scope` has other side effects including cleared control flow and gradient tape. Sometimes the usage of `init_scope` can become too complex to manage realistically.\n```\nclassModel(tf.Module):\n  def__init__(self):\n    self.v = tf.Variable(0)\n    self.counter = 0\n\n  @tf.function\n  def__call__(self):\n    if self.counter == 0:\n      # Lifts ops out of function-building graphs\n      with tf.init_scope():\n        self.counter += 1\n        self.v.assign_add(1)\n\n    return self.v\n\nm = Model()\nfor n in range(3):\n  print(m().numpy()) # prints 1, 1, 1\n\n```\n```\n1\n1\n1\n\n```\n\nIn summary, as a rule of thumb, you should avoid mutating python objects such as integers or containers like lists that live outside the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). Instead, use arguments and TF objects. For example, the section [\"Accumulating values in a loop\"](https://www.tensorflow.org/guide/function#accumulating_values_in_a_loop) has one example of how list-like operations can be implemented.\nYou can, in some cases, capture and manipulate state if it is a [`tf.Variable`](https://www.tensorflow.org/guide/variable). This is how the weights of Keras models are updated with repeated calls to the same `ConcreteFunction`.\n#### Using Python iterators and generators\nMany Python features, such as generators and iterators, rely on the Python runtime to keep track of state. In general, while these constructs work as expected in eager mode, they are examples of Python side effects and therefore only happen during tracing.\n```\n@tf.function\ndefbuggy_consume_next(iterator):\n  tf.print(\"Value:\", next(iterator))\n\niterator = iter([1, 2, 3])\nbuggy_consume_next(iterator)\n# This reuses the first value from the iterator, rather than consuming the next value.\nbuggy_consume_next(iterator)\nbuggy_consume_next(iterator)\n\n```\n```\nValue: 1\nValue: 1\nValue: 1\n\n```\n\nJust like how TensorFlow has a specialized [`tf.TensorArray`](https://www.tensorflow.org/api_docs/python/tf/TensorArray) for list constructs, it has a specialized [`tf.data.Iterator`](https://www.tensorflow.org/api_docs/python/tf/data/Iterator) for iteration constructs. See the section on [AutoGraph transformations](https://www.tensorflow.org/guide/function#autograph_transformations) for an overview. Also, the [`tf.data`](https://www.tensorflow.org/guide/data) API can help implement generator patterns:\n```\n@tf.function\ndefgood_consume_next(iterator):\n  # This is ok, iterator is a tf.data.Iterator\n  tf.print(\"Value:\", next(iterator))\n\nds = tf.data.Dataset.from_tensor_slices([1, 2, 3])\niterator = iter(ds)\ngood_consume_next(iterator)\ngood_consume_next(iterator)\ngood_consume_next(iterator)\n\n```\n```\nValue: 1\nValue: 2\nValue: 3\n\n```\n\n### All outputs of a tf.function must be return values\nWith the exception of [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s, a tf.function must return all its outputs. Attempting to directly access any tensors from a function without going through return values causes \"leaks\".\nFor example, the function below \"leaks\" the tensor `a` through the Python global `x`:\n```\nx = None\n\n@tf.function\ndefleaky_function(a):\n  global x\n  x = a + 1  # Bad - leaks local tensor\n  return a + 2\n\ncorrect_a = leaky_function(tf.constant(1))\n\nprint(correct_a.numpy())  # Good - value obtained from function's returns\ntry:\n  x.numpy()  # Bad - tensor leaked from inside the function, cannot be used here\nexcept AttributeError as expected:\n  print(expected)\n\n```\n```\n3\n'SymbolicTensor' object has no attribute 'numpy'\n\n```\n\nThis is true even if the leaked value is also returned:\n```\n@tf.function\ndefleaky_function(a):\n  global x\n  x = a + 1  # Bad - leaks local tensor\n  return x  # Good - uses local tensor\n\ncorrect_a = leaky_function(tf.constant(1))\n\nprint(correct_a.numpy())  # Good - value obtained from function's returns\ntry:\n  x.numpy()  # Bad - tensor leaked from inside the function, cannot be used here\nexcept AttributeError as expected:\n  print(expected)\n\n@tf.function\ndefcaptures_leaked_tensor(b):\n  b += x  # Bad - `x` is leaked from `leaky_function`\n  return b\n\nwith assert_raises(TypeError):\n  captures_leaked_tensor(tf.constant(2))\n\n```\n```\n2\n'SymbolicTensor' object has no attribute 'numpy'\nCaught expected exception \n  <class 'TypeError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/566849597.py\", line 21, in <module>\n    captures_leaked_tensor(tf.constant(2))\nTypeError: <tf.Tensor 'add:0' shape=() dtype=int32> is out of scope and cannot be used here. Use return values, explicit Python locals or TensorFlow collections to access it.\nPlease see https://www.tensorflow.org/guide/function#all_outputs_of_a_tffunction_must_be_return_values for more information.\n\n<tf.Tensor 'add:0' shape=() dtype=int32> was defined here:\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n    File \"/tmpfs/tmp/ipykernel_167534/566849597.py\", line 7, in <module>\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 833, in __call__\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 889, in _call\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 696, in _initialize\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 178, in trace_function\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 283, in _maybe_define_function\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\", line 310, in _create_concrete_function\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 1059, in func_graph_from_py_func\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 599, in wrapped_fn\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py\", line 41, in autograph_handler\n    File \"/tmpfs/tmp/ipykernel_167534/566849597.py\", line 4, in leaky_function\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/override_binary_operator.py\", line 113, in binary_op_wrapper\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/ops/tensor_math_operator_overrides.py\", line 28, in _add_dispatch_factory\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py\", line 1260, in op_dispatch_handler\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py\", line 1701, in _add_dispatch\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 490, in add_v2\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py\", line 796, in _apply_op_helper\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\", line 670, in _create_op_internal\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 2682, in _create_op_internal\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\", line 1177, in from_node_def\n\nThe tensor <tf.Tensor 'add:0' shape=() dtype=int32> cannot be accessed from here, because it was defined in FuncGraph(name=leaky_function, id=139959630636096), which is out of scope.\n\n```\n\nUsually, leaks such as these occur when you use Python statements or data structures. In addition to leaking inaccessible tensors, such statements are also likely wrong because they count as Python side effects, and are not guaranteed to execute at every function call.\nCommon ways to leak local tensors also include mutating an external Python collection, or an object:\n```\nclassMyClass:\n\n  def__init__(self):\n    self.field = None\n\nexternal_list = []\nexternal_object = MyClass()\n\ndefleaky_function():\n  a = tf.constant(1)\n  external_list.append(a)  # Bad - leaks tensor\n  external_object.field = a  # Bad - leaks tensor\n\n```\n\n### Recursive tf.functions are not supported\nRecursive [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s are not supported and could cause infinite loops. For example,\n```\n@tf.function\ndefrecursive_fn(n):\n  if n > 0:\n    return recursive_fn(n - 1)\n  else:\n    return 1\n\nwith assert_raises(Exception):\n  recursive_fn(tf.constant(5))  # Bad - maximum recursion error.\n\n```\n```\nCaught expected exception \n  <class 'Exception'>:\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 9, in <module>\n    recursive_fn(tf.constant(5))  # Bad - maximum recursion error.\ntensorflow.python.autograph.impl.api.StagingError: in user code:\n\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n    File \"/tmpfs/tmp/ipykernel_167534/2233998312.py\", line 4, in recursive_fn  *\n        return recursive_fn(n - 1)\n\n    RecursionError: maximum recursion depth exceeded while calling a Python object\n\n```\n\nEven if a recursive [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) seems to work, the Python function will be traced multiple times and could have performance implications. For example,\n```\n@tf.function\ndefrecursive_fn(n):\n  if n > 0:\n    print('tracing')\n    return recursive_fn(n - 1)\n  else:\n    return 1\n\nrecursive_fn(5)  # Warning - multiple tracings\n\n```\n```\ntracing\ntracing\ntracing\ntracing\ntracing\n<tf.Tensor: shape=(), dtype=int32, numpy=1>\n\n```\n\n## Known Issues\nIf your [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is not evaluating correctly, the error may be explained by these known issues which are planned to be fixed in the future.\n### Depending on Python global and free variables\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) creates a new `ConcreteFunction` when called with a new value of a Python argument. However, it does not do that for the Python closure, globals, or nonlocals of that [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). If their value changes in between calls to the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), the [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) will still use the values they had when it was traced. This is different from how regular Python functions work.\nFor that reason, you should follow a functional programming style that uses arguments instead of closing over outer names.\n```\n@tf.function\ndefbuggy_add():\n  return 1 + foo\n\n@tf.function\ndefrecommended_add(foo):\n  return 1 + foo\n\nfoo = 1\nprint(\"Buggy:\", buggy_add())\nprint(\"Correct:\", recommended_add(foo))\n\n```\n```\nBuggy: tf.Tensor(2, shape=(), dtype=int32)\nCorrect: tf.Tensor(2, shape=(), dtype=int32)\n\n```\n```\nprint(\"Updating the value of `foo` to 100!\")\nfoo = 100\nprint(\"Buggy:\", buggy_add())  # Did not change!\nprint(\"Correct:\", recommended_add(foo))\n\n```\n```\nUpdating the value of `foo` to 100!\nBuggy: tf.Tensor(2, shape=(), dtype=int32)\nCorrect: tf.Tensor(101, shape=(), dtype=int32)\n\n```\n\nAnother way to update a global value is to make it a [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) and use the [`Variable.assign`](https://www.tensorflow.org/api_docs/python/tf/Variable#assign) method instead.\n```\n@tf.function\ndefvariable_add():\n  return 1 + foo\n\nfoo = tf.Variable(1)\nprint(\"Variable:\", variable_add())\n\n```\n```\nVariable: tf.Tensor(2, shape=(), dtype=int32)\n\n```\n```\nprint(\"Updating the value of `foo` to 100!\")\nfoo.assign(100)\nprint(\"Variable:\", variable_add())\n\n```\n```\nUpdating the value of `foo` to 100!\nVariable: tf.Tensor(101, shape=(), dtype=int32)\n\n```\n\n### Depending on Python objects\nPassing custom Python objects as arguments to [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is supported but has certain limitations.\nFor maximum feature coverage, consider transforming the objects into [Extension types](https://www.tensorflow.org/guide/extension_type) before passing them to [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function). You can also use Python primitives and [`tf.nest`](https://www.tensorflow.org/api_docs/python/tf/nest)-compatible structures.\nHowever, as covered in the [rules of tracing](https://www.tensorflow.org/guide/function#rules_of_tracing), when a custom `TraceType` is not provided by the custom Python class, [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) is forced to use instance-based equality which means it will **not create a new trace** when you pass the **same object with modified attributes**.\n```\nclassSimpleModel(tf.Module):\n  def__init__(self):\n    # These values are *not* tf.Variables.\n    self.bias = 0.\n    self.weight = 2.\n\n@tf.function\ndefevaluate(model, x):\n  return model.weight * x + model.bias\n\nsimple_model = SimpleModel()\nx = tf.constant(10.)\nprint(evaluate(simple_model, x))\n\n```\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n```\n```\nprint(\"Adding bias!\")\nsimple_model.bias += 5.0\nprint(evaluate(simple_model, x))  # Didn't change :(\n\n```\n```\nAdding bias!\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n```\n\nUsing the same [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to evaluate the modified instance of the model will be buggy since it still has the [same instance-based TraceType](https://www.tensorflow.org/guide/function#rules_of_tracing) as the original model.\nFor that reason, you're recommended to write your [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) to avoid depending on mutable object attributes or implement the [Tracing Protocol](https://www.tensorflow.org/guide/function#use_the_tracing_protocol) for the objects to inform [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) about such attributes.\nIf that is not possible, one workaround is to make new [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function)s each time you modify your object to force retracing:\n```\ndefevaluate(model, x):\n  return model.weight * x + model.bias\n\nnew_model = SimpleModel()\nevaluate_no_bias = tf.function(evaluate).get_concrete_function(new_model, x)\n# Don't pass in `new_model`. `tf.function` already captured its state during tracing.\nprint(evaluate_no_bias(x))\n\n```\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n```\n```\nprint(\"Adding bias!\")\nnew_model.bias += 5.0\n# Create new `tf.function` and `ConcreteFunction` since you modified `new_model`.\nevaluate_with_bias = tf.function(evaluate).get_concrete_function(new_model, x)\nprint(evaluate_with_bias(x)) # Don't pass in `new_model`.\n\n```\n```\nAdding bias!\ntf.Tensor(25.0, shape=(), dtype=float32)\n\n```\n\nAs [retracing can be expensive](https://www.tensorflow.org/guide/intro_to_graphs#tracing_and_performance), you can use [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s as object attributes, which can be mutated (but not changed, careful!) for a similar effect without needing a retrace.\n```\nclassBetterModel:\n\n  def__init__(self):\n    self.bias = tf.Variable(0.)\n    self.weight = tf.Variable(2.)\n\n@tf.function\ndefevaluate(model, x):\n  return model.weight * x + model.bias\n\nbetter_model = BetterModel()\nprint(evaluate(better_model, x))\n\n```\n```\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n```\n```\nprint(\"Adding bias!\")\nbetter_model.bias.assign_add(5.0)  # Note: instead of better_model.bias += 5\nprint(evaluate(better_model, x))  # This works!\n\n```\n```\nAdding bias!\ntf.Tensor(25.0, shape=(), dtype=float32)\n\n```\n\n### Creating tf.Variables\n[`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) only supports singleton [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s created once on the first call, and reused across subsequent function calls. The code snippet below would create a new [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) in every function call, which results in a `ValueError` exception.\nExample:\n```\n@tf.function\ndeff(x):\n  v = tf.Variable(1.0)\n  return v\n\nwith assert_raises(ValueError):\n  f(1.0)\n\n```\n```\nCaught expected exception \n  <class 'ValueError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/3018268426.py\", line 7, in <module>\n    f(1.0)\nValueError: in user code:\n\n    File \"/tmpfs/tmp/ipykernel_167534/3018268426.py\", line 3, in f  *\n        v = tf.Variable(1.0)\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n\n```\n\nA common pattern used to work around this limitation is to start with a Python None value, then conditionally create the [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable) if the value is None:\n```\nclassCount(tf.Module):\n  def__init__(self):\n    self.count = None\n\n  @tf.function\n  def__call__(self):\n    if self.count is None:\n      self.count = tf.Variable(0)\n    return self.count.assign_add(1)\n\nc = Count()\nprint(c())\nprint(c())\n\n```\n```\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\n\n```\n\n#### Using with multiple Keras optimizers\nYou may encounter `ValueError: tf.function only supports singleton tf.Variables created on the first call.` when using more than one Keras optimizer with a `tf.function`. This error occurs because optimizers internally create `tf.Variable`s when they apply gradients for the first time.\n```\nopt1 = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nopt2 = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n\n@tf.function\ndeftrain_step(w, x, y, optimizer):\n   with tf.GradientTape() as tape:\n       L = tf.reduce_sum(tf.square(w*x - y))\n   gradients = tape.gradient(L, [w])\n   optimizer.apply_gradients(zip(gradients, [w]))\n\nw = tf.Variable(2.)\nx = tf.constant([-1.])\ny = tf.constant([2.])\n\ntrain_step(w, x, y, opt1)\nprint(\"Calling `train_step` with different optimizer...\")\nwith assert_raises(ValueError):\n  train_step(w, x, y, opt2)\n\n```\n```\nCalling `train_step` with different optimizer...\nCaught expected exception \n  <class 'ValueError'>:\nTraceback (most recent call last):\n  File \"/tmpfs/tmp/ipykernel_167534/3551158538.py\", line 8, in assert_raises\n    yield\n  File \"/tmpfs/tmp/ipykernel_167534/950644149.py\", line 18, in <module>\n    train_step(w, x, y, opt2)\nValueError: in user code:\n\n    File \"/tmpfs/tmp/ipykernel_167534/950644149.py\", line 9, in train_step  *\n        optimizer.apply_gradients(zip(gradients, [w]))\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 291, in apply_gradients  **\n        self.apply(grads, trainable_variables)\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 330, in apply\n        self.build(trainable_variables)\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/adam.py\", line 97, in build\n        self.add_variable_from_reference(\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 36, in add_variable_from_reference\n        return super().add_variable_from_reference(\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 227, in add_variable_from_reference\n        return self.add_variable(\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 201, in add_variable\n        variable = backend.Variable(\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/backend/common/variables.py\", line 163, in __init__\n        self._initialize_with_initializer(initializer)\n    File \"/tmpfs/src/tf_docs_env/lib/python3.9/site-packages/keras/src/backend/tensorflow/core.py\", line 40, in _initialize_with_initializer\n        self._value = tf.Variable(\n\n    ValueError: tf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\n\n```\n\nIf you need to change a stateful object between calls, it's simplest to define a [`tf.Module`](https://www.tensorflow.org/api_docs/python/tf/Module) subclass, and create instances to hold those objects:\n```\nclassTrainStep(tf.Module):\n  def__init__(self, optimizer):\n    self.optimizer = optimizer\n\n  @tf.function\n  def__call__(self, w, x, y):\n    with tf.GradientTape() as tape:\n       L = tf.reduce_sum(tf.square(w*x - y))\n    gradients = tape.gradient(L, [w])\n    self.optimizer.apply_gradients(zip(gradients, [w]))\n\n\nopt1 = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nopt2 = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n\ntrain_o1 = TrainStep(opt1)\ntrain_o2 = TrainStep(opt2)\n\ntrain_o1(w, x, y)\ntrain_o2(w, x, y)\n\n```\n\nYou could also do this manually by creating multiple instances of the [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) wrapper, one for each optimizer:\n```\nopt1 = tf.keras.optimizers.Adam(learning_rate = 1e-2)\nopt2 = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n\n# Not a tf.function.\ndeftrain_step(w, x, y, optimizer):\n   with tf.GradientTape() as tape:\n       L = tf.reduce_sum(tf.square(w*x - y))\n   gradients = tape.gradient(L, [w])\n   optimizer.apply_gradients(zip(gradients, [w]))\n\nw = tf.Variable(2.)\nx = tf.constant([-1.])\ny = tf.constant([2.])\n\n# Make a new tf.function and ConcreteFunction for each optimizer.\ntrain_step_1 = tf.function(train_step)\ntrain_step_2 = tf.function(train_step)\nfor i in range(10):\n  if i % 2 == 0:\n    train_step_1(w, x, y, opt1)\n  else:\n    train_step_2(w, x, y, opt2)\n\n```\n\n#### Using with multiple Keras models\nYou may also encounter `ValueError: tf.function only supports singleton tf.Variables created on the first call.` when passing different model instances to the same `tf.function`.\nThis error occurs because Keras models (which [do not have their input shape defined](https://www.tensorflow.org/guide/keras/custom_layers_and_models#best_practice_deferring_weight_creation_until_the_shape_of_the_inputs_is_known)) and Keras layers create [`tf.Variable`](https://www.tensorflow.org/api_docs/python/tf/Variable)s when they are first called. You may be attempting to initialize those variables inside a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), which has already been called. To avoid this error, try calling `model.build(input_shape)` to initialize all the weights before training the model.\n## Further reading\nTo learn about how to export and load a [`tf.function`](https://www.tensorflow.org/api_docs/python/tf/function), see the [SavedModel guide](https://www.tensorflow.org/guide/saved_model). To learn more about graph optimizations that are performed after tracing, see the [Grappler guide](https://www.tensorflow.org/guide/graph_optimization). To learn how to optimize your data pipeline and profile your model, see the [Profiler guide](https://www.tensorflow.org/guide/profiler).\n"
}