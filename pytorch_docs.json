{
  "https://pytorch.org/docs/stable/": "> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).  \n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable": "> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).  \n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/accelerator.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/build_ci_governance.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/checkpoint.html": "Checkpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward propagation. This can cause persistent states like the RNG state to be more advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply to or to omit stashing and restoring the RNG state during each checkpoint.  \nThe stashing logic saves and restores the RNG state for CPU and another device type (infer the device type from Tensor arguments excluding CPU tensors by ) to the . If there are multiple device, device state will only be saved for devices of a single device type, and the remaining devices will be ignored. Consequently, if any checkpointed functions involve randomness, this may result in incorrect gradients. (Note that if CUDA devices are among the devices detected, it will be prioritized; otherwise, the first device encountered will be selected.) If there are no CPU-tensors, the default device type state (default value is , and it could be set to other device by ) will be saved and restored. However, the logic has no way to anticipate if the user will move Tensors to a new device within the itself. Therefore, if you move Tensors to a new device (“new” meaning not belonging to the set of [current device + devices of Tensor arguments]) within , deterministic output compared to non-checkpointed passes is never guaranteed.     \nActivation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.\nIf the invocation during the backward pass differs from the forward pass, e.g., due to a global variable, the checkpointed version may not be equivalent, potentially causing an error being raised or leading to silently incorrect gradients.\nThe parameter should be passed explicitly. In version 2.4 we will raise an exception if is not passed. If you are using the variant, please refer to the note below for important considerations and potential limitations.\n  * Non-reentrant checkpoint stops recomputation as soon as all needed intermediate activations have been recomputed. This feature is enabled by default, but can be disabled with . Reentrant checkpoint always recomputes in its entirety during the backward pass.\n  * The reentrant variant does not record the autograd graph during the forward pass, as it runs with the forward pass under . The non-reentrant version does record the autograd graph, allowing one to perform backward on the graph within checkpointed regions.\n  * At least one input and output must have for the reentrant variant. If this condition is unmet, the checkpointed part of the model will not have gradients. The non-reentrant version does not have this requirement.\n  * The reentrant checkpoint does not support checkpointed regions with detached tensors from the computational graph, whereas the non-reentrant version does. For the reentrant variant, if the checkpointed segment contains tensors detached using or with , the backward pass will raise an error. This is because makes all the outputs require gradients and this causes issues when a tensor is defined to have no gradient in the model. To avoid this, detach the tensors outside of the function.\n\n    \n  * – describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes , should correctly use the first input as and the second input as \n  * ( is not passed. If , will use an implementation that does not require reentrant autograd. This allows to support additional functionality, such as working as expected with and support for keyword arguments input into the checkpointed function.\n  * () – A callable returning a tuple of two context managers. The function and its recomputation will be run under the first and second context managers respectively. This argument is only supported if .\n  * () – A string specifying the determinism check to perform. By default it is set to which compares the shapes, dtypes, and devices of the recomputed tensors against those the saved tensors. To turn off this check, specify . Currently these are the only two supported values. Please open an issue if you would like to see more determinism checks. This argument is only supported if , if , the determinism check is always disabled.\n\n    \nSequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will not store the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.     \n  * ( is not passed. If , will use an implementation that does not require reentrant autograd. This allows to support additional functionality, such as working as expected with and support for keyword arguments input into the checkpointed function.\n\n    \nContext manager that sets whether checkpoint should print additional debug information when running. See the flag for for more information. Note that when set, this context manager overrides the value of passed to checkpoint. To defer to the local setting, pass to this context.     \n\n              \n  *     * If a policy function is provided, it should accept a , the , args and kwargs to the op, and return a enum value indicating whether the execution of the op should be recomputed or not.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/backends.html": "      \nNote that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run on a machine with working CUDA drivers and devices, we would be able to use it.     \nWhen PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available. For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance. This flag (a \n  * User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.\n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs.     \n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs.     \nWhen PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a \n  * User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.\n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect for your application’s inputs.                         \nThis context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.     \nNote that this doesn’t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.     \nTo make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named . However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.     \nTo make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named . However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.     \nYou must install opt-einsum in order for torch to automatically optimize einsum. To make opt-einsum available, you can install it along with torch: or by itself: . If the package is installed, torch will import it automatically and use it accordingly. Use this function to check whether opt-einsum was installed and properly imported by torch.     \nA is . By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal” strategies are also supported. Note that the “optimal” strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum’s docs (\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/bottleneck.html": "Due to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.  \nTo decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (“CPU total time is much greater than CUDA total time”). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.\nOf course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you’re evaluating. If the profiler outputs don’t help, you could try looking at the result of with . However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Similarly, helps to analyze performance on Intel platforms further with .\nIf you are profiling CUDA code, the first profiler that runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/benchmark_utils.html": "      \n  1.     \nWhen measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the API by conceptually merging and . (Exact algorithms are discussed in method docstrings.) The method is replicated for cases where an adaptive strategy is not desired.\n  2. \n    \n  * () – Callable which returns the current time. If PyTorch was built without CUDA or there is no GPU present, this defaults to ; otherwise it will synchronize CUDA before measuring the time.\n  * Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be “float” or “int”, so that it is easy to differentiate: “ReLU(x + 1): (float)”\n  * String to distinguish measurements with identical label and sub_label. The principal use of is to signal to the columns of data. For instance one might set it based on the input size to create a table of the form:\n  * () – This tag indicates that otherwise identical tasks were run in different environments, and are therefore not equivalent, for instance when A/B testing a change to a kernel. will treat Measurements with different specification as distinct when merging replicate runs.\n  * (. Single threaded performance is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency, so the default is set to one. This is in contrast to the default PyTorch threadpool size which tries to utilize all cores.\n\n         \n>   1. A large block size better amortizes the cost of invocation, and results in a less biased measurement. This is important because CUDA synchronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.\n> \n    \nUnlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, however this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.\nBecause there is a process boundary between the caller (this process) and the execution, cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, ’s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to for them to transfer properly.          \nThis property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of , so z can approximate .\nThe significant figure estimation used in conjunction with the method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for .          \nWhen comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:     \nOne common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is “why”. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis.     \nmatches the semantics of callgrind. If True, the counts include instructions executed by children. is useful for identifying hot spots in code; is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(…) for more details)          \nSeveral instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.     \nThis can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc.     \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/amp.html": "provides convenience methods for mixed precision, where some operations use the () datatype and other operations use lower precision floating point datatype (): () or . Some ops, like linear layers and convolutions, are much faster in . Other ops, like reductions, often require the dynamic range of . Mixed precision tries to match each op to its appropriate datatype.  \nOrdinarily, “automatic mixed precision training” with datatype of uses and together, as shown in the and . However, and are modular, and may be used separately if desired. As shown in the CPU example section of , “automatic mixed precision training/inference” on CPU with datatype of only uses .\n  * \n    \nshould wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.\nFloating-point Tensors produced in an autocast-enabled region may be . After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to (or other dtype if desired). If a Tensor from the autocast region is already , the cast is a no-op, and incurs no additional overhead. CUDA Example:\n```\n\n    \n    \n    \n    \n\n \n    \n    \n    \n       \n    \n       \n\n\n   \n\n```\n\nsubregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular . Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to before use:\n```\n\n    \n    \n    \n    \n\n \n       \n      \n        \n        \n           \n\n    \n    \n       \n\n```\n\nThe autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects and when used with more than one GPU per process (see ).     \n  * () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’, and ‘hpu’. The type is the same as the attribute of a . Thus, you may obtain the device type of a tensor using .\n\n         \n  * ( or None, optional, default=None) – If not , when runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes with autocast disabled. If , ’s internal ops execute with the current autocast state.\n\n    \nIf the forward pass for a particular op has inputs, the backward pass for that op will produce gradients. Gradient values with small magnitudes may not be representable in . These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\nTo prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss or gradients when running with AMP/fp16, verify your model is compatible.\nOnly out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region can autocast, but and cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\nThe backward passes of (and , which wraps it) can produce gradients that aren’t representable in . In autocast-enabled regions, the forward input may be , which means the backward gradient must be representable in (autocasting forward inputs to doesn’t help, because that cast must be reversed in backward). Therefore, and raise an error in autocast-enabled regions.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/contribution_guide.html": "  * The majority of open source contributions come from people scratching their own itches. However, if you don’t know what you want to work on, or are just looking to get more acquainted with the project, here are some tips for how to find appropriate tasks:  \n  * The majority of pull requests are small; in that case, no need to let us know about what you want to do, just get cracking. But if the change is going to be large, it’s usually a good idea to get some design comments about it first by \n    * Some feature additions are very standardized; for example, lots of people add new operators or optimizers to PyTorch. Design discussion in these cases boils down mostly to, “Do we want this operator/optimizer?” Giving evidence for its utility, e.g., usage in peer reviewed papers, or existence in other frameworks, helps a bit when making this case.\n      * is generally not accepted unless there is overwhelming evidence that this newly published work has ground-breaking results and will eventually become a standard in the field. If you are not sure where your method falls, open an issue first before implementing a PR.\n    * Core changes and refactors can be quite difficult to coordinate since the pace of development on the PyTorch main branch is quite fast. Definitely reach out about fundamental or cross-cutting changes; we can often give guidance about how to stage such changes into more easily reviewable pieces.\n  *     * If you are not ready for the pull request to be reviewed, create a draft pull request first - you can later convert it to a full PR by pressing “Ready for review” button. You can also prepend the title of the PR with “[WIP]” (“work in progress”) while it’s still in draft. We will ignore draft PRs when doing review passes. If you are working on a complex change, it’s good to start things off as a draft, because you will need to spend time looking at CI results to see if things worked out or not.\n    * Find an appropriate reviewer for your change. We have some folks who regularly go through the PR queue and try to review everything, but if you happen to know who the maintainer for a given subsystem affected by your patch is, feel free to include them directly on the pull request. You can learn more about that could review your code.\n  *     * We’ll try our best to minimize the number of review round trips and block PRs only when there are major issues. For the most common issues in pull requests, take a look at .\n\n\nNew feature ideas are best discussed on a specific issue. Please include as much information as you can, any accompanying data, and your proposed solution. The PyTorch team and community frequently review new issues and comments where they think they can help. If you feel confident in your solution, go ahead and implement it.\nIf you want to fix a specific issue, it’s best to comment on the individual issue with your intent. However, we do not lock or assign issues except in cases where we have worked with the developer before. It’s best to strike up a conversation on the issue and discuss your proposed solution. The PyTorch team can provide guidance that saves you time.\nWe aim to produce high quality documentation and tutorials. On rare occasions that content includes typos or bugs. If you find something you can fix, send us a pull request for consideration.\nWe appreciate your help reviewing and commenting on pull requests. Our team strives to keep the number of open pull requests at a manageable size, we respond quickly for more information if we need it, and we merge PRs that we think are useful. However, due to the high level of interest, additional eyes on the pull requests are always appreciated.\nImproving code readability helps everyone. It is often better to submit a small number of pull requests that touch a few files versus a large pull request that touches many files. Starting a discussion in the PyTorch forum or on an issue related to your improvement is the best way to get started.\nYour use of PyTorch in your projects, research papers, write ups, blogs, or general discussions around the internet helps to raise awareness for PyTorch and our growing community. Please reach out to \nIf you feel that an issue could benefit from a particular tag or level of complexity, comment on the issue and share your opinion. If you feel an issue isn’t categorized properly, comment and let the team know.\n  * People often want to “claim” an issue when they decide to work on it, to ensure that there isn’t wasted work when someone else ends up working on it. This doesn’t really work too well in open source, since someone may decide to work on something, and end up not having time to do it. Feel free to give information in an advisory fashion, but at the end of the day, we will take running code and rough consensus to move forward quickly.\n  * Unlike in a corporate environment, where the person who wrote code implicitly “owns” it and can be expected to take care of it for the code’s lifetime, once a pull request is merged into an open source project, it immediately becomes the collective responsibility of all maintainers on the project. When we merge code, we are saying that we, the maintainers, can review subsequent changes and make a bugfix to the code. This naturally leads to a higher standard of contribution.\n\n\n  *     *       1. to help us tell if the patch is correct in the first place (yes, we did review it, but as Knuth says, “beware of the following code, for I have not run it, merely proven it correct”)\n    * When is it OK not to add a test? Sometimes a change can’t be conveniently tested, or the change is so obviously correct (and unlikely to be broken) that it’s OK not to test it. On the contrary, if a change seems likely (or is known to be likely) to be accidentally broken, it’s important to put in the time to work out a testing strategy.\n  *     * When is it OK to submit a large PR? It helps a lot if there was a corresponding design discussion in an issue, with sign off from the people who are going to review your diff. We can also help give advice about how to split up a large change into individually shippable parts. Similarly, it helps if there is a complete description of the contents of the PR: it’s easier to review code if we know what’s inside!\n  * To prevent major regressions, pull requests that touch core components receive extra scrutiny. Make sure you’ve discussed your changes with the team before undertaking major changes.\n  * If you want to add new features, comment your intention on the related issue. Our team tries to comment on and provide feedback to the community. It’s better to have an open discussion with the team and the rest of the community before building new features. This helps us stay aware of what you’re working on and increases the chance that it’ll be merged.\n\n\n  * There is lots of value if community developers reproduce issues, try out new functionality, or otherwise help us identify or troubleshoot issues. Commenting on tasks or pull requests with your environment details is helpful and appreciated.\n  * Maybe your PR is based off a broken main branch? You can try to rebase your change on top of the latest main branch. You can also see the current status of main branch’s CI at .\n  * Sometimes another community member will provide a patch or fix to your pull request or branch. This is often needed for getting CI tests to pass.\n\n\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/data.html": "An iterable-style dataset is an instance of a subclass of that implements the protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.  \nWhen using a with . The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See documentations for how to achieve this.\nThe rest of this section concerns the case with . classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.\nA sequential or shuffled sampler will be automatically constructed based on the argument to a . Alternatively, users may use the argument to specify a custom object that at each time yields the next index/key to fetch.\nThis is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).\nWhen (default ) is not , the data loader yields batched samples instead of individual samples. and arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify , which yields a list of keys at a time.\nThe and arguments essentially are used to construct a from . For map-style datasets, the is either provided by user or constructed based on the argument. For iterable-style datasets, the is a dummy infinite one. See on more details on samplers.\nIn certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where is used to collate the samples), but let the data loader directly return each member of the object.\n, is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default ().\nFor instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple , the default collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default has the following properties:\n  * It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for s, s, s, etc.\n\n\nIn this mode, data fetching is done in the same process a is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.\nAfter several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is ). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out \nIn this mode, each time an iterator of a is created (e.g., when you call ), worker processes are created. At this point, the , , and are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including ) runs in the worker process.\nreturns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns in main process. Users may use this function in dataset code and/or to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.\nFor map-style datasets, the main process generates the indices using and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.\nFor iterable-style datasets, since each worker process gets a replica of the object, naive multi-process loading will often result in duplicated data. Using and/or , users may configure each replica independently. (See documentations for how to achieve this. ) For similar reasons, in multi-process loading, the argument drops the last non-full batch of each worker’s iterable-style dataset replica.\nIt is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see ). Instead, we recommend using (i.e., setting ), which enables fast data transfer to CUDA-enabled GPUs.\n\n\n  * Wrap most of you main script’s code within block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and instance creation logic here, as it doesn’t need to be re-executed in workers.\n  * Make sure that any custom , or code is declared as top level definitions, outside of the check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not .)\n\n\nBy default, each worker will have its PyTorch seed set to , where is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified . However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See in FAQ.).\nThe default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a method on your custom type(s).          \n  * () – If , the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your returns a batch that is a custom type, see the example below.\n  * () – set to to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: )\n  * () – Number of batches loaded in advance by each worker. means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is . Otherwise, if value of default is ).\n\n\nheuristic is based on the length of the sampler used. When is an , it instead returns an estimate based on , with proper rounding depending on , regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user code in correctly handling multi-process loading to avoid duplicate data.\nHowever, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when is set. Unfortunately, PyTorch can not detect such cases in general.     \nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite , supporting fetching a data sample for a given key. Subclasses could also optionally overwrite , which is expected to return the size of the dataset by many implementations and the default options of . Subclasses could also optionally implement , for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.     \nWhen a subclass is used with , each item in the dataset will be yielded from the iterator. When , each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. , when called in a worker process, returns information about the worker. It can be used in either the dataset’s method or the ‘s option to modify each copy’s behavior.\n```\n \n       \n        \n            \n          \n          \n\n     \n          \n             \n              \n              \n          \n            \n                  \n              \n                  \n                 \n          \n\n\n   \n\n\n \n\n\n\n\n \n\n\n\n \n\n\n```\n\n```\n \n       \n        \n            \n          \n          \n\n     \n          \n\n\n   \n\n\n \n\n\n\n \n\n\n\n \n      \n        \n      \n      \n    \n          \n      \n          \n         \n\n\n\n\n  \n\n\n\n  \n\n\n```\n                   \n  * () – Optional dictionary mapping from element type to the corresponding collate function. If the element type isn’t present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key.\n\n    \n```\n\n   \n\n\n  \n\n\n       \n\n\n    \n   \n\n\n   \n\n\n   \n\n\n\n \n      \n        \n         \n      \n         \n\n   \n     \n \n  \n\n```\n    \nIf the input is a , , or , it tries to convert each element inside to a . If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both and are NOT defined in .     \n\n\nWhen used in a passed over to , this method can be useful to set up each worker process differently, for instance, using to configure the object to only read a specific fraction of a sharded dataset, or use to seed other libraries used in dataset code.          \nEvery Sampler subclass has to provide an method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and may provide a method that returns the length of the returned iterators.               \n  * (, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.\n\n              \n  * () – if , then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If , the sampler will add extra indices to make the data evenly divisible across the replicas. Default: .\n\n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpp_index.html": "allows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in the . This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including:  \nTorchScript can be augmented with user-supplied code through custom operators and custom classes. Once registered with TorchScript, these operators and classes can be invoked in TorchScript code run from Python or from C++ as part of a serialized TorchScript model. The tutorial walks through interfacing TorchScript with OpenCV. In addition to wrapping a function call with a custom operator, C++ classes and structs can be bound into TorchScript through a pybind11-like interface which is explained in the tutorial.\n\n\nThe “author in TorchScript, infer in C++” workflow requires model authoring to be done in TorchScript. However, there might be cases where the model has to be authored in C++ (e.g. in workflows where a Python component is undesirable). To serve such use cases, we provide the full capability of authoring and training a neural net model purely in C++, with familiar components such as / / that closely resemble the Python API.\n\n\nFor guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see: . Note that on Linux there are two types of libtorch binaries provided: one compiled with GCC pre-cxx11 ABI and the other with GCC cxx11 ABI, and you should make the selection based on the GCC ABI your system is using.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/config_mod.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/governance.html": "\n  \nBeyond the maintainers, the community is encouraged to contribute, file issues, make proposals, review pull requests and be present in the community. Given contributions and willingness to invest, anyone can be accepted as a maintainer and provided write access or ownership of parts of the codebase.\nTechnical governance is strictly separated from business governance. Separating technical from business governance ensures that there is no way for any person or company to “buy their way into” the technical guidance of the project. Additionally, membership in the technical governance process is for , not companies. That is, there are no seats reserved for specific companies, and membership is associated with the person rather than the company employing that person.\nEach maintainer group should publish publicly available communication for their module (a vision, rough roadmap, design docs, any disputes and dispute resolutions) so that contributors and other interested parties understand the future direction of the project and can participate in discussion.\n\n\nThe core maintainers as a group have the power to veto any decision made at a Module maintainer level. The core maintainers have power to resolve disputes as they see fit. The core maintainers should publicly articulate their decision-making, and give a clear reasoning for their decisions, vetoes and dispute resolution.\nThere may be decisions in which the core maintainers cannot come to a consensus. To make such difficult decisions, the core maintainers have an assigned and publicly declared Lead Core Maintainer amongst them, also commonly known in open-source governance models as a BDFL.\n  * Membership in module maintainer groups is given to on after they demonstrated strong expertise of the component through contributions, reviews and discussions and are aligned with how the component fits in overall PyTorch direction.\n  * Light criteria of moving module maintenance to ‘emeritus’ status if they don’t actively participate over long periods of time. Each module maintainer group may define the inactive period that’s appropriate for that module.\n\n\n  *   * The core maintainers then evaluate all information and make a final decision to Confirm or Decline the nomination. The decision of the core maintainers has to be articulated well and would be public.\n\n\n  *   * The core maintainers then evaluate all information and make a final decision to Confirm or Decline the removal. The decision of the core maintainers has to be articulated well and would be public.\n\n\n  * \n\n  * After a removal of the Lead Core Maintainer or in unforeseen circumstances (such as permanent unavailability of the Lead Core Maintainer), the core maintainers follow a Ranked-Choice voting method to elect a new Lead Core Maintainer.\n\n\nThe core maintainers together are responsible for taking decisions on adding, removing and re-scoping new modules in the PyTorch org, either as new repositories in the PyTorch GitHub org, or as folders in the \nThey invite proposals from members in the community (including themselves) for such changes. The proposals are open-ended, but should have some basic ground-work to make a convincing case to make change. The following is an example approach to this process:\n  1. Create a state of the world - make sure this change is necessary, for example adding a new project or module is worth the maintenance cost; or removing a project or module will not remove too much value from PyTorch;\n\n\nPrimary work happens through issues and pull requests on GitHub. Maintainers should avoid pushing their changes directly to the PyTorch repository, instead relying on pull requests. Approving a pull request by a core or module maintainer allows it to be merged without further process. Core and module maintainers, as listed on the page and within \nNotifying relevant experts about an issue or a pull request is important. Reviews from experts in the given interest area are strongly preferred, especially on pull request approvals. Failure to do so might end up with the change being reverted by the relevant expert.\n\n\nPyTorch participants acknowledge that the copyright in all new contributions will be retained by the copyright holder as independent works of authorship and that no contributor or copyright holder will be required to assign copyrights to the project. Except as described below, all code contributions to the project must be made using the 3-Clause-BSD License available here: \nThis is absolutely possible. The first step is to start contributing to the existing project area and supporting its health and success. In addition to this, you can make a proposal through a GitHub issue for new functionality or changes to improve the project area.\nNo, the PyTorch project is strictly driven by the a maintainer project philosophy and clearly separates technical governance from business governance. However, if you want to be involved in sponsorship and support, you can become involved in the PyTorch Foundation (PTF) and sponsorship through this. You can also have individual engineers look to become maintainers, but this is not guaranteed and is merit-based.\nNo, not at this point. We are however looking at ways to better support the community of independent developers around PyTorch. If you have suggestions or inputs, please reach out on the PyTorch forums to discuss.\nIf the change is relatively minor, a pull request on GitHub can be opened up immediately for review and merge by the project committers. For larger changes, please open an issue to make a proposal to discuss prior. Please also see the \nUnfortunately, the current commit process to PyTorch involves an interaction with Facebook infrastructure that can only be triggered by Facebook employees. We are however looking at ways to expand the committer base to individuals outside of Facebook and will provide an update when the tooling exists to allow this.\nNo, we encourage community members to showcase their work wherever and whenever they can. Please reach out to \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpu.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/design.html": "This document is designed to help contributors and module maintainers understand the high-level design principles that have developed over time in PyTorch. These are not meant to be hard-and-fast rules, but to serve as a guide to help trade off different concerns and to resolve disagreements that may come up while developing PyTorch. For more information on contributing, module maintainership, and how to escalate a disagreement to the Core Maintainers, please see .  \nWe believe the ability to maintain our flexibility to support researchers who are building on top of our abstractions remains critical. We can’t see what the future of what workloads will be, but we know we want them to be built first on PyTorch and that requires flexibility.\nIn more concrete terms, we operate in a manner and try to avoid jumping to regimes (for example, static shapes, graph-mode only) without a clear-eyed view of the tradeoffs. Often there is a temptation to impose strict user restrictions upfront because it can simplify implementation, but this comes with risks:\n\n\nWe want users to be able to seamlessly move their PyTorch code to different hardware and software platforms, to interoperate with different libraries and frameworks, and to experience the full richness of the PyTorch user experience, not a least common denominator subset.\n\n\nIn this specific case, and as a general design philosophy, PyTorch favors exposing simple and explicit building blocks rather than APIs that are easy-to-use by practitioners. The simple version is immediately understandable and debuggable by a new PyTorch user: you get a clear error if you call an operator requiring cross-device movement at the point in the program where the operator is actually invoked. The easy solution may let a new user move faster initially, but debugging such a system can be complex: How did the system make its determination? What is the API for plugging into such a system and how are objects represented in its IR?\nA caveat here is that this does not mean that higher-level “easy” APIs are not valuable; certainly there is a value in, for example, higher-levels in the stack to support efficient tensor computations across heterogeneous compute in a large cluster. Instead, what we mean is that focusing on simple lower-level building blocks helps inform the easy API while still maintaining a good experience when users need to leave the beaten path. It also allows space for innovation and the growth of more opinionated tools at a rate we cannot support in the PyTorch core library, but ultimately benefit from, as evidenced by our . In other words, not automating at the start allows us to potentially reach levels of good automation faster.\nOne thing PyTorch has needed to deal with over the years is Python overhead: we first rewrote the engine in C++, then the majority of operator definitions, then developed TorchScript and the C++ frontend.\nStill, working in Python provides easily the best experience for our users: it is flexible, familiar, and perhaps most importantly, has a huge ecosystem of scientific computing libraries and extensions available for use. This fact motivates a few of our most recent contributions, which attempt to hit a Pareto optimal point close to the Python usability end of the curve:\n\n\nThese design principles are not hard-and-fast rules, but hard won choices and anchor how we built PyTorch to be the debuggable, hackable and flexible framework it is today. As we have more contributors and maintainers, we look forward to applying these core principles with you across our libraries and ecosystem. We are also open to evolving them as we learn new things and the AI space evolves, as we know it will.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpp_extension.html": "      \nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.     \nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.\nBy default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).\nThe +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better.\nNote that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.\nIf you want to reference device symbols across compilation units (across object files), the object files need to be built with (-rdc=true or -dc). An exception to this rule is “dynamic parallelism” (nested kernel launches) which is not used a lot anymore. is less optimized so it needs to be used only on object files that need it. Using (Device Link Time Optimization) at the device code compilation step and step helps reduce the protentional perf degradation of . Note that it needs to be used at both steps to be useful.\nIf you have objects you need to have an extra (device linking) step before the CPU symbol linking step. There is also a case where is used without : when an extension is linked against a static lib containing rdc-compiled objects like the [NVSHMEM library](     \nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.\nBy default the extension will be compiled to run on all archs of the cards visible during the building process of the extension. If down the road a new card is installed the extension may need to be recompiled. You can override the default behavior using to explicitly specify which device architectures you want the extension to support:\nNote that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.     \nWhen using , it is allowed to supply a dictionary for (rather than the usual list) that maps from languages/compilers (the only expected values are , or ) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++, CUDA and SYCL compiler during mixed compilation.\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the environment variable to a non-negative number.     \nTo load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.\nBy default, the directory to which the build file is emitted and the resulting library compiled to is , where is the temporary folder on the current platform and the name of the extension. This location can be overridden in two ways. First, if the environment variable is set, it replaces and all extensions will be compiled into subfolders of this directory. Second, if the argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.\nTo compile the sources, the default system compiler () is used, which can be overridden by setting the environment variable. To pass additional arguments to the compilation process, or can be provided. For example, to compile your extension with optimizations, pass . You can also use to pass further include directories.\nCUDA support with mixed compilation is provided. Simply pass CUDA source files ( or ) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking . You can pass additional flags to nvcc via , just like with for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the environment variable is the safest option.\nSYCL support with mixed compilation is provided. Simply pass SYCL source files () along with other sources. Such files will be detected and compiled with SYCL compiler (such as Intel DPC++ Compiler) rather than the C++ compiler. You can pass additional flags to SYCL compiler via , just like with for C++. SYCL compiler is expected to be found via system PATH environment variable.     \n  * () – Determines whether CUDA headers and libraries are added to the build. If set to (default), this value is automatically determined based on the existence of or in . Set it to to force CUDA headers and libraries to be included.\n  * () – Determines whether SYCL headers and libraries are added to the build. If set to (default), this value is automatically determined based on the existence of in . Set it to to force SYCL headers and libraries to be included.\n\n         \nSources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to are first concatenated into a single file. This file is then prepended with .\nFurthermore, if the argument is supplied, bindings will be automatically generated for each function specified. can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.\nThe sources in are concatenated into a separate file and prepended with , and includes. The and files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the (and include its name in ).\nThe sources in are concatenated into a separate file and prepended with , includes. The and files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in per se. To bind to a SYCL kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the (and include its name in ).     \n  * – Determines whether CUDA headers and libraries are added to the build. If set to (default), this value is automatically determined based on whether is provided. Set it to to force CUDA headers and libraries to be included.\n  * – Determines whether SYCL headers and libraries are added to the build. If set to (default), this value is automatically determined based on whether is provided. Set it to to force SYCL headers and libraries to be included.\n  * – Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function is called via an intermediary function. This redirection might cause issues in obscure cases of cpp. This flag should be set to when this redirect causes issues.\n\n\nSince load_inline will just-in-time compile the source code, please ensure that you have the right toolchains installed in the runtime. For example, when loading C++, make sure a C++ compiler is available. If you’re loading a CUDA extension, you will need to additionally install the corresponding CUDA toolkit (nvcc and any other dependencies your code has). Compiling toolchains are not included when you install torch and must be additionally installed.\nDuring compiling, by default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the environment variable to a non-negative number.     \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/persons_of_interest.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/complex_numbers.html": "Complex numbers are numbers that can be expressed in the form , where a and b are real numbers, and is called the imaginary unit, which satisfies the equation . Complex numbers frequently occur in mathematics and engineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have handled complex numbers by representing the data in float tensors with shape where the last dimension contains the real and imaginary values.  \nTensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on complex tensors (e.g., , ) are likely to be faster and more memory efficient than operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized to use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\nThe default dtype for complex tensors is determined by the default floating point dtype. If the default floating point dtype is then complex numbers are inferred to have a dtype of , otherwise they are assumed to have a dtype of .\nUsers who currently worked around the lack of complex tensors with real tensors of shape can easily to switch using the complex tensors in their code using and . Note that these functions don’t perform any copy and return a view of the input tensor.\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative, the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers can be implemented to work out of the box with complex parameters. For more details, check out the note .\nand will compute the same updates on the parameters, though there may be slight numerical discrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers and capturable vs default optimizers. For more details, see .\nSpecifically, while you can think of our optimizer’s handling of complex tensors as the same as optimizing over their and pieces separately, the implementation details are not precisely that. Note that the equivalent will convert a complex tensor to a real tensor with shape , whereas splitting a complex tensor into two tensors is 2 tensors of size . This distinction has no impact on pointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS). We currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue if you have a use case that requires precisely defining this behavior.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html": "DDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in . A few built-in communication hooks are provided, and users can easily apply any of these hooks to optimize communication. Besides, the hook interface can also support user-defined communication strategies for more advanced use cases.  \nA communication hook provides a flexible way to allreduce gradients. Therefore, it mainly operates on the gradients on each replica before allreduce, which are bucketized to increase the overlap between communication and computation. Particularly, represents a bucket of gradient tensors to be allreduced.     \nThis class mainly passes a flattened gradient tensor (returned by ) to DDP communication hook. This tensor can be further decomposed into a list of per-parameter tensors within this bucket (returned by ) to apply layer-wise operations.          \nIf user registers this DDP communication hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won’t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.     \nThis DDP communication hook implements a simple gradient compression approach that casts tensor to half-precision floating-point format () and then divides it by the process group size. It allreduces those gradient tensors. Once compressed gradient tensors are allreduced, the chained callback casts it back to the input data type (such as ).     \nThis DDP communication hook implements a simple gradient compression approach that casts tensor to half-precision ) and then divides it by the process group size. It allreduces those gradient tensors. Once compressed gradient tensors are allreduced, the chained callback casts it back to the input data type (such as ).     \nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (), and casts the resulting tensor of the given hook back to the input data type, such as . Therefore, is equivalent to .     \nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision ), and casts the resulting tensor of the given hook back to the input data type, such as .     \n  1. > \n\nTo tune , we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, …), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.\n  1. defers PowerSGD compression until step , and vanilla allreduce runs prior to step . This hybrid scheme of can effectively improve the accuracy, even a relatively small is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.\n\n\nTo tune , we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, typically should be no less than the number of warm-up steps.\n  1. is the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where . If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.\n\n\n  1. can be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy.\n  2. controls whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e., arg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set to if the compression / decompression computation is a bottleneck.\n\n\nIf error feedback or warm-up is enabled, the minimum value of allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.     \n  1. >   2. >   3. > 3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;\n\n\nNote that this communication hook enforces vanilla allreduce for the first iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.     \n  * () – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.\n\n    \nIncreasing here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider first, and only consider this variant when a satisfactory accuracy can be achieved when is 1.\n\n\nNote that this communication hook enforces vanilla allreduce for the first iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.     \n  * () – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.\n\n    \nThis hook should be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fsdp.html": "      \nThis ensures that the FSDP instance’s compute device is the destination device. For option 1 and 3, the FSDP initialization always occurs on GPU. For option 2, the FSDP initialization happens on module’s current device, which may be a CPU.\nIf you’re using the flag, you need to ensure that the module is on a GPU or use the argument to specify a CUDA device that FSDP will move the module to in the FSDP constructor. This is necessary because requires GPU communication.\nWith , you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.\nFSDP replaces managed modules’ parameters with views during forward and backward computation for autograd-related reasons. If your module’s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP’s newly created views, and autograd will not work correctly.\n  * FSDP currently does not support gradient accumulation outside when using CPU offloading. This is because FSDP uses the newly-reduced gradient instead of accumulating with any existing gradient, which can lead to incorrect results.\n  * FSDP does not support running the forward pass of a submodule that is contained in an FSDP instance. This is because the submodule’s parameters will be sharded, but the submodule itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately.\n  * FSDP has some constraints when freezing parameters. For , each FSDP instance must manage parameters that are all frozen or all non-frozen. For , FSDP supports mixing frozen and non-frozen parameters, but it’s recommended to avoid doing so to prevent higher than expected gradient memory usage.\n\n    \n  * () – This is the process group over which the model is sharded and thus the one used for FSDP’s all-gather and reduce-scatter collective communications. If , then FSDP uses the default process group. For hybrid sharding strategies such as , users can pass in a tuple of process groups, representing the groups over which to shard and replicate, respectively. If , then FSDP constructs process groups for the user to shard intra-node and replicate inter-node. (Default: )\n  * This specifies a policy to apply FSDP to submodules of , which is needed for communication and computation overlap and thus affects performance. If , then FSDP only applies to , and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments , , and and should return a specifying whether the passed-in should have FSDP applied if or if the traversal should continue into the module’s subtree if . Users may add additional arguments to the callable. The in gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.\n  * () – This configures explicit backward prefetching of all-gathers. If , then FSDP does not backward prefetch, and there is no communication and computation overlap in the backward pass. See for details. (Default: )\n  * () – This configures native mixed precision for FSDP. If this is set to , then no mixed precision is used. Otherwise, parameter, buffer, and gradient reduction dtypes can be set. See for details. (Default: )\n  * () – Modules whose own parameters and child modules’ parameters and buffers are ignored by this instance. None of the modules directly in should be instances, and any child modules that are already-constructed instances will not be ignored if they are nested under this instance. This argument may be used to avoid sharding specific parameters at module granularity when using an or if parameters’ sharding is not managed by FSDP. (Default: )\n  * A that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via and either applies if specified or calls otherwise. For both cases, the implementation should initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX’s ( API, where the deferred modules are initialized by calling if specified or torchdistX’s default otherwise. If is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.\n  * () – An or giving the CUDA device on which FSDP initialization takes place, including the module initialization if needed and the parameter sharding. This should be specified to improve initialization speed if is on CPU. If the default CUDA device was set (e.g. via ), then the user may pass to this. (Default: )\n  * (, then each FSDP module will broadcast module parameters and buffers from rank 0 to ensure that they are replicated across ranks (adding communication overhead to this constructor). This can help load checkpoints via in a memory efficient way. See for an example of this. (Default: )\n  * (, then FSDP prefetches the next forward-pass all-gather before the current forward computation. This is only useful for CPU-bound workloads, in which case issuing the next all-gather earlier may improve overlap. This should only be used for static-graph models since the prefetching follows the first iteration’s execution order. (Default: )\n  * (, then FSDP explicitly synchronizes the CPU thread to ensure GPU memory usage from only consecutive FSDP instances (the current instance running computation and the next instance whose all-gather is prefetched). If , then FSDP allows the CPU thread to issue all-gathers without any extra synchronization. (Default: ) We often refer to this feature as the “rate limiter”. This flag should only be set to for specific CPU-bound workloads with low memory pressure in which case the CPU thread can aggressively issue all kernels without concern for the GPU memory usage.\n  * ( has FSDP use ‘s original parameters. FSDP exposes those original parameters to the user via instead of FSDP’s internal s. This means that the optimizer step runs on the original parameters, enabling per-original-parameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded , respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size-0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. is required to use . Setting this to exposes FSDP’s internal s to the user via . (Default: )\n  * () – Ignored parameters or modules that will not be managed by this FSDP instance, meaning that the parameters are not sharded and their gradients are not reduced across ranks. This argument unifies with the existing argument, and we may deprecate soon. For backward compatibility, we keep both and , but FSDP only allows one of them to be specified as not .\n  * () – DeviceMesh can be used as an altenative to process_group. When device_mesh is passed, FSDP will use the underlying process groups for all-gather and reduce-scatter collective communications. Therefore, these two args need to be mutually exclusive. For hybrid sharding strategies such as , users can pass in a 2D DeviceMesh instead of a tuple of process groups. For 2D FSDP + TP, users are required to pass in device_mesh instead of process_group. For more DeviceMesh info, please visit: \n\n         \nIf at least some FSDP instance uses a sharded strategy (i.e. one other than ), then you should use this method instead of since this method handles the fact that gradients are sharded across ranks.\nThe total norm returned will have the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics. For example, if parameters/gradients use a low precision dtype, then the returned norm’s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm’s dtype will be FP32.                    \n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n\n                   \nWithin this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.          \n\n         \n\n    \nThis is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like .     \n  * () – Callable, which has one of the following signatures: 1) : This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns ; 2) : This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns . Callables with signature 1 are expected to handle gradient communication for a case. Callables with signature 2 are expected to handle gradient communication for sharded cases.\n\n         \nBoth and may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.     \n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n\n    \nAlso takes (optional) configuration for the model’s and optimizer’s state dict. The target module does not have to be a FSDP module. If the target module is a FSDP module, its will also be changed.\nThis API enables users to transparently use the conventional API to take model checkpoints in cases where the root FSDP module is wrapped by another . For example, the following will ensure is called on all non-FSDP instances, while dispatching into implementation for FSDP:     \nBoth and may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.     \n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n\n              \nCan be useful forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the argument.\nThe full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless , in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when , or config, the modification is persisted regardless of .\nNote that in conjunction with is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\nNote that and will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use with .     \n  * () – if , full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting with is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\n  * () – If , full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or config). It is recommended to use with to avoid redundant copies of model parameters being offloaded to the same CPU memory.\n\n    \n  * : This enables the most overlap but increases memory usage the most. This prefetches the next set of parameters the current set of parameters’ gradient computation. This overlaps the and the , and at the peak, it holds the current set of parameters, next set of parameters, and current set of gradients in memory.\n  * : This enables less overlap but requires less memory usage. This prefetches the next set of parameters the current set of parameters’ gradient computation. This overlaps the and the , and it frees the current set of parameters before allocating memory for the next set of parameters, only holding the next set of parameters and current set of gradients in memory at the peak.\n\n\nFor more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders.     \n  * : Parameters, gradients, and optimizer states are sharded. For the parameters, this strategy unshards (via all-gather) before the forward, reshards after the forward, unshards before the backward computation, and reshards after the backward computation. For gradients, it synchronizes and shards them (via reduce-scatter) after the backward computation. The sharded optimizer states are updated locally per rank.\n  * : Gradients and optimizer states are sharded during computation, and additionally, parameters are sharded outside computation. For the parameters, this strategy unshards before the forward, does not reshard them after the forward, and only reshards them after the backward computation. The sharded optimizer states are updated locally per rank. Inside , the parameters are not resharded after the backward computation.\n  * : Parameters, gradients, and optimizer states are not sharded but instead replicated across ranks similar to PyTorch’s API. For gradients, this strategy synchronizes them (via all-reduce) after the backward computation. The unsharded optimizer states are updated locally per rank.\n  * : Apply within a node, and replicate parameters across nodes. This results in reduced communication volume as expensive all-gathers and reduce-scatters are only done within a node, which can be more performant for medium -sized models.\n  * : Apply within a node, and replicate parameters across nodes. This is like , except this may provide even higher throughput since the unsharded parameters are not freed after the forward pass, saving the all-gathers in the pre-backward.\n\n         \n  * () – This specifies the dtype for model parameters during forward and backward and thus the dtype for forward and backward computation. Outside forward and backward, the parameters are kept in full precision (e.g. for the optimizer step), and for model checkpointing, the parameters are always saved in full precision. (Default: )\n  * () – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is but is not , then this takes on the value, still running gradient reduction in low precision. This is permitted to differ from , e.g. to force gradient reduction to run in full precision. (Default: )\n  * () – This specifies the dtype for buffers. FSDP does not shard buffers. Rather, FSDP casts them to in the first forward pass and keeps them in that dtype thereafter. For model checkpointing, the buffers are saved in full precision except for . (Default: )\n  * (, then FSDP upcasts gradients to full precision after the backward pass in preparation for the optimizer step. If , then FSDP keeps the gradients in the dtype used for gradient reduction, which can save memory if using a custom optimizer that supports running in low precision. (Default: )\n  * (, then this FSDP module casts its forward args and kwargs to . This is to ensure that parameter and input dtypes match for forward computation, as required by many ops. This may need to be set to when only applying mixed precision to some but not all FSDP modules, in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default: )\n  * () – (Sequence[Type[nn.Module]]): This specifies module classes to ignore for mixed precision when using an : Modules of these classes will have FSDP applied to them separately with mixed precision disabled (meaning that the final FSDP construction would deviate from the specified policy). If is not specified, then this does not do anything. This API is experimental and subject to change. (Default: )\n\n\nLayer norm and batch norm accumulate in even when their inputs are in a low precision like or . Disabling FSDP’s mixed precision for those norm modules only means that the affine parameters are kept in . However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.\nhas and by default. For the root FSDP instance, its takes precedence over its . For non-root FSDP instances, their values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same configuration and only needs to cast inputs to the at the beginning of the model’s forward pass.\nFor nested FSDP instances with different configurations, we recommend setting individual values to configure casting inputs or not before each instance’s forward. In such a case, since the casts happen before each FSDP instance’s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different configuration.          \nis a config class meant to be used with . We recommend enabling both and when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the context manager as follows:\n```\n     \n   \n   \n   \n      \n\n\n    \n   \n\n      \n    \n\n\n  \n    \n    \n    \n    \n\n\n\n```\n              \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/export.html": "\n  \n  * is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code.\n\n\n  * : When runs into an untraceable part of a model, it will “graph break” and fall back to running the program in the eager Python runtime. In comparison, aims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Since produces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages.\n  * : Since is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible. will instead require users to provide more information or rewrite their code to make it traceable.\n\n\nCompared to , traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, is expected to work on more user programs, and produce lower-level graphs (at the operator level). Note that users can still use as a preprocessing step before .\nCompared to , does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).\nCompared to , is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.\n```\n\n     \n             \n            \n                     \n\n            \n                \n\n            \n               \n                   \n             \n\n \n    \n        \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n        \n        \n            \n                 \n                \n                \n            \n        \n    \n  \n\n```\n\n  * The resulting shape and dtype of tensors produced by each node in the graph is noted. For example, the node will result in a tensor of dtype and shape (1, 16, 256, 256).\n\n\nIn PyTorch 2.3, we introduced a new mode of tracing called . It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag.\nIn , we trace through the program using the Python interpreter. Your code will execute exactly as it would in eager mode; the only difference is that all Tensor objects will be replaced by ProxyTensors, which will record all their operations into a graph.\nIn mode, which is currently the default, we first trace through the program using TorchDynamo, a bytecode analysis engine. TorchDynamo does not actually execute your Python code. Instead, it symbolically analyzes it and builds a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported.\nAn example of a case where one might want to use non-strict mode is if you run into a unsupported TorchDynamo feature that might not be easily solved, and you know the python code is not exactly needed for computation. For example:\nIn this example, the first call using non-strict mode (through the flag) traces successfully whereas the second call using strict mode (default) results with a failure, where TorchDynamo is unable to support context managers. One option is to rewrite the code (see ), but seeing as the context manager does not affect the tensor computations in the model, we can go with the non-strict mode’s result.\nIn this API, we produce the most generic IR that contains all ATen operators (including both functional and non-functional) which can be used to train in eager PyTorch Autograd. This API is intended for eager training use cases such as PT2 Quantization and will soon be the default IR of torch.export.export. To read further about the motivation behind this change, please refer to \nFrom the above output, you can see that produces pretty much the same ExportedProgram as except for the operators in the graph. You can see that we captured batch_norm in the most general form. This op is non-functional and will be lowered to different ops when running inference.\nBy default will trace the program assuming all input shapes are , and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be specified by using the API to create them and by passing them into through the argument. An example:\n  * Through the API and the argument, we specified the first dimension of each input to be dynamic. Looking at the inputs and , they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs. is a symbol representing that this dimension can be a range of values.\n  * describes the ranges of each symbol appearing in the graph. In this case, we see that has the range [0, int_oo]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. See \n\n\nWe can also specify more expressive relationships between input shapes, such as where a pair of shapes might differ by one, a shape might be double of another, or a shape is even. An example:\n  * By specifying for the first input, we see that the resulting shape of the first input is now dynamic, being . And now by specifying for the second input, we see that the resulting shape of the second input is also dynamic. However, because we expressed , instead of ’s shape containing a new symbol, we see that it is now being represented with the same symbol used in , . We can see that relationship of is being shown through .\n\n\nA value is one that can change from run to run. These behave like normal arguments to a Python function—you can pass different values for an argument and expect your function to do the right thing. Tensor is treated as dynamic.\nA value is a value that is fixed at export time and cannot change between executions of the exported program. When the value is encountered during tracing, the exporter will treat it as a constant and hard-code it into the graph.\nWhen an operation is performed (e.g. ) and all inputs are static, then the output of the operation will be directly hard-coded into the graph, and the operation won’t show up (i.e. it will get constant-folded).\nBy default, will trace the program specializing on the input tensors’ shapes, unless a dimension is specified as dynamic via the argument to . This means that if there exists shape-dependent control flow, will specialize on the branch that is being taken with the given sample inputs. For example:\nThe conditional of () does not appear in the because the example inputs have the static shape of (10, 2). Since specializes on the inputs’ static shapes, the else branch () will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, will need to be used to specify the dimension of the input tensor () to be dynamic, and the source code will need to be .\nBecause integers are specialized, the operations are all computed with the hard-coded constant , rather than . If a user passes a different value for at runtime, like 2, than the one used during export time, 1, this will result in an error. Additionally, the iterator used in the loop is also “inlined” in the graph through the 3 repeated calls, and the input is never used.\nAs is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of , an unsupported operation will cause a “graph break” and the unsupported operation will be run with default Python evaluation. In contrast, will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.\nGraph breaks can also be encountered on data-dependent control flow () when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators. Currently, we support to express if-else like control flow (more coming soon!).     \ntakes any nn.Module along with example inputs, and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different inputs or serialized. The traced graph (1) produces normalized operators in the functional ATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control flow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to show that this normalization and control-flow elimination is sound for future inputs.\n\n\nIf any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested fixes to the specification that are needed to validate the assumptions. For example might suggest the following fix to the definition of a dynamic dimension , say appearing in the shape associated with input , that was previously defined as :\nThis example means the generated code requires dimension 0 of input to be less than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension definitions and then copy them verbatim into your code without needing to change the argument to your call.     \n  * An optional argument where the type should either be: 1) a dict from argument names of to their dynamic shape specifications, 2) a tuple that specifies dynamic shape specifications for each input in original order. If you are specifying dynamism on keyword args, you will need to pass them in the order that is defined in the original function signature.\nThe dynamic shape of a tensor argument can be specified as either (1) a dict from dynamic dimension indices to types, where it is not required to include static dimension indices in this dict, but when they are, they should be mapped to None; or (2) a tuple / list of types or None, where the types correspond to dynamic dimensions, and static dimensions are denoted by None. Arguments that are dicts or tuples / lists of tensors are recursively specified by using mappings or sequences of contained specifications.\n\n         \n\n         \n\n         \nconstructs a type analogous to a named symbolic integer with a range. It can be used to describe multiple possible values of a dynamic tensor dimension. Note that different dynamic dimensions of the same tensor, or of different tensors, can be described by the same type.          \nWhen exporting with , export may fail with a ConstraintViolation error if the specification doesn’t match the constraints inferred from tracing the model. The error message may provide suggested fixes - changes that can be made to to export successfully.          \nRun a set of decompositions on the exported program and returns a new exported program. By default we will run the Core ATen decompositions to get operators in the .     \nExport Graph is functional and does not access “states” like parameters or buffers within the graph via nodes. Instead, gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.\n```\n \n       \n         \n\n        \n          \n\n        \n         \n         \n\n       \n        \n                  \n\n        \n         \n\n         \n\n```\n    \nThis is a custom dictionary that is specifically used for handling decomp_table in export. The reason we need this is because in the new world, you can only an op from decomp table to preserve it. This is problematic for custom ops because we don’t know when the custom op will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations until we really need to materialize it (which is when we run decomposition pass.)     \n\n    \nExport Graph is functional and does not access “states” like parameters or buffers within the graph via nodes. Instead, gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.\n```\n \n       \n         \n\n        \n          \n\n        \n         \n         \n\n       \n        \n                  \n\n        \n         \n\n         \n\n```\n    \nA module that carries a sequence of InterpreterModules corresponding to a sequence of calls of that module. Each call to the module dispatches to the next InterpreterModule, and wraps back around after the last.     \nUnflatten an ExportedProgram, producing a module with the same module hierarchy as the original eager module. This can be useful if you are trying to use with another system that expects a module hierachy instead of the flat graph that usually produces.\nThe args/kwargs of unflattened modules will not necessarily match the eager module, so doing a module swap (e.g. ) will not necessarily work. If you need to swap a module out, you need to set the parameter of .          \n  * () – The device to move the exported program to. If a string, it is interpreted as a device name. If a dict, it is interpreted as a mapping from the existing device to the intended one\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fft.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/deterministic.html": "      \nFilling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance and still be deterministic.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/deploy.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/future_mod.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/dlpack.html": "      \nThe returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.     \n```\n \n  \n\n\n  \n    \n\n\n\n\n\n\n  \n\n\n  \n\n\n    \n\n\n\n\n\n\n\n```\n    \nis a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to . The more idiomatic use of DLPack is to call directly on the tensor object - this works when that object has a method, which PyTorch and most other libraries indeed have now.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/func.html": "This library is currently in . What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don’t have full coverage over PyTorch operations.  \nIf you have suggestions on the API or use-cases you’d like to be covered, please open an GitHub issue or reach out. We’d love to hear about how you’re using the library.\n\n\n\n\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributions.html": "It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples , the pathwise derivative requires the derivative . The next sections discuss these two in a reinforcement learning example. For more details see   \nIn practice we would sample an action from the output of a network, apply this action in an environment, and then use to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:\nThe other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:          \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (where for univariate distributions).     \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to . This method calls on the distribution’s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in , when an instance is first created.     \nThe default behavior mimics Python’s statement: validation is on by default, but is disabled if Python is run in optimized mode (via ). Validation may be expensive, so you may want to disable it once a model is working.     \nThis class is an intermediary between the class and distributions which belong to an exponential family mainly to check the correctness of the and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).                    \nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.          \nThe distribution is supported in [0, 1] and parameterized by ‘probs’ (in (0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’ does not correspond to a probability and ‘logits’ does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.               \nThis is mainly useful for changing the shape of the result of . For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:               \nLKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by parameter to make the probability of the correlation matrix generated from a Cholesky factor proportional to . Because of that, when , we have a uniform distribution over Cholesky factors of correlation matrices:\nNote that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the distribution. For sampling, this uses the Onion method from [1] Section 3.               \n\n    \nThe distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a “selecting distribution” (over component) and a component distribution, i.e., a with a rightmost batch shape (equal to ) which indexes each (batch of) component.\n```\n\n\n  \n   \n   \n\n\n\n  \n  \n           \n   \n\n\n\n  \n  \n          \n   \n\n```\n    \nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.     \nThe multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix or a positive definite precision matrix or a lower-triangular matrix with positive-valued diagonal entries, such that . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.          \n\n         \nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.                                             \nSampling is always done in double precision internally to avoid a hang in _rejection_sample() for small values of the concentration, which starts to happen for single precision around 1e-4 (see issue #88443).     \nOnly one of or or can be specified. Using will be more efficient: all computations internally are based on . If or is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. ‘torch.distributions.LKJCholesky’ is a restricted Wishart distribution.[1]\n[1] Wang, Z., Wu, Y. and Chu, H., 2018. . [2] Sawyer, S., 2007. . [3] Anderson, T. W., 2003. . [4] Odell, P. L. & Feiveson, A. H., 1966. . JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. .     \nIn some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return values in . In those cases, the user should validate the samples and either fix the value of or adjust value for argument in accordingly.               \nTransforms an uncontrained real vector with length into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:\n>   1. For each row of the lower triangular part, we apply a version of class to transform into a unit Euclidean length vector using the following steps: - Scales into the interval domain: . - Transforms into an unsigned domain: . - Applies . - Transforms back into signed domain: .\n> \n         \nWrapper around another transform to treat -many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out -many of the rightmost dimensions in .                    \nThis transform arises as an iterated sigmoid transform in a stick-breaking construction of the distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.     \nCaching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:     \n\n    \nThe registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:\nAn example where and differ is : returns a that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, returns a that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.          \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/futures.html": "           \nAppend the given callback function to this , which will be run when the is completed. Multiple callbacks can be added to the same , but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this . The callback function can use the method to get the value. Note that if this is already completed, the given callback will be run inline.\nWe recommend that you use the method as it provides a way to synchronize after your callback has completed. can be cheaper if your callback does not return anything. But both and use the same callback registration API under the hood.\nNote that if the callback function throws, either through the original future being completed with an exception and calling , or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.     \nIf the value contains tensors that reside on GPUs, will return even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see ).     \nSet an exception for this , which will mark this as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this , the exception set here will be raised inline.     \nIf the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it’s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn’t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this .     \nAppend the given callback function to this , which will be run when the is completed. Multiple callbacks can be added to the same , but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: ). The callback must take one argument, which is the reference to this . The callback function can use the method to get the value. Note that if this is already completed, the given callback will be run immediately inline.\nIf the ’s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven’t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn’t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of .\nSimilarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn’t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.\nNote that if the callback function throws, either through the original future being completed with an exception and calling , or through other code in the callback, the future returned by will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.     \nIf the value contains tensors that reside on GPUs, then this method will perform any additional synchronization. This should be done beforehand, separately, through a call to (except within callbacks, for which it’s already being taken care of by ).     \nIf the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn’t change streams.          \nWaits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fx.html": "```\n \n\n\n\n \n       \n        \n           \n           \n\n      \n            \n\n\n  \n\n   \n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n  \nThe performs “symbolic execution” of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the and documentation.\nThe is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or instances), and return values. More information about the IR can be found in the documentation for . The IR is the format on which transformations are applied.\nis what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph’s semantics. This functionality is wrapped up in , which is a instance that holds a as well as a method generated from the Graph.\nTaken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!\n```\n \n \n\n  \n                    \n    \n\n    \n    \n    \n    \n        \n\n    \n      \n\n    \n      \n\n```\n\nYour transform will take in a , acquire a from it, do some modifications, and return a new . You should think of the that your FX transform returns as identical to a regular – you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a will allow for composability.\nFull treatment of the semantics of graphs can be found in the documentation, but we are going to cover the basics here. A is a data structure that represents a method on a . The information that this requires is:\n  * What are the inputs to the method? In FX, method inputs are specified via special nodes. In this case, we have a single node with a of , meaning we have a single (non-self) argument named x.\n\n\nOne approach to building this new is to directly manipulate your old one. To aid in this, we can simply take the we obtain from symbolic tracing and modify it. For example, let’s say we desire to replace calls with calls.\n```\n \n \n\n\n \n       \n          \n\n  \n                    \n        \n    \n    \n       \n        \n        \n           \n            \n            \n               \n                  \n\n     \n                 \n\n      \n\n```\n\nWe can also do more involved rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the documentation. An example of using these APIs to append a call can be found below.\n```\n\n\n \n    \n      \n         \n\n    \n    \n    \n    \n\n```\n\nFX also provides another level of automation on top of direct graph manipulation. The API is essentially a “find/replace” tool for editing s. It allows you to specify a and function and it will trace through those functions, find instances of the group of operations in the graph, and replace those instances with copies of the graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.\nAnother way of manipulating s is by reusing the machinery used in symbolic tracing. For example, let’s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every call into . One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the , and then clean up the original . However, we can automate this process by using objects to automatically record operations into the .\nTo use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with objects as arguments. These objects will capture the operations that are performed on them and append them to the .\n```\n\n \n         \n\n  \n  \n\n  \n                    \n\n\n\n\n\n        \n      \n      \n      \n       \n               \n            \n            \n            \n            \n              \n                          \n              \n\n            \n            \n            \n            \n              \n              \n        \n            \n            \n                 \n              \n      \n\n```\n\nIn addition to avoiding explicit graph manipulation, using s also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling we also passed a tracer pointing to the underlying variable . This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using especially when the underlying operators can not be safely assumed to be unary.\nA useful code organizational pattern in FX is to loop over all the s in a and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with s. For example, suppose we want to run a and record the shape and dtype properties on the nodes as we see them at runtime. That might look like:\n```\n \n \n   \n\n   \n\n \n\n\n\n\n\n\n\n\n\n      \n          \n          \n          \n\n      \n          \n             \n\n         \n                \n\n           \n              \n              \n                \n                   \n                     \n                   \n             \n\n           \n               \n                  \n               \n                  \n               \n                   \n               \n                   \n                  \n                    \n               \n                   \n\n            \n            \n            \n              \n                  \n                  \n\n              \n\n         \n\n```\n\nAs you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the class, which encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides.\nIn addition to executing operations, we can also generate a new by feeding values through an interpreter. Similarly, we provide the class to encompass this pattern. behaves similarly to , but instead of calling the method to get a concrete output value from the Module, you would call the method to return a new which was subject to any transformation rules you installed as overridden methods.\nOften in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.\n  * Nondeterministic iteration order. In Python, the datatype is unordered. Using to contain collections of objects like s, for example, can cause unexpected nondeterminism. An example is iterating over a set of s to insert them into a . Because the data type is unordered, the ordering of the operations in the output program will be nondeterministic and can change across program invocations. The recommended alternative is to use a data type, which is can be used equivalently to a set by storing values to be deduplicated in the keys of the .\n\n\nBecause the output of most deep learning modules consists of floating point instances, checking for equivalence between the results of two is not as straightforward as doing a simple equality check. To motivate this, let’s use an example:\nHere, we’ve tried to check equality of the values of two deep learning models with the equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:\n```\n \n \n   \n\n         \n      \n    \n    \n\n    \n      \n\n  \n  \n\n     \n\n\n\n\n  \n\n\n\n```\n\nIf you’d like to run the same code multiple times, then it can be a bit tedious to step to the right code with . In that case, one approach is to simply copy-paste the generated pass into your code and examine it from there.\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n     \n        \n\n    \n    \n      \n          \n                  \n         \n\n\n\n\n  \n  \n\n```\n\nis a method in that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in , it may be easier to examine modules and parameters using .\nNow that we’ve identified that a transformation is creating incorrect code, it’s time to debug the transformation itself. First, we’ll check the section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our transformation. There may be a quick answer in , but, if not, there are several ways to examine our traced module:\n```\n\n \n       \n           \n\n\n  \n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nopcode         name    target                   args    kwargs\n\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\n\noutput         output  output                   (add,)  {}\n\n\n```\n\nUsing the utility functions above, we can compare our traced Module before and after we’ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it’s still not clear what’s going wrong, a debugger like can be a good next step.\n```\n\n         \n    \n      \n\n\n\n\n\n      \n\n\n  \n\n\n\n\n\n```\n\nUsing the above example, let’s say that the call to showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a session. We can see what’s happening during the transform by breaking on , then pressing to “step into” the call to .\nThe most common Python debugger is by typing into the command line, where is the name of the file you want to debug. After that, you can use the ) when you start , then call to run the program until that point. This prevents you from having to step through each line of execution (using or ) to get to the part of the code you want to examine. Alternatively, you can write before the line you want to break at. If you add , your program will automatically start in debug mode when you run it. (In other words, you can just type into the command line instead of .) Once you’re running your file in debug mode, you can step through the code and examine your program’s internal state using certain commands. There are many excellent tutorials on online, including RealPython’s \nIDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use by pulling up a terminal window in your IDE (e.g. View → Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around ).\nFX uses a system of (a.k.a in that it executes the program (really a or function) to record operations. It is in that the data flowing through the program during this execution is not real data, but rather symbols ( in FX parlance).\n```\n \n       \n         \n    \n         \n\n  \n\n\n\n\n\n\n\n\n\n\n\n```\n\nThe condition to the statement relies on the value of , which relies on the value of , a function input. Since can change (i.e. if you pass a new input tensor to the traced function), this is . The traceback walks back up through your code to show you where this situation happens.\nOn the other hand, so-called is supported. Static control flow is loops or statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example:\n```\n \n \n\n \n          \n        \n          \n           \n\n      \n          \n        \n        \n         \n              \n         \n\n  \n  \n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n```\n\nThe if-statement does not depend on any function inputs, thus it is static. can be considered to be a hyper-parameter, and the traces of different instances of with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.\nMany instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to attributes or by binding concrete values to arguments during symbolic tracing:\nIn the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see ) or function (see ) rather than tracing through them.\n```\n \n \n   \n\n \n\n\n\n       \n\n\n \n\n  \n\n\n\n\n\n\n\n\n\n```\n\n```\n \n    \n    \n    \n    \n\n\n\n \n      \n            \n\n  \n\n  \n\n\n   \n\n```\n\nLeaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard module instances. For example:\n```\n \n      \n         \n\n \n     \n        \n           \n          \n\n      \n         \n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n```\n\n  *     * The deterministic constructors (, ) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, or may be a viable substitute.\n  *     * When using functionals like , it will be common for the training argument to be passed in as . During FX tracing, this will likely be baked in as a constant value.\n> ```\n \n \n\n \n    \n      \n\n\n  \n\n\n\n\n\n\n\n\n\n   \n \n\n\n\n\n\n\n\n\n```\n\n\n         \nThis function can be called at module-level scope to register fn_or_name as a “leaf function”. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through:\nA wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through.               \n  * () – can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that is a Module, any references to Module-based objects (via qualified name) in the Graph’s Nodes’ field will be copied over from the respective place within ’s Module hierarchy into the GraphModule’s module hierarchy. In the case that is a dict, the qualified name found in a Node’s will be looked up directly in the dict’s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule’s module hierarchy.\n  * ( denotes the name of this GraphModule for debugging purposes. If it’s unset, all error messages will report as originating from . It may be helpful to set this to ’s original name or a name that makes sense within the context of your transform.\n\n              \nA Module is considered “used” if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a node 3. It has a non-Module attribute that is used from a node               \nis the main data structure used in the FX Intermediate Representation. It consists of a series of s, each representing callsites (or other syntactic constructs). The list of s, taken together, constitute a valid Python function.          \n\n         \n\n         \n\n         \n\n    \nDead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations or you supply your own custom function for detecting side-effectful nodes.               \n\n         \n  * () – a dictionary that will be populated with a mapping from nodes in to nodes in . Note that can be passed in with values in it already to override copying of certain values.\n\n                        \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule          \n\n         \n>          \n> This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.\n> ```\n   \n\n\n\n\n\n\n \n      \n\n\n\n\n  \n\n\n\n\n\n      \n              \n    \n\n\n\n  \n\n```\n\n> ```\n\n\n   \n    \n    \n      \n\n\n\n\n\n```\n              \n  * () – an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).\n  * () – The default value this function argument should take on. NOTE: to allow for as a default value, should be passed as this argument to specify that the parameter does _not_ have a default value.\n\n         \nis the data structure that represents individual operations within a . For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each has a function specified by its property. The semantics for each value of are as follows:\n  * represents a function input. The attribute specifies the name this value will take on. is similarly the name of the argument. holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. is don’t-care. Placeholders correspond to the function parameters (e.g. ) in the graph printout.\n  * applies a module in the module hierarchy’s method to given arguments. is as previous. is the fully-qualified name of the module in the module hierarchy to call. and represent the arguments to invoke the module on, .\n\n                   \n\n              \nReturns normalized arguments to Python targets. This means that will be matched up to the module/functional’s signature and return exclusively kwargs in positional order if is true. Also populates default values. Does not support positional-only parameters or varargs parameters.                    \nReturn the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by . To record stack traces during tracing for debug purposes, set on the instance. When traced with dynamo, this property will be populated by default by .     \n>     \nBy default, the behavior is to check if the called module is a leaf module via . If it is, emit a node referring to in the . Otherwise, call the normally, tracing through the operations in its function.     \n\n    \n  1. \n                   \nBy default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the , so that future calls will reuse the proxy rather than creating a new one.     \nLeaf modules are the atomic units that appear in the IR, referenced by calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.          \nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.                    \nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.     \nNote that after this call, may be different from the passed in here. For example, when a free function is passed to , we will create an instance to use as the root and add embedded constants to.     \n\n    \nThere are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:          \nRun via interpretation and return the result. This uses the “boxed” calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.                                             \n  * () – An optional starting environment for execution. This is a dict mapping to any value. This can be used, for example, to pre-populate results for certain so as to do only partial evaluation within the interpreter.\n\n                        \nThe above code will first match in the method of . Pattern-matching is done based on use-def relationships, not node names. For example, if you had in , you could match in the original function, despite the variable names being different ( vs ).\nThe statement in is matched based on its value only; it may or may not match to the statement in the larger graph. In other words, the pattern doesn’t have to extend to the end of the larger graph.\nWhen the pattern is matched, it will be removed from the larger function and replaced by . If there are multiple matches for in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (“First” here being defined as the first in a topological ordering of the Nodes’ use-def relationships. In most cases, the first Node is the parameter that appears directly after , while the last Node is whatever the function returns.)\nOne important thing to note is that the parameters of the Callable must be used in the Callable itself, and the parameters of the Callable must match the pattern. The first rule is why, in the above code block, the function has parameters , but the function only has parameters . doesn’t use , so it shouldn’t specify as a parameter. As an example of the second rule, consider replacing\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/library.html": "torch.library is a collection of APIs for extending PyTorch’s core library of operators. It contains utilities for testing custom operators, creating new custom operators, and extending operators defined with PyTorch’s C++ operator registration APIs (e.g. aten operators).       \nThat is, when you use the torch.library/TORCH_LIBRARY APIs to create a custom op, you specified metadata (e.g. mutability info) about the custom op and these APIs require that the functions you pass them satisfy certain properties (e.g. no data pointer access in the fake/meta/abstract kernel) tests these metadata and properties.\n  * test_schema: If the schema matches the implementation of the operator. For example: if the schema specifies a Tensor is mutated, then we check the implementation mutates the Tensor. If the schema specifies that we return a new Tensor, then we check that the implementation returns a new Tensor (instead of an existing one or a view of an existing one).\n  * test_autograd_registration: If the operator supports training (autograd): we check that its autograd formula is registered via torch.library.register_autograd or a manual registration to one or more DispatchKey::Autograd keys. Any other DispatchKey-based registrations may lead to undefined behavior.\n  * test_faketensor: If the operator has a FakeTensor kernel (and if it is correct). The FakeTensor kernel is necessary ( but not sufficient) for the operator to work with PyTorch compilation APIs (torch.compile/export/FX). We check that a FakeTensor kernel (also sometimes known as a meta kernel) was registered for the operator and that it is correct. This test takes the result of running the operator on real tensors and the result of running the operator on FakeTensors and checks that they have the same Tensor metadata (sizes/strides/dtype/device/etc).\n  * test_aot_dispatch_dynamic: If the operator has correct behavior with PyTorch compilation APIs (torch.compile/export/FX). This checks that the outputs (and gradients, if applicable) are the same under eager-mode PyTorch and torch.compile. This test is a superset of and is an e2e test; other things it tests are that the operator supports functionalization and that the backward pass (if it exists) also supports FakeTensor and functionalization.\n\n\nFor best results, please call multiple times with a representative set of inputs. If your operator supports autograd, please use with inputs with ; if your operator supports multiple devices (e.g. CPU and CUDA), please use with inputs on all supported devices.     \n\n    \nReasons why you may want to create a custom op include: - Wrapping a third-party library or custom kernel to work with PyTorch subsystems like Autograd. - Preventing torch.compile/export/FX tracing from peeking inside your function.     \n  * () – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.\n  * () – The device type(s) the function is valid for. If no device type is provided, then the function is used as the default implementation for all device types. Examples: “cpu”, “cuda”. When registering a device-specific implementation for an operator that accepts no Tensors, we require the operator to have a “device: torch.device argument”.\n\n\nWe recommend not passing in a arg and instead letting us infer it from the type annotations. It is error-prone to write your own schema. You may wish to provide your own schema if our interpretation of the type annotation is not what you want. For more info on how to write a schema string, see      \nThis is a more structured way of using triton kernels with PyTorch. Prefer using triton kernels with no custom operator wrappers (like , ) because that is simpler; only use / if you want to create an operator that behaves like PyTorch built-in operators. For example, you may use a wrapper API to define the behavior of the triton kernel when passed a tensor subclass or under a TorchDispatchMode.     \n  * () – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.\n\n    \n```\n \n \n     \n   \n   \n\n\n \n    \n    \n    \n    \n     \n\n      \n        \n         \n        \n         \n         \n        \n        \n\n  \n      \n      \n\n     \n          \n\n        \n     \n\n   \n   \n   \n\n\n\n\n\n\n\n\n\n\n\n```\n         \n\n         \n\n    \nIn order for an operator to work with autograd, you need to register a backward formula: 1. You must tell us how to compute gradients during the backward pass by providing us a “backward” function. 2. If you need any values from the forward to compute gradients, you can use to save values for backward.\nruns during the backward pass. It accepts : - is one or more gradients. The number of gradients matches the number of outputs of the operator. The object is used by . The semantics of are the same as .\nruns during the forward pass. Please save quantities needed for backward onto the object via either or assigning them as attributes of . If your custom op has kwarg-only arguments, we expect the signature of to be .\nBoth and must be traceable. That is, they may not directly access and they must not depend on or mutate global state. If you need a non-traceable backward, you can make it a separate custom_op that you call inside .     \nAn “FakeTensor implementation” specifies the behavior of this operator on Tensors that carry no data (“FakeTensor”). Given some input Tensors with certain properties (sizes/strides/storage_offset/device), it specifies what the properties of the output Tensors are.\nThe FakeTensor implementation has the same signature as the operator. It is run for both FakeTensors and meta tensors. To write a FakeTensor implementation, assume that all Tensor inputs to the operator are regular CPU/CUDA/Meta tensors, but they do not have storage, and you are trying to return regular CPU/CUDA/Meta tensor(s) as output. The FakeTensor implementation must consist of only PyTorch operations (and may not directly access the storage or data of any input or intermediate Tensors).\n```\n \n   \n   \n\n\n \n        \n     \n\n\n   \n       \n       \n       \n       \n       \n       \n\n         \n\n \n       \n       \n      \n        \n\n    \n\n\n \n    \n      \n       \n      \n\n\n \n\n\n\n\n      \n      \n       \n       \n     \n\n   \n\n       \n   \n\n\n  \n\n```\n    \nFor each arg in , has a corresponding . It is if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.\nThe return of the function is a tuple of . Similar to , should be of the same structure as and contain one per output that specifies if the output has the vmapped dimension and what index it is in.          \n\n    \n  * () – The name of the operator in the schema. If is None, then the name is not included in the inferred schema. Note that the input schema to requires a operator name.\n\n         \nThe low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible. This blog post <     \nA class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.\nTo create a library to override operators in an existing library (with name ns), set the kind to “IMPL”. To create a new library (with name ns) to register new operators, set the kind to “DEF”. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to “FRAGMENT”.          \n  * () – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.\n\n         \n\n         \n\n    \nIn PyTorch, defining an op (short for “operator”) is a two step-process: - we need to define the op (by providing an operator name and schema) - we need to implement behavior for how the operator interacts with various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.     \n  * () – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.\n\n    \nYou may pass “default” for to register this implementation as the default implementation for ALL device types. Please only use this if the implementation truly supports all device types; for example, this is true if it is a composition of built-in PyTorch operators.\n```\n \n   \n\n\n \n\n\n \n \n     \n\n  \n  \n  \n\n\n \n      \n            \n     \n\n\n \n\n\n \n\n \n     \n\n\n  \n\n  \n    \n  \n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/masked.html": "“Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.  \nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored.\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they’d like during computation.\n\n\nUnary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we’ll continue to mask out the data.\nAs you may have seen in the tutorial, also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics.\nThe following reductions are available (with autograd support). For more information, the tutorial details some examples of reductions, while the tutorial has some further in-depth discussions about how we decided on certain reduction semantics.\nWe’ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a . For a quick example, consider :\n```\n    \n\n\n\n\n             \n   \n \n\n \n\n \n\n\n\n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fx.experimental.html": "For clients: the size at this dimension must be within 'vr' (which specifies a lower and upper bound, inclusive-inclusive) AND it must be non-negative and should not be 0 or 1 (but see NB below).  \n---  \nGiven a function f, return a new function which when executed with valid arguments to f, returns an FX GraphModule representing the set of operations that were executed during the course of execution.  \n---  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/meta.html": "  * Models can be loaded on the meta device, allowing you to load a representation of the model without actually loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data.  \n  * Most operations can be performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations like or . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we typically prefer representing the CUDA behavior faithfully in this situation.\n\n\nAlthough in principle meta tensor computation should always be faster than an equivalent CPU/CUDA computation, many meta tensor implementations are implemented in Python and have not been ported to C++ for speed, so you may find that you get lower absolute framework latency with small CPU tensors.\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores no data and we do not know what the correct data values for your new tensor are:\ncontains undocumented utilities for taking an arbitrary Tensor and constructing an equivalent meta Tensor with high fidelity. These APIs are experimental and may be changed in a BC breaking way at any time.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/hub.html": "Here is a code snippet specifies an entrypoint for model if we expand the implementation in . In most case importing the right function in is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in   \n  * Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here.\n  * Pretrained weights can either be stored locally in the GitHub repo, or loadable by . If less than 2GB, it’s recommended to attach it to a handles , alternatively you can put the following logic in the entrypoint definition.\n\n         \n  * () – if , torchhub will check that the branch or commit specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n  * \n         \n  * () – if , torchhub will check that the ref specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n  * \n         \n  * ( is ‘github’, this should correspond to a github repo with format with an optional ref (tag or branch), for example ‘pytorch/vision:0.10’. If is not specified, the default branch is assumed to be if it exists, and otherwise . If is ‘local’ then it should be a path to a local directory.\n  *   * () – if , torchhub will check that the branch or commit specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n\n              \n  * () – If True, the filename part of the URL should follow the naming convention where is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False\n\n\nTo help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.     \nUsers can force a reload by calling . This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.\nTorch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches and which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the dict; more details can be found in \nA known limitation that is worth mentioning here: users load two different branches of the same repo in the . It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/linalg.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/logging.html": "PyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component’s log messages can be completely disabled, while another component’s log messages can be set to maximum verbosity.  \nThe environment variable is a comma-separated list of pairs, where is a component specified below. The prefix will decrease the log level of the component, displaying more log messages while the prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in . In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with or will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be . This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the environment variable (see torch._logging.set_logs for the python API):               \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/genindex.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/monitor.html": "The stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.       \n>     \nStat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every duration. When the window closes the stats are logged via the event handlers as a event.\nIf is set, the stat will cap the number of samples per window by discarding calls once adds have occurred. If it’s not set, all calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.     \nEvent represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.     \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.functional.html": "Takes LongTensor with index values of shape and returns a tensor of shape that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.  \n---  \nReverses the operation by rearranging elements in a tensor of shape to a tensor of shape , where r is the .  \n---  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mtia.memory.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.attention.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mtia.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.html": "Creates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input (a 2D mini-batch ) and output (which is a 1D tensor of target class indices, ):  \n---  \nCreates a criterion that measures the triplet loss given input tensors , , and (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").  \nUtility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the for more information on how to implement your own parametrizations.\nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the documentation.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.init.html": "      \nIn order to implement instead of . This gives the initial weights a variance of , which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for sacrifices the normalization effect for more stable gradient flow in rectangular layers.          \nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .          \n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .          \n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nThe values are effectively drawn from the normal distribution with values outside redrawn until they are within the bounds. The method used for generating the random values works best when .     \nDescribed in - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.          \n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/amp_examples.html": "\n  \n```\n\n  \n   \n\n\n  \n\n   \n        \n        \n\n        \n          \n              \n               \n\n        \n        \n        \n        \n\n        \n        \n        \n        \n\n        \n        \n\n```\n\nAll gradients produced by are scaled. If you wish to modify or inspect the parameters’ attributes between and , you should unscale them first. For example, gradient clipping manipulates a set of gradients such that their global norm (see ) or maximum magnitude (see ) is some user-imposed threshold. If you attempted to clip unscaling, the gradients’ norm/maximum magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for gradients) would be invalid.\n```\n  \n\n   \n        \n        \n          \n              \n               \n        \n\n        \n        \n\n        \n         \n\n        \n        \n        \n\n        \n        \n\n```\n\nGradient accumulation adds gradients over an effective batch of size ( if distributed). The scale should be calibrated for the effective batch, which means inf/NaN checking, step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity. Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective batch are accumulated. If grads are unscaled (or the scale factor changes) before accumulation is complete, the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor) after which it’s impossible to recover the accumulated unscaled grads must apply.\nTherefore, if you want to grads (e.g., to allow clipping unscaled grads), call just before , after all (scaled) grads for the upcoming have been accumulated. Also, only call at the end of iterations where you called for a full effective batch:\n```\n  \n\n   \n        \n        \n          \n              \n               \n\n        \n          \n                                                 \n                                                 \n\n        \n        \n          \n                \n\n        \n          \n              \n               \n                  \n              \n                \n\n        \n        \n        \n\n        \n\n        \n        \n        \n\n```\n\nIf your network has multiple losses, you must call on each of them individually. If your network has multiple optimizers, you may call on any of them individually, and you must call on each of them individually.\n```\n  \n\n   \n        \n        \n        \n          \n              \n              \n                     \n                     \n\n        \n        \n        \n        \n\n        \n        \n        \n\n        \n        \n\n        \n\n```\n\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision whether or not to skip the step. This may result in one optimizer skipping the step while the other one does not. Since step skipping occurs rarely (every several hundred iterations) this should not impede convergence. If you observe poor convergence after adding gradient scaling to a multiple-optimizer model, please report a bug.\nHere may spawn a side thread to run the forward pass on each device, like . : apply autocast as part of your model’s method to ensure it’s enabled in side threads.\nIn all cases, if you’re importing the function and can’t alter its definition, a safe fallback is to disable autocast and force execution in ( or ) at any points of use where errors occur:\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mps.html": "This package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See   \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nested.html": "Nested tensors allow for ragged-shaped data to be contained within and operated upon as a single tensor. Such data is stored underneath in an efficient packed representation, while exposing a standard PyTorch tensor interface for applying operations.  \nA common application of nested tensors is for expressing batches of variable-length sequential data present in various domains, such as varying sentence lengths, image sizes, and audio / video clip lengths. Traditionally, such data has been handled by padding sequences to that of the max length within a batch, performing computation on the padded form, and subsequently masking to remove padding. This is inefficient and error-prone, and nested tensors exist to address these problems.\nThe API for calling operations on a nested tensor is no different from that of a regular , allowing for seamless integration with existing models, with the main difference being .\nThere are two forms of nested tensors present within PyTorch, distinguished by layout as specified during construction. Layout can be one of or . We recommend utilizing the layout whenever possible. While it currently only supports a single ragged dimension, it has better op coverage, receives active development, and integrates well with . These docs adhere to this recommendation and refer to nested tensors with the layout as “NJTs” for brevity throughout.\nConstruction is straightforward and involves passing a list of tensors to the constructor. A nested tensor with the layout (AKA an “NJT”) supports a single ragged dimension. This constructor will copy the input tensors into a packed, contiguous block of memory according to the layout described in the section below.\nEach tensor in the list must have the same number of dimensions, but the shapes can otherwise vary along a single dimension. If the dimensionalities of the input components don’t match, the constructor throws an error.\ncan be used to preserve autograd history from the tensors passed to the constructor. When this constructor is utilized, gradients will flow through the nested tensor back into the original components. Note that this constructor still copies the input components into a packed, contiguous block of memory.\n```\n    \n    \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nNote that the nested tensor acts as a view over the original padded dense tensor, referencing the same memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more limited, so if you run into support gaps, it’s always possible to convert to a contiguous NJT using .\nFor efficiency, nested tensors generally pack their tensor components into a contiguous chunk of memory and maintain additional metadata to specify batch item boundaries. For the layout, the contiguous chunk of memory is stored in the component, with the component delineating batch item boundaries for the ragged dimension.\nAn NJT has a well-defined shape with dimensionality 1 greater than that of its components. The underlying structure of the ragged dimension is represented by a symbolic value ( in the example below).\nNJTs must have the same ragged structure to be compatible with each other. For example, to run a binary operation involving two NJTs, the ragged structures must match (i.e. they must have the same ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact tensor, so both NJTs must have the same tensor to be compatible with each other.\nIn the above example, even though the conceptual shapes of the two NJTs are the same, they don’t share a reference to the same tensor, so their shapes differ, and they are not compatible. We recognize that this behavior is unintuitive and are working hard to relax this restriction for the beta release of nested tensors. For a workaround, see the section of this document.\nIn addition to the metadata, NJTs can also compute and cache the minimum and maximum sequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA). There are currently no public APIs for accessing these, but this will change for the beta release.\nThis section contains a list of common operations over nested tensors that you may find useful. It is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While a sizeable subset of these are supported for nested tensors today, full support is a large task. The ideal state for nested tensors is full support of all PyTorch operations that are available for non-nested tensors. To help us accomplish this, please consider:\n```\n \n   \n   \n    \n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n```\n\n```\n \n   \n   \n    \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThis can be useful as an escape hatch to work around NJT support gaps, but ideally such conversions should be avoided when possible for optimal memory usage and performance, as the more efficient nested tensor layout does not materialize padding.\nThe reverse conversion can be accomplished using , which applies ragged structure to a given dense tensor to produce an NJT. Note that by default, this operation does not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be useful to explicitly call here if a contiguous NJT is desired.\nAs variable-length sequences are common inputs to attention mechanisms, nested tensors support important attention operators and . See for usage examples of NJT with SDPA and for usage examples of NJT with FlexAttention.\nNJTs are designed to be used with for optimal performance, and we always recommend utilizing with NJTs when possible. NJTs work out-of-the-box and graph-break-free both when passed as inputs to a compiled function or module OR when instantiated in-line within the function.\nIf you’re not able to utilize for your use case, performance and memory usage may still benefit from the use of NJTs, but it’s not as clear-cut whether this will be the case. It is important that the tensors being operated on are large enough so the performance gains are not outweighed by the overhead of python tensor subclasses.\nIf you run into problems or arcane errors when utilizing NJT + , please file a PyTorch issue. Full subclass support within is a long-term effort and there may be some rough edges at this time.\nThis error occurs when calling an op that operates over multiple NJTs with incompatible ragged structures. Currently, it is required that input NJTs have the exact same constituent in order to have the same symbolic ragged structure symbol (e.g. ).\nAs a workaround for this situation, it is possible to construct NJTs from the and components directly. With both NJTs referencing the same components, they are considered to have the same ragged structure and are thus compatible.\nThis error occurs when calling an op that does data-dependent operation within torch.compile; this commonly occurs for ops that need to examine the values of the NJT’s to determine the output shape. For example:\nIn this example, calling on the batch dimension of the NJT requires examination of the NJT’s data to delineate batch item boundaries within the packed ragged dimension. As a workaround, there are a couple torch.compile flags that can be set:\nIf, after setting these, you still see data-dependent operator errors, please file an issue with PyTorch. This area of is still in heavy development and certain aspects of NJT support may be incomplete.\nIf you’d like to contribute to nested tensor development, one of the most impactful ways to do so is to add nested tensor support for a currently-unsupported PyTorch op. This process generally consists of a couple simple steps:\n\n\nThe most common way to implement an op is to unwrap the NJT into its constituents, redispatch the op on the underlying buffer, and propagate the relevant NJT metadata (including ) to a new output NJT. If the output of the op is expected to have a different shape from the input, new , etc. metadata must be computed.\n  * For operation on the ragged dimension, consider converting to padded dense with a properly-selected padding value that won’t negatively bias the output, running the op, and converting back to NJT. Within , these conversions can be fused to avoid materializing the padded intermediate.\n\n         \n\n    \nConstructs a jagged layout nested tensor from the given jagged components. The jagged layout consists of a required values buffer with the jagged dimension packed into a single dimension. The offsets / lengths metadata determines how this dimension is split into batch elements and are expected to be allocated on the same device as the values buffer.     \n  * offsets: Indices within the packed dimension splitting it into heterogeneously-sized batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3]. Note that both the beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\n  * lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3].\n\n\nNote that it can be useful to provide both offsets and lengths. This describes a nested tensor with “holes”, where the offsets indicate the start position of each batch item and the length specifies the total number of elements (see example below).     \n  * () – The underlying buffer in the shape of (sum_B(*), D_1, …, D_N). The jagged dimension is packed into a single dimension, with the offsets / lengths metadata used to distinguish batch elements.\n  * () – If set, uses the specified value as the cached minimum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n  * () – If set, uses the specified value as the cached maximum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n\n\n```\n   \n       \n   \n\n\n\n\n\n\n\n   \n     \n    \n\n    \n    \n\n  \n\n\n  \n\n\n  \n\n\n```\n    \nIf a nested tensor is passed, it will be returned directly unless the device / dtype / layout differ. Note that converting device / dtype will result in a copy, while converting layout is not currently supported by this function.\nIf a non-nested tensor is passed, it is treated as a batch of constituents of consistent size. A copy will be incurred if the passed device / dtype differ from those of the input OR if the input is non-contiguous. Otherwise, the input’s storage will be used directly.     \n\n         \n  * () – The size of the output tensor. If given, it must be large enough to contain all nested data; else, will infer by taking the max size of each nested sub-tensor along each dimension.\n\n\n```\n     \n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n      \n\n\n\n\n\n\n\n\n      \n\n\n```\n    \nConstructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is represented with the offsets, this is unlike where the output is collapsed to a 1D tensor.     \nConstructs a nested tensor (which might be a view) from , a strided tensor. This follows similar semantics to torch.Tensor.narrow, where in the -th dimension the new nested tensor shows only the elements in the interval . As nested representations allow for a different and at each ‘row’ of that dimension, and can also be tensors of shape .\nThere’s some differences depending on the layout you use for the nested tensor. If using strided layout, torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular representation is really useful for representing kv-caches in Transformer models, as specialized SDPA kernels can deal with format easily, resulting in performance improvements.     \n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/broadcasting.html": "\n  \n```\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n\n\n```\n\n\n\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal. The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements.\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape, but are broadcastable and have the same number of elements. For Example:\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]). In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set to , which will generate a python warning in such cases.\n```\n\n \n\n\n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/gradcheck.html": "It will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where argument is passed (referred to as fast gradcheck below).  \n\n\n\n\nFor the simple real-to-real case, we write as the Jacobian matrix associated with of size . This matrix contains all the partial derivatives such that the entry at position contains . Backward mode AD is then computing, for a given vector of size , the quantity . Forward mode AD on the other hand is computing, for a given vector of size , the quantity .\nFor functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at .\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called below) and the Conjugate Wirtinger derivative (called below). Both and need to be propagated because in general, despite their name, one is not the complex conjugate of the other.\nTo avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\nUnder this assumption, using and definitions, we can show that (we use to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered. To simplify internal computations, PyTorch uses as the value it backwards and returns when the user asks for gradients. Similarly to the real case, when the output is actually in , backward mode AD does not compute but only for a given vector .\nFor forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in . Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in and in this case, using and definitions, we can show that for the intermediary functions. To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes . Similarly to the real case, when the input is actually in , forward mode AD does not compute but only for a given vector .\nTo test a function , we reconstruct the full Jacobian matrix of size in two ways: analytically and numerically. The analytical version uses our backward mode AD while the numerical version uses finite difference. The two reconstructed Jacobian matrices are then compared elementwise for equality.\nThis formula easily generalizes for multiple outputs () by having be a column vector of size like . In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely and ).\nIt is more computationally expensive to handle the case with multiple inputs (). In this scenario, we loop over all the inputs one after the other and apply the perturbation for each element of one after the other. This allows us to reconstruct the matrix column by column.\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computes . For functions with a single output, we simply use to recover the full Jacobian matrix with a single backward pass.\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where each is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the matrix row by row.\nNote that and , in the above equation, are derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. This allows us to compute the matrix and then multiply it by .\n```\n\n\n\n\n\n  \n    \n\n        \n\n        \n        \n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\n```\n\nSince backward mode AD computes exactly twice the derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. To solve this, we will replace the test of the function (where can be either or ), with two functions: and such that:\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the or functions manually by passing the arguments to the different functions. When , then we are considering . When , then we are considering .\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices. This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the matrix is complex-valued and so in this case, we will compare to complex scalars.\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\nWe first consider how to compute with a numerical method. To do so, keeping in mind that we’re considering with , and that , we rewrite it as follows:\n\\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\\ &= 2 * v^T (\\frac{1}{2} * \\frac{\\partial y}{\\partial a} ur + i * \\frac{1}{2} * \\frac{\\partial y}{\\partial b} ui) \\\\\\ &= v^T (\\frac{\\partial y}{\\partial a} ur + i * \\frac{\\partial y}{\\partial b} ui) \\\\\\ &= v^T ((\\frac{\\partial y}{\\partial a} ur) + i * (\\frac{\\partial y}{\\partial b} ui)) \\end{aligned} \nIn this formula, we can see that and can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued vector.\n\\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\\ &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\\\\\ &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui \\end{aligned} \nWe can thus use the fact that the backward mode AD provides us with an efficient way to compute and then perform a dot product of the real part with and the imaginary part with before reconstructing the final complex scalar .\nAt this point, you might be wondering why we did not select a complex and just performed the reduction . To dive into this, in this paragraph, we will use the complex version of noted . Using such complex , the problem is that when doing the numerical evaluation, we would need to compute:\n\\begin{aligned} 2*CW u' &= (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})(ur' + i ui') \\\\\\ &= \\frac{\\partial y}{\\partial a} ur' + i \\frac{\\partial y}{\\partial a} ui' + i \\frac{\\partial y}{\\partial b} ur' - \\frac{\\partial y}{\\partial b} ui' \\end{aligned} \nWhich would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\nThis feature is implemented by considering the function and use the gradcheck defined above on this function. Note that in this case is just a random vector with the same type as .\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/hip.html": "ROCm™ is AMD’s open source software platform for GPU-accelerated high performance computing and machine learning. HIP is ROCm’s C++ dialect designed to ease conversion of CUDA applications to portable C++ code. HIP is used when converting existing CUDA applications like PyTorch to portable C++ and for new projects that require portability between AMD and NVIDIA.  \n```\n       \n    \n    \n\n    \n\n   \n\n\n \n    \n        \n\n    \n       \n    \n\n    \n       \n    \n\n        \n    \n\n        \n    \n\n    \n    \n       \n      \n      \n    \n\n```\n\nWhether you are using PyTorch for CUDA or HIP, the result of calling will be the same. If you are using a PyTorch that has been built with GPU support, it will return . If you must check which version of PyTorch you are using, refer to this example below:\nPyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in . You can use and to monitor memory occupied by tensors, and use and to monitor the total amount of memory managed by the caching allocator. Calling releases all cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\nFor more advanced users, we offer more comprehensive memory benchmarking via . We also offer the capability to capture a complete snapshot of the memory allocator state via , which can help you understand the underlying allocation patterns produced by your code.\nFor each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that handle and stream combination executes a hipBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless is called; note that it’s the same function for CUDA or HIP. The workspace size per allocation can be specified via the environment variable with the format . As an example, the environment variable specifies a total size of or 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force hipBLAS to avoid using workspaces, set . For convenience, is also accepted.\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/faq.html": "As the error message suggests, you have run out of memory on your GPU. Since we often deal with large amounts of data in PyTorch, small mistakes can rapidly cause your program to use up all of your GPU; fortunately, the fixes in these cases are often simple. Here are a few common things to check:  \nBy default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.\nIf you assign a Tensor or Variable to a local, Python will not deallocate until the local goes out of scope. You can free this reference by using . Similarly, if you assign a Tensor or Variable to a member variable of an object, it will not deallocate until the object goes out of scope. You will get the best memory usage if you don’t hold onto temporaries you don’t need.\nThe amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long.\nA linear layer uses memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to \nBut find that when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the clause.\nYou are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via . See ’s documentation for how to properly set up random seeds in workers with its option.\nThere is a subtlety in using the pattern in a with or . Input to each the on each device will only be part of the entire input. Because the unpack operation by default only pads up to the longest input it sees, i.e., the longest on that particular device, size mismatches will happen when results are gathered together. Therefore, you can instead take advantage of the argument of to make sure that the calls return sequences of same length. For example, you can write:\n```\n    \n\n \n    \n\n    \n    \n    \n    \n       \n            \n           \n                                            \n           \n            \n                                        \n         \n\n\n  \n  \n\n```\n\nAdditionally, extra care needs to be taken when batch dimension is dim (i.e., ) with data parallelism. In this case, the first argument of pack_padded_sequence will be of shape and should be scattered along dim , but the second argument will be of shape and should be scattered along dim . Extra code to manipulate the tensor shapes will be needed.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/extending.func.html": "  * you wish to call code that does not contain PyTorch operations and have it work with function transforms. That is, the ’s forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.  \n\n\n\n\nBecause accepts only and , the only quantities that can be saved are either objects (such as Tensors) in the inputs or outputs or quantities (like ) derived from them. If you wish to save a non-input intermediate activation from for backward, then you’ll need to return it as an output from so that it gets passed to .\nIn order for the to be arbitrarily composable with function transforms, we recommend that all other staticmethods other than and must be transformable: that is, they must consist of only PyTorch operators or call other (that may call into C++/CUDA/etc).\n```\n \n   \n\n \n     \n\n \n    \n    \n      \n          \n          \n           \n           \n            \n        \n        \n         \n            \n             \n            \n             \n            \n             \n        \n\n    \n    \n    \n    \n       \n           \n        \n        \n        \n        \n        \n            \n         \n        \n        \n         \n        \n          \n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n           \n             \n\n \n    \n        \n          \n          \n          \n            \n\n    \n       \n             \n         \n          \n\n    \n      \n           \n             \n            \n\n```\n\nAnother common case is an that is implemented with PyTorch operations. PyTorch is able to compute gradients for PyTorch operations automatically, but perhaps we wish to customize how the gradients are computed. Some reasons why we may want a custom backward different from the one PyTorch gives us are:\n```\n \n    \n     \n            \n        \n        \n        \n              \n          \n\n    \n       \n          \n           \n         \n\n    \n       \n           \n        \n        \n                  \n         \n\n```\n\nPlease do not capture Tensors that are being transformed over, have requires_grad=True, or are dual tensors, into the methods of the . The way to be completely safe is to ensure that the only Tensors being used inside any method of the must be directly passed as inputs (or via the ctx object) rather than come from outside the .\ndoes not handle Tensors in pytrees (arbitrary nested Python data structures that may or may not contain Tensors). For those Tensors to be tracked by autograd, they must be passed directly as an argument to . This is in contrast to jax.{custom_vjp, custom_jvp}, which do accept pytrees.\nIf your fulfills the following additional constraints, then we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you want custom behavior under vmap, please manually define a vmap staticmethod (see next section).\nConceptually, the vmap staticmethod is responsible for defining how the should behave under . That is, it defines how to transform the to run over inputs with an additional dimension (the dimension being vmapped over). This is similar to how is implemented over PyTorch operations: for each operation, we define a vmap rule (sometimes also referred to as a “batching rule”).\n  * For each arg in , has a corresponding . It is if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.\n  * The return of the vmap staticmethod is a tuple of . Similar to , should be of the same structure as and contain one per output that specifies if the output has the vmapped dimension and what index it is in.\n\n\n```\n \n     \n\n \n    \n      \n          \n          \n           \n           \n            \n         \n             \n             \n             \n        \n\n    \n       \n           \n            \n         \n         \n          \n\n    \n        \n           \n             \n\n    \n    \n    \n    \n        \n        \n        \n        \n        \n        \n        \n           \n\n        \n        \n        \n        \n           \n        \n                    \n             \n\n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n        \n               \n\n \n    \n        \n          \n          \n          \n            \n\n    \n       \n             \n         \n          \n\n    \n      \n           \n             \n            \n\n    \n          \n             \n\n        \n        \n        \n\n        \n                  \n                  \n\n          \n               \n                  \n              \n\n        \n        \n           \n           \n           \n\n        \n        \n               \n\n  \n         \n     \n\n   \n  \n   \n\n```\n\nIt is a legitimate use case to write a custom vmap staticmethod for a that PyTorch is able to generate a vmap rule for via . You may wish to do this if the generated vmap rule doesn’t have the semantics you’re looking for.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/extending.html": "PyTorch offers a large library of operators that work on Tensors (e.g. , , etc). However, you may wish to bring a new custom operation to PyTorch and have it behave like PyTorch’s built-in operators. In order to do so, you must register the custom operation with PyTorch via the Python or C++ TORCH_LIBRARY APIs.  \nThe first part of this doc is focused on backward mode AD as it is the most widely used feature. A section at the end discusses the extensions for forward mode AD.\nIn general, implement a custom function if you want to perform computations in your model that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine.\nIn some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a , you can wrap them in to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together.\nIf you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function.\n  * is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. arguments that track history (i.e., with ) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single output, or a to find descriptions of useful methods that can be called only from .\n  * (optional). One can either write a “combined” that accepts a object or (as of PyTorch 2.0) a separate that does not accept and a method where the modification happens. The should have the compute and should only be responsible for the modification (and not have any compute). In general the separate and is closer to how PyTorch native operations work and therefore more composable with various PyTorch subsystems. See for more details.\n  * (or ) defines the gradient formula. It will be given as many arguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient ( is a tuple of booleans indicating whether each input needs gradient computation), or were non- objects, you can return . Also, if you have optional arguments to you can return more gradients than there were inputs, as long as they’re all \n\n\n  * must be used to save any tensors to be used in the backward pass. Non-tensors should be stored directly on . If tensors that are neither input nor output are saved for backward your may not support double backward (see step 3).\n  * must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.\n  * can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True.\n\n\nIf your does not support double backward you should explicitly declare this by decorating backward with the . With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward.\nIt is recommended that you use to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.\n```\n\n \n\n    \n    \n       \n          \n            \n              \n         \n\n    \n    \n    \n       \n            \n          \n\n    \n    \n      \n        \n        \n        \n        \n        \n            \n              \n\n        \n        \n        \n        \n         \n              \n         \n              \n              \n              \n\n           \n\n```\n\nNow, to make it easier to use these custom ops, we recommend either aliasing them or wrapping them in a function. Wrapping in a function lets us support default arguments and keyword arguments:\n```\n \n    \n      \n           \n\n    \n       \n        \n        \n           \n          \n\n    \n      \n        \n        \n            \n\n```\n\n```\n \n    \n      \n           \n\n    \n       \n           \n        \n          \n\n    \n      \n        \n        \n           \n              \n\n        \n        \n            \n\n```\n\nIf you need any “intermediate” Tensors computed in to be saved, either they must be returned as outputs, or combine and (see ). Note that this means if you want gradients to flow through those intermediate values, you need to define the gradient formula for them (see also ):\n```\n \n    \n     \n        \n        \n              \n            \n          \n\n    \n       \n          \n           \n         \n\n    \n       \n           \n        \n        \n        \n                  \n         \n\n\n \n       \n     \n\n```\n\nInputs to , i.e., , can also be tensors that track history. So if is implemented with differentiable operations, (e.g., invocation of another custom ), higher order derivatives will work. In this case, the tensors saved with can also be used in the backward and have gradients flowing back but tensors saved in the won’t have gradients flowing back for them. If you need gradients to flow back for a Tensor saved in the , you should make it an output of the custom and save it with .\n```\n   \n\n\n\n\n   \n     \n\n\n```\n\nSee for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the function from the same package to check higher order derivatives.\nWe recommend the second option (separate and ) because that is closer to how PyTorch native operations are implemented and it composes with transforms. However, we plan to support both approaches going forward; combining with : leads to more flexibility since you are able to save intermediates without returning them as output.\nIt will be given as many arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The will be called just after the method, before the returns.\n  * The function must match the view/inplace behavior of . For example, if the th input is modified inplace, then the th gradient must be updated inplace. Similarly, if the th output is a view of the th input. Then the returned th output gradient must be a view of the given th input gradient.\n\n\nexports two kinds of interfaces - modules and their functional versions. You can extend it in both ways, but we recommend using modules for all kinds of layers, that hold any parameters or buffers, and recommend using a functional form parameter-less operations like activation functions, pooling, etc.\nSince heavily utilizes , adding a new requires implementing a that performs the operation and can compute the gradient. From now on let’s assume that we want to implement a module and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented:\n\n\n```\n \n        \n        \n          \n          \n\n        \n        \n        \n        \n        \n        \n        \n           \n         \n              \n        \n            \n            \n             \n\n        \n          \n            \n              \n\n      \n        \n           \n\n     \n        \n        \n         \n                 \n        \n\n```\n\nYou can create custom types that emulate by defining a custom class with methods that match . But what if you want to be able to pass these types to functions like in the top-level namespace that accept operands?\nIf your custom Python type defines a method named , PyTorch will invoke your implementation when an instance of your custom class is passed to a function in the namespace. This makes it possible to define custom implementations for any of the functions in the namespace which your implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for . This works with “duck” types that are unrelated to as well as user-defined subclasses of .\nTo make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order and value along the diagonal entries, :\nThe method takes four arguments: , a reference to the torch API function that is being overridden, , the list of types of Tensor-likes that implement , , the tuple of arguments passed to the function, and , the dict of keyword arguments passed to the function. It uses a global dispatch table named to store custom implementations. The keys of this dictionary are functions in the namespace and the values are implementations for .\nThis class definition isn’t quite enough to make do the right thing when we pass it a – we also need to define an implementation for for operands and add the implementation to the dispatch table dictionary. One way of doing this is to define a decorator:\nOf course is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines , for example for :\nThis version has a fast path for when both operands are instances and also a slower path which degrades to converting the data to tensors when either operand is not a . That makes the override function correctly when either operand is a or a regular :\nFor speed and flexibility the dispatch mechanism does not check that the signature of an override function matches the signature of the function being overridden in the API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with , user implementations of torch API functions should take care to exactly emulate the API of the function that is being overridden.\nFunctions in the API that do not have explicit overrides will return from . If all operands with defined on them return , PyTorch will raise a . This means that most of the time operations that do not have explicit overrides for a type will raise a when an instance of such a type is passed:\nIn practice this means that if you would like to implement your overrides using a implementation along these lines, you will need to explicitly implement the full API or the entire subset of the API that you care about for your use case. This may be a tall order as the full API is quite extensive.\nAnother option is to not return for operations that are not handled but to instead pass a to the original function when no override is available. For example, if we change our implementation of for to the one below:\nThe protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a . This is especially true for subclasses, where all three of , and must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. If one requires the implementation of a function from subclasses, they must use inside their implementation.\nHowever, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using and matching with .\nAnother useful case is a type that wraps a , either as an attribute or via subclassing. Below we implement a special case of this sort of type, a that attaches a dictionary of metadata to a that is propagated through operations. Since this is a generic sort of wrapping for the full API, we do not need to individually implement each override so we can make the implementation more permissive about what operations are allowed:\n\n\nOne troublesome aspect of implementing is that if some operations do and others do not have overrides, users will at best see an inconsistent experience, or at worst will see errors raised at runtime when they use a function that does not have an override. To ease this process, PyTorch provides a developer-facing API for ensuring full support for overrides. This API is private and may be subject to changes without warning in the future.\nFirst, to get a listing of all overridable functions, use . This returns a dictionary whose keys are namespaces in the Python API and whose values are a list of functions in that namespace that can be overridden. For example, let’s print the names of the first 5 functions in that can be overridden:\nThis listing of functions makes it possible to iterate over all overridable functions, however in practice this is not enough to write tests for all of these functions without laboriously and manually copying the signature of each function for each test. To ease this process, the function returns a dictionary mapping overridable functions in the API to dummy lambda functions that have the same signature as the original function but unconditionally return -1. These functions are most useful to use with to analyze the function signature of the original function:\nWhile allows one to effectively extend PyTorch’s pure Python components’ behavior, it does not allow one to extend the parts of PyTorch implemented in C++. To that end, a subclass can also define which will be able to override the behavior at the C++ level.\nTo effectively use this feature, it is important to know how the native part of PyTorch is implemented. The most important component there is what we call the “dispatcher” (the best description can be found in this , the dispatcher will inspect both arguments, figure out which “feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally call all the right kernels. A very common thing done by a kernel is to “redispatch”. For example, when running your neural network on GPU with autocast, the first call will be the autocast kernel that will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit.\nOne configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in inside the enum. For the purpose of extending torch, the important subset of the ordering for this discussion is:\nThe most important key for the purpose of this discussion is as every Tensor subclass with the method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided again will perform a “redispatch”.\n  * This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc).\n  * If any high level feature implements a given function without redispatching, it will never reach the key and so the callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead.\n  * When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None).\n  * Our native functions are lazily populated as as callable Python objects to enable easily interacting with them from Python. The object given to is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.\n\n\nIn a similar way where is able to interpose on all of torch’s Python API and Tensor methods, is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: and will lead to exactly the same aten call. Most of these functions are defined in which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen. Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.\nIt is also possible to add native functions using . This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.\n  * the consists of all other arguments, no matter how they were passed to the operator (positional vs keyword). If an arg is equal to its default value, and it is the right-most positional arg or all the args to the right of it are not passed, it will not be passed.\n\n\nUnfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch’s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive.\nTo simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass. This means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first.\n```\n \n    \n          \n           \n          \n          \n\n           \n\n \n\n      \n          \n           \n          \n          \n\n       \n\n    \n             \n            \n            \n\n```\n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/custom_operators.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html": "---  \nIntel GPUs support (Prototype) is ready from PyTorch* 2.5 for Intel® Client GPUs and Intel® Data Center GPU Max Series on both Linux and Windows, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack with consistent user experience to embrace more AI application scenarios.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/ddp.html": "Let us start with a simple example. This example uses a as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.  \n```\n \n   \n   \n   \n   \n \n     \n\n\n  \n    \n      \n    \n       \n    \n       \n    \n      \n       \n\n    \n       \n       \n    \n     \n    \n    \n\n \n      \n    \n        \n        \n        \n\n \n    \n    \n    \n      \n      \n    \n\n```\n\nDDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model wrapper before compiling the model, such that torchdynamo can apply (graph-break optimizations) based on DDP bucket sizes. (See for more information.)\n  * : The DDP constructor takes a reference to the local module, and broadcasts from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. Then, each DDP process creates a local , which later will take care of the gradients synchronization during the backward pass. To improve communication efficiency, the organizes parameter gradients into buckets, and reduces one bucket at a time. Bucket size can be configured by setting the argument in DDP constructor. The mapping from parameter gradients to buckets is determined at the construction time, based on the bucket size limit and parameter sizes. Model parameters are allocated into buckets in (roughly) the reverse order of from the given model. The reason for using the reverse order is because DDP expects gradients to become ready during the backward pass in approximately that order. The figure below shows an example. Note that, the and are in , and the other two gradients are in . Of course, this assumption might not always be true, and when that happens it could hurt DDP backward speed as the cannot kick off the communication at the earliest possible time. Besides bucketing, the also registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.\n  * : The DDP takes the input and passes it to the local model, and then analyzes the output from the local model if is set to . This mode allows running backward on a subgraph of the model, and DDP finds out which parameters are involved in the backward pass by traversing the autograd graph from the model output and marking all unused parameters as ready for reduction. During the backward pass, the would only wait for unready parameters, but it would still reduce all buckets. Marking a parameter gradient as ready does not help DDP skip buckets as for now, but it will prevent DDP from waiting for absent gradients forever during the backward pass. Note that traversing the autograd graph introduces extra overheads, so applications should only set to when necessary.\n  * : The function is directly invoked on the loss , which is out of DDP’s control, and DDP uses autograd hooks registered at construction time to trigger gradients synchronizations. When one gradient becomes ready, its corresponding DDP hook on that grad accumulator will fire, and DDP will then mark that parameter gradient as ready for reduction. When gradients in one bucket are all ready, the kicks off an asynchronous on that bucket to calculate mean of gradients across all processes. When all buckets are ready, the will block waiting for all operations to finish. When this is done, averaged gradients are written to the field of all parameters. So after the backward pass, the field on the same corresponding parameter across different DDP processes should be the same.\n  * : From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.\n\n\nDDP requires instances on all processes to invoke in exactly the same order, which is done by always running in the bucket index order instead of actual bucket ready order. Mismatched order across processes can lead to wrong results or DDP backward hang.\n  * function for the module which call into C++ libraries. Its function performs intra-process parameter synchronization when one DDP process works on multiple devices, and it also broadcasts model buffers from the process with rank 0 to all other processes. The inter-process parameter synchronization happens in .\n  * \n\nDDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards. AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph, because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\nTorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break the graph during backwards, and the simplest implementation is to break the forward graphs and then call AotAutograd and compilation on each section. This allows DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications to overlap with compute.\nTo Debug DDPOptimizer, set for full graph dumps. For logs without graphs, add any of ‘dynamo’, ‘distributed’, or ‘dist_ddp’ to (for basic info about bucket boundaries). To disable DDPOptimizer, set . DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/fsdp.html": "Implicit prefetching refers to relying on issuing the all-gathers from a separate CUDA stream to allow for overlapping an all-gather with compute issued before it (from the CPU perspective). For example, if we have layer 0 all-gather -> layer 0 compute -> layer 1 all-gather -> …, then layer 1 all-gather can overlap with layer 0 compute even though the CPU thread issued it afterwards. (The 1st all-gather will not be able to overlap with anything.)  \nExplicit prefetching refers to changing the CPU thread’s issue order: e.g. layer 0 all-gather -> layer 1 all-gather -> layer 0 compute -> …. In eager mode, there is no way to know in general which layer is the next layer (e.g. layer 1 in the example) when still executing on layer 0. Therefore, explicit prefetching should only be used for models whose execution order is fixed from iteration to iteration (which we sometimes call “static graph”). An example of a model that does not satisfy this constraint is ).\nExplicit prefetching only saves the time taken to issue a layer’s compute kernels at the cost that the next all-gather’s output tensor must be allocated while the current one is still in use. By issuing the next all- gather before the current compute kernels, the next all-gather can start sooner on GPU. For most LLM workloads, this is not the case, so there is no motivation for enabling .\nIn contrast, for , we must use explicit prefetching or else there will be 0 overlap of communication and computation. The reason is because we use a single NCCL process group for both all-gather and reduce-scatter (partially because in earlier NCCL versions, it was not safe to use multiple concurrently on the same device over the same ranks). A single NCCL process group means a single internal NCCL stream on which reduce-scatters and all-gathers run serially. As such, unless we explicitly reorder the CPU issue order to be next all-gather -> current reduce-scatter, then the current reduce-scatter would block the next all-gather and hence the next computation, preventing the current reduce-scatter from overlapping.\nIn the FSDP design, the communication payload per rank is determined as follows: Each call to creates one communication group consisting of the parameters in except any already assigned to a nested instance. For example, for Llama, if you apply to every transformer block and also to the root module, then there is one communication group for each transformer block and finally one communication group with the initial embedding and final linear. Each communication group corresponds to a single all-gather call and single reduce-scatter call. In that way, how you apply determines the communication size. In general, applying FSDP to each transformer block is a good heuristic for LLMs, and it is hard to do better than that given the current design.\nLet’s consider an example where we have a Transformer-based model sharded over 8 GPUs, where the sharding happens at the transformer block-level only, and each transformer block contains 1.6B parameters and the parameters are in fp32 (4 bytes each). Which means that once sharded, each transformer block will contain 0.2B parameters on each rank.\nIn other words there will be 3 communications with a payload of each. If the model was comprised of 10 transformer blocks there would be a total of 30 communications for a total of .\nPlease note that in this example we didn’t include the additional communications required for the embedding, which should be accounted for as well. And the math would depend on whether the input and output embeddings are tied or not. If they aren’t tied there will be 2x more communications.\nWhile the implicit prefetching (, default) case of the same sequence in theory should need only 1 buffer, in reality it’s still 2x all-gather-sized buffers. The reason is that in the flat-parameter FSDP design, we do not copy-out of the all-gather buffer. The parameters used for compute are directly viewed into the all-gather buffer (in fact, the main benefit of the “flat parameter” is exactly this reason). In that case, while ‘layer 1 all-gather’ is overlapping with ‘layer 0 forward compute’, the ‘layer 0 forward compute’ is using the parameters viewed into the ‘layer 0 all-gather’ buffer.\nA natural question then is, when would you want ? For static-graph models (like most LLMs), there is a major technical reason. It is more that, practically, we added this option quickly for some CPU-bound internal models and have not tested every code path with it in unit testing, so we are less confident in it. can be slightly easier to reason about since we do not have to check the recorded forward order as a possible ‘failure mode’; a module’s all-gather can always be found under its own label in its profiler trace.\nThe current FSDP design uses to manage allocations produced in one stream consumed in another, which can lead to more memory usage than expected. How much more can be “non-deterministic” in that it depends on GPU kernel timing relative to the CPU. The argument is a mitigation to that - for more details refer to this discussion is .\n  * This means that in , we end up with (ND -> 1D) and (which is a concat). In particular, each individual gradient is computed as a separate allocation, and an explicit concat happens to construct the reduce-scatter input buffer. This implies actually a 2x buffer size for reduce-scatter at that peak memory point.\n\n\nOnce the sharded parameters are gathered from all ranks, they require an additional buffer of for the full parameters - so continuing the example from earlier if each transformer block is 1.6B parameters and the parameters are in fp32, then it’d be buffer.\n  * Putting this together, this means that the root’s flat parameter including the embedding and final projection are all-gathered to begin forward and kept in GPU memory until the end of backward.\n  * If the embedding and final linear are not weight-tied, then we _could_ further apply FSDP to the embedding and to the final linear. For weight-tied parameters, we require them to be part of the same flat parameter (or else it would get double-counted). That would allow the embedding to be freed after its usage in forward and only all-gathered toward the end of backward.\n  * Hopefully, this gives a better sense – each FSDP module gets assigned parameters in its except those already assigned to another nested FSDP module, and the FSDP module’s defines the ‘live’ interval for its parameters. Hence, the nested structure can affect the all-gather/free schedule and hence the memory/throughput performance.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html": "The note assumes that you either build PyTorch from source in your organization or have an ability to statically link additional code to be loaded when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that can be triggered once in a centralized place, e.g. in static initialization code.  \nPyTorch comes with capable of measuring time taken by individual operators on demand. One can use the same mechanism to do “always ON” measurements for any process running PyTorch. It might be useful for gathering information about PyTorch workloads running in a given process or across the entire set of machines.\nNew callbacks for any operator invocation can be added with . Hooks will be called with struct that describes invocation context (e.g. ). If enabled, contains arguments of the function represented as variant type. Note, that inputs logging is relatively expensive and thus has to be enabled explicitly.\nThe operator callbacks also have access to interface that returns a pointer to the struct holding the debug information. This debug information can be set earlier by using object. Debug information is propagated through the forward (including async tasks) and backward passes and can be useful for passing some extra information about execution environment (e.g. model id) from the higher layers of the application down to the operator callbacks.\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nWhen running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in one-off python scripts, the callback fires only once for a given process for each of the APIs.\nTorchScript modules can be saved as an archive file that bundles serialized parameters and module code as TorchScript (see ). It’s often convenient to bundle additional information together with the model, for example, description of model producer or auxiliary artifacts.\nIt can be achieved by passing the argument to and to store and retrieve arbitrary binary blobs during saving process. Since TorchScript files are regular ZIP archives, extra information gets stored as regular files inside archive’s directory.\nThere’s also a global hook allowing to attach extra files to any TorchScript archive produced in the current process. It might be useful to tag models with producer metadata, akin to JPEG metadata produced by digital cameras. Example usage might look like:\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/package.html": "adds support for creating packages containing both artifacts and arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using .  \nIt is possible to construct malicious pickle data which will . Never unpackage data that could have come from an untrusted source, or that could have been tampered with.\n  * \n\nallows for the customization of how classes are packaged. This behavior is accessed through defining the method on a class and by defining a corresponding de-packaging function. This is similar to defining for Python’s normal pickling process.\n  1. Define the method on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the when it encounters an instance of the target class.\n  2. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be a instance, and the rest of the parameters are user defined.\n\n\n```\n\n    \n \n\n\n \n       \n        \n          \n          \n          \n\n       \n\n\n\n\n\n\n\n\n\n        \n        \n        \n          \n        \n            \n            \n              \n        \n          \n\n        \n           \n\n\n \n         \n  \n\n\n\n\n\n      \n       \n      \n      \n     \n\n```\n\nA will add the attribute to every module that it initializes. Your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not.\n: in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to hard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring your code so that it behaves the same way no matter how it was loaded.\n```\n   \n    \n    \n\n    \n    \n     \n\n\n\n\n    \n\n    \n    \n    \n    \n                                \n                                \n\n  \n  \n\n```\n\nUsing is the recommended way to access package contents from within packaged code, since it complies with the Python standard. However, it is also possible to access the parent instance itself from within packaged code.\n```\n\n  \n\n\n \n      \n\n\n\n \n      \n\n```\n\nTo tell if an object’s code is from a , use the function. Note: if an object is from a package but its definition is from a module marked or from , this check will return .\nTo package a TorchScript model, use the same and APIs as you would with any other object. Saving TorchScript objects that are attributes or submodules is supported as well with no extra work.\nThe directory is owned by torch.package, and its contents are considered to be a private implementation detail. The format makes no guarantees about the contents of , but any changes made will be backward compatible (that is, newer version of PyTorch will always be able to load older ).\n\n\nWhen a Python module is identified as a dependency, walks the module’s python AST representation and looks for import statements with full support for the standard forms: , , , etc. When one of these import statements are encountered, registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.\nautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution. For each module that the dependency resolver finds, you must specify an to take.\n\n\nNote that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out. This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a module, so that’s what uses.\nIf a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined, and the first action will be taken.\nThis action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from , you will need to the module torchvision.models.resnet.\nOn package import, when your packaged code tries to import an -ed module, PackageImporter will look inside your package for that module. If it can’t find that module, an error will be raised. This ensures that each is isolated from the loading environment—even if you have available in both your package and the loading environment, will only use the version in your package.\n: Only Python source modules can be -ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if you attempt to them. These kinds of modules need to be -ed or -ed.\nOn package import, when the packaged code tries to import an -ed module, will use the default Python importer to find that module, as if you did . If it can’t find that module, an error will be raised.\nIf a module is -ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve objects from it (so that will not error), but any use of that object will raise a .\nshould be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents. For example, initialization/configuration code, or code only used for debugging/training.\n: In general, should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code, which may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.\nThe best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some guidelines for writing code with clean dependencies (which are also generally good practices!):\n. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try to process them.\n. For example, instead of writing import foo and later using , prefer to write . This more precisely specifies your real dependency () and lets the dependency resolver know you don’t need all of .\n. If your module contains a hodge-podge of unrelated functionality, any module that depends on will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define single-purpose modules that can be packaged independently of one another.\n\n\nPython makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to names this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable global state.\nEvery creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure they are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug errors.\nIn this example, and are . In this specific example, and have exactly the same implementation, so you might think it’s okay to consider them the same class. But consider the situation where is coming from an older package with an entirely different implementation of — in that case, it’s unsafe to consider them the same class.\nThat means you should not expect checks to work when one of the arguments is from a package and the other is not. If you need this functionality, consider the following options:\n  * Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tag and have client code check for the value of instead of checking the type directly.\n\n\nEach instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import other packaged modules, or modules marked . If you use multiple instances to load a single package, you will get multiple independent environments that do not interact.\nWhen you invoke , will construct and return a new module, much as the system importer does. However, patches the returned module to use (i.e. that instance) to fulfill future import requests by looking in the package rather than searching the user’s Python environment.\nName mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print statements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult in .     \nImports can load this code in a hermetic way, such that code is loaded from the package rather than the normal Python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.\nThe code contained in packages is copied file-by-file from the original source when it is created, and the file format is a specially organized zip file. Future users of the package can unzip the package, and edit the code in order to perform custom modifications to it.\nThe importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external using . The file in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.\nWhen source code is added to the package, the exporter can optionally scan it for further code dependencies (). It looks for import statements, resolves relative references to qualified module names, and performs an action specified by the user (See: , , and ).          \n\n         \n\n    \nInclude in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package.     \n  * ( method must be matched to some module during packaging. If an extern module glob pattern is added with , and is called (either explicitly or via ) before any modules match that pattern, an exception is thrown. If , no such exception is thrown.\n\n         \n  * ( method must be matched to some module during packaging. If an module glob pattern is added with , and is called (either explicitly or via ) before any modules match that pattern, an exception is thrown. If , no such exception is thrown.\n\n    \nReplace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code.     \n  * A string e.g. , or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically.\n  * ( method must be matched to some module during packaging. If a mock is added with , and is called (either explicitly or via ) and the mock has not been matched to a module used by the package being exported, an exception is thrown. If , no such exception is thrown.\n\n                        \nSave a python object to the archive using pickle. Equivalent to but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.\nTo be able to save an object where is , must resolve to the class of the object according to the order. When saving objects that have previously been packaged, the importer’s method will need to be present in the list for this to work.               \n\n    \nImporters allow you to load code written to packages by . Code is loaded in a hermetic way, using files from the package rather than the normal python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.\nThe importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external during export. The file in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.          \n  * () – A method to determine if a externally provided module should be allowed. Can be used to ensure packages loaded do not depend on modules that the server does not support. Defaults to allowing anything.\n\n         \n  * () – An optional string e.g. , or optional list of strings for the names of the files to be included in the zipfile representation. This can also be a glob-style pattern, as described in \n\n                   \n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/libtorch_stable_abi.html": "  1. Schema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.  \n\n\nOur confidently supported types are the ones in the table that have completed rows. For a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. You can work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator with aoti_torch_call_dispatcher.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html": "Many operations in PyTorch support batched computation, where the same operation is performed for the elements of the batches of inputs. An example of this is and . It is possible to implement batched computation as a loop over batch elements, and apply the necessary math operations to the individual batch elements, for efficiency reasons we are not doing that, and typically perform computation for the whole batch. The mathematical libraries that we are calling, and PyTorch internal implementations of operations can produces slightly different results in this case, compared to non-batched computations. In particular, let and be 3D tensors with the dimensions suitable for batched matrix multiplication. Then (the first element of the batched result) is not guaranteed to be bitwise identical to (the matrix product of the first elements of the input batches) even though mathematically it’s an identical computation.  \nSimilarly, an operation applied to a tensor slice is not guaranteed to produce results that are identical to the slice of the result of the same operation applied to the full tensor. E.g. let be a 2-dimensional tensor. is not guaranteed to be bitwise equal to .\nWhen inputs contain large values such that intermediate results may overflow the range of the used datatype, the end result may overflow too, even though it is representable in the original datatype. E.g.:\nThe external libraries (backends) that uses provide no guarantees on their behaviour when the inputs have non-finite values like or . As such, neither does PyTorch. The operations may return a tensor with non-finite values, or raise an exception, or even segfault.\nand assume that the input matrix is invertible. If it is close to being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return incorrect results. These matrices are said to be .\nSpectral operations like , , and may also return incorrect results (and their gradients may be infinite) when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions struggle to converge for these inputs.\nRunning the computation in (as NumPy does by default) often helps, but it does not solve these issues in all cases. Analyzing the spectrum of the inputs via or their condition number via may help to detect these issues.\nOn Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions. When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read. This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input). By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32. We recommend enabling TF32 tensor cores for matrix multiplications with if your network does not need full float32 precision. If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with .\nHalf-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., values when the final result should be be representable in half-precision). If reduced-precision reductions are problematic, they can be turned off with \nA naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul.\nOn AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.\nrocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.\nWhen training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/randomness.html": "However, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.  \nSome PyTorch operations may use random numbers internally. does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine.\nPlease check the documentation for for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: \nWhile disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either or is set. The latter setting controls only this behavior, unlike which will make other PyTorch operations behave deterministically, too.\nOperations like and can return tensors with uninitialized memory that contain undefined values. Using such a tensor as an input to another operation is invalid if determinism is required, because the output will be nondeterministic. But there is nothing to actually prevent such invalid code from being run. So for safety, is set to by default, which will fill the uninitialized memory with a known value if is set. This will prevent the possibility of this kind of nondeterministic behavior.\nHowever, filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/windows.html": "```\n\n\n\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\n\n\n\n\n\n\n cuda102\n release\ncurl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4__.7z -o magma.7z\n7z x -aoa magma.7z -omagma\n\n\n \n \n \n\n```\n  \nThis type of extension has better support compared with the previous one. However, it still needs some manual configuration. First, you should open the . And then, you can start your compiling process.\nThe problem is caused by the missing of the essential files. Actually, we include almost all the essential files that PyTorch need for the conda package except VC2017 redistributable and some mkl libraries. You can resolve this by typing the following command.\nThe implementation of is different on Windows, which uses instead of . So we have to wrap the code with an if-clause to protect the code from executing multiple times. Refactor your code into the following structure.\nThis issue happens when the child process ends before the parent process finishes sending data. There may be something wrong with your code. You can debug your code by reducing the of to zero and see if the issue persists.\nPlease update your graphics driver. If this persists, this may be that your graphics card is too old or the calculation is too heavy for your card. Please update the TDR settings according to this \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html": "When a is sent to another process, the data is shared. If is not , it is also shared. After a without a field is sent to the other process, it creates a standard process-specific that is not automatically shared across all processes, unlike how the ’s data has been shared.  \nUnlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. It is implemented under the hood but requires users to follow the best practices for the program to run correctly. For example, the sending process must stay alive as long as the consumer process has references to the tensor, and the refcounting can not save you if the consumer process exits abnormally via a fatal signal. See .\nThere are a lot of things that can go wrong when a new process is spawned, with the most common cause of deadlocks being background threads. If there’s any thread that holds a lock or imports a module, and is called, it’s very likely that the subprocess will be in a corrupted state and will deadlock or fail in a different way. Note that even if you don’t, Python built in libraries do - no need to look further than , that doesn’t use any additional threads.\nWe’re trying our best to make it easy for you and ensure these deadlocks don’t happen but some things are out of our control. If you have any issues you can’t cope with for a while, try reaching out on forums, and we’ll see if it’s an issue we can fix.\nUsing , it is possible to train a model asynchronously, with parameters either shared all the time, or being periodically synchronized. In the first case, we recommend sending over the whole model object, while in the latter, we advise to only send the .\nWe recommend using start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.\nAssuming there are N vCPUs available on the machine, executing the above command will generate 4 subprocesses. Each subprocess will allocate N vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the machine only has N vCPUs available. Consequently, the different processes will compete for resources, leading to frequent process switching.\n  1. High CPU Utilization: By using the command, you can observe that the CPU utilization is consistently high, often reaching or exceeding its maximum capacity. This indicates that the demand for CPU resources exceeds the available physical cores, causing contention and competition among processes for CPU time.\n  2. Frequent Context Switching with Low System Efficiency: In an oversubscribed CPU scenario, processes compete for CPU time, and the operating system needs to rapidly switch between different processes to allocate resources fairly. This frequent context switching adds overhead and reduces the overall system efficiency.\n\n\nIn this case, a solution would be to specify the appropriate number of threads in the subprocesses. This can be achieved by setting the number of threads for each process using the function in subprocess.\nAssuming there are N vCPUs on the machine and M processes will be generated, the maximum value used by each process would be . To avoid CPU oversubscription in the mnist_hogwild example, the following changes are needed for the file in \nSet for each process using . where you replace N with the number of vCPUs available and M with the chosen number of processes. The appropriate value will vary depending on the specific task at hand. However, as a general guideline, the maximum value for the should be to avoid CPU oversubscription. In the \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/mps.html": "device enables high-performance training on GPU for MacOS devices with Metal programming framework. It introduces a new device to map Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively.  \n```\n\n  \n      \n        \n              \n    \n        \n              \n\n\n      \n\n    \n       \n    \n       \n\n    \n        \n\n    \n      \n    \n\n    \n      \n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/serialization.html": "  * \n  \nWhen PyTorch saves tensors it saves their storage objects and tensor metadata separately. This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors. In the above snippet, for example, only a single storage is written to ‘tensors.pt’.\nIn some cases, however, saving the current storage objects may be unnecessary and create prohibitively large files. In the following snippet a storage much larger than the saved tensor is written to a file:\nWhen saving tensors with fewer elements than their storage objects, the size of the saved file can be reduced by first cloning the tensors. Cloning a tensor produces a new tensor with a new storage object containing only the values in the tensor:\nSince the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.\n```\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nInstead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, , to restore their states from a state dict:     \n\n\nWhen saving, PyTorch will ensure that the local file header of each file is padded to an offset that is a multiple of 64 bytes, ensuring that the offset of each file is 64-byte aligned.\nAs discussed in the documentation for , restricts the unpickler used in to only executing functions/building classes required for of plain as well as some other primitive types. Further, unlike the default provided by the module, the Unpickler is not allowed to dynamically import anything during unpickling.\nAs mentioned above, saving a module’s is a best practice when using . If loading an old checkpoint that contains an , we recommend . When loading a checkpoint that contains tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.\nTo get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use which will return a list of strings of the form . If you trust these functions/classes, you can import them and allowlist them per the error message either via or the context manager .\nA caveat is that analyzes the checkpoint statically, some types might be built dynamically during the unpickling process and hence will not be reported by . One such example is in numpy. In after allowlisting all the functions/classes reported by you might see an error like\nScriptModules can be serialized as a TorchScript program and loaded using . This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python).\nThe distinction between and may not be immediately clear. saves Python objects with pickle. This is especially useful for prototyping, researching, and training. , on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models.\nThe above module has an if statement that is not triggered by the traced inputs, and so is not part of the traced module and not serialized with it. The scripted module, however, contains the if statement and is serialized with it. See the for more on scripting and tracing.\nThe PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s \nThe behavior of is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see perform floor division when given two integer inputs even when loaded with newer versions of PyTorch. ScriptModules using and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.\nThe behavior of is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.full return float tensors by default, even when given bool or integer fill values. ScriptModules using and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.     \nRegisters callables for tagging and deserializing storage objects with an associated priority. Tagging associates a device with a storage object at save time while deserializing moves a storage object to an appropriate device at load time. and are run in the order given by their until a tagger/deserializer returns a value that is not .     \n\n              \nWithin the serialized format, each function is identified with its full path as . When calling this API, you can provide this full path that should match the one in the checkpoint otherwise the default will be used.\n```\n \n \n    \n   \n   \n     \n\n\n\n    \n     \n\n\n\n```\n         \n```\n \n \n    \n   \n   \n     \n\n\n\n     \n         \n\n\n   \n\n```\n    \nFor the save path, storages will still be saved, but the space that their bytes would usually be written to will be empty space. The storage bytes can then be populated in a separate pass.\n> \n\n>   * : If this config is set to , offsets for storages will be calculated rather than read via random reads when using . This minimizes random reads, which can be helpful when the file is being loaded over a network. (Default : )\n> \n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/optim.html": "To construct an you have to give it an iterable containing the parameters (all should be s) or named parameters (tuples of (str, )) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.  \ns also support specifying per-parameter options. To do this, instead of passing an iterable of s, pass in an iterable of key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.\nYou can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.\nAlso consider the following example related to the distinct penalization of parameters. Remember that returns an iterable that contains all learnable parameters, including biases and other parameters that may prefer distinct penalization. To address this, one can specify individual penalization weights for each parameter group:\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.     \nParameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.\nMany of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user.\nWe have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that.\nIn general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. We summarize the stability status for each implementation on the second table below, you are welcome to try them out though!\nMost learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling ) before the optimizer’s update (calling ), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling at the wrong time.\nThe function stores the optional content from the loaded state dict if present. However, the process of loading the optimizer state is not affected, as the order of the parameters matters to maintain compatibility (in case of different ordering). To utilize the loaded parameters names from the loaded state dict, a custom needs to be implemented according to the desired behavior.\nLet’s say that implements an expert (MoE), and we want to duplicate it and resume training for two experts, both initialized the same way as the layer. For the following we create two layers identical to and resume training by loading the model weights and optimizer states from into both and of (and adjust them accordingly):\nTo load the state dict for with the state dict of the previous optimizer such that both and will be initialized with a copy of optimizer states (to resume training for each layer from ), we can use the following hook:\nThis ensures that the adapted state_dict with the correct states for the layers of will be used during model loading. Note that this code is designed specifically for this example (e.g., assuming a single parameter group), and other cases might require different adaptations.\nThe following example shows how to handle missing parameters in a loaded when the model structure changes. The adds a new layer, which is not present in the original . To resume training, a custom hook is used to adapt the optimizer’s , ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged (as initialized in this example). This approach enables smooth loading and resuming of the optimizer state despite model changes. The new bypass layer will be trained from scratch:\nimplements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), implements the SWA learning rate scheduler and is a utility function used to update SWA/EMA batch normalization statistics at the end of training.\nDecay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to , the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.\nFor SWA and EMA, this call is usually done right after the optimizer . In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.\n  * allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the functions. This function must update the averaged parameters in-place.\n\n\nTypically, in SWA the learning rate is set to a high constant value. is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:\nassumes that each batch in the dataloader is either a tensors or a list of tensors where the first element is the tensor that the network should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the by doing a forward pass with the on each element of the dataset.\nIn the example below, is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:\nIn the example below, is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately.          \n\n\nThe utility assumes that each data batch in is either a tensor or a list or tuple of tensors; in the latter case it is assumed that should be called on the first element of the list or tuple corresponding to the data batch.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/profiler.html": "PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler’s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.            \n\n\nEnabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.     \n\n    \nPreset a user defined metadata when the profiler is not started and added into the trace file later. Metadata is in the format of a string key and a valid json value               \n\n\nUse to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.\nEnabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.\n```\n\n\n\n \n    \n         \n    \n\n \n    \n        \n        \n    \n\n    \n    \n    \n    \n    \n    \n    \n\n    \n        \n        \n        \n        \n    \n    \n    \n      \n           \n            \n            \n            \n\n```\n    \nReturns a callable that can be used as profiler argument. The profiler will skip the first steps, then wait for steps, then do the warmup for the next steps, then do the active recording for the next steps and then repeat the cycle starting with steps. The optional number of cycles is specified with the parameter, the zero value means that the cycles will continue until the profiling is finished.\nThe parameter controls whether the first stage should be skipped. This can be useful if a user wants to wait longer than between cycles, but not for the first profile. For example, if is 10 and is 20, the first cycle will wait 10 + 20 = 30 steps before warmup if is zero, but will wait only 10 steps if is non-zero. All subsequent cycles will then wait 20 steps between the last active and warmup.     \nOutputs tracing files to directory of , then that directory can be directly delivered to tensorboard as logdir. should be unique for each worker in distributed scenario, it will be set to ‘[hostname]_[pid]’ by default.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/sparse.html": "Now, some users might decide to represent data such as graph adjacency matrices, pruned weights or points clouds by Tensors whose . We recognize these are important applications and aim to provide performance optimizations for these use cases via sparse storage formats.  \nVarious sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been developed over the years. While they differ in exact layouts, they all compress data through efficient representation of zero valued elements. We call the uncompressed values in contrast to , compressed elements.\nBy compressing repeat zeros sparse storage formats aim to save memory and computational resources on various CPUs and GPUs. Especially for high degrees of sparsity or highly structured sparsity this can have significant performance implications. As such sparse storage formats can be seen as a performance optimization.\nPlease feel encouraged to open a GitHub issue if you analytically expected to see a stark increase in performance but measured a degradation instead. This helps us prioritize the implementation of efficient kernels and wider performance optimizations.\nIn the next example we convert a 2D Tensor with default dense (strided) layout to a 2D Tensor backed by the COO memory layout. Only values and indices of non-zero elements are stored in this case.\nWe currently offer a very simple version of batching where each component of a sparse format itself is batched. This also requires the same number of specified elements per batch entry. In this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.\nIn this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension from a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is not stored. If however any of the values in the row are non-zero, they are stored entirely. This reduces the number of indices since we need one index one per row instead of one per element. But it also increases the amount of storage for the values. Since only rows that are zero can be emitted and the presence of any non-zero valued elements cause the entire row to be stored.\nFundamentally, operations on Tensor with sparse storage formats behave the same as operations on Tensor with strided (or other) storage formats. The particularities of storage, that is the physical layout of the data, influences the performance of an operation but should not influence the semantics.\nAs shown in the example above, we don’t support non-zero preserving unary operators such as cos. The output of a non-zero preserving unary operation will not be able to take advantage of sparse storage formats to the same extent as the input and potentially result in a catastrophic increase in memory. We instead rely on the user to explicitly convert to a dense Tensor first and then run the operation.\nWe are aware that some users want to ignore compressed zeros for operations such as instead of preserving the exact semantics of the operation. For this we can point to torch.masked and its MaskedTensor, which is in turn also backed and powered by sparse storage formats and kernels.\nAlso note that, for now, the user doesn’t have a choice of the output layout. For example, adding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some users might prefer for this to stay a sparse layout, because they know the result will still be sufficiently sparse.\nWe acknowledge that access to kernels that can efficiently produce different output layouts can be very useful. A subsequent operation might significantly benefit from receiving a particular layout. We are working on an API to control the result layout and recognize it is an important feature to plan a more optimal path of execution for any given model.\nSparse semi-structured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.\nThis sparse layout stores elements out of every elements, with being determined by the width of the Tensor’s data type (dtype). The most frequently used dtype is float16, where , thus the term “2:4 structured sparsity.”\nIn PyTorch, semi-structured sparsity is implemented via a Tensor subclass. By subclassing, we can override , allowing us to use faster sparse kernels when performing matrix multiplication. We can also store the tensor in it’s compressed form inside the subclass to reduce memory overhead.\nThe specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single flat compressed tensor. They are appended to each other to form a contiguous chunk of memory.\n\n\nM_{dense} = r \\times c \\times e \\\\\\ M_{sparse} = M_{specified} + M_{metadata} = r \\times \\frac{c}{2} \\times e + r \\times \\frac{c}{2} \\times 2 = \\frac{rce}{2} + rc =rce(\\frac{1}{2} +\\frac{1}{e}) \nTo construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format. To do this we tile a small 1x4 strip to create a 16x16 dense float16 tensor. Afterwards, we can call function to compress it for accelerated inference.\n```\n   \n      \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nPyTorch implements the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,\nFor example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least bytes when using COO tensor layout and bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.\nA sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function .\nSuppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write:\n> \n\nWe use (M + K)-dimensional tensor to denote a N-dimensional sparse hybrid tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write\n> \n\nTo be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via keyword argument, or globally using context manager instance. By default, the sparse tensor invariants checks are disabled.\nPyTorch sparse COO tensor format permits sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, and , for the same index , that leads to an 1-D uncoalesced tensor:\nFor the most part, you shouldn’t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a sparse coalesced or uncoalesced tensor.\nWhen working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on a sparse uncoalesced tensor could be implemented by multiplying all the uncoalesced values with the scalar because holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because does not hold in general.\nIn PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, computes the softmax with the assumption that the fill value is negative infinity.\nSparse Compressed Tensors represents a class of sparse tensors that have a common feature of compressing the indices of a certain dimension using an encoding that enables certain optimizations on linear algebra kernels of sparse compressed tensors. This encoding is based on the \nWe use (B + M + K)-dimensional tensor to denote a N-dimensional sparse compressed hybrid tensor, where B, M, and K are the numbers of batch, sparse, and dense dimensions, respectively, such that holds. The number of sparse dimensions for sparse compressed tensors is always two, .\n\n\nTo be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via keyword argument, or globally using context manager instance. By default, the sparse tensor invariants checks are disabled.\nThe generalization of sparse compressed layouts to N-dimensional tensors can lead to some confusion regarding the count of specified elements. When a sparse compressed tensor contains batch dimensions the number of specified elements will correspond to the number of such elements per-batch. When a sparse compressed tensor has dense dimensions the element considered is now the K-dimensional array. Also for block sparse compressed layouts the 2-D block is considered as the element being specified. Take as an example a 3-dimensional block sparse tensor, with one batch dimension of length , and a block shape of . If this tensor has specified elements, then in fact we have blocks specified per batch. This tensor would have with shape . This interpretation of the number of specified elements comes from all sparse compressed layouts being derived from the compression of a 2-dimensional matrix. Batch dimensions are treated as stacking of sparse matrices, dense dimensions change the meaning of the element from a simple scalar value to an array with its own dimensions.\n>   * The tensor consists of compressed row indices. This is a 1-D tensor of size (the number of rows plus 1). The last element of is the number of specified elements, . This tensor encodes the index in and depending on where the given row starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given row.\n> \n\nThe index tensors and should have element type either (default) or . If you want to use MKL-enabled matrix operations, use . This is as a result of the default linking of pytorch being with MKL LP64, which uses 32 bit integer indexing.\nThe batches of sparse CSR tensors are dependent: the number of specified elements in all batches must be the same. This somewhat artificial constraint allows efficient storage of the indices of different CSR batches.\nWith the same example data of , the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least bytes when using CSR tensor layout. Notice the 1.6 and 310 fold savings from using CSR storage format compared to using the COO and strided formats, respectively.\nSparse CSR tensors can be directly constructed by using the function. The user must supply the row and column indices and values tensors separately where the row indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and if it is not present.\nThe values of sparse dimensions in deduced is computed from the size of and the maximal index value in . If the number of columns needs to be larger than in the deduced then the argument must be specified explicitly.\nThe simplest way of constructing a 2-D sparse CSR tensor from a strided or sparse COO tensor is to use method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:\nThe sparse CSC (Compressed Sparse Column) tensor format implements the CSC format for storage of 2 dimensional tensors with an extension to supporting batches of sparse CSC tensors and values being multi-dimensional tensors.\n>   * The tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified elements, . This tensor encodes the index in and depending on where the given column starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given column.\n> \n\nSparse CSC tensors can be directly constructed by using the function. The user must supply the row and column indices and values tensors separately where the column indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\nThe (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from any two-dimensional tensor using method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:\nThe sparse BSR (Block compressed Sparse Row) tensor format implements the BSR format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSR tensors and values being blocks of multi-dimensional tensors.\n>   * The tensor consists of compressed row indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified blocks, . This tensor encodes the index in and depending on where the given column block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given row.\n> \n\nSparse BSR tensors can be directly constructed by using the function. The user must supply the row and column block indices and values tensors separately where the row block indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\n```\n    \n     \n       \n                            \n                            \n                            \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\n```\n       \n                           \n                           \n                           \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThe sparse BSC (Block compressed Sparse Column) tensor format implements the BSC format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSC tensors and values being blocks of multi-dimensional tensors.\n>   * The tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified blocks, . This tensor encodes the index in and depending on where the given row block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given column.\n> \n\nSparse BSC tensors can be directly constructed by using the function. The user must supply the row and column block indices and values tensors separately where the column block indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\n```\n    \n     \n       \n                            \n                            \n                            \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nAll sparse compressed tensors — CSR, CSC, BSR, and BSC tensors — are conceptionally very similar in that their indices data is split into two parts: so-called compressed indices that use the CSR encoding, and so-called plain indices that are orthogonal to the compressed indices. This allows various tools on these tensors to share the same implementations that are parameterized by tensor layout.\nSparse CSR, CSC, BSR, and CSC tensors can be constructed by using function that have the same interface as the above discussed constructor functions , , , and , respectively, but with an extra required argument. The following example illustrates a method of constructing CSR and CSC tensors using the same input data by specifying the corresponding layout parameter to the function:\nThe following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here denotes a tensor with a given layout. Similarly, denotes a matrix (2-D PyTorch tensor), and denotes a vector (1-D PyTorch tensor). In addition, denotes a scalar (float or 0-D PyTorch tensor), is element-wise multiplication, and is matrix multiplication.\nIf you find that we are missing a zero-preserving unary function that you need, please feel encouraged to open an issue for a feature request. As always please kindly try the search function first before opening an issue.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/py-modindex.html": "If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like ) you must provide an file. \n",
  "https://docs.pytorch.org/docs/stable/storage.html": "\n  \nA is a contiguous, one-dimensional array of elements. Its length is equal to the number of bytes of the tensor. The storage serves as the underlying data container for tensors. In general, a tensor created in PyTorch using regular constructors such as , or will produce tensors where there is a one-to-one correspondence between the tensor storage and the tensor itself.\nHowever, a storage is allowed to be shared by multiple tensors. For instance, any view of a tensor (obtained through or some, but not all, kinds of indexing like integers and slices) will point to the same underlying storage as the original tensor. When serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors continue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage can be faster than deserializing multiple independent tensors.\nA tensor storage can be accessed through the method. This will return an object of type . Fortunately, storages have a unique identifier called accessed through the method. In regular settings, two tensors with the same data storage will have the same storage . However, tensors themselves can point to two separate storages, one for its data attribute and another for its grad attribute. Each will require a of its own. In general, there is no guarantee that a and match and this should not be assumed to be true.\nUntyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors with different dtypes or shape can point to the same storage. It also implies that a tensor storage can be changed, as the following example shows:\nPlease note that directly modifying a tensor’s storage as shown in this example is not a recommended practice. This low-level manipulation is illustrated solely for educational purposes, to demonstrate the relationship between tensors and their underlying storages. In general, it’s more efficient and safer to use standard methods, such as and , to achieve the same results.\nOther than , untyped storage also have other attributes such as (in case the storage points to a file on disk), or for device checks. A storage can also be manipulated in-place or out-of-place with methods like , or . FOr more information, check the API reference below. Keep in mind that modifying storages is a low-level API and comes with risks! Most of these APIs also exist on the tensor level: if present, they should be prioritized over their storage counterparts.\nWe mentioned that a tensor that has a non-None attribute has actually two pieces of data within it. In this case, will return the storage of the attribute, whereas the storage of the gradient can be obtained through .     \n\n              \nis the number of elements in the storage. If is , then the file must contain at least bytes ( is the type of storage, in the case of an the file must contain at least bytes). If is the file will be created if needed.               \nThis is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.\nWhen all references to a storage in shared memory are deleted, the associated shared memory object will also be deleted. PyTorch has a special cleanup process to ensure that this happens even if the current process exits unexpectedly.\n  1. will call on the object after mapping it to make sure the shared memory object is freed when no process has the object open. does not unlink the file. This file is persistent and will remain until it is deleted by the user.\n\n\nFor historical context, PyTorch previously used typed storage classes, which are now deprecated and should be avoided. The following details this API in case you should encounter it, although its usage is highly discouraged. All storage classes except for will be removed in the future, and will be used in all cases.\nA is a contiguous, one-dimensional array of elements of a particular . It can be given any , and the internal data will be interpreted appropriately. contains a which holds the data as an untyped array of bytes.                                        \n  * (, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/rpc.html": "CUDA support was introduced in PyTorch 1.9 and is still a feature. Not all features of the RPC package are yet compatible with CUDA support and thus their use is discouraged. These unsupported features include: RRefs, JIT compatibility, dist autograd and dist optimizer, and profiling. These shortcomings will be addressed in future releases.  \nThe distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.\n  1. supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs: (synchronous), (asynchronous), and (asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. The API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case, and are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future.\n  2. serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is neither the creator (the caller of ) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases.\n  3. stitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass.\n  4. ’s constructor takes a (e.g., , , etc.) and a list of parameter RRefs, creates an instance on each distinct RRef owner, and updates parameters accordingly when running . When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor and API.\n\n         \n  * () – The options passed to the RpcAgent constructor. It must be an agent-specific subclass of and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using , meaning that environment variables and need to be set properly. See for more information and find which options are available.\n\n\nThe following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a as an argument or a return value, the destination worker will try to create a with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.\nTorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.          \n  * () – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n         \nMake a non-blocking RPC call to run function on worker . RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a that can be awaited on.     \n  * () – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n\nUsing GPU tensors as arguments or return values of is not supported since we don’t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of .\nThe API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned completes.          \nMake a remote call to run on worker and return an to the result value immediately. Worker will be the owner of the returned , and the worker calling is a user. The owner manages the global reference count of its , and the owner is only destructed when globally there are no living references to it.     \n  * () – timeout in seconds for this remote call. If the creation of this on worker is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as ), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n\nThe API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the API.\nErrors such as timeouts for the API are handled on a best-effort basis. This means that when remote calls initiated by fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as or fork call), then future uses of the will appropriately raise errors. However, it is possible that the user application will use the before the errors are handled. In this case, errors may not be raised as they have not yet been handled.     \nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If , this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if , this is a local shutdown, and it does not wait for other RPC processes to reach this method.          \nA structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through and the result can be passed in to functions such as , , to avoid copying a string on every invocation.     \nA decorator for a function indicating that the return value of the function is guaranteed to be a object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the returned by the wrapped function and installs subsequent processing steps as a callback to that . The installed callback will read the value from the when completed and send the value back as the RPC response. That also means the returned only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function’s () execution needs to pause and resume due to, e.g., containing or waiting for other signals.\nTo enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with or , needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by .     \n```\n   \n\n\n\n\n\n    \n    \n    \n    \n    \n    \n    \n    \n        \n            \n    \n\n\n  \n    \n    \n       \n\n  \n\n```\n\nThe RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the function, by passing a certain value of the enum. Regardless of what backend is used, the rest of the RPC API won’t change. Each backend also defines its own subclass of the class, an instance of which can also be passed to to configure the backend’s behavior.     \nAn abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to in order to initialize RPC with specific configurations, such as the RPC timeout and to be used.\nThe TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.          \n  * () – The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in and if necessary.\n  * () – Device placement mappings from this worker to the callee. Key is the callee worker name and value the dictionary ( of , , or ) that maps this worker’s devices to the callee worker’s devices. (default: )\n  * (List[int, str, or ], optional) – all local CUDA devices used by RPC agent. By Default, it will be initialized to all local devices from its own and corresponding devices from its peers’ . When processing CUDA RPC requests, the agent will properly synchronize CUDA streams for all devices in this .\n\n    \n```\n\n  \n      \n          \n\n\n  \n    \n      \n\n\n  \n\n\n\n    \n    \n    \n    \n    \n\n\n  \n     \n\n\n\n\n  \n  \n\n```\n    \nThe RPC framework does not automatically retry any , and calls. The reason being that there is no way the RPC framework can determine whether an operation is idempotent or not and whether it is safe to retry. As a result, it is the application’s responsibility to deal with failures and retry if necessary. RPC communication is based on TCP and as a result failures could happen due to network failures or intermittent network connectivity issues. In such scenarios, the application needs to retry appropriately with reasonable backoffs to ensure the network isn’t overwhelmed by aggressive retries.\nAn (Remote REFerence) is a reference to a value of some type (e.g. ) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See for more details.     \nA class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker. A will be deleted when 1) no references to it in both the application code and in the local RRef context, or 2) the application has called a graceful shutdown. Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation only offers best-effort error detection, and applications should not use after .          \n> Runs the backward pass using the RRef as the root of the backward pass. If is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, should be used to retrieve the gradients. If is , it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.     \n  * () – If , the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to to run backward multiple times (default: False).\n\n    \nCreate a helper proxy to easily launch a using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the creation of this is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as ), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see for specific timeout semantics for .     \nCreate a helper proxy to easily launch an using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.     \nCreate a helper proxy to easily launch an using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.     \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.\nis an easy way to create an nn.Module remotely on a different process. The actual module resides on a remote host, but the local host has a handle to this module and invoke this module similar to a regular nn.Module. The invocation however incurs RPC calls to the remote end and can be performed asynchronously if needed via additional APIs supported by RemoteModule.     \n> It creates a user-specified module on a specified remote node. It behaves like a regular except that the method is executed on the remote node. It takes care of autograd recording to ensure the backward pass propagates gradients back to the corresponding remote module.     \nThis module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see .     \nKicks off the distributed backward pass using the provided roots. This currently implements the which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.\nWe accumulate the gradients in the appropriate on each of the nodes. The autograd context to be used is looked up given the that is passed in when is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the API.     \n  * () – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.\n\n    \nContext object to wrap forward and backward passes when using distributed autograd. The generated in the statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this , which is required to correctly execute a distributed autograd pass.     \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/random.html": "           \n  * () – devices for which to fork the RNG. CPU RNG state is always forked. By default, operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed\n\n    \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/size.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/testing.html": "      \nIf and are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are checked individually. Indices, namely for COO, and for CSR and BSR, or and for CSC and BSC layouts, respectively, are always checked for equality whereas the values are checked for closeness according to the definition above.     \n  * () – Optional error message to use in case a failure occurs during the comparison. Can also passed as callable in which case it will be called with the generated message and should return the new message.\n\n    \n\n\n```\n\n   \n   \n  \n  \n  \n\n\n       \n       \n \n\n```\n\n```\n    \n  \n\n \n\n\n      \n\n\n\n: \n\n\n \n\n\n: \n\n\n\n  \n\n```\n\n```\n    \n    \n\n  \n\n\n: \n\n\n\n        \n\n\n\n: \n\n\n\n\n\n\n\n\n\n```\n    \nIf or are specified and are outside the range of the ’s representable finite values then they are clamped to the lowest or highest representable finite value, respectively. If , then the following table describes the default values for and , which depend on .     \n  * () – Sets the lower limit (inclusive) of the given range. If a number is provided it is clamped to the least representable finite value of the given dtype. When (default), this value is determined based on the (see the table above). Default: .\n  * Sets the upper limit (exclusive) of the given range. If a number is provided it is clamped to the greatest representable finite value of the given dtype. When (default) this value is determined based on the (see the table above). Default: .\n  * () – If then zeros are replaced with the dtype’s small positive value depending on the . For bool and integer types zero is replaced with one. For floating point types it is replaced with the dtype’s smallest positive normal number (the “tiny” value of the ’s object), and for complex types it is replaced with a complex number whose real and imaginary parts are both the smallest positive normal number representable by the complex type. Default .\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/signal.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/special.html": "           \n\\begin{align} \\text{entr(x)} = \\begin{cases} -x * \\ln(x) & x > 0 \\\\\\ 0 & x = 0.0 \\\\\\ -\\infty & x < 0 \\end{cases} \\end{align}                                                                                 \n  * (, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to before the operation is performed. This is useful for preventing data type overflows. Default: None.\n\n    \nReturns a new tensor with the logit of the elements of . is clamped to [eps, 1 - eps] when eps is not None. When eps is None and < 0 or > 1, the function will yields NaN.\n\\begin{align} y_{i} &= \\ln(\\frac{z_{i}}{1 - z_{i}}) \\\\\\ z_{i} &= \\begin{cases} x_{i} & \\text{if eps is None} \\\\\\ \\text{eps} & \\text{if } x_{i} < \\text{eps} \\\\\\ x_{i} & \\text{if } \\text{eps} \\leq x_{i} \\leq 1 - \\text{eps} \\\\\\ 1 - \\text{eps} & \\text{if } x_{i} > 1 - \\text{eps} \\end{cases} \\end{align}                                    \n  * (, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to before the operation is performed. This is useful for preventing data type overflows. Default: None.\n\n    \n\\text{out}_{i} = \\begin{cases} \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\\ 0 & \\text{if } \\text{input}_{i} = 0.0 \\text{ and } \\text{other}_{i} != \\text{NaN} \\\\\\ \\text{input}_{i} * \\text{log1p}(\\text{other}_{i})& \\text{otherwise} \\end{cases}      \n\\text{out}_{i} = \\begin{cases} \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\\ 0 & \\text{if } \\text{input}_{i} = 0.0 \\\\\\ \\text{input}_{i} * \\log{(\\text{other}_{i})} & \\text{otherwise} \\end{cases}      \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org": "> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).  \n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch_environment_variables.html": "PyTorch leverages environment variables for adjusting various settings that influence its runtime behavior. These variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels, specifying the number of threads for parallel processing tasks and many more.  \nMoreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN, which also utilize environment variables to modify their functionality. This interplay of settings allows for a highly customizable development environment that can be optimized for efficiency, debugging, and computational resource management.\nPlease note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive. If you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/utils.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/xpu.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.overrides.html": "           \nA tuple of functions that are publicly available in the torch API but cannot be overridden with . Mostly this is because none of the arguments of these functions are tensors or tensor-likes.               \nA dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines .          \nCheck for __torch_function__ implementations in the elements of an iterable or if a __torch_function__ mode is enabled. Considers exact s and s non-dispatchable. Use this to guard a call to ; don’t use it to test if something is Tensor-like, use instead. :param relevant_args: Iterable or arguments to check for __torch_function__ methods. :type relevant_args: iterable               \nThis decorator may reduce the performance of your code. Generally, it’s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you’re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/type_info.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.compiler.html": "is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. is written in Python and it marks the transition of PyTorch from C++ to Python.  \n  * is the default deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through possible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key building block.\n\n\nAs mentioned above, to run your workflows faster, through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as , TorchDynamo has a list of supported backends developed by our partners, which can be see by running each of which with its optional dependencies.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.html": "The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.  \nWithin the PyTorch repo, we define an “Accelerator” as a that is being used alongside a CPU to speed up computation. These device use an asynchronous execution scheme, using and as their main way to perform synchronization. We also assume that only one such accelerator can be available at once on a given host. This allows us to use the current accelerator as the default device for relevant concepts such as pinned memory, Stream device_type, FSDP, etc.\nMany tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading or intra-op parallelism), it is thus important to delay as much as possible any operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect. In practice, you should keep in mind that checking is a compile-time check by default, it is thus always fork-safe. On the contrary, passing the flag to this function or calling will usually prevent later fork.\nReturns a tensor where each row contains indices sampled from the multinomial (a stricter definition would be multivariate, refer to for more details) probability distribution located in the corresponding row of tensor .  \n---  \n\n\nThe context managers , , and are helpful for locally disabling and enabling gradient computation. See for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the module, etc.\nReturns a namedtuple where is the mode value of each row of the tensor in the given dimension , i.e. a value which appears most often in that row, and is the index location of each mode value found.  \n---  \nReturns the indices of the lower triangular part of a -by- matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \n---  \nReturns the indices of the upper triangular part of a by matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \nComputes the QR decomposition of a matrix or a batch of matrices , and returns a namedtuple (Q, R) of tensors such that with being an orthogonal matrix or batch of orthogonal matrices and being an upper triangular matrix or batch of upper triangular matrices.  \n---                 \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/community/index.html": "If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like ) you must provide an file. \n",
  "https://docs.pytorch.org/docs/stable/notes/amp.html": "If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like ) you must provide an file. \n",
  "https://docs.pytorch.org/docs/stable/notes/index.html": "If this is your site, make sure that the filename case matches the URL as well as any file permissions. For root URLs (like ) you must provide an file. \n",
  "https://docs.pytorch.org/docs/stable/autograd.html": "It requires minimal changes to the existing code - you only need to declare s for which gradients should be computed with the keyword. As of now, we only support autograd for floating point types ( half, float, double and bfloat16) and complex types (cfloat, cdouble).  \nThis API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as you can use it as .\nSee for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see for a list of functions that can be used to locally disable gradients.\nThe default behavior (letting s be before the first , such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to or will not affect layouts.\nIf you need manual control over ’s strides, assign a zeroed tensor with desired strides before the first , and never reset it to . 3 guarantees your layout is preserved as long as . 4 indicates your layout is preserved even if .\nSupporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\nAll s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.\nThe Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with set to . Below please find a quick guide on what has changed:     \nAutograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using . nvprof based (registers both CPU and GPU activity) using . and vtune profiler based using .     \nUnder the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks     \n  * () – If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.\n  * () – If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operator’s input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.\n\n\n```\n    \n   \n         \n            \n        \n\n\n\nName                                 Self CPU total   CPU time avg     Number of Calls\n\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\n\n\n\n\n```\n    \nUnfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or can load the results for inspection e.g. in Python REPL.     \n  * () – If , the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: Non-tensor arguments will be represented by . Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: \n\n\nWhen viewing a profile created using in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, appends sequence number information to the ranges it generates.\nDuring the forward pass, each function range is decorated with . is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s call is decorated with . is the sequence number that the backward object was created with. By comparing numbers in backward with numbers in forward, you can track down which forward op created each backward Function.\nAny functions executed during the backward pass are also decorated with . During default backward (with ) this information is irrelevant, and in fact, may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ methods are useful, as a way to correlate these Function objects with the earlier forward pass.\nIf, on the other hand, a backward pass with is underway (in other words, if you are setting up for a double-backward), each function’s execution during backward is given a nonzero, useful . Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ ranges are still tagged with numbers, which can be compared to numbers from the backward pass.     \nThe Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI.     \n  * () – If , the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: Non-tensor arguments will be represented by . Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: \n\n    \n\n\n```\n \n   \n \n    \n      \n         \n    \n      \n        \n         \n         \n \n      \n     \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n    \nThe attribute of a holds a if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or otherwise.\nSome operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the and can be accessed. For example:\n```\n     \n  \n \n\n\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n \n\n\n```\n\nYou can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see .     \nIn that context, the function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using but also those recorded by a PyTorch-defined operation). The output of is then stored in the computation graph instead of the original tensor.\nThe is called when the saved tensor needs to be accessed, namely when executing or . It takes as argument the object returned by and should return a tensor which has the same content as the original tensor (passed as input to the corresponding ).     \nWhen performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.\n```\n    \n    \n    \n\n   \n                   \n     \n              \n                   \n     \n\n    \n     \n\n\n  \n\n\n```\n         \nUnder the mode, the hook will be called after gradients with respect to every tensor in have been computed. If a tensor is in but is not part of the graph, or if a tensor is not needed to compute the gradients for any specified for the current or call, this tensor will be ignored and the hook will not wait for its gradient to be computed.     \nUnder this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it’s used during backward.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.optim.html": "exposes DistributedOptimizer, which takes a list of remote parameters () and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer to apply the gradients on each worker.       \nConcurrent calls to , either from the same or different clients, will be serialized on each worker – as each worker’s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.\ncreates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow      \n\n    \nThis will call on each worker containing parameters to be optimized, and will block until all workers return. The provided will be used to retrieve the corresponding that contains the gradients that should be applied to the parameters.     \n```\n \n   \n   \n   \n   \n   \n  \n  \n\n\n  \n     \n\n\n\n    \n \n\n\n\n\n   \n  \n    \n     \n\n\n\n\n\n    \n   \n      \n   \n   \n\n```\n         \nThe local optimizer instance in each rank is only responsible for updating approximately parameters and hence only needs to keep optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. can be used in conjunction with to reduce per-rank peak memory consumption.\nuses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.     \n  * () – if , is overlapped with ‘s gradient synchronization; this requires (1) either a functional optimizer for the argument or one with a functional equivalent and (2) registering a DDP communication hook constructed from one of the functions in ; parameters are packed into buckets matching those in , meaning that the argument is ignored. If , runs disjointly after the backward pass (per normal). (default: )\n\n\nIf you pass , be wary of the following: Given the way that overlapping with is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if or , respectively. This is because it needs information about the gradient bucketing strategy used by , which is not finalized until the second forward pass if or until the third forward pass if . To adjust for this, one option is to prepend dummy inputs.     \nThis method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.                         \nand this method is called before this instance has been fully initialized, which happens once gradient buckets have been rebuilt; or if this method is called without a preceding call to .     \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.elastic.html": "To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.checkpoint.html": "\n           \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform writes. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n\n\nsave_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .          \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform ‘stage’ and ‘save’. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n\n    \nEach rank must have the same keys in their provided to this API. Mismatched keys may result in hangs or errors. If unsure, you can use the API to check (but may incur communication costs).\nFor each object (having both a and a ), load will first call before attempting deserialization, followed by once the deserialization is complete. For each non- object, load will deserailize the object, and then replace it in the with the deserialized object.     \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform reads. If this is not specified, DCP will automatically infer the reader based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n\n\nload_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .     \nThis protocol is meant to provide customization and extensibility for dcp.async_save, allowing users to customize how data is staged previous to executing the usual dcp.save path in parallel. The expected order of operations (concretely defined in ) is the following:\n  1.     \nThis call gives the AsyncStager the opportunity to ‘stage’ the state_dict. The expectation and purpose of staging in this context is to create a “training-safe” representation of the state dict, meaning that any updates to module data after staging is complete should not be reflected in the state dict returned from this method. For example, in the default case a copy of the entire state dict is created on CPU RAM and returned here, allowing users to continue training without risking changes to data which is being serialized.\n  2.     \nthe serialization thread starts and before returning from dcp.async_save. If this is set to False, the assumption is the user has defined a custom synchronization point for the the purpose of further optimizing save latency in the training loop (for example, by overlapping staging with the forward/backward pass), and it is the respondsibility of the user to call at the appropriate time.\n\n    \nAn implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete. This implementation also provides an option to optimize stage latency using pinned memory.                                   \nCalls to indicates a brand new checkpoint read is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint read. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage.     \n() – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is more like a key-value store. (Default: )                         \nCalls to indicates a brand new checkpoint write is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint write. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage.     \n() – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )     \nReturn the storage-specific metadata. This is used to store additional information in a checkpoint that can be useful for providing request-level observability. StorageMeta is passed to the during save calls. Returns None by default.          \n\n\nRewriting state_dict. This is the simplest way to extend the load process as it doesn’t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place     \nThe provided tensor is the same one returned by the call to . This method is only needed if this LoadPlanner needs to post process prior to copying it back to the one in the state_dict.     \nThe tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that’s not possible, the planner can use the method to copy the data back to the one in state_dict.     \n\n\nFinally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:                    \nflatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode allow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.\nDue to legacy design decisions, the state dictionaries of and may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).\nTo tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.     \ncan process any module that is parallelized by PyTorch FSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any combination of these parallelisms. The main functions of are: 1.) returning a model and optimizer state_dict that can be resharded with a different number of trainers and/or different parallelisms. 2.) hiding the parallelism-specific state_dict APIs. Users don’t have to call these APIs. 3.) sanity checking the result state_dict.\nThe keys of the result state dictionary are the canonical FQNs (Fully Qualified Names). A canonical FQN refers to the FQN based on a parameter’s position in an nn.Module hierarchy. More specifically, a canonical FQN to a parameter is the FQN returned by or when the module is not distributed by any parallelisms. Since the optimizer internally uses parameter IDs to represent a parameter, there will be a conversion from the parameter IDs to the canonical FQNs when calling this API.     \n\n         \n\n         \n\n    \nThe counterpart of to set the state_dict to the model and optimizers. The given and do not have to be returned by but must meet the following requirements: 1) all FQNs are canonical FQNs as defined in , 2) if a tensor is sharded, it must be either a ShardedTensor or DTensor, 3) optimizer state_dict cannot contain the parameter IDs; the keys should be the canonical FQNs.     \n  * () – (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]): the model state_dict to load. If the key of the is nn.Module, the key is a submodule of and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.\n\n         \n  * () – (Dict[str, ValueType]): the model state_dict to load. If the key of the is nn.Module, the key is a submodule of and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.\n\n         \n\n    \n  * (deprecated): when is not None, this option indicates whether to keep the submodule prefixes from the state_dict keys. or example, if the submodule is and the full FQN of the parameter is of the param. When this option is True, the parameter’s key in the returned state_dict will be . If the options is False, the key will be . Note that if is False, there may be conflicted FQNs, hence there should be only one submodule in .\n  *     \nfull state_dict and will broadcast the tensors in the state_dict/ optim_state_dict one by one to other ranks. Other ranks will receive the tensors and shard according to the local shards in the model and optimizer. must be set to True when using this option. This option currently only supports DTensor, not the legacy ShardedTensor.\n\n                        \nExtension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict, avoiding the need to read metadata from disk. This is useful when reading formats which don’t have a metadata file, like Torch Save files.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html": "\n  \n  * FSDP2 uses -based dim-0 per-parameter sharding for a simpler sharding representation compared to FSDP1’s flat-parameter sharding, while preserving similar throughput performance. More specifically, FSDP2 chunks each parameter on dim-0 across the data parallel workers (using ), whereas FSDP1 flattens, concatenates, and chunks a group of tensors together, making reasoning about what data is present on each worker and resharding to different parallelisms complex. Per-parameter sharding provides a more intuitive user experience, relaxes constraints around frozen parameters, and allows for communication-free (sharded) state dicts, which otherwise require all-gathers in FSDP1.\n  * FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly support full state dicts. Instead, users can reshard the sharded state dicts containing s to full state dicts themselves using APIs like or by using higher-level APIs like ‘s distributed state dict APIs. Also, some other args have been removed; see \n\n    \nAt initialization, FSDP shards the module’s parameters across the data parallel workers given by . Before forward, FSDP all-gathers the sharded parameters across the data-parallel workers to get the unsharded parameters for forward computation. If is , then FSDP frees the unsharded parameters after forward and re-all-gathers them in backward before gradient computation. After gradient computation, FSDP frees the unsharded parameters and reduce-scatters the unsharded gradients across data-parallel workers.\nThis implementation represents the sharded parameters as s sharded on dim-0, while the unsharded parameters will be like the original parameters on (e.g. if originally ). A module on all-gathers the parameters, and a module on frees them (if needed). Similar backward hooks all-gather parameters and later free parameters and reduce-scatter gradients.\nSince grouping multiple tensors together for one collective is critical for communication efficiency, this implementation makes this grouping first class. Calling on constructs one group that includes the parameters in except those already assigned to a group from an earlier call on a submodule. This means that should be called bottom-up on your model. Each group’s parameters are all-gathered in one collective, and its gradients are reduce-scattered in one collective. Partitioning the model into multiple groups (“layer by layer”) allows for peak memory savings and communication/computation overlap. Users generally should call only on the topmost root module.     \n  * () – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.\n  *     * If an , then this represents the world size to reshard to after forward. It should be a non-trivial divisor of the shard dim size (i.e. excluding 1 and the dim size itself). A choice may be the intra-node size (e.g. ). This allows the all-gather in backward to be over a smaller world size at the cost of higher memory usage than setting to .\n    * After forward, the parameters registered to the module depend on to this: The registered parameters are the sharded parameters if ; unsharded parameters if ; and the paramters resharded to the smaller mesh otherwise. To modify the parameters between forward and backward, the registered parameters must be the sharded parameters. For or an , this can be done by manually resharding via .\n  * () – This callable can be used to override the sharding placement for a parameter to shard a parameter on a dimension other than dim-0. If this callable returns a placement (not ), then FSDP will shard according to that placement (e.g. ). If sharding on a nonzero dim, we currently require even sharding, i.e. the tensor dim size on that dim must be divisible by the FSDP shard mesh size.\n\n\nCalling dynamically constructs a new class that subclasses and an FSDP class . For example, if we call on a module , then FSDP constructs a new class and changes ‘s type to this. Otherwise, does not change the module structure and parameter fully-qualified names. The class allows providing some FSDP-specific methods on the module.               \n  * () – Stream to run the all-reduce hook in. This should only be set if not using native HSDP. If using native HSDP, the hook will run in the internally defined all-reduce stream used by the native HSDP all-reduce.\n\n    \nSets whether the next backward is the last one. On the last backward, FSDP waits on pending gradient reduction and clears internal data data structures for backward prefetching. This can be useful for microbatching.     \nSets the FSDP modules for which this FSDP module should explicitly prefetch all-gathers in backward. This overrides the default backward pretching implementation that prefetches the next FSDP module based on the reverse post-forward order.\nPassing a singleton list containing the previous FSDP module gives the same all-gather overlap behavior as the default overlap behavior. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory.     \nPassing a singleton list containing the next FSDP module gives the same all-gather overlap behavior as the default overlap behavior, except the prefetched all-gather is issued earlier from the CPU. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory.     \nBy default, the root FSDP module waits the all-gather streams on the current stream to ensure that the optimizer step has finished before all-gathering. However, this may introduce false dependencies if there is unrelated computation after the optimizer step. This API allows the user to provide their own event to wait on. After the root waits on the event, the event is discarded, so this API should be called with a new event each iteration.     \nSets if the module should reshard parameters after backward. This can be used during gradient accumulation to trade off higher memory for reduced communication since the unsharded parameters do not need to be re-all-gathered before the next forward.     \nSets whether the FSDP module’s parameters need to be unsharded in backward. This can be used in expert cases when the user knows that all parameters in this FSDP module’s parameter group are not needed for backward computation (e.g. embedding).          \nFSDP all-gathers parameters pre-forward and optionally frees parameters post-forward (depending on ). FSDP only knows to do this for by default. This function patches a user-specified method to run the pre/post-forward hooks before/after the method, respectively. If is not an , then this is a no-op.     \nThis configures FSDP’s mixed precision. Unlike autocast, this applies mixed precision at the module level, not op level, which means low-precision activations are saved for backward and high-to-low-precision casts are incurred only at module boundaries.\nFSDP works well with module-level mixed precision since it keeps the high-precision sharded parameters in memory anyway. In other words, FSDP does not require any extra memory to keep a high-precision copy of the parameters for the optimizer step.     \n  * () – This specifies the dtype for the unsharded parameter and hence the dtype for forward/backward computation and the parameter all-gather. If this is , then the unsharded parameter uses the original dtype. The optimizer step uses the sharded parameter in the original dtype. (Default: )\n  * () – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is but is not , then the reduction uses the compute dtype. This can be used to run gradient reduction in full precision while using low precision for compute. If also gradient reduction is disabled via , then FSDP will accumulate gradients using . (Default: )\n\n    \nThis offload policy offloads parameters, gradients, and optimizer states to CPU. Sharded parameters are copied host-to-device before all-gather. The all-gathered parameters are freed according to . Sharded gradients are copied device-to-host in backward, and the optimizer step runs on CPU with CPU optimizer states.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.algorithms.join.html": "The generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: , , and . For a tutorial, see .       \nThe context manager requires that all attributes in the objects are the same. If there are multiple objects, then the of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if is enabled, both of which using an all- reduce.     \n\n                   \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cuda.html": "      \nSome operations could be implemented using more than one library or more than one technique. For example, a GEMM could be implemented for CUDA or ROCm using either the cublas/cublasLt libraries or hipblas/hipblasLt libraries, respectively. How does one know which implementation is the fastest and should be chosen? That’s what TunableOp provides. Certain operators have been implemented using multiple strategies as Tunable Operators. At runtime, all strategies are profiled and the fastest is selected for all subsequent operations.\nThese APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must ensure that their system is appropriately configured to use GPUDirect Storage per the \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.html": "supports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.  \nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)\n  *     * Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)\n\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):\nIf you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: . The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.\nYou may also use to get more details about a specific aspect of NCCL. For example, would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set to inspect the detailed detection result and save as reference if further help from NCCL team is needed.\n- NCCL performs automatic tuning based on its topology detection to save users’ tuning effort. On some socket-based systems, users may still try tuning and to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP.\nThe package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by and in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.\n  * Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.\n  * Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.\n\n\nInitialization is not thread-safe. Process group creation should be performed from a single thread, to prevent inconsistent ‘UUID’ assignment across ranks, and to prevent races during initialization that can lead to hangs.               \n  * () – The backend to use. Depending on build-time configurations, valid values include , , , , or one that is registered by a third-party plugin. Since 2.6, if is not provided, c10d will use a backend registered for the device type indicated by the kwarg (if provided). The known default registrations today are: for , for . If neither nor is provided, c10d will detect the accelerator on the run-time machine and use a backend registered for that detected accelerator (or ). This field can be given as a lowercase string (e.g., ), which can also be accessed via attributes (e.g., ). If using multiple processes per machine with backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlock or NCCL invalid usage. backend is experimental.\n  * () – Timeout for operations executed against the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends. This is the duration after which collectives will be aborted asynchronously and the process will crash. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\n  * () – process group options specifying what additional options need to be passed in during the construction of specific process groups. As of now, the only options we support is for the backend, can be specified so that the nccl backend can pick up high priority cuda streams when there’re compute kernels waiting. For other availble options to config nccl, See \n  * () – a single, specific device to “bind” this process to, allowing for backend-specific optimizations. Currently this has two effects, only under NCCL: the communicator is immediately formed (calling immediately rather than the normal lazy call) and sub-groups will use when possible to avoid unnecessary overhead of group creation. If you want to know NCCL initialization error early, you can also use this field.\n\n\nSupport for multiple backends is experimental. Currently when no backend is specified, both and backends will be created. The backend will be used for collectives with CPU tensors and the backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g. “cpu:gloo,cuda:custom_backend”.     \nfollows SPMD programming model, meaning the same PyTorch Python program runs on all processes/ranks in the cluster. Ensure (the dimensions of the nD array describing device layout) is identical across all ranks. Inconsistent may lead to hanging.     \n  * () – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of . Each string in must be unique.\n\n    \nThe existence of environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..\nThere are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired . The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.\nAnother initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired . The URL should start with and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next call on the same file path/name.\nThis method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time is called.\n\n              \n\n              \nThe simplest pattern to follow is to destroy every process group and backend by calling with the default value of None for the argument, at a point in the training script where communications are no longer needed, usually near the end of main(). The call should be made once per trainer-process, not at the outer process-launcher level.\nif is not called by all ranks in a pg within the timeout duration, especially when there are multiple process-groups in the application e.g. for N-D parallelism, hangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort, which must be called collectively, but the order of calling ProcessGroupNCCL’s destructor if called by python’s GC is not deterministic. Calling helps by ensuring ncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort during ProcessGroupNCCL’s destructor.\ncan also be used to destroy individual process groups. One use case could be fault tolerant training, where a process group may be destroyed and then a new one initialized during runtime. In this case, it’s critical to synchronize the trainer processes using some means other than torch.distributed primitives _after_ calling destroy and before subsequently initializing. This behavior is currently unsupported/untested, due to the difficulty of achieving this synchronization, and is considered a known issue. Please file a github issue or RFC if this is a use case that’s blocking you.\nBy default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).     \nThis function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\nWhen using async variants of torch.distributed communication APIs, a work object is returned and the communication kernel is enqueued on a separate CUDA stream, allowing overlap of communication and computation. Once one or more async ops have been issued on one process group, they must be synchronized with other cuda streams by calling before using another process group.     \n  * () – The backend to use. Depending on build-time configurations, valid values are and . By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., ), which can also be accessed via attributes (e.g., ). If is passed in, the backend corresponding to the default process group will be used. Default is .\n  * () – process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the backend, can be specified so that process group can pick up high priority cuda streams. For other availble options to config nccl, See \n\n         \nDeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators). It allows user to easily create inter node and intra node process groups without worrying about how to set up the ranks correctly for different sub process groups, and it helps manage those distributed process group easily. function can be used to create new DeviceMesh, with a mesh shape describing the device topology.     \nDeviceMesh represents a mesh of devices, where layout of devices could be represented as a n-d dimension array, and each value of the n-d dimensional array is the global id of the default process group ranks.\nDeviceMesh follows SPMD programming model, which means the same PyTorch Python program is running on all processes/ranks in the cluster. Therefore, users need to make sure the array (which describes the layout of devices) should be identical across all ranks. Inconsistent will lead to silent hang.\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. A reduction over the first dimension of mesh will reduce across columns (0, 4), .. and (3, 7), a reduction over the second dimension of mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).     \nThe constructed device mesh has number of dimensions equal to the number of groups passed. For example, if a single process group is passed in, the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in, the resulted DeviceMesh is a 2D mesh.\nIf more than one group is passed, then the and arguments are required. The order of the process groups passed in determines the topology of the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh. The tensor passed in must have the same number of dimensions as the number of process groups passed in, and the order of the dimensions in the tensor must match the order in the process groups passed in.     \n  * () – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of . Each string in must be unique. Default is None.\n\n         \nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.          \n\n         \n\n         \n\n         \n\n         \n\n\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .          \n\n\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .          \n() – A list of point-to-point operations(type of each operator is ). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.\nIn addition, if this API is the first collective call in the passed to , all ranks of the must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the , batched P2P operations involving only a subset of ranks of the are allowed.          \n\n\n- the default mode, when is set to . When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see . See the below script to see examples of differences in these semantics for CPU and CUDA operations.\n- when is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:\n  * - in the case of CPU collectives, returns if completed. In the case of CUDA operations, returns if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.\n  * - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).\n  * - returns object. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs, call might become redundant.\n\n\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n```\n\n  \n  \n  \n   \n\n\n\n \n    \n    \n   \n    \n    \n    \n    \n\n```\n         \n\n         \n\n\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .          \n\n\n```\n\n\n  \n          \n\n\n\n \n\n\n\n\n```\n\n```\n\n\n  \n           \n        \n\n\n\n \n\n\n\n\n```\n         \n\n         \n\n\n```\n\n\n  \n  \n          \n\n\n\n\n          \n\n\n\n \n\n\n\n\n```\n\n```\n\n\n  \n          \n\n\n\n\n  \n           \n        \n\n\n\n \n\n\n\n\n```\n         \n  * () – Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of “concatenation”, see ; (ii) a stack of all the input tensors along the primary dimension; for definition of “stack”, see . Examples below may better explain the supported output forms.\n\n\n```\n\n\n  \n          \n\n\n\n\n      \n \n\n\n\n\n     \n \n\n\n\n\n\n\n```\n         \n\n    \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input . If the calling rank is not part of the group, the passed in will be unmodified.\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via .          \n\n    \n```\n\n  \n  \n     \n   \n           \n\n      \n  \n\n\n\nNone                                                                   # Rank 1\n\n```\n         \n\n\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via .          \n\n    \n```\n\n   \n  \n  \n   \n   \n    \n    \n       \n         \n       \n\n      \n  \n\n\n\n\n\n```\n    \nSimilar to , but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of . Note that all objects in must be picklable in order to be scattered.     \n\n    \n```\n\n   \n   \n    \n          \n\n    \n        \n  \n  \n\n\n\n\n```\n         \n\n         \n  * () – Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of “concatenation”, see . For definition of “stack”, see .\n\n\n```\n\n\n  \n    \n\n      \n\n\n\n \n\n\n\n\n    \n\n\n\n\n\n \n\n\n\n\n```\n         \n\n\n```\n      \n\n\n\n\n\n   \n \n\n\n\n\n\n\n```\n\n```\n\n\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n  \n   \n\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n```\n\n```\n\n  \n                \n        \n\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n   \n \n\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n\n```\n         \n\n\n```\n      \n  \n\n\n\n\n\n   \n \n\n\n\n\n\n\n```\n\n```\n\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n  \n\n\n\n\n\n  \n \n\n\n\n\n\n\n```\n\n```\n\n  \n                \n        \n  \n\n\n\n\n\n   \n \n\n\n\n\n\n\n```\n         \n\n    \nIt is able to report ranks that did not pass this barrier within the provided timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\nThis collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application’s collective calls to check if any ranks are desynchronized.     \n  * () – Whether to collect all failed ranks or not. By default, this is and on rank 0 will throw on the first failed rank it encounters in order to fail fast. By setting will collect all failed ranks and throw an error containing information about all failed ranks.\n\n    \n```\n\n   \n   \n     \n\n\n   \n     \n\n\n\n```\n         \nIn the example above, work will be done on GPU using NCCL backend, will return after synchronizing the appropriate NCCL streams with PyTorch’s current device streams to ensure we can have asynchronous CUDA execution and it does not wait for the entire operation to complete on GPU. Note that does not support flag or NCCL’s . In addition, if a callback function was added by , it will wait until ’s NCCL streams synchronize with ’s dedicated callback stream and invoke the callback inline after running the callback on the callback stream. will return another that holds the return value of the callback and a that recorded the callback stream.\n> \n         \nusers can use to blocking wait for the completion of the work and get the WorkResult by . Also, users can use to register a callback function to be called when the work is completed, without blocking the current thread.     \nIn normal cases, users do not need to set the timeout. calling wait() is the same as calling synchronize(): Letting the current stream block on the completion of the NCCL work. However, if timeout is set, it will block the CPU thread until the NCCL work is completed or timed out. If timeout, exception will be thrown.     \nThe distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in (by explicitly creating the store as an alternative to specifying .) There are 3 choices for Key-Value Stores: , , and .          \nThe first call to add for a given creates a counter associated with in the store, initialized to . Subsequent calls to add with the same increment the counter by the specified . Calling with a key that has already been set in the store by will result in an exception.          \nThe call to check whether a given list of have value stored in the store. This call immediately returns in normal cases but still suffers from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed. Calling with a list of keys that one wants to check whether stored in the store or not.                         \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by and since one key is used to coordinate all the workers using the store.\nWhen used with the , returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.                    \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as to insert a key-value pair, to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection.     \n  * () – If specified, the underlying will listen on this file descriptor, which must be a socket already bound to . Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to ).\n\n              \nNote that you can use (recommended, only available after 1.8.1) or to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (, , ) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\nThe multi-GPU functions (which stand for multiple GPUs per CPU thread) are deprecated. As of today, PyTorch Distributed’s preferred programming model is one device per thread, as exemplified by the APIs in this document. If you are a backend developer and want to support multiple devices per thread, please contact PyTorch Distributed’s maintainers.\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to and . The capability of third-party backends are decided by their own implementations.\nThe utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.\nIn both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (), and each process will be operating on a single GPU from .\n1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.\n2. In your training program, you must parse the command-line argument: , which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\nFor backward compatibility, it may be necessary for users to handle both cases in their argument parsing code. This means including both and in the argument parser. If only is provided, the launcher will trigger an error: “error: unrecognized arguments: –local-rank=<rank>”. For training code that only supports PyTorch 2.0.0+, including should be sufficient.\n3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that . Other init methods (e.g. ) may work, but is the one that is officially supported by this module.\n4. In your training program, you can either use regular distributed functions or use module. If your training program uses GPUs for training and you would like to use module, here is how to configure it.\nPlease ensure that argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the needs to be , and needs to be in order to use this utility\n5. Another way to pass to the subprocesses via environment variable . This behavior is enabled when you launch the script with . You must adjust the subprocess example above to replace with ; the launcher will not pass when you specify this flag.\nThe package also provides a function in . This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.\nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. provides a suite of tools to help debug training applications in a self-serve fashion:\nIt is extremely convenient to use python’s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all. PyTorch offers a customized wrapper around pdb that streamlines the process.\nmakes this process easy. Internally, it customizes ’s breakpoint behavior in two ways but otherwise behaves as normal . 1. Attaches the debugger only on one rank (specified by the user). 2. Ensures all other ranks stop, by using a that will release once the debugged rank issues a 3. Reroutes stdin from the child process such that it connects to your terminal.\nAs of v1.10, exists as an alternative to which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into within the provided timeout. implements a host-side barrier using / communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into (in practice this could be due to an application bug or hang in a previous collective):\nWith , the environment variable can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. can be set to either (default), , or depending on the debugging level required. Please note that the most verbose option, may impact the application performance and thus should only be used when debugging issues.\nSetting will result in additional debug logging when models trained with are initialized, and will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\nIn addition, enhances crash logging in due to unused parameters in the model. Currently, must be passed into initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify to be instead computed as , then does not receive a gradient in the backwards pass, and thus results in failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\nSetting will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP ). This is done by creating a wrapper process group that wraps all process groups returned by and APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a , which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into :\nWith the backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables and reruns the application, the following error message reveals the root cause:\nIn addition, can be used in conjunction with to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use collective calls backed by process groups created with the and APIs.\nIn addition to explicit debugging support via and , the underlying C++ library of also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of and environment variables.\n\n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n"
}