{
  "https://pytorch.org/docs/stable/": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nFeatures described in this documentation are classified by release status:\n> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.\n\n\n\n\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nFeatures described in this documentation are classified by release status:\n> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.\n\n\n\n\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/bottleneck.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nis a tool that can be used as an initial step for debugging bottlenecks in your program. It summarizes runs of your script with the Python profiler and PyTorch’s autograd profiler.\nwhere [args] are any number of arguments to , or run for more usage instructions.\nBecause your script will be profiled, please ensure that it exits in a finite amount of time.\nDue to the asynchronous nature of CUDA kernels, when running against CUDA code, the cProfile output and CPU-mode autograd profilers may not show correct timings: the reported CPU time reports the amount of time used to launch the kernels but does not include the time the kernel spent executing on a GPU unless the operation does a synchronize. Ops that do synchronize appear to be extremely expensive under regular CPU-mode profilers. In these case where timings are incorrect, the CUDA-mode autograd profiler may be helpful.\nTo decide which (CPU-only-mode or CUDA-mode) autograd profiler output to look at, you should first check if your script is CPU-bound (“CPU total time is much greater than CUDA total time”). If it is CPU-bound, looking at the results of the CPU-mode autograd profiler will help. If on the other hand your script spends most of its time executing on the GPU, then it makes sense to start looking for responsible CUDA operators in the output of the CUDA-mode autograd profiler.\nOf course the reality is much more complicated and your script might not be in one of those two extremes depending on the part of the model you’re evaluating. If the profiler outputs don’t help, you could try looking at the result of with . However, please take into account that the NVTX overhead is very high and often gives a heavily skewed timeline. Similarly, helps to analyze performance on Intel platforms further with .\nIf you are profiling CUDA code, the first profiler that runs (cProfile) will include the CUDA startup time (CUDA buffer allocation cost) in its time reporting. This should not matter if your bottlenecks result in code much slower than the CUDA startup time.\nFor more complicated uses of the profilers (like in a multi-GPU case), please see for more information.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/accelerator.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nCheck if the current accelerator is available at runtime: it was build, all the required drivers are available and at least one device is visible.  \n---  \nReturn the device of the accelerator available at compilation time.  \nReturn the index of a currently selected device for the current .  \nReturn the index of a currently selected device for the current .  \nWait for all kernels in all streams on the given device to complete.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/backends.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nPossible values: - “DEFAULT” - “VSX” - “Z VECTOR” - “NO AVX” - “AVX2” - “AVX512” - “SVE256”     \nNote that this doesn’t necessarily mean CUDA is available; just that if this PyTorch binary were run on a machine with working CUDA drivers and devices, we would be able to use it.     \ncontains the cuFFT plan caches for each CUDA device. Query a specific device ’s cache via .     \nOverride the library PyTorch uses for BLAS operations. Choose between cuBLAS, cuBLASLt, and CK [ROCm-only].\nWhen PyTorch runs a CUDA BLAS operation it defaults to cuBLAS even if both cuBLAS and cuBLASLt are available. For PyTorch built for ROCm, hipBLAS, hipBLASLt, and CK may offer different performance. This flag (a \n  * If (the default) is set then heuristics will be used to pick between the other options.\n  * When no input is given, this function returns the currently preferred library.\n  * User may use the environment variable TORCH_BLAS_PREFER_CUBLASLT=1 to set the preferred library to cuBLASLt globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.\n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs.     \n[ROCm-only] Override the backend PyTorch uses in ROCm environments for Flash Attention. Choose between AOTriton and CK\nWhen Flash Attention is enabled and desired, PyTorch defaults to using AOTriton as the backend. This flag (a \n  * If is set then the default backend will be used wherever possible. Currently AOTriton.\n  * When no input is given, this function returns the currently preferred library.\n  * User may use the environment variable TORCH_ROCM_FA_PREFER_CK=1 to set the preferred library to CK globally.\n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s library selection is incorrect for your application’s inputs.     \nOverride the heuristic PyTorch uses to choose between cuSOLVER and MAGMA for CUDA linear algebra operations.\nWhen PyTorch runs a CUDA linear algebra operation it often uses the cuSOLVER or MAGMA libraries, and if both are available it decides which to use with a heuristic. This flag (a \n  * If (the default) is set then heuristics will be used to pick between cuSOLVER and MAGMA if both are available.\n  * When no input is given, this function returns the currently preferred library.\n  * User may use the environment variable TORCH_LINALG_PREFER_CUSOLVER=1 to set the preferred library to cuSOLVER globally. This flag only sets the initial value of the preferred library and the preferred library may still be overridden by this function call later in your script.\n\n\nNote: When a library is preferred other libraries may still be used if the preferred library doesn’t implement the operation(s) called. This flag may achieve better performance if PyTorch’s heuristic library selection is incorrect for your application’s inputs.     \nReturns whether flash scaled dot product attention is enabled or not.          \nReturns whether memory efficient scaled dot product attention is enabled or not.          \nReturns whether math scaled dot product attention is enabled or not.          \nReturns whether fp16/bf16 reduction in math scaled dot product attention is enabled or not.     \nEnables or disables fp16/bf16 reduction in math scaled dot product attention.     \nReturns whether cuDNN scaled dot product attention is enabled or not.          \nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments.          \n  * () – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.\n\n    \nTrue if FlashAttention can be used with the given parameters; otherwise, False.\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments.          \n  * () – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.\n\n    \nTrue if efficient_attention can be used with the given parameters; otherwise, False.\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments.          \n  * () – An instance of SDPAParams containing the tensors for query, key, value, an optional attention mask, dropout rate, and a flag indicating if the attention is causal.\n\n    \nTrue if cuDNN can be used with the given parameters; otherwise, False.\nThis function is dependent on a CUDA-enabled build of PyTorch. It will return False in non-CUDA environments.     \nThis context manager can be used to temporarily enable or disable any of the three backends for scaled dot product attention. Upon exiting the context manager, the previous state of the flags will be restored.     \nA is True. Set to zero to try every available algorithm. Note that this setting only affects convolutions dispatched via the cuDNN v8 API.          \nReturns whether fast path for TransformerEncoder and MultiHeadAttention is enabled, or if jit is scripting.\nThe fastpath might not be run even if returns unless all conditions on inputs are met.          \nNote that this doesn’t necessarily mean MPS is available; just that if this PyTorch binary were run a machine with working MPS drivers and devices, we would be able to use it.     \nTo make it easier to debug performance issues, oneMKL can dump verbose messages containing execution information like duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named . However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.     \nTo make it easier to debug performance issues, oneDNN can dump verbose messages containing information like kernel size, input data size and execution duration while executing the kernel. The verbosing functionality can be invoked via an environment variable named . However, this methodology dumps messages in all steps. Those are a large amount of verbose messages. Moreover, for investigating the performance issues, generally taking verbose messages for one single iteration is enough. This on-demand verbosing functionality makes it possible to control scope for verbose message dumping. In the following example, verbose messages will be dumped out for the second inference only.     \n– Verbose level - : Disable verbosing - : Enable verbosing - : Enable verbosing, including oneDNN kernel creation     \nYou must install opt-einsum in order for torch to automatically optimize einsum. To make opt-einsum available, you can install it along with torch: or by itself: . If the package is installed, torch will import it automatically and use it accordingly. Use this function to check whether opt-einsum was installed and properly imported by torch.     \nReturn the opt_einsum package if opt_einsum is currently available, else None.     \nIf opt_einsum is not available, torch.einsum will fall back to the default contraction path of left to right.     \nA is . By default, torch.einsum will try the “auto” strategy, but the “greedy” and “optimal” strategies are also supported. Note that the “optimal” strategy is factorial on the number of inputs as it tries all possible paths. See more details in opt_einsum’s docs (\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/checkpoint.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nCheckpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward propagation. This can cause persistent states like the RNG state to be more advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply to or to omit stashing and restoring the RNG state during each checkpoint.\nThe stashing logic saves and restores the RNG state for CPU and another device type (infer the device type from Tensor arguments excluding CPU tensors by ) to the . If there are multiple device, device state will only be saved for devices of a single device type, and the remaining devices will be ignored. Consequently, if any checkpointed functions involve randomness, this may result in incorrect gradients. (Note that if CUDA devices are among the devices detected, it will be prioritized; otherwise, the first device encountered will be selected.) If there are no CPU-tensors, the default device type state (default value is , and it could be set to other device by ) will be saved and restored. However, the logic has no way to anticipate if the user will move Tensors to a new device within the itself. Therefore, if you move Tensors to a new device (“new” meaning not belonging to the set of [current device + devices of Tensor arguments]) within , deterministic output compared to non-checkpointed passes is never guaranteed.     \nActivation checkpointing is a technique that trades compute for memory. Instead of keeping tensors needed for backward alive until they are used in gradient computation during backward, forward computation in checkpointed regions omits saving tensors for backward and recomputes them during the backward pass. Activation checkpointing can be applied to any part of a model.\nThere are currently two checkpointing implementations available, determined by the parameter. It is recommended that you use . Please refer the note below for a discussion of their differences.\nIf the invocation during the backward pass differs from the forward pass, e.g., due to a global variable, the checkpointed version may not be equivalent, potentially causing an error being raised or leading to silently incorrect gradients.\nThe parameter should be passed explicitly. In version 2.4 we will raise an exception if is not passed. If you are using the variant, please refer to the note below for important considerations and potential limitations.\nThe reentrant variant of checkpoint () and the non-reentrant variant of checkpoint () differ in the following ways:\n  * Non-reentrant checkpoint stops recomputation as soon as all needed intermediate activations have been recomputed. This feature is enabled by default, but can be disabled with . Reentrant checkpoint always recomputes in its entirety during the backward pass.\n  * The reentrant variant does not record the autograd graph during the forward pass, as it runs with the forward pass under . The non-reentrant version does record the autograd graph, allowing one to perform backward on the graph within checkpointed regions.\n  * The reentrant checkpoint only supports the API for the backward pass without its argument, while the non-reentrant version supports all ways of performing the backward pass.\n  * At least one input and output must have for the reentrant variant. If this condition is unmet, the checkpointed part of the model will not have gradients. The non-reentrant version does not have this requirement.\n  * The reentrant version does not consider tensors in nested structures (e.g., custom objects, lists, dicts, etc) as participating in autograd, while the non-reentrant version does.\n  * The reentrant checkpoint does not support checkpointed regions with detached tensors from the computational graph, whereas the non-reentrant version does. For the reentrant variant, if the checkpointed segment contains tensors detached using or with , the backward pass will raise an error. This is because makes all the outputs require gradients and this causes issues when a tensor is defined to have no gradient in the model. To avoid this, detach the tensors outside of the function.\n\n    \n  * – describes what to run in the forward pass of the model or part of the model. It should also know how to handle the inputs passed as the tuple. For example, in LSTM, if user passes , should correctly use the first input as and the second input as \n  * () – Omit stashing and restoring the RNG state during each checkpoint. Note that under torch.compile, this flag doesn’t take effect and we always preserve RNG state. Default: \n  * ( is not passed. If , will use an implementation that does not require reentrant autograd. This allows to support additional functionality, such as working as expected with and support for keyword arguments input into the checkpointed function.\n  * () – A callable returning a tuple of two context managers. The function and its recomputation will be run under the first and second context managers respectively. This argument is only supported if .\n  * () – A string specifying the determinism check to perform. By default it is set to which compares the shapes, dtypes, and devices of the recomputed tensors against those the saved tensors. To turn off this check, specify . Currently these are the only two supported values. Please open an issue if you would like to see more determinism checks. This argument is only supported if , if , the determinism check is always disabled.\n  * () – If , error messages will also include a trace of the operators ran during the original forward computation as well as the recomputation. This argument is only supported if .\n\n    \nSequential models execute a list of modules/functions in order (sequentially). Therefore, we can divide such a model in various segments and checkpoint each segment. All segments except the last will not store the intermediate activations. The inputs of each checkpointed segment will be saved for re-running the segment in the backward pass.\nThe parameter should be passed explicitly. In version 2.4 we will raise an exception if is not passed. If you are using the .     \n  * – A or the list of modules or functions (comprising the model) to run sequentially.\n  * () – Omit stashing and restoring the RNG state during each checkpoint. Default: \n  * ( is not passed. If , will use an implementation that does not require reentrant autograd. This allows to support additional functionality, such as working as expected with and support for keyword arguments input into the checkpointed function.\n\n    \nContext manager that sets whether checkpoint should print additional debug information when running. See the flag for for more information. Note that when set, this context manager overrides the value of passed to checkpoint. To defer to the local setting, pass to this context.     \n  * : The operation’s output will be saved during the forward pass and will not be recomputed during the backward pass\n  * : The operation’s output will not be saved during the forward pass and will be recomputed during the backward pass\n\n\nUse over to indicate that the policy should not be overridden by other subsystems like .\nA policy function that returns every op is NOT equivalent to not using checkpointing. Using such a policy would save additional tensors not limited to ones that are actually needed for gradient computation.     \nThis class is used to pass relevant metadata to the policy function during selective checkpointing. The metadata includes whether the current invocation of the policy function is during recomputation or not.     \nUse this with to control which operations are recomputed during the backward pass.     \n  *     * If a policy function is provided, it should accept a , the , args and kwargs to the op, and return a enum value indicating whether the execution of the op should be recomputed or not.\n    * If a list of operations is provided, it is equivalent to a policy returning for the specified operations and for all other operations.\n  * () – By default, an error is raised if any tensors cached by selective activation checkpoint are mutated in order to ensure correctness. If set to , this check is disabled.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/complex_numbers.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nComplex numbers are numbers that can be expressed in the form , where a and b are real numbers, and is called the imaginary unit, which satisfies the equation . Complex numbers frequently occur in mathematics and engineering, especially in topics like signal processing. Traditionally many users and libraries (e.g., TorchAudio) have handled complex numbers by representing the data in float tensors with shape where the last dimension contains the real and imaginary values.\nTensors of complex dtypes provide a more natural user experience while working with complex numbers. Operations on complex tensors (e.g., , ) are likely to be faster and more memory efficient than operations on float tensors mimicking them. Operations involving complex numbers in PyTorch are optimized to use vectorized assembly instructions and specialized kernels (e.g. LAPACK, cuBlas).\nComplex tensors is a beta feature and subject to change.\nThe default dtype for complex tensors is determined by the default floating point dtype. If the default floating point dtype is then complex numbers are inferred to have a dtype of , otherwise they are assumed to have a dtype of .\nUsers who currently worked around the lack of complex tensors with real tensors of shape can easily to switch using the complex tensors in their code using and . Note that these functions don’t perform any copy and return a view of the input tensor.\n```\n   \n\n\n\n\n  \n\n\n\n\n\n\n\n```\n\nThe real and imaginary values of a complex tensor can be accessed using the and .\nAccessing and attributes doesn’t allocate any memory, and in-place updates on the and tensors will update the original complex tensor. Also, the returned and tensors are not contiguous.\n```\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThe angle and absolute values of a complex tensor can be computed using and .\nMany linear algebra operations, like , , etc., support complex numbers. If you’d like to request an operation we don’t currently support, please \nComplex tensors can be serialized, allowing data to be saved as complex values.\nPyTorch supports autograd for complex tensors. The gradient computed is the Conjugate Wirtinger derivative, the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, all the existing optimizers can be implemented to work out of the box with complex parameters. For more details, check out the note .\nSemantically, we define stepping through a PyTorch optimizer with complex parameters as being equivalent to stepping through the same optimizer on the equivalent of the complex params. More concretely:\nand will compute the same updates on the parameters, though there may be slight numerical discrepancies between the two optimizers, similar to numerical discrepancies between foreach vs forloop optimizers and capturable vs default optimizers. For more details, see .\nSpecifically, while you can think of our optimizer’s handling of complex tensors as the same as optimizing over their and pieces separately, the implementation details are not precisely that. Note that the equivalent will convert a complex tensor to a real tensor with shape , whereas splitting a complex tensor into two tensors is 2 tensors of size . This distinction has no impact on pointwise optimizers (like AdamW) but will cause slight discrepancy in optimizers that do global reductions (like LBFGS). We currently do not have optimizers that do per-Tensor reductions and thus do not yet define this behavior. Open an issue if you have a use case that requires precisely defining this behavior.\nIf any of these would help your use case, please \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/benchmark_utils.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nFor a full tutorial on how to use this class, see: \nThe PyTorch Timer is based on (and in fact uses internally), but with several key differences:\n  1.     \nTimer will perform warmups (important as some elements of PyTorch are lazily initialized), set threadpool size so that comparisons are apples-to-apples, and synchronize asynchronous CUDA functions when necessary.\n  2.     \nWhen measuring code, and particularly complex kernels / models, run-to-run variation is a significant confounding factor. It is expected that all measurements should include replicates to quantify noise and allow median computation, which is more robust than mean. To that effect, this class deviates from the API by conceptually merging and . (Exact algorithms are discussed in method docstrings.) The method is replicated for cases where an adaptive strategy is not desired.\n  3.     \nWhen defining a Timer, one can optionally specify , , , and . (Defined later) These fields are included in the representation of result object and by the class to group and display results for comparison.\n  4.     \nIn addition to wall times, Timer can run a statement under Callgrind and report instructions executed.\n\n    \n  * () – Callable which returns the current time. If PyTorch was built without CUDA or there is no GPU present, this defaults to ; otherwise it will synchronize CUDA before measuring the time.\n  * () – A dict which defines the global variables when is being executed. This is the other method for providing variables which needs.\n  * () – String which summarizes . For instance, if is “torch.nn.functional.relu(torch.add(x, 1, out=out))” one might set label to “ReLU(x + 1)” to improve readability.\n  * Provide supplemental information to disambiguate measurements with identical stmt or label. For instance, in our example above sub_label might be “float” or “int”, so that it is easy to differentiate: “ReLU(x + 1): (float)”\n  * String to distinguish measurements with identical label and sub_label. The principal use of is to signal to the columns of data. For instance one might set it based on the input size to create a table of the form:\n  * () – This tag indicates that otherwise identical tasks were run in different environments, and are therefore not equivalent, for instance when A/B testing a change to a kernel. will treat Measurements with different specification as distinct when merging replicate runs.\n  * (. Single threaded performance is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency, so the default is set to one. This is in contrast to the default PyTorch threadpool size which tries to utilize all cores.\n\n    \nSimilar to but also checks for variablility in measurements and repeats until iqr/median is smaller than or is reached.     \nA object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)     \nMeasure many replicates while keeping timer overhead to a minimum.\nNote the variable in the inner loop. The choice of block size is important to measurement quality, and must balance two competing objectives:\n>   1. A small block size results in more replicates and generally better statistics.\n>   2. A large block size better amortizes the cost of invocation, and results in a less biased measurement. This is important because CUDA synchronization time is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement.\n> \n\nblocked_autorange sets block_size by running a warmup period, increasing block size until timer overhead is less than 0.1% of the overall computation. This value is then used for the main measurement loop.     \nA object that contains measured runtimes and repetition counts, and can be used to compute statistics. (mean, median, etc.)     \nUnlike wall times, instruction counts are deterministic (modulo non-determinism in the program itself and small amounts of jitter from the Python interpreter.) This makes them ideal for detailed performance analysis. This method runs in a separate process so that Valgrind can instrument the program. Performance is severely degraded due to the instrumentation, however this is ameliorated by the fact that a small number of iterations is generally sufficient to obtain good measurements.\nIn order to to use this method , , and must be installed.\nBecause there is a process boundary between the caller (this process) and the execution, cannot contain arbitrary in-memory data structures. (Unlike timing methods) Instead, globals are restricted to builtins, ’s, and TorchScripted functions/modules to reduce the surprise factor from serialization and subsequent deserialization. The class provides more detail on this subject. Take particular care with nn.Modules: they rely on pickle and you may need to add an import to for them to transfer properly.\nBy default, a profile for an empty statement will be collected and cached to indicate how many instructions are from the Python loop which drives .     \nA object which provides instruction counts and some basic facilities for analyzing and manipulating results.          \nThis class stores one or more measurements of a given statement. It is serializable and provides several convenience methods (including a detailed __repr__) for downstream consumers.     \nMerge will extrapolate times to and will not transfer any metadata. (Since it might differ between replicates)     \nThis property is intended to give a convenient way to estimate the precision of a measurement. It only uses the interquartile region to estimate statistics to try to mitigate skew from the tails, and uses a static z value of 1.645 since it is not expected to be used for small values of , so z can approximate .\nThe significant figure estimation used in conjunction with the method to provide a more human interpretable data summary. __repr__ does not use this method; it simply displays raw values. Significant figure estimation is intended for .     \nManipulation is generally done using the FunctionCounts class, which is obtained by calling . Several convenience methods are provided as well; the most significant is .     \nWhen comparing two different sets of instruction counts, on stumbling block can be path prefixes. Callgrind includes the full filepath when reporting a function (as it should). However, this can cause issues when diffing profiles. If a key component such as Python or PyTorch was built in separate locations in the two profiles, which can result in something resembling:\nStripping prefixes can ameliorate this issue by regularizing the strings and causing better cancellation of equivalent call sites when diffing.          \nOne common reason to collect instruction counts is to determine the the effect that a particular change will have on the number of instructions needed to perform some unit of work. If a change increases that number, the next logical question is “why”. This generally involves looking at what part if the code increased in instruction count. This function automates that process so that one can easily diff counts on both an inclusive and exclusive basis.     \nConceptually, the FunctionCounts returned can be thought of as a tuple of (count, path_and_function_name) tuples.\nmatches the semantics of callgrind. If True, the counts include instructions executed by children. is useful for identifying hot spots in code; is useful for reducing noise when diffing counts from two different runs. (See CallgrindStats.delta(…) for more details)          \n  1. A function which strips CPython calls which are known to be non-deterministic and quite noisy.\n\n    \nSeveral instructions in the CPython interpreter are rather noisy. These instructions involve unicode to dictionary lookups which Python uses to map variable names. FunctionCounts is generally a content agnostic container, however this is sufficiently important for obtaining reliable results to warrant an exception.     \nKeep only the elements where applied to function name returns True.     \nThis can be used to regularize function names (e.g. stripping irrelevant parts of the file path), coalesce entries by mapping multiple functions to the same name (in which case the counts are added together), etc.     \nHelper class for displaying the results of many measurements in a formatted table.\nThe table format is based on the information fields provided in (, , , , etc).\nThe table can be directly printed using or casted as a .\nFor a full tutorial on how to use this class, see:           \nEnables trimming of significant figures when building the formatted table.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/amp.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nprovides convenience methods for mixed precision, where some operations use the () datatype and other operations use lower precision floating point datatype (): () or . Some ops, like linear layers and convolutions, are much faster in . Other ops, like reductions, often require the dynamic range of . Mixed precision tries to match each op to its appropriate datatype.\nOrdinarily, “automatic mixed precision training” with datatype of uses and together, as shown in the and . However, and are modular, and may be used separately if desired. As shown in the CPU example section of , “automatic mixed precision training/inference” on CPU with datatype of only uses .\n  *     *     *     * \n         \n( attribute of a . Thus, you may obtain the device type of a tensor using .     \nInstances of serve as context managers or decorators that allow regions of your script to run in mixed precision.\nIn these regions, ops run in an op-specific dtype chosen by autocast to improve performance while maintaining accuracy. See the for details.\nWhen entering an autocast-enabled region, Tensors may be any type. You should not call or on your model(s) or inputs when using autocasting.\nshould wrap only the forward pass(es) of your network, including the loss computation(s). Backward passes under autocast are not recommended. Backward ops run in the same type that autocast used for corresponding forward ops.\n```\n\n  \n   \n\n    \n    \n\n    # Enables autocasting for the forward pass (model + loss)\n     \n          \n           \n\n    \n    \n    \n\n```\n\nSee the for usage (along with gradient scaling) in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).\ncan also be used as a decorator, e.g., on the method of your model:\nFloating-point Tensors produced in an autocast-enabled region may be . After returning to an autocast-disabled region, using them with floating-point Tensors of different dtypes may cause type mismatch errors. If so, cast the Tensor(s) produced in the autocast region back to (or other dtype if desired). If a Tensor from the autocast region is already , the cast is a no-op, and incurs no additional overhead. CUDA Example:\n```\n# Creates some tensors in default dtype (here assumed to be float32)\n    \n    \n    \n    \n\n \n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    \n       \n    \n       \n\n# After exiting autocast, calls f_float16.float() to use with d_float32\n   \n\n```\n\n```\n\n  \n   \n\n   \n        \n        \n\n        \n          \n              \n               \n\n        \n        \n\n```\n\n```\n\n  \n\n  \n       \n        \n          \n\n```\n\n```\n \n       \n        \n           \n      \n         \n\n  \n  \n   \n\n# For now, we suggest to disable the Jit Autocast Pass,\n\n\n\n \n        \n  \n\n   \n     \n\n```\n\nType mismatch errors an autocast-enabled region are a bug; if this is what you observe, please file an issue.\nsubregions can be nested in autocast-enabled regions. Locally disabling autocast can be useful, for example, if you want to force a subregion to run in a particular . Disabling autocast gives you explicit control over the execution type. In the subregion, inputs from the surrounding region should be cast to before use:\n```\n# Creates some tensors in default dtype (here assumed to be float32)\n    \n    \n    \n    \n\n \n       \n      \n        \n        # (necessary because e_float16 was created in an autocasted region)\n           \n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n       \n\n```\n\nThe autocast state is thread-local. If you want it enabled in a new thread, the context manager or decorator must be invoked in that thread. This affects and when used with more than one GPU per process (see ).     \n  * () – Device type to use. Possible values are: ‘cuda’, ‘cpu’, ‘mtia’, ‘xpu’, and ‘hpu’. The type is the same as the attribute of a . Thus, you may obtain the device type of a tensor using .\n  * () – Whether autocasting should be enabled in the region. Default: \n  * () – Data type for ops run in autocast. It uses the default value ( for CUDA and for CPU), given by , if is . Default: \n  * () – Whether the weight cache inside autocast should be enabled. Default: \n\n         \n  * ( attribute of a . Thus, you may obtain the device type of a tensor using .\n  * ( or None, optional, default=None) – If not , when runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes with autocast disabled. If , ’s internal ops execute with the current autocast state.\n\n\nIf the decorated is called outside an autocast-enabled region, is a no-op and has no effect.     \nCreate a helper decorator for backward methods of custom autograd functions.\nAutograd functions are subclasses of . Ensures that executes with the same autocast state as . See the for more detail.     \n( attribute of a . Thus, you may obtain the device type of a tensor using .\nIf the forward pass for a particular op has inputs, the backward pass for that op will produce gradients. Gradient values with small magnitudes may not be representable in . These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\nTo prevent underflow, “gradient scaling” multiplies the network’s loss(es) by a scale factor and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\nEach parameter’s gradient ( attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.\nAMP/fp16 may not work for every model! For example, most bf16-pretrained models cannot operate in the fp16 numerical range of max 65504 and will cause gradients to overflow instead of underflow. In this case, the scale factor may decrease under 1 as an attempt to bring gradients to a number representable in the fp16 dynamic range. While one may expect the scale to always be above 1, our GradScaler does NOT make this guarantee to maintain performance. If you encounter NaNs in your loss or gradients when running with AMP/fp16, verify your model is compatible.\nOps that run in or non-floating-point dtypes are not eligible, and will run in these types whether or not autocast is enabled.\nOnly out-of-place ops and Tensor methods are eligible. In-place variants and calls that explicitly supply an Tensor are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region can autocast, but and cannot. For best performance and stability, prefer out-of-place ops in autocast-enabled regions.\nOps called with an explicit argument are not eligible, and will produce output that respects the argument.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nIf an op is unlisted, we assume it’s numerically stable in . If you believe an unlisted op is numerically unstable in , please file an issue.\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\nThe backward passes of (and , which wraps it) can produce gradients that aren’t representable in . In autocast-enabled regions, the forward input may be , which means the backward gradient must be representable in (autocasting forward inputs to doesn’t help, because that cast must be reversed in backward). Therefore, and raise an error in autocast-enabled regions.\nMany models use a sigmoid layer right before the binary cross entropy layer. In this case, combine the two layers using or . and are safe to autocast.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nIf an op is unlisted, we assume it’s numerically stable in . If you believe an unlisted op is numerically unstable in , please file an issue.\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\nThe following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a , as a function, or as a method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.\nOps not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.\nIf an op is unlisted, we assume it’s numerically stable in . If you believe an unlisted op is numerically unstable in , please file an issue. shares the lists of .\nThese ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match. If all of the inputs are , the op runs in . If any of the inputs is , autocast casts all inputs to and runs the op in .\nSome ops not listed here (e.g., binary ops like ) natively promote inputs without autocasting’s intervention. If inputs are a mixture of and , these ops run in and produce output, regardless of whether autocast is enabled.\n  *     *       *       *       * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/config_mod.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nReturn a human-readable string with descriptions of the configuration of PyTorch.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/deterministic.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nA is set to . Floating point and complex values are set to NaN, and integer values are set to the maximum value.\nFilling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance and still be deterministic.\nThe following operations will fill uninitialized memory when this setting is turned on:\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/dlpack.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nThe returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.     \nIf is a tensor (or ndarray) object, it must support the protocol (i.e., have a method). Otherwise may be a DLPack capsule, which is an opaque instance, typically produced by a function or method.\n```\n \n  \n\n# Convert a tensor directly (supported in PyTorch >= 1.10)\n  \n    \n\n\n\n\n\n# The old-style DLPack usage, with an intermediate capsule object\n  \n\n\n  \n\n\n    \n\n\n\n\n\n\n\n```\n    \nReturns an opaque object (a “DLPack capsule”) representing the tensor.\nis a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to . The more idiomatic use of DLPack is to call directly on the tensor object - this works when that object has a method, which PyTorch and most other libraries indeed have now.\nOnly call once per capsule produced with . Behavior when a capsule is consumed multiple times is undefined.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpp_index.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIf you are looking for the PyTorch C++ API docs, directly go .\nPyTorch provides several features for working with C++, and it’s best to choose from them based on your needs. At a high level, the following support is available:\nallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in the . This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including:\n  * Doing simple model modifications if needed (e.g. pulling out submodules)\n  * Constructing the input and doing preprocessing using C++ Tensor API\n\n\nTorchScript can be augmented with user-supplied code through custom operators and custom classes. Once registered with TorchScript, these operators and classes can be invoked in TorchScript code run from Python or from C++ as part of a serialized TorchScript model. The tutorial walks through interfacing TorchScript with OpenCV. In addition to wrapping a function call with a custom operator, C++ classes and structs can be bound into TorchScript through a pybind11-like interface which is explained in the tutorial.\nMost of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include:\n  * methods such as / / . For the full list of methods available, please see: \n  * C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see: \n  * The tensor autograd APIs and the package that are crucial for building dynamic neural networks in C++ frontend. For more details, please see: \n\n\nThe “author in TorchScript, infer in C++” workflow requires model authoring to be done in TorchScript. However, there might be cases where the model has to be authored in C++ (e.g. in workflows where a Python component is undesirable). To serve such use cases, we provide the full capability of authoring and training a neural net model purely in C++, with familiar components such as / / that closely resemble the Python API.\n  * For an overview of the PyTorch C++ model authoring and training API, please see: \n  * For a detailed tutorial on how to use the API, please see: \n\n\nFor guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see: . Note that on Linux there are two types of libtorch binaries provided: one compiled with GCC pre-cxx11 ABI and the other with GCC cxx11 ABI, and you should make the selection based on the GCC ABI your system is using.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/data.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nAt the heart of PyTorch data loading utility is the class. It represents a Python iterable over a dataset, with support for\n\n\nThese options are configured by the constructor arguments of a , which has signature:\nThe sections below describe in details the effects and usages of these options.\nThe most important argument of constructor is , which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\nA map-style dataset is one that implements the and protocols, and represents a map from (possibly non-integral) indices/keys to data samples.\nFor example, such a dataset, when accessed with , could read the -th image and its corresponding label from a folder on the disk.\nAn iterable-style dataset is an instance of a subclass of that implements the protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.\nFor example, such a dataset, when called , could return a stream of data reading from a database, a remote server, or even logs generated in real time.\nWhen using a with . The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See documentations for how to achieve this.\nFor , data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).\nThe rest of this section concerns the case with . classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.\nA sequential or shuffled sampler will be automatically constructed based on the argument to a . Alternatively, users may use the argument to specify a custom object that at each time yields the next index/key to fetch.\nA custom that yields a list of batch indices at a time can be passed as the argument. Automatic batching can also be enabled via and arguments. See for more details on this.\nNeither nor is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.\nsupports automatically collating individual fetched data samples into batches via arguments , , , and (which has a default function).\nThis is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).\nWhen (default ) is not , the data loader yields batched samples instead of individual samples. and arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify , which yields a list of keys at a time.\nThe and arguments essentially are used to construct a from . For map-style datasets, the is either provided by user or constructed based on the argument. For iterable-style datasets, the is a dummy infinite one. See on more details on samplers.\nWhen fetching from with , the argument drops the last non-full batch of each worker’s dataset replica.\nAfter fetching a list of samples using the indices from sampler, the function passed as the argument is used to collate lists of samples into batches.\nIn this case, loading from a map-style dataset is roughly equivalent with:\nand loading from an iterable-style dataset is roughly equivalent with:\nA custom can be used to customize collation, e.g., padding sequential data to max length of a batch. See on more about .\nIn certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where is used to collate the samples), but let the data loader directly return each member of the object.\nWhen both and are (default value for is already ), automatic batching is disabled. Each sample obtained from the is processed with the function passed as the argument.\n, the default simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.\nIn this case, loading from a map-style dataset is roughly equivalent with:\nand loading from an iterable-style dataset is roughly equivalent with:\nThe use of is slightly different when automatic batching is enabled or disabled.\n, is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default simply converts NumPy arrays in PyTorch tensors.\n, is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default ().\nFor instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple , the default collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default has the following properties:\n  * It always prepends a new dimension as the batch dimension.\n  * It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.\n  * It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for s, s, s, etc.\n\n\nUsers may use customized to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.\nIf you run into a situation where the outputs of have dimensions or type that is different from your expectation, you may want to check your .\nIn this mode, data fetching is done in the same process a is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.\nSetting the argument as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.\nAfter several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is ). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out \nIn this mode, each time an iterator of a is created (e.g., when you call ), worker processes are created. At this point, the , , and are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including ) runs in the worker process.\nreturns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns in main process. Users may use this function in dataset code and/or to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.\nFor map-style datasets, the main process generates the indices using and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.\nFor iterable-style datasets, since each worker process gets a replica of the object, naive multi-process loading will often result in duplicated data. Using and/or , users may configure each replica independently. (See documentations for how to achieve this. ) For similar reasons, in multi-process loading, the argument drops the last non-full batch of each worker’s iterable-style dataset replica.\nWorkers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.\nIt is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see ). Instead, we recommend using (i.e., setting ), which enables fast data transfer to CUDA-enabled GPUs.\n  * On Unix, is the default , child workers typically can access the and Python argument functions directly through the cloned address space.\n  * On Windows or MacOS, is the default , another interpreter is launched which runs your main script, followed by the internal worker function that receives the , and other arguments through \n\n\nThis separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:\n  * Wrap most of you main script’s code within block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and instance creation logic here, as it doesn’t need to be re-executed in workers.\n  * Make sure that any custom , or code is declared as top level definitions, outside of the check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not .)\n\n\nBy default, each worker will have its PyTorch seed set to , where is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified . However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See in FAQ.).\nIn , you may access the PyTorch seed set for each worker with either or , and use it to seed other libraries before data loading.\nHost to GPU copies are much faster when they originate from pinned (page-locked) memory. See for more details on when and how to use pinned memory generally.\nFor data loading, passing to a will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.\nThe default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a method on your custom type(s).     \nData loader combines a dataset and a sampler, and provides an iterable over the given dataset.\nThe supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.     \n  * () – set to to have the data reshuffled at every epoch (default: ).\n  * () – defines the strategy to draw samples from the dataset. Can be any with implemented. If specified, must not be specified.\n  * () – like , but returns a batch of indices at a time. Mutually exclusive with , , , and .\n  * () – how many subprocesses to use for data loading. means that the data will be loaded in the main process. (default: )\n  * () – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.\n  * () – If , the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your returns a batch that is a custom type, see the example below.\n  * () – set to to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: )\n  * () – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: )\n  * () – If not , this will be called on each worker subprocess with the worker id (an int in ) as input, after seeding and before data loading. (default: )\n  * () – If not , this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate for workers. (default: )\n  * () – Number of batches loaded in advance by each worker. means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is . Otherwise, if value of default is ).\n  * () – If , the data loader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers instances alive. (default: )\n  * () – the device to on if is . If not given, the current will be the default. This argument is discouraged and subject to deprecated.\n  * () – If , the data loader will not enforce that batches are returned in a first-in, first-out order. Only applies when . (default: )\n\n\nIf the start method is used, cannot be an unpicklable object, e.g., a lambda function. See on more details related to multiprocessing in PyTorch.\nheuristic is based on the length of the sampler used. When is an , it instead returns an estimate based on , with proper rounding depending on , regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user code in correctly handling multi-process loading to avoid duplicate data.\nHowever, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when is set. Unfortunately, PyTorch can not detect such cases in general.\nSee for more details on these two types of datasets and how interacts with .\nSee , and , and notes for random seed related questions.\nSetting to can harm reproducibility and may lead to a skewed data distribution being fed to the trainer in cases with imbalanced data.     \nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite , supporting fetching a data sample for a given key. Subclasses could also optionally overwrite , which is expected to return the size of the dataset by many implementations and the default options of . Subclasses could also optionally implement , for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\nby default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.     \nAll datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.\nAll subclasses should overwrite , which would return an iterator of samples in this dataset.\nWhen a subclass is used with , each item in the dataset will be yielded from the iterator. When , each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. , when called in a worker process, returns information about the worker. It can be used in either the dataset’s method or the ‘s option to modify each copy’s behavior.\n```\n \n       \n        \n            \n          \n          \n\n     \n          \n             \n              \n              \n          \n            \n                  \n              \n                  \n                 \n          \n\n# should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n   \n\n\n \n\n\n\n# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n \n\n\n\n \n\n\n```\n\n```\n \n       \n        \n            \n          \n          \n\n     \n          \n\n# should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n   \n\n\n \n\n\n\n \n\n\n# Define a `worker_init_fn` that configures each dataset copy differently\n \n      \n        \n      \n      \n    # configure the dataset to only process the split workload\n          \n      \n          \n         \n\n\n\n# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n  \n\n\n\n  \n\n\n```\n    \nEach sample will be retrieved by indexing tensors along the first dimension.     \n() – tensors that have the same size of the first dimension.     \nThis class is useful to assemble different parts of complex input data, given as datasets.     \n\n         \nThis class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.          \n  * () – Indices in the whole set selected for subset\n\n    \nGeneral collate function that handles collection type of element within each batch.\nThe function also opens function registry to deal with specific element types. provides default collate functions for tensors, numpy arrays, numbers and strings.     \n  * () – Optional dictionary mapping from element type to the corresponding collate function. If the element type isn’t present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key.\n\n\n```\n   \n    \n      \n \n       \n      \n\n \n\n```\n\nEach collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as .     \nTake in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\nThe exact output type can be a , a of , a Collection of , or left unchanged, depending on the input type. This is used as the default function for collation when or is defined in .\nHere is the general input type (based on the type of the element within the batch) to output type mapping:\n> \n\n```\n\n   \n\n\n  \n\n\n       \n{'A': tensor([  0, 100]), 'B': tensor([  1, 100])}\n\n    \n   \n\n\n   \n\n\n   \n\n# Two options to extend `default_collate` to handle specific type\n# Option 1: Write custom collate function and invoke `default_collate`\n \n      \n        \n         \n      \n         \n\n   \n     \n \n  \n\n```\n    \nIf the input is a , , or , it tries to convert each element inside to a . If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both and are NOT defined in .\nThe general input type to output type mapping is similar to that of . See the description there for more details.\n```\n\n\n\n\n \n\n\n    \n \n\n \n\n\n   \n\n\n```\n    \nWhen called in a worker, this returns an object guaranteed to have the following attributes:\n  * : the random seed set for the current worker. This value is determined by main process RNG and the worker id. See ’s documentation for more details.\n  * : the copy of the dataset object in process. Note that this will be a different object in a different process than the one in the main process.\n\n\nWhen used in a passed over to , this method can be useful to set up each worker process differently, for instance, using to configure the object to only read a specific fraction of a sharded dataset, or use to seed other libraries used in dataset code.     \nRandomly split a dataset into non-overlapping new datasets of given lengths.\nIf a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided.\nAfter computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.     \n  * () – lengths or fractions of splits to be produced\n\n    \nEvery Sampler subclass has to provide an method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and may provide a method that returns the length of the returned iterators.     \n() – This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.\nThe method isn’t strictly required by , but is expected in any calculation involving the length of a .          \nSamples elements randomly. If without replacement, then sample from a shuffled dataset.     \n\n    \nSamples elements randomly from a given list of indices, without replacement.     \n\n         \n  * () – a sequence of weights, not necessary summing up to one\n  * (, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.\n\n         \n  * (, the sampler will drop the last batch if its size would be less than \n\n\n```\n  \n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n  \n\n\n```\n    \nSampler that restricts data loading to a subset of the dataset.\nIt is especially useful in conjunction with . In such a case, each process can pass a instance as a sampler, and load a subset of the original dataset that is exclusive to it.\nDataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order.     \n  * () – Number of processes participating in distributed training. By default, is retrieved from the current distributed group.\n  * () – Rank of the current process within . By default, is retrieved from the current distributed group.\n  * () – random seed used to shuffle the sampler if . This number should be identical across all processes in the distributed group. Default: .\n  * () – if , then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If , the sampler will add extra indices to make the data evenly divisible across the replicas. Default: .\n\n\nIn distributed mode, calling the method at the beginning of each epoch creating the iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpu.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis package implements abstractions found in to facilitate writing device-agnostic code.\nWaits for all kernels in all streams on the CPU device to complete.  \n---  \nWrapper around the Context-manager StreamContext that selects a given stream.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/deploy.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cpp_extension.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nConvenience method that creates a with the bare minimum (but often sufficient) arguments to build a C++ extension.\nAll arguments are forwarded to the constructor. Full list arguments can be found at \nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.     \nConvenience method that creates a with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.\nAll arguments are forwarded to the constructor. Full list arguments can be found at \nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.\nBy default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, PyTorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).\nYou can override the default behavior using to explicitly specify which CCs you want the extension to support:\nThe +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC >= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better.\nNote that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.\nNote that CUDA-11.5 nvcc will hit internal compiler error while parsing torch/extension.h on Windows. To workaround the issue, move python binding logic to pure C++ file.\nIf you want to reference device symbols across compilation units (across object files), the object files need to be built with (-rdc=true or -dc). An exception to this rule is “dynamic parallelism” (nested kernel launches) which is not used a lot anymore. is less optimized so it needs to be used only on object files that need it. Using (Device Link Time Optimization) at the device code compilation step and step helps reduce the protentional perf degradation of . Note that it needs to be used at both steps to be useful.\nIf you have objects you need to have an extra (device linking) step before the CPU symbol linking step. There is also a case where is used without : when an extension is linked against a static lib containing rdc-compiled objects like the [NVSHMEM library](\nNote: Ninja is required to build a CUDA Extension with RDC linking.     \nConvenience method that creates a with the bare minimum (but often sufficient) arguments to build a SYCL/C++ extension.\nThe PyTorch python API (as provided in libtorch_python) cannot be built with the flag . When this flag is passed, it is the user’s responsibility in their library to not use APIs from libtorch_python (in particular pytorch/python bindings) and to only use APIs from libtorch (aten objects, operators and the dispatcher). For example, to give access to custom ops from python, the library should register the ops through the dispatcher.\nContrary to CPython setuptools, who does not define -DPy_LIMITED_API as a compile flag when py_limited_api is specified as an option for the “bdist_wheel” command in , PyTorch does! We will specify -DPy_LIMITED_API=min_supported_cpython to best enforce consistency, safety, and sanity in order to encourage best practices. To target a different version, set min_supported_cpython to the hexcode of the CPython version of choice.\nBy default the extension will be compiled to run on all archs of the cards visible during the building process of the extension. If down the road a new card is installed the extension may need to be recompiled. You can override the default behavior using to explicitly specify which device architectures you want the extension to support:\nNote that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.     \nThis subclass takes care of passing the minimum required compiler flags (e.g. ) as well as mixed C++/CUDA/SYCL compilation (and support for CUDA/SYCL files in general).\nWhen using , it is allowed to supply a dictionary for (rather than the usual list) that maps from languages/compilers (the only expected values are , or ) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++, CUDA and SYCL compiler during mixed compilation.\n(bool): If is (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard . Fallbacks to the standard distutils backend if Ninja is not available.\nBy default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the environment variable to a non-negative number.     \nTo load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.\nBy default, the directory to which the build file is emitted and the resulting library compiled to is , where is the temporary folder on the current platform and the name of the extension. This location can be overridden in two ways. First, if the environment variable is set, it replaces and all extensions will be compiled into subfolders of this directory. Second, if the argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.\nTo compile the sources, the default system compiler () is used, which can be overridden by setting the environment variable. To pass additional arguments to the compilation process, or can be provided. For example, to compile your extension with optimizations, pass . You can also use to pass further include directories.\nCUDA support with mixed compilation is provided. Simply pass CUDA source files ( or ) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking . You can pass additional flags to nvcc via , just like with for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the environment variable is the safest option.\nSYCL support with mixed compilation is provided. Simply pass SYCL source files () along with other sources. Such files will be detected and compiled with SYCL compiler (such as Intel DPC++ Compiler) rather than the C++ compiler. You can pass additional flags to SYCL compiler via , just like with for C++. SYCL compiler is expected to be found via system PATH environment variable.     \n  * – The name of the extension to build. This MUST be the same as the name of the pybind11 module!\n  * () – A list of relative or absolute paths to C++ source files.\n  * – optional list of compiler flags to forward to the build.\n  * – optional list of compiler flags to forward to nvcc when building CUDA sources.\n  * – optional list of compiler flags to forward to SYCL compiler when building SYCL sources.\n  * – optional list of linker flags to forward to the build.\n  * – optional list of include directories to forward to the build.\n  * () – Determines whether CUDA headers and libraries are added to the build. If set to (default), this value is automatically determined based on the existence of or in . Set it to to force CUDA headers and libraries to be included.\n  * () – Determines whether SYCL headers and libraries are added to the build. If set to (default), this value is automatically determined based on the existence of in . Set it to to force SYCL headers and libraries to be included.\n  * – If (default), imports the produced shared library as a Python module. If , behavior depends on .\n  * – If (default) loads the constructed extension into the process as a plain dynamic library. If , build a standalone executable.\n\n         \nReturns nothing. (The shared library is loaded into the process as a side effect.)     \nReturn the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)     \nLoad a PyTorch C++ extension just-in-time (JIT) from string sources.\nThis function behaves exactly like , but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of is identical to .\nSources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to are first concatenated into a single file. This file is then prepended with .\nFurthermore, if the argument is supplied, bindings will be automatically generated for each function specified. can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.\nThe sources in are concatenated into a separate file and prepended with , and includes. The and files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the (and include its name in ).\nThe sources in are concatenated into a separate file and prepended with , includes. The and files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in per se. To bind to a SYCL kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the (and include its name in ).     \n  * – A string, or list of strings, containing C++ source code.\n  * – A string, or list of strings, containing CUDA source code.\n  * – A string, or list of strings, containing SYCL source code.\n  * – A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).\n  * – Determines whether CUDA headers and libraries are added to the build. If set to (default), this value is automatically determined based on whether is provided. Set it to to force CUDA headers and libraries to be included.\n  * – Determines whether SYCL headers and libraries are added to the build. If set to (default), this value is automatically determined based on whether is provided. Set it to to force SYCL headers and libraries to be included.\n  * – Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function is called via an intermediary function. This redirection might cause issues in obscure cases of cpp. This flag should be set to when this redirect causes issues.\n\n\nSince load_inline will just-in-time compile the source code, please ensure that you have the right toolchains installed in the runtime. For example, when loading C++, make sure a C++ compiler is available. If you’re loading a CUDA extension, you will need to additionally install the corresponding CUDA toolkit (nvcc and any other dependencies your code has). Compiling toolchains are not included when you install torch and must be additionally installed.\nDuring compiling, by default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the environment variable to a non-negative number.     \nGet the include paths required to build a C++ or CUDA or SYCL extension.     \nDetermine if the given compiler is ABI-compatible with PyTorch alongside its version.     \nA tuple that contains a boolean that defines if the compiler is (likely) ABI-incompatible with PyTorch, followed by a string that contains the compiler version separated by dots.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/ddp_comm_hooks.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nDDP communication hook is a generic interface to control how to communicate gradients across workers by overriding the vanilla allreduce in . A few built-in communication hooks are provided, and users can easily apply any of these hooks to optimize communication. Besides, the hook interface can also support user-defined communication strategies for more advanced use cases.\nTo use a communication hook, the user just needs to let the DDP model register the hook before the training loop as below.\nA communication hook provides a flexible way to allreduce gradients. Therefore, it mainly operates on the gradients on each replica before allreduce, which are bucketized to increase the overlap between communication and computation. Particularly, represents a bucket of gradient tensors to be allreduced.     \nThis class mainly passes a flattened gradient tensor (returned by ) to DDP communication hook. This tensor can be further decomposed into a list of per-parameter tensors within this bucket (returned by ) to apply layer-wise operations.     \nSince the buckets are rebuilt after the first iteration, should not rely on the indices at the beginning of training.     \nThe index of a bucket that stores gradients of a few contiguous layers. All the gradients are bucketized.          \nA flattened 1D buffer, which can be further decomposed into a list of per-parameter tensors within this bucket.          \nA list of . Each tensor in the list corresponds to a gradient.          \nWhether this bucket is the last bucket to allreduce in an iteration. This also means that this bucket corresponds to the first few layers in the forward pass.     \nReplaces the tensor in the bucket with the input tensor buffer.          \nA list of . Each tensor in the list corresponds to a model parameter.\nDefault communication hooks are simple hooks, so the input state in is either a process group or . The input is a object.     \nOnce gradient tensors are aggregated across all workers, its callback takes the mean and returns the result.\nIf user registers this DDP communication hook, DDP results is expected to be same as the case where no hook was registered. Hence, this won’t change behavior of DDP and user can use this as a reference or modify this hook to log useful information or any other purposes while unaffecting DDP behavior.     \nThis DDP communication hook implements a simple gradient compression approach that casts tensor to half-precision floating-point format () and then divides it by the process group size. It allreduces those gradient tensors. Once compressed gradient tensors are allreduced, the chained callback casts it back to the input data type (such as ).     \nWarning: This API is experimental, and it requires NCCL version later than 2.9.6.\nThis DDP communication hook implements a simple gradient compression approach that casts tensor to half-precision ) and then divides it by the process group size. It allreduces those gradient tensors. Once compressed gradient tensors are allreduced, the chained callback casts it back to the input data type (such as ).\nAdditionally, a communication hook wrapper is provided to support or as a wrapper, which can be combined with other communication hooks.     \nCast input tensor to , cast result of hook back to input dtype.\nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision floating point format (), and casts the resulting tensor of the given hook back to the input data type, such as . Therefore, is equivalent to .     \nWarning: This API is experimental, and it requires NCCL version later than 2.9.6.\nThis wrapper casts the input gradient tensor of a given DDP communication hook to half-precision ), and casts the resulting tensor of the given hook back to the input data type, such as .\nPowerSGD ( hook, and the user needs to provide a state object defined as below.     \nStore both the algorithm’s hyperparameters and internal state for all gradients during training.\nParticularly, and are the main hyperparameters that should be tuned by the user. For performance, we suggest to keep binary hyperparameters and on.\n  1. controls the size of compressed low-rank tensors, which determines the compression rate. The lower the rank, the stronger the compression.\n> 1.1. If is too low, the full model quality will need more training steps to reach or will never reach and yield loss in accuracy.\n> 1.2. The increase of can substantially increase the computation costs of the compression, and the accuracy may not be further improved beyond a certain threshold.\n\n\nTo tune , we suggest to start from 1 and increase by factors of 2 (like an exponential grid search, 1, 2, 4, …), until a satisfactory accuracy is reached. Typically only a small value 1-4 is used. For some NLP tasks (as shown in Appendix D of the original paper), this value has been increased to 32.\n  1. defers PowerSGD compression until step , and vanilla allreduce runs prior to step . This hybrid scheme of can effectively improve the accuracy, even a relatively small is used. This is because that, the beginning of training phase is usually very sensitive to inaccurate gradients, and compressing gradients too early may make the training quickly take a suboptimal trajectory, which can result in an irrecoverable impact on the accuracy.\n\n\nTo tune , we suggest to start with 10% of total training steps, and increase it until a satisfactory accuracy is reached. If there is a warm-up stage in the training, typically should be no less than the number of warm-up steps.\n  1. is the minimum compression rate required when a layer is compressed. Due to the computation overheads incurred by the compression, a tensor is worth compressing only if there can be sufficient saving in bandwidth, where . If the specified compression rate threshold cannot be satisfied, the tensor will be directly allreduced without compression.\n\n\n  1. can be a very small value (e.g., 1e-8) added to every normalized matrix column in orthogonalization step, to prevent div-by-zero error if any column has all 0s. If this can already be prevented (e.g., by batch normalization), an epsilon of 0 is recommended for accuracy.\n  2. controls whether to compress and decompress tensors with same shape in a batched operation to achieve higher parallelism. Note that you should also increase the bucket size (i.e., arg in DDP constructor) to make more same-shaped tensors appear in the same bucket, however this may reduce the overlap between computation and communication, and increase the memory footprint due to stacking the tensors of the same shape. Set to if the compression / decompression computation is a bottleneck.\n\n\nIf error feedback or warm-up is enabled, the minimum value of allowed in DDP is 2. This is because there is another internal optimization that rebuilds buckets at iteration 1 in DDP, and this can conflict with any tensor memorized before the rebuild process.\nPowerSGD typically requires extra memory of the same size as the model’s gradients to enable error feedback, which can compensate for biased compressed communication and improve accuracy.     \nThis DDP communication hook implements PowerSGD gradient compression algorithm described in the \n  1. Views the input flattened 1D gradient tensor as a list of per-parameter tensors, and divides all the tensors into two groups:\n> 1.1 The tensors that should be compressed before allreduce, because the compression can give enough saving in bandwidth.\n> 1.2 Rest of the tensors will be directly allreduced without compression, including all the vector tensors (for biases).\n  2. > 2.1. Allocate contiguous memory for those uncompressed tensors, and allreduces all the uncompressed tensors as a batch, without compression;\n> 2.2. Copies the individual uncompressed tensors from the contiguous memory back to the input tensor.\n  3. Handles the tensors that should be compressed by PowerSGD compression:\n> 3.1. For each tensor M, creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;\n> 3.2. Computes each P in Ps, which is equal to MQ;\n> 3.5. Computes each Q in Qs, which is approximately equal to M^TP;\n> 3.7. Computes each M among all the compressed tensors, which is approximately equal to PQ^T.\n\n\nNote that this communication hook enforces vanilla allreduce for the first iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.     \n  * () – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune , and .\n  * () – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.\n\n    \nFuture handler of the communication, which updates the gradients in place.     \nThis DDP communication hook implements a simplified PowerSGD gradient compression algorithm described in the than , but usually results in a , unless is 1.\nIncreasing here may not necessarily increase the accuracy, because batching per-parameter tensors without column/row alignment can destroy low-rank structure. Therefore, the user should always consider first, and only consider this variant when a satisfactory accuracy can be achieved when is 1.\nOnce gradient tensors are aggregated across all workers, this hook applies compression as follows:\n  1. Views the input flattened 1D gradient tensor as a square-shaped tensor M with 0 paddings;\n  2. Creates two low-rank tensors P and Q for decomposing M, such that M = PQ^T, where Q is initialized from a standard normal distribution and orthogonalized;\n\n\nNote that this communication hook enforces vanilla allreduce for the first iterations. This not only gives the user more control over the tradeoff between speedup and accuracy, but also helps abstract away some complexity of the internal optimization of DDP for future communication hook developers.     \n  * () – State information to configure the compression rate and support error feedback, warm start, etc. To tune the compression configs, mainly need to tune and .\n  * () – Bucket that stores a 1D flattened gradient tensor that batches multiple per-variable tensors. Note that since DDP comm hook only supports single process single device mode, only exactly one tensor is stored in this bucket.\n\n    \nFuture handler of the communication, which updates the gradients in place.\nAs the name implies, debugging communication hooks are used for debugging and performance optimization purpose.\nDebugging communication hooks do not necessarily output the correct results.     \nReturn a future that wraps the input, so it is a no-op that does not incur any communication overheads.\nThis hook should be used for headroom analysis of allreduce optimization, instead of the normal gradient synchronization. For example, if only less than 10% speedup of training time can be observed after this hook is registered, it usually implies that allreduce is not a performance bottleneck for this case. Such instrumentation can be particularly useful if GPU traces cannot be easily retrieved or the trace analysis is complicated some factors such as the overlap between allreduce and computation or the desynchronization across ranks.\nA stateful communication hook can be saved as a part of model checkpointing to enable trainer restarts. To make a hook serializable, and should be defined.          \nHere is a simple, end-to-end example of saving and reloading PowerSGD state and hook.\n```\n \n \n \n \n   \n   \n   \n   \n\n   \n     \n\n \n     \n        \n          \n          \n          \n\n      \n         \n\n  \n      \n      \n\n    \n      \n\n \n    \n\n  \n    \n        \n        \n        \n        \n\n  \n     \n\n        \n\n      \n       \n\n      \n      \n\n       \n     \n\n      \n         \n         \n         \n\n       \n         \n\n    \n           \n       \n\n       \n    \n      \n      \n\n     \n\n       \n        \n\n    \n\n   \n      \n        \n      \n     \n\n```\n\nMany thanks to PowerSGD paper author for the code review on PowerSGD communication hook, as well as the \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributions.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the \nIt is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples , the pathwise derivative requires the derivative . The next sections discuss these two in a reinforcement learning example. For more details see \nWhen the probability density function is differentiable with respect to its parameters, we only need and to implement REINFORCE:\nwhere are the parameters, is the learning rate, is the reward and is the probability of taking action in state given policy .\nIn practice we would sample an action from the output of a network, apply this action in an environment, and then use to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:\n```\n  \n# Note that this is equivalent to what used to be called multinomial\n  \n  \n   \n    \n\n\n```\n\nThe other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:\n```\n  \n  \n# Any distribution with .has_rsample == True could work based on the application\n  \n     \n  \n\n\n```\n         \nReturns a dictionary from argument names to objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.          \nReturns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be (where for univariate distributions).\nNote that this enumerates over all batched tensors in lock-step . With , enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, .     \nReturns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to . This method calls on the distribution’s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in , when an instance is first created.     \n\n         \nReturns the log of the probability density/mass function evaluated at .          \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.     \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.     \nGenerates n samples or n batches of samples if the distribution parameters are batched.     \nThe default behavior mimics Python’s statement: validation is on by default, but is disabled if Python is run in optimized mode (via ). Validation may be expensive, so you may want to disable it once a model is working.     \nExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below\np_{F}(x; \\theta) = \\exp(\\langle t(x), \\theta\\rangle - F(\\theta) + k(x))\nwhere denotes the natural parameters, denotes the sufficient statistic, is the log normalizer function for a given family and is the carrier measure.\nThis class is an intermediary between the class and distributions which belong to an exponential family mainly to check the correctness of the and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).     \nMethod to compute the entropy using Bregman divergence of the log normalizer.     \nSamples are binary (0 or 1). They take the value with probability and with probability .     \n\n         \n  * () – 1st concentration parameter of the distribution (often referred to as alpha)\n  * () – 2nd concentration parameter of the distribution (often referred to as beta)\n\n    \nCreates a Binomial distribution parameterized by and either or (but not both). must be broadcastable with /.\n```\n       \n  \ntensor([   0.,   22.,   71.,  100.])\n\n     \n  \n\n\n\n```\n    \n\n    \nIf is 1-dimensional with length-, each element is the relative probability of sampling the class at that index.\nIf is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.\nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.     \nSamples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means follows a Cauchy distribution.\n```\n   \n  # sample from a Cauchy distribution with loc=0 and scale=1\n\n\n```\n    \n\n    \nCreates a Chi-squared distribution parameterized by shape parameter . This is exactly equivalent to      \nThe distribution is supported in [0, 1] and parameterized by ‘probs’ (in (0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’ does not correspond to a probability and ‘logits’ does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.     \n\n\n[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.      \n```\n   \n  \n\n\n```\n    \n() – concentration parameter of the distribution (often referred to as alpha)          \n() – rate = 1 / scale of the distribution          \n\n         \n  * () – shape parameter of the distribution (often referred to as alpha)\n  * () – rate parameter of the distribution (often referred to as beta), rate = 1 / scale\n\n    \nCreates a Geometric distribution parameterized by , where is the probability of success of Bernoulli trials.\n-th trial is the first success hence draws samples in , whereas -th trial is the first success hence draws samples in .\n```\n  \n  # underlying Bernoulli has 30% chance 1; 70% chance 0\n\n\n```\n    \n  * () – the probability of sampling . Must be in range (0, 1]\n\n         \n\n              \nReinterprets some of the batch dims of a distribution as event dims.\nThis is mainly useful for changing the shape of the result of . For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:          \n  * () – shape parameter of the distribution (often referred to as alpha)\n  * () – rate = 1 / scale of the distribution (often referred to as beta)\n\n    \n```\n   \n  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\n\n\n```\n    \n  * () – 1st concentration parameter of the distribution (often referred to as alpha)\n  * () – 2nd concentration parameter of the distribution (often referred to as beta)\n\n    \nLKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by parameter to make the probability of the correlation matrix generated from a Cholesky factor proportional to . Because of that, when , we have a uniform distribution over Cholesky factors of correlation matrices:\nNote that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the distribution. For sampling, this uses the Onion method from [1] Section 3.\n```\n   \n  # l @ l.T is a sample of a correlation 3x3 matrix\n\n\n\n\n```\n    \n  * () – concentration/shape parameter of the distribution (often referred to as eta)\n\n\n[1] Generating random correlation matrices based on vines and extended onion method (2009), Daniel Lewandowski, Dorota Kurowicka, Harry Joe. Journal of Multivariate Analysis. 100. 10.1016/j.jmva.2009.04.008          \n\n         \n\n    \nCreates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by and :     \n  * () – mean of the distribution with shape \n  * () – factor part of low-rank form of covariance matrix with shape \n  * () – diagonal part of low-rank form of covariance matrix with shape \n\n\nThe computation for determinant and inverse of covariance matrix is avoided when thanks to      \nThe distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a “selecting distribution” (over component) and a component distribution, i.e., a with a rightmost batch shape (equal to ) which indexes each (batch of) component.\n```\n# Construct Gaussian Mixture Model in 1D consisting of 5 equally\n\n  \n   \n   \n\n# Construct Gaussian Mixture Model in 2D consisting of 5 equally\n\n  \n  \n           \n   \n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n\n  \n  \n          \n   \n\n```\n    \n  * () – -like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the . Must have either scalar or matching \n\n    \nCreates a Multinomial distribution parameterized by and either or (but not both). The innermost dimension of indexes over categories. All other dimensions index over batches.\nNote that need not be specified if only is called (see example below)\nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.\n\n\n```\n       \n    \n\n\n   \n\n\n```\n    \nCreates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.\nThe multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix or a positive definite precision matrix or a lower-triangular matrix with positive-valued diagonal entries, such that . This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.     \n\n\nUsing will be more efficient: all computations internally are based on . If or is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.     \nCreates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before failures are achieved. The probability of success of each Bernoulli trial is .     \n  * () – non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count\n  * () – Event probabilities of success in the half open interval [0, 1)\n\n         \n  * () – mean of the distribution (often referred to as mu)\n  * () – standard deviation of the distribution (often referred to as sigma)\n\n    \nThe argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. will return this normalized value. The argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. will return this normalized value.\n```\n       \n  \n\n\n```\n    \n```\n   \n  # sample from a Pareto distribution with scale=1 and alpha=1\n\n\n```\n    \n\n         \nCreates a RelaxedBernoulli distribution, parametrized by , and either or (but not both). This is a relaxed version of the distribution, so the values are in (0, 1), and has reparametrizable samples.     \n\n    \nCreates a LogitRelaxedBernoulli distribution parameterized by or (but not both), which is the logit of a RelaxedBernoulli distribution.\nSamples are logits of values in (0, 1). See [1] for more details.     \n\n\n[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al., 2017)     \nCreates a RelaxedOneHotCategorical distribution parametrized by , and either or . This is a relaxed version of the distribution, so its samples are on simplex, and are reparametrizable.     \n\n    \nCreates a Student’s t-distribution parameterized by degree of freedom , mean and scale .     \n\n    \nExtension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:\nNote that the of a is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.\n```\n\n\n\n\n   \n    \n   \n\n```\n    \nComputes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.     \nComputes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.     \nScores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.     \nGenerates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies for every transform in the list.     \nGenerates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies for every transform in the list.          \nThis implementation uses polar coordinates. The and args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.          \nThe sampling algorithm for the von Mises distribution is based on the following paper: D.J. Best and N.I. Fisher, “Efficient simulation of the von Mises distribution.” Applied Statistics (1979): 152-157.\nSampling is always done in double precision internally to avoid a hang in _rejection_sample() for small values of the concentration, which starts to happen for single precision around 1e-4 (see issue #88443).     \n```\n   \n  \n\n\n```\n    \n\n    \nCreates a Wishart distribution parameterized by a symmetric positive definite matrix , or its Cholesky decomposition \n```\n   \n  \n# variance(x_ij)=`df` for i != j and variance(x_ij)=`2 * df` for i == j\n\n```\n    \n  * () – real-valued parameter larger than the (dimension of Square matrix) - 1\n\n\nOnly one of or or can be specified. Using will be more efficient: all computations internally are based on . If or is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition. ‘torch.distributions.LKJCholesky’ is a restricted Wishart distribution.[1]\n[1] Wang, Z., Wu, Y. and Chu, H., 2018. On equivalence of the LKJ distribution and the restricted Wishart distribution. [2] Sawyer, S., 2007. . [3] Anderson, T. W., 2003. . [4] Odell, P. L. & Feiveson, A. H., 1966. . JASA, 61(313):199-203. [5] Ku, Y.-C. & Bloomfield, P., 2010. Generating Random Wishart Matrices with Fractional Degrees of Freedom in OX.     \nIn some cases, sampling algorithm based on Bartlett decomposition may return singular matrix samples. Several tries to correct singular samples are performed by default, but it may end up returning singular matrix samples. Singular samples may return values in . In those cases, the user should validate the samples and either fix the value of or adjust value for argument in accordingly.     \nKL(p \\| q) = \\int p(x) \\log\\frac {p(x)} {q(x)} \\,dx \n\nKL divergence is currently implemented for the following distribution pairs:\n    \nLookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a is raised. For example to resolve the ambiguous situation:     \nTransform via the pointwise affine mapping .     \n  * (. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.\n\n    \nTransform functor that applies a sequence of transforms component-wise to each submatrix at , of length , in a way compatible with .     \nComposes multiple transforms in a chain. The transforms being composed are responsible for caching.     \nTransforms an uncontrained real vector with length into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:\n>   1. First we convert x into a lower triangular matrix in row order.\n>   2. For each row of the lower triangular part, we apply a version of class to transform into a unit Euclidean length vector using the following steps: - Scales into the interval domain: . - Transforms into an unsigned domain: . - Applies . - Transforms back into signed domain: .\n> \n    \nTransform via the cumulative distribution function of a probability distribution.     \n() – Distribution whose cumulative distribution function to use for the transformation.     \nWrapper around another transform to treat -many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out -many of the rightmost dimensions in .     \nTransform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.\nThis is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.     \nUnit Jacobian transform to reshape the rightmost part of a tensor.\nNote that and must have the same number of elements, just as for .     \n\n    \nTransform via the mapping and .     \nTransform via the mapping . The implementation reverts to the linear function when .     \nHowever this might not be numerically stable, thus it is recommended to use instead.     \nTransform from unconstrained space to the simplex via then normalizing.\nThis is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.     \nTransform functor that applies a sequence of transforms component-wise to each submatrix at in a way compatible with .     \nTransform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.\nThis transform arises as an iterated sigmoid transform in a stick-breaking construction of the distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.\nThis is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.     \nAbstract class for invertable transformations with computable log det jacobians. They are primarily used in .\nCaching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:\nHowever the following will error when caching due to dependency reversal:\nDerived classes should implement one or both of or . Derived classes that set should also implement .     \n  * () – The constraint representing valid inputs to this transform.\n  * () – The constraint representing valid outputs to this transform which are inputs to the inverse transform.\n  * ( is bijective iff and for every in the domain and in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties and .\n  * () – For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.\n\n    \nReturns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.     \nInfers the shape of the forward computation, given the input shape. Defaults to preserving shape.     \nInfers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.     \nA constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.     \nReturns a byte tensor of indicating whether each event in value satisfies this constraint.     \nPyTorch provides two global objects that link objects to objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.\n  1. looks up a bijective from to the given . The returned transform is guaranteed to have and should implement .\n  2. looks up a not-necessarily bijective from to the given . The returned transform is not guaranteed to implement .\n\n\nThe registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:\nThe registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained are propagated in an unconstrained space, and algorithms are typically rotation invariant.:\nAn example where and differ is : returns a that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, returns a that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.\nThe and objects can be extended by user-defined constraints and transforms using their method either as a function on singleton constraints:\nYou can create your own registry by creating a new object.               \n  * (subclass of ) – A subclass of , or a singleton object of the desired class.\n  * () – A callable that inputs a constraint object and returns a object.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/export.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis feature is a prototype under active development and there WILL BE BREAKING CHANGES in the future.\ntakes a and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different outputs or serialized.\n```\n\n     \n             \n            \n               \n\n            \n               \n\n            \n                 \n             \n\n     \n        \n            \n                \n                     \n                    \n                    \n                    \n                \n                \n                     \n                    \n                    \n                    \n                \n            \n            \n                \n                     \n                    \n                    \n                \n            \n        \n      \n\n```\n\nproduces a clean intermediate representation (IR) with the following invariants. More specifications about the IR can be found .\n  * : It is guaranteed to be a sound representation of the original program, and maintains the same calling conventions of the original program.\n  * : There are no Python semantics within the graph. Submodules from the original programs are inlined to form one fully flattened computational graph.\n  * : The graph is purely functional, meaning it does not contain operations with side effects such as mutations or aliasing. It does not mutate any intermediate values, parameters, or buffers.\n  * : The graph contains metadata captured during tracing, such as a stacktrace from user’s code.\n\n\n  * is an internal API that uses a CPython feature called the Frame Evaluation API to safely trace PyTorch graphs. This provides a massively improved graph capturing experience, with much fewer rewrites needed in order to fully trace the PyTorch code.\n  * provides a functionalized PyTorch graph and ensures the graph is decomposed/lowered to the ATen operator set.\n  * is the underlying representation of the graph, allowing flexible Python-based transformations.\n\n\nalso utilizes the same PT2 stack as , but is slightly different:\n  * : is a JIT compiler whereas which is not intended to be used to produce compiled artifacts outside of deployment.\n  * : When runs into an untraceable part of a model, it will “graph break” and fall back to running the program in the eager Python runtime. In comparison, aims to get a full graph representation of a PyTorch model, so it will error out when something untraceable is reached. Since produces a full graph disjoint from any Python features or runtime, this graph can then be saved, loaded, and run in different environments and languages.\n  * : Since is able to fallback to the Python runtime whenever it reaches something untraceable, it is a lot more flexible. will instead require users to provide more information or rewrite their code to make it traceable.\n\n\nCompared to , traces using TorchDynamo which operates at the Python bytecode level, giving it the ability to trace arbitrary Python constructs not limited by what Python operator overloading supports. Additionally, keeps fine-grained track of tensor metadata, so that conditionals on things like tensor shapes do not fail tracing. In general, is expected to work on more user programs, and produce lower-level graphs (at the operator level). Note that users can still use as a preprocessing step before .\nCompared to , does not capture Python control flow or data structures, but it supports more Python language features than TorchScript (as it is easier to have comprehensive coverage over Python bytecodes). The resulting graphs are simpler and only have straight line control flow (except for explicit control flow operators).\nCompared to , is sound: it is able to trace code that performs integer computation on sizes and records all of the side-conditions necessary to show that a particular trace is valid for other inputs.\nThe main entrypoint is through , which takes a callable (, function, or method) and sample inputs, and captures the computation graph into an . An example:\n```\n\n     \n             \n            \n                     \n\n            \n                \n\n            \n               \n                   \n             \n\n \n    \n        \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n            \n                 \n                \n                \n                \n            \n        \n        \n            \n                 \n                \n                \n            \n        \n    \n  \n\n```\n\n  * The contains the computation graph of the original program, along with records of the original code for easy debugging.\n  * The parameters (weight and bias to conv) are lifted as inputs to the graph, resulting in no nodes in the graph, which previously existed in the result of .\n  * The models the input and output signature, along with specifying which inputs are parameters.\n  * The resulting shape and dtype of tensors produced by each node in the graph is noted. For example, the node will result in a tensor of dtype and shape (1, 16, 256, 256).\n\n\nIn PyTorch 2.3, we introduced a new mode of tracing called . It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag.\nIn , we trace through the program using the Python interpreter. Your code will execute exactly as it would in eager mode; the only difference is that all Tensor objects will be replaced by ProxyTensors, which will record all their operations into a graph.\nIn mode, which is currently the default, we first trace through the program using TorchDynamo, a bytecode analysis engine. TorchDynamo does not actually execute your Python code. Instead, it symbolically analyzes it and builds a graph based on the results. This analysis allows torch.export to provide stronger guarantees about safety, but not all Python code is supported.\nAn example of a case where one might want to use non-strict mode is if you run into a unsupported TorchDynamo feature that might not be easily solved, and you know the python code is not exactly needed for computation. For example:\n```\n \n \n\n \n     \n          \n     \n          \n        \n          \n\n \n      \n         \n               \n\n     \n    \n\n```\n\nIn this example, the first call using non-strict mode (through the flag) traces successfully whereas the second call using strict mode (default) results with a failure, where TorchDynamo is unable to support context managers. One option is to rewrite the code (see ), but seeing as the context manager does not affect the tensor computations in the model, we can go with the non-strict mode’s result.\nIn PyTorch 2.5, we introduced a new API called . It’s still going through hardening, so if you run into any issues, please file them to Github with the “oncall: export” tag.\nIn this API, we produce the most generic IR that contains all ATen operators (including both functional and non-functional) which can be used to train in eager PyTorch Autograd. This API is intended for eager training use cases such as PT2 Quantization and will soon be the default IR of torch.export.export. To read further about the motivation behind this change, please refer to \nWhen this API is combined with , you should be able to get inference IR with any desired decomposition behavior.\n```\n\n     \n                         \n                 \n                \n                       \n             \n\n```\n\nFrom the above output, you can see that produces pretty much the same ExportedProgram as except for the operators in the graph. You can see that we captured batch_norm in the most general form. This op is non-functional and will be lowered to different ops when running inference.\nYou can also go from this IR to an inference IR via with arbitrary customizations.\n```\n# Lower to core aten inference IR, but keep conv2d\n  \n \n  \n\n\n\n```\n\n```\n\n     \n                         \n                 \n                \n                     \n               \n               \n               \n                \n\n```\n\nHere you can see that we kept op in the IR while decomposing the rest. Now the IR is a functional IR containing core aten operators except for .\nYou can do even more customization by directly registering your chosen decomposition behaviors.\nYou can do even more customizations by directly registering custom decomp behaviour\n```\n# Lower to core aten inference IR, but customize conv2d\n  \n\n          \n                \n\n  \n  \n\n\n\n```\n\n```\n\n     \n                         \n                           \n                \n                \n                     \n               \n               \n               \n                \n\n```\n\nBy default will trace the program assuming all input shapes are , and specializing the exported program to those dimensions. However, some dimensions, such as a batch dimension, can be dynamic and vary from run to run. Such dimensions must be specified by using the API to create them and by passing them into through the argument. An example:\n```\n \n    \n\n \n     \n        \n\n          \n              \n        \n          \n              \n        \n          \n\n       \n          \n          \n            \n\n     \n\n\n  \n# Specify that the first dimension of each input is that batch size\n       \n\n   \n      \n\n\n\n```\n\n```\n\n \n                   \n\n         \n             \n           \n\n         \n             \n           \n\n         \n            \n          \n\n    \n\n```\n\n  * Through the API and the argument, we specified the first dimension of each input to be dynamic. Looking at the inputs and , they have a symbolic shape of (s0, 64) and (s0, 128), instead of the (32, 64) and (32, 128) shaped tensors that we passed in as example inputs. is a symbol representing that this dimension can be a range of values.\n  * describes the ranges of each symbol appearing in the graph. In this case, we see that has the range [0, int_oo]. For technical reasons that are difficult to explain here, they are assumed to be not 0 or 1. This is not a bug, and does not necessarily mean that the exported program will not work for dimensions 0 or 1. See \n\n\nWe can also specify more expressive relationships between input shapes, such as where a pair of shapes might differ by one, a shape might be double of another, or a shape is even. An example:\n  * By specifying for the first input, we see that the resulting shape of the first input is now dynamic, being . And now by specifying for the second input, we see that the resulting shape of the second input is also dynamic. However, because we expressed , instead of ’s shape containing a new symbol, we see that it is now being represented with the same symbol used in , . We can see that relationship of is being shown through .\n  * Looking at the range constraints, we see that has the range [3, 6], which is specified initially, and we can see that has the solved range of [4, 7].\n\n\nTo save the , users can use the and APIs. A convention is to save the using a file extension.\nA key concept in understanding the behavior of is the difference between and values.\nA value is one that can change from run to run. These behave like normal arguments to a Python function—you can pass different values for an argument and expect your function to do the right thing. Tensor is treated as dynamic.\nA value is a value that is fixed at export time and cannot change between executions of the exported program. When the value is encountered during tracing, the exporter will treat it as a constant and hard-code it into the graph.\nWhen an operation is performed (e.g. ) and all inputs are static, then the output of the operation will be directly hard-coded into the graph, and the operation won’t show up (i.e. it will get constant-folded).\nWhen a value has been hard-coded into the graph, we say that the graph has been to that value.\nBy default, will trace the program specializing on the input tensors’ shapes, unless a dimension is specified as dynamic via the argument to . This means that if there exists shape-dependent control flow, will specialize on the branch that is being taken with the given sample inputs. For example:\nThe conditional of () does not appear in the because the example inputs have the static shape of (10, 2). Since specializes on the inputs’ static shapes, the else branch () will never be reached. To preserve the dynamic branching behavior based on the shape of a tensor in the traced graph, will need to be used to specify the dimension of the input tensor () to be dynamic, and the source code will need to be .\nNote that tensors that are part of the module state (e.g. parameters and buffers) always have static shapes.\nalso specializes on Python primtivies, such as , , , and . However they do have dynamic variants such as , , and .\n```\n\n     \n             \n            \n                \n                \n                \n             \n\n```\n\nBecause integers are specialized, the operations are all computed with the hard-coded constant , rather than . If a user passes a different value for at runtime, like 2, than the one used during export time, 1, this will result in an error. Additionally, the iterator used in the loop is also “inlined” in the graph through the 3 repeated calls, and the input is never used.\nAs is a one-shot process for capturing a computation graph from a PyTorch program, it might ultimately run into untraceable parts of programs as it is nearly impossible to support tracing all PyTorch and Python features. In the case of , an unsupported operation will cause a “graph break” and the unsupported operation will be run with default Python evaluation. In contrast, will require users to provide additional information or rewrite parts of their code to make it traceable. As the tracing is based on TorchDynamo, which evaluates at the Python bytecode level, there will be significantly fewer rewrites required compared to previous tracing frameworks.\nWhen a graph break is encountered, is a great resource for learning about the kinds of programs that are supported and unsupported, along with ways to rewrite programs to make them traceable.\nAn option to get past dealing with this graph breaks is by using \nGraph breaks can also be encountered on data-dependent control flow () when shapes are not being specialized, as a tracing compiler cannot possibly deal with without generating code for a combinatorially exploding number of paths. In such cases, users will need to rewrite their code using special control flow operators. Currently, we support to express if-else like control flow (more coming soon!).\nWhen tracing, a FakeTensor kernel (aka meta kernel, abstract impl) is required for all operators. This is used to reason about the input/output shapes for this operator.\nIn the unfortunate case where your model uses an ATen operator that is does not have a FakeTensor kernel implementation yet, please file an issue.\n\n    \ntakes any nn.Module along with example inputs, and produces a traced graph representing only the Tensor computation of the function in an Ahead-of-Time (AOT) fashion, which can subsequently be executed with different inputs or serialized. The traced graph (1) produces normalized operators in the functional ATen operator set (as well as any user-specified custom operators), (2) has eliminated all Python control flow and data structures (with certain exceptions), and (3) records the set of shape constraints needed to show that this normalization and control-flow elimination is sound for future inputs.\nWhile tracing, takes note of shape-related assumptions made by the user program and the underlying PyTorch operator kernels. The output is considered valid only when these assumptions hold true.\nTracing makes assumptions on the shapes (not values) of input tensors. Such assumptions must be validated at graph capture time for to succeed. Specifically:\n  * Assumptions on static shapes of input tensors are automatically validated without additional effort.\n  * Assumptions on dynamic shape of input tensors require explicit specification by using the API to construct dynamic dimensions and by associating them with example inputs through the argument.\n\n\nIf any assumption can not be validated, a fatal error will be raised. When that happens, the error message will include suggested fixes to the specification that are needed to validate the assumptions. For example might suggest the following fix to the definition of a dynamic dimension , say appearing in the shape associated with input , that was previously defined as :\nThis example means the generated code requires dimension 0 of input to be less than or equal to 5 to be valid. You can inspect the suggested fixes to dynamic dimension definitions and then copy them verbatim into your code without needing to change the argument to your call.     \n  * () – We will trace the forward method of this module.\n  * An optional argument where the type should either be: 1) a dict from argument names of to their dynamic shape specifications, 2) a tuple that specifies dynamic shape specifications for each input in original order. If you are specifying dynamism on keyword args, you will need to pass them in the order that is defined in the original function signature.\nThe dynamic shape of a tensor argument can be specified as either (1) a dict from dynamic dimension indices to types, where it is not required to include static dimension indices in this dict, but when they are, they should be mapped to None; or (2) a tuple / list of types or None, where the types correspond to dynamic dimensions, and static dimensions are denoted by None. Arguments that are dicts or tuples / lists of tensors are recursively specified by using mappings or sequences of contained specifications.\n\n\n\n    \nUnder active development, saved files may not be usable in newer versions of PyTorch.\nSaves an to a file-like object. It can then be loaded using the Python API .     \n  * () – implement write and flush) or a string containing a file name.\n  * () – Map from filename to contents which will be stored as part of f.\n  * () – A map of opset names to the version of this opset\n\n\n```\n \n \n\n \n      \n           \n\n   \n\n\n \n\n\n  \n \n\n\n   \n  \n\n```\n    \nUnder active development, saved files may not be usable in newer versions of PyTorch.     \n  * () – A file-like object (has to implement write and flush) or a string containing a file name.\n  * () – The extra filenames given in this map would be loaded and their content would be stored in the provided map.\n  * () – A map of opset names to expected opset versions\n\n\n```\n \n \n\n\n  \n\n\n    \n      \n\n  \n\n\n     \n   \n\n\n\n```\n         \n  * () – The serialized name for the dataclass. This is\n  * (_required if you want to serialize the pytree TreeSpec containing_) – \n\n    \nconstructs a type analogous to a named symbolic integer with a range. It can be used to describe multiple possible values of a dynamic tensor dimension. Note that different dynamic dimensions of the same tensor, or of different tensors, can be described by the same type.     \n\n    \nA type that can be used in dynamic shape specifications for tensors.     \nThis is the default decomposition table which contains decomposition of all ATEN operators to core aten opset. Use this API together with      \nBuilder for dynamic_shapes. Used to assign dynamic shape specifications to tensors that appear in inputs.\nThis is useful particularly when is a nested input structure, and it’s easier to index the input tensors, than to replicate the structure of in the specification.\n```\n      \n\n  \n  \n      \n     \n\n# dynamic_shapes = {\"x\": (dim, dim + 1, 8), \"others\": [{0: dim * 2}, None]}\n\n  \n\n```\n    \nWhen exporting with , export may fail with a ConstraintViolation error if the specification doesn’t match the constraints inferred from tracing the model. The error message may provide suggested fixes - changes that can be made to to export successfully.\n```\n \n\n          \n        \n          # dy was specified as an independent dim, but is actually tied to dx with this relation\n\n```\n\nThis is a helper function that takes the ConstraintViolation error message and the original spec, and returns a new spec that incorporates the suggested fixes.     \nPackage of a program from . It contains an that represents Tensor computation, a state_dict containing tensor values of all lifted parameters and buffers, and various metadata.\nYou can call an ExportedProgram like the original callable traced by with the same calling convention.\nTo perform transformations on the graph, use property to access an . You can then use to rewrite the graph. Afterwards, you can simply use again to construct a correct ExportedProgram.     \nReturns a self contained GraphModule with all the parameters/buffers inlined.          \nReturns an iterator over original module buffers, yielding both the name of the buffer as well as the buffer itself.          \nReturns an iterator over original module parameters, yielding both the name of the parameter as well as the parameter itself.     \nRun a set of decompositions on the exported program and returns a new exported program. By default we will run the Core ATen decompositions to get operators in the .     \n() – An optional argument that specifies decomp behaviour for Aten ops (1) If None, we decompose to core aten decompositions (2) If empty, we don’t decompose any operator\nIf you want to get a core aten operator set except for certain operator, you can do following:     \nmodels the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.\nExport Graph is functional and does not access “states” like parameters or buffers within the graph via nodes. Instead, gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.\n```\n \n       \n         \n\n        \n          \n\n        \n         \n         \n\n       \n        # Use the parameter, buffers, and both inputs in the forward method\n                  \n\n        # Mutate one of the buffers (e.g., increment it by 1)\n         \n\n         \n\n```\n    \nThis is a custom dictionary that is specifically used for handling decomp_table in export. The reason we need this is because in the new world, you can only an op from decomp table to preserve it. This is problematic for custom ops because we don’t know when the custom op will actually be loaded to the dispatcher. As a result, we need to record the custom ops operations until we really need to materialize it (which is when we run decomposition pass.)     \n  1. We materialize ALL ops when user ever reads from the table to make it more likely that dispatcher picks up the custom op.\n  2. We load the final time during export, right before calling run_decompositions()\n\n    \nmodels the input/output signature of Export Graph, which is a fx.Graph with stronger invariants gurantees.\nExport Graph is functional and does not access “states” like parameters or buffers within the graph via nodes. Instead, gurantees that parameters, buffers, and constant tensors are lifted out of the graph as inputs. Similarly, any mutations to buffers are not included in the graph either, instead the updated values of mutated buffers are modeled as additional outputs of Export Graph.\n```\n \n       \n         \n\n        \n          \n\n        \n         \n         \n\n       \n        # Use the parameter, buffers, and both inputs in the forward method\n                  \n\n        # Mutate one of the buffers (e.g., increment it by 1)\n         \n\n         \n\n```\n    \nReplace all uses of the old name with new name in the signature.          \nA module that uses torch.fx.Interpreter to execute instead of the usual codegen that GraphModule uses. This provides better stack trace information and makes it easier to debug execution.     \nA module that carries a sequence of InterpreterModules corresponding to a sequence of calls of that module. Each call to the module dispatches to the next InterpreterModule, and wraps back around after the last.     \nUnflatten an ExportedProgram, producing a module with the same module hierarchy as the original eager module. This can be useful if you are trying to use with another system that expects a module hierachy instead of the flat graph that usually produces.\nThe args/kwargs of unflattened modules will not necessarily match the eager module, so doing a module swap (e.g. ) will not necessarily work. If you need to swap a module out, you need to set the parameter of .     \n  * () – Adapt flat args if input TreeSpec does not match with exported module’s.\n\n    \nAn instance of , which has the same module hierarchy as the original eager module pre-export.          \n  * () – The device to move the exported program to. If a string, it is interpreted as a device name. If a dict, it is interpreted as a mapping from the existing device to the intended one\n\n\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/future_mod.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nSets whether to assign new tensors to the parameters instead of changing the existing parameters in-place when converting an .\nWhen enabled, the following methods will assign new parameters to the module:\n\n    \nReturns whether to assign new tensors to the parameters instead of changing the existing parameters in-place when converting an . Defaults to .     \nSets whether to use instead of setting to change the existing parameters in-place when converting an and instead of when loading a state dict into an .\nWhen enabled, the following methods will swap the existing parameters in-place:\n\n\n    \nReturns whether to use instead of setting .data to change the existing parameters in-place when converting an . Defaults to .\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fx.experimental.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThese APIs are experimental and subject to change without notice.\nFor clients: the size at this dimension must be within 'vr' (which specifies a lower and upper bound, inclusive-inclusive) AND it must be non-negative and should not be 0 or 1 (but see NB below).  \n---  \nFor clients: no explicit constraint; constraint is whatever is implicitly inferred by guards from tracing.  \nRepresent and decide various kinds of equality constraints between input sources.  \nData structure specifying how we should create symbols in ; e.g., should they be static or dynamic.  \nCreate symbols in via a symbolic_context determination as given by a cache of Source:Symbol.  \nThe correct symbolic context for a given inner tensor of a traceable tensor subclass may differ from that of the outer symbolic context.  \nCustom solver for a system of constraints on symbolic dimensions.  \nEncapsulates all shape env settings that could potentially affect FakeTensor dispatch.  \nRetrieve the hint for an int (based on the underlying real values as observed at runtime).  \nUtility to check if underlying object in SymInt is concrete value.  \nUtility to check if underlying object in SymBool is concrete value.  \nUtility to check if underlying object in SymInt is concrete value.  \nReturns True only if we can tell that a is True, possibly introducing a guard in the process.  \nReturns True only if we can tell that a is False, possibly introducing a guard in the process.  \nPerform a guard on a symbolic boolean expression in a size oblivious way.  \nLike ==, but when run on list/tuple, it will recursively test equality and use sym_and to join the results together, without guarding.  \nApplies a constraint that the passed in SymInt must lie between min-max inclusive-inclusive, WITHOUT introducing a guard on the SymInt (meaning that it can be used on unbacked SymInts).  \nGiven two SymInts, constrain them so that they must be equal.  \nCanonicalize a boolean expression by transforming it into a lt / le inequality and moving all the non-constant terms to the rhs.  \nReturns True if x can be simplified to a constant and is true.  \nTest that two \"meta\" values (typically either Tensor or SymInt) have the same values, e.g., after retracing.  \nAfter having run fake tensor propagation and producing example_value result, traverse example_value looking for freshly bound unbacked symbols and record their paths for later.  \nSuppose we are retracing a pre-existing FX graph that previously had fake tensor propagation (and therefore unbacked SymInts).  \nGiven a function f, return a new function which when executed with valid arguments to f, returns an FX GraphModule representing the set of operations that were executed during the course of execution.  \n---  \nCall into the currently active proxy tracing mode to do a SymInt/SymFloat/SymBool dispatch trace on a function that operates on these arguments.  \nCurrent the currently active proxy tracing mode, or None if we are not currently tracing.  \nWithin this context manager, if you are doing make_fx tracing, we will thunkify all SymNode compute and avoid tracing it into the graph unless it is actually needed.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/hub.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPytorch Hub is a pre-trained model repository designed to facilitate research reproducibility.\nPytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights) to a GitHub repository by adding a simple file;\ncan have multiple entrypoints. Each entrypoint is defined as a python function (example: a pre-trained model you want to publish).\n```\n  \n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    \n\n```\n\nHere is a code snippet specifies an entrypoint for model if we expand the implementation in . In most case importing the right function in is sufficient. Here we just want to use the expanded version as an example to show how it works. You can see the full script in \n```\n  \n     \n\n\n  \n\n\n\n\n    \n       \n     \n\n```\n\n  * variable is a of package names required to the model. Note this might be slightly different from dependencies required for training a model.\n  * Docstring of the function works as a help message. It explains what does the model do and what are the allowed positional/keyword arguments. It’s highly recommended to add a few examples here.\n  * Entrypoint function can either return a model(nn.module), or auxiliary tools to make the user workflow smoother, e.g. tokenizers.\n  * Callables prefixed with underscore are considered as helper functions which won’t show up in .\n  * Pretrained weights can either be stored locally in the GitHub repo, or loadable by . If less than 2GB, it’s recommended to attach it to a handles , alternatively you can put the following logic in the entrypoint definition.\n\n\n```\n \n    # For checkpoint saved in local GitHub repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n      \n       \n      \n    \n\n    \n      \n     \n\n```\n\n  * The published models should be at least in a branch/tag. It can’t be a random commit.\n\n\nPytorch Hub provides convenient APIs to explore all available models in hub through , show docstring and examples through and load the pre-trained models using .     \nList all callable entrypoints available in the repo specified by .     \n  * ( is not specified, the default branch is assumed to be if it exists, and otherwise . Example: ‘pytorch/vision:0.10’\n  * () – whether to discard the existing cache and force a fresh download. Default is .\n  * () – if , torchhub will check that the branch or commit specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n  * , , or . This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n    * If , a prompt will ask the user whether the repo should be trusted.\n    * If , the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n    * If , the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the option.\n    * If : this will raise a warning, inviting the user to set to either , or . This is only present for backward compatibility and will be removed in v2.0.\n  * () – If , mute messages about hitting local caches. Note that the message about first download cannot be muted. Default is .\n\n         \n  * ( is not specified, the default branch is assumed to be if it exists, and otherwise . Example: ‘pytorch/vision:0.10’\n  * () – whether to discard the existing cache and force a fresh download. Default is .\n  * () – if , torchhub will check that the ref specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n  * , , or . This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n    * If , a prompt will ask the user whether the repo should be trusted.\n    * If , the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n    * If , the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the option.\n    * If : this will raise a warning, inviting the user to set to either , or . This is only present for backward compatibility and will be removed in v2.0.\n\n    \nLoad a model from a github repo or a local directory.\nNote: Loading a model is the typical use case, but this can also be used to for loading other objects such as tokenizers, loss functions, etc.\nIf is ‘github’, is expected to be of the form with an optional ref (a tag or a branch).\nIf is ‘local’, is expected to be a path to a local directory.     \n  * ( is ‘github’, this should correspond to a github repo with format with an optional ref (tag or branch), for example ‘pytorch/vision:0.10’. If is not specified, the default branch is assumed to be if it exists, and otherwise . If is ‘local’ then it should be a path to a local directory.\n  * () – ‘github’ or ‘local’. Specifies how is to be interpreted. Default is ‘github’.\n  * , , or . This parameter was introduced in v1.12 and helps ensuring that users only run code from repos that they trust.\n    * If , a prompt will ask the user whether the repo should be trusted.\n    * If , the repo will be added to the trusted list and loaded without requiring explicit confirmation.\n    * If , the repo will be checked against the list of trusted repos in the cache. If it is not present in that list, the behaviour will fall back onto the option.\n    * If : this will raise a warning, inviting the user to set to either , or . This is only present for backward compatibility and will be removed in v2.0.\n  * () – whether to force a fresh download of the github repo unconditionally. Does not have any effect if . Default is .\n  * () – If , mute messages about hitting local caches. Note that the message about first download cannot be muted. Does not have any effect if . Default is .\n  * () – if , torchhub will check that the branch or commit specified by the argument properly belongs to the repo owner. This will make requests to the GitHub API; you can specify a non-default GitHub token by setting the environment variable. Default is .\n\n    \nDownload object at the given URL to a local path.     \n  * () – If not None, the SHA256 downloaded file should start with . Default: None\n  * () – whether or not to display a progress bar to stderr Default: True\n\n    \nIf downloaded file is a zip file, it will be automatically decompressed.\nIf the object is already present in , it’s deserialized and returned. The default value of is where is the directory returned by .     \n  * () – a function or a dict specifying how to remap storage locations (see torch.load)\n  * () – whether or not to display a progress bar to stderr. Default: True\n  * () – If True, the filename part of the URL should follow the naming convention where is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False\n  * () – name for the downloaded file. Filename from will be used if not set.\n  * () – If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See for more details.\n\n\nNote that and in are used to a model. After you have loaded a model, how can you find out what you can do with the model? A suggested workflow is\n\n\nTo help users explore without referring to documentation back and forth, we strongly recommend repo owners make function help messages clear and succinct. It’s also helpful to include a minimal working example.     \nGet the Torch Hub cache directory used for storing downloaded models & weights.\nIf is not called, default path is where environment variable defaults to . follows the X Design Group specification of the Linux filesystem layout, with a default value if the environment variable is not set.     \nOptionally set the Torch Hub directory used to save downloaded models & weights.\nBy default, we don’t clean up files after loading it. Hub uses the cache by default if it already exists in the directory returned by .\nUsers can force a reload by calling . This will delete the existing GitHub folder and downloaded weights, reinitialize a fresh download. This is useful when updates are published to the same branch, users can keep up with the latest release.\nTorch hub works by importing the package as if it was installed. There are some side effects introduced by importing in Python. For example, you can see new items in Python caches and which is normal Python behavior. This also means that you may have import errors when importing different models from different repos, if the repos have the same sub-package names (typically, a subpackage). A workaround for these kinds of import errors is to remove the offending sub-package from the dict; more details can be found in \nA known limitation that is worth mentioning here: users load two different branches of the same repo in the . It’s just like installing two packages with the same name in Python, which is not good. Cache might join the party and give you surprises if you actually try that. Of course it’s totally fine to load them in separate processes.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fft.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nComputes the one dimensional discrete Fourier transform of a Hermitian symmetric signal.  \n---  \nComputes the 2-dimensional discrete Fourier transform of a Hermitian symmetric signal.  \nComputes the n-dimensional discrete Fourier transform of a Hermitian symmetric signal.  \nComputes the discrete Fourier Transform sample frequencies for a signal of size .  \n---  \nReorders n-dimensional FFT data, as provided by , to have negative frequency terms first.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/futures.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis package provides a type that encapsulates an asynchronous execution and a set of utility functions to simplify operations on objects. Currently, the type is primarily used by the .     \nWrapper around a which encapsulates an asynchronous execution of a callable, e.g. . It also exposes a set of APIs to add callback functions and set results.     \nAppend the given callback function to this , which will be run when the is completed. Multiple callbacks can be added to the same , but the order in which they will be executed cannot be guaranteed. The callback must take one argument, which is the reference to this . The callback function can use the method to get the value. Note that if this is already completed, the given callback will be run inline.\nWe recommend that you use the method as it provides a way to synchronize after your callback has completed. can be cheaper if your callback does not return anything. But both and use the same callback registration API under the hood.\nWith respect to GPU tensors, this method behaves in the same way as .     \n() – a that takes in one argument, which is the reference to this .\nNote that if the callback function throws, either through the original future being completed with an exception and calling , or through other code in the callback, error handling must be carefully taken care of. For example, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.     \n```\n \n    \n    \n  \n\n\n\n\n\n```\n    \nReturn if this is done. A is done if it has a result or an exception.\nIf the value contains tensors that reside on GPUs, will return even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, because at such stage the result is already usable, provided one performs the appropriate synchronizations (see ).     \nSet an exception for this , which will mark this as completed with an error and trigger all attached callbacks. Note that when calling wait()/value() on this , the exception set here will be raised inline.     \nSet the result for this , which will mark this as completed and trigger all attached callbacks. Note that a cannot be marked completed twice.\nIf the result contains tensors that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those tensors haven’t yet completed running on the device, provided that the streams on which those kernels were enqueued are set as the current ones when this method is called. Put simply, it’s safe to call this method immediately after launching those kernels, without any additional synchronization, as long as one doesn’t change streams in between. This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of this .     \nAppend the given callback function to this , which will be run when the is completed. Multiple callbacks can be added to the same , but the order in which they will be executed cannot be guaranteed (to enforce a certain order consider chaining: ). The callback must take one argument, which is the reference to this . The callback function can use the method to get the value. Note that if this is already completed, the given callback will be run immediately inline.\nIf the ’s value contains tensors that reside on GPUs, the callback might be invoked while the async kernels that are populating those tensors haven’t yet finished executing on the device. However, the callback will be invoked with some dedicated streams set as current (fetched from a global pool) which will be synchronized with those kernels. Hence any operation performed by the callback on these tensors will be scheduled on the device after the kernels complete. In other words, as long as the callback doesn’t switch streams, it can safely manipulate the result without any additional synchronization. This is similar to the non-blocking behavior of .\nSimilarly, if the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernels that are producing these tensors are still running on the device, as long as the callback didn’t change streams during its execution. If one wants to change streams, one must be careful to re-synchronize them with the original streams, that is, those that were current when the callback was invoked.     \nA new object that holds the return value of the and will be marked as completed when the given finishes.\nNote that if the callback function throws, either through the original future being completed with an exception and calling , or through other code in the callback, the future returned by will be marked appropriately with the encountered error. However, if this callback later completes additional futures, those futures are not marked as completed with an error and the user is responsible for handling completion/waiting on those futures independently.     \n```\n \n    \n  \n# The inserted callback will print the return value when\n\n  \n  \n       \n\n\n\n\n\n```\n    \nThis method should only be called after a call to has completed, or inside a callback function passed to . In other cases this may not yet hold a value and calling could fail.\nIf the value contains tensors that reside on GPUs, then this method will perform any additional synchronization. This should be done beforehand, separately, through a call to (except within callbacks, for which it’s already being taken care of by ).     \nThe value held by this . If the function (callback or RPC) creating the value has thrown an error, this method will also throw an error.     \nIf the value contains tensors that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may be asynchronously populating those tensors. Such sync is non-blocking, which means that will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the async kernels but, once that is done, will return, even if those kernels are still running. No further synchronization is required when accessing and using the values, as long as one doesn’t change streams.     \nThe value held by this . If the function (callback or RPC) creating the value has thrown an error, this method will also throw an error.     \nCollects the provided objects into a single combined that is completed when all of the sub-futures are completed.     \n```\n  \n  \n   \n\n\n  \n\n\n\n\n\n```\n    \nWaits for all provided futures to be complete, and returns the list of completed values. If any of the futures encounters an error, the method will exit early and report the error not waiting for other futures to complete.     \nA list of the completed results. This method will throw an error if on any throws.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/func.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis library is currently in . What this means is that the features generally work (unless otherwise documented) and we (the PyTorch team) are committed to bringing this library forward. However, the APIs may change under user feedback and we don’t have full coverage over PyTorch operations.\nIf you have suggestions on the API or use-cases you’d like to be covered, please open an GitHub issue or reach out. We’d love to hear about how you’re using the library.\n  * A “function transform” is a higher-order function that accepts a numerical function and returns a new function that computes a different quantity.\n  * has auto-differentiation transforms ( returns a function that computes the gradient of ), a vectorization/batching transform ( returns a function that computes over batches of inputs), and others.\n  * These function transforms can compose with each other arbitrarily. For example, composing computes a quantity called per-sample-gradients that stock PyTorch cannot efficiently compute today.\n\n\nThere are a number of use cases that are tricky to do in PyTorch today:\n\n\nComposing , , and transforms allows us to express the above without designing a separate subsystem for each. This idea of composable function transforms comes from the \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fsdp.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nA wrapper for sharding module parameters across data parallel workers.\nUsing FSDP involves wrapping your module and then initializing your optimizer after. This is required since FSDP changes the parameter variables.\nWhen setting up FSDP, you need to consider the destination CUDA device. If the device has an ID (), you have three options:\n\n\nThis ensures that the FSDP instance’s compute device is the destination device. For option 1 and 3, the FSDP initialization always occurs on GPU. For option 2, the FSDP initialization happens on module’s current device, which may be a CPU.\nIf you’re using the flag, you need to ensure that the module is on a GPU or use the argument to specify a CUDA device that FSDP will move the module to in the FSDP constructor. This is necessary because requires GPU communication.\nFSDP also takes care of moving input tensors to the forward method to the GPU compute device, so you don’t need to manually move them from CPU.\nFor , exposes the unsharded parameters, not the sharded parameters after forward, unlike . If you want to inspect the gradients, you can use the method with .\nWith , you may see a gap in the FSDP pre-forward where the CPU thread is not issuing any kernels. This is intentional and shows the rate limiter in effect. Synchronizing the CPU thread in that way prevents over-allocating memory for subsequent all-gathers, and it should not actually delay GPU kernel execution.\nFSDP replaces managed modules’ parameters with views during forward and backward computation for autograd-related reasons. If your module’s forward relies on saved references to the parameters instead of reacquiring the references each iteration, then it will not see FSDP’s newly created views, and autograd will not work correctly.\nFinally, when using with the sharding process group being intra-node and the replication process group being inter-node, setting can help improve the all-reduce times over the replication process group for some cluster setups.\nThere are several limitations to be aware of when using FSDP:\n  * FSDP currently does not support gradient accumulation outside when using CPU offloading. This is because FSDP uses the newly-reduced gradient instead of accumulating with any existing gradient, which can lead to incorrect results.\n  * FSDP does not support running the forward pass of a submodule that is contained in an FSDP instance. This is because the submodule’s parameters will be sharded, but the submodule itself is not an FSDP instance, so its forward pass will not all-gather the full parameters appropriately.\n  * FSDP does not work with double backwards due to the way it registers backward hooks.\n  * FSDP has some constraints when freezing parameters. For , each FSDP instance must manage parameters that are all frozen or all non-frozen. For , FSDP supports mixing frozen and non-frozen parameters, but it’s recommended to avoid doing so to prevent higher than expected gradient memory usage.\n  * As of PyTorch 1.12, FSDP offers limited support for shared parameters. If enhanced shared parameter support is needed for your use case, please post in \n  * You should avoid modifying the parameters between forward and backward without using the context, as the modifications may not persist.\n\n    \n  * () – This is the module to be wrapped with FSDP.\n  * () – This is the process group over which the model is sharded and thus the one used for FSDP’s all-gather and reduce-scatter collective communications. If , then FSDP uses the default process group. For hybrid sharding strategies such as , users can pass in a tuple of process groups, representing the groups over which to shard and replicate, respectively. If , then FSDP constructs process groups for the user to shard intra-node and replicate inter-node. (Default: )\n  * () – This configures the sharding strategy, which may trade off memory saving and communication overhead. See for details. (Default: )\n  * () – This configures CPU offloading. If this is set to , then no CPU offloading happens. See for details. (Default: )\n  * This specifies a policy to apply FSDP to submodules of , which is needed for communication and computation overlap and thus affects performance. If , then FSDP only applies to , and users should manually apply FSDP to parent modules themselves (proceeding bottom-up). For convenience, this accepts directly, which allows users to specify the module classes to wrap (e.g. the transformer block). Otherwise, this should be a callable that takes in three arguments , , and and should return a specifying whether the passed-in should have FSDP applied if or if the traversal should continue into the module’s subtree if . Users may add additional arguments to the callable. The in gives an example callable that applies FSDP to a module if the parameters in its subtree exceed 100M numel. We recommend printing the model after applying FSDP and adjusting as needed.\n  * () – This configures explicit backward prefetching of all-gathers. If , then FSDP does not backward prefetch, and there is no communication and computation overlap in the backward pass. See for details. (Default: )\n  * () – This configures native mixed precision for FSDP. If this is set to , then no mixed precision is used. Otherwise, parameter, buffer, and gradient reduction dtypes can be set. See for details. (Default: )\n  * () – Modules whose own parameters and child modules’ parameters and buffers are ignored by this instance. None of the modules directly in should be instances, and any child modules that are already-constructed instances will not be ignored if they are nested under this instance. This argument may be used to avoid sharding specific parameters at module granularity when using an or if parameters’ sharding is not managed by FSDP. (Default: )\n  * A that specifies how modules that are currently on the meta device should be initialized onto an actual device. As of v1.12, FSDP detects modules with parameters or buffers on meta device via and either applies if specified or calls otherwise. For both cases, the implementation should initialize the parameters/buffers of the module, not those of its submodules. This is to avoid re-initialization. In addition, FSDP also supports deferred initialization via torchdistX’s ( API, where the deferred modules are initialized by calling if specified or torchdistX’s default otherwise. If is specified, then it is applied to all meta-device modules, meaning that it should probably case on the module type. FSDP calls the initialization function before parameter flattening and sharding.\n```\n  \n  \n    \n    \n    \n \n\n   \n\n   \n\n```\n\n  * () – An or giving the CUDA device on which FSDP initialization takes place, including the module initialization if needed and the parameter sharding. This should be specified to improve initialization speed if is on CPU. If the default CUDA device was set (e.g. via ), then the user may pass to this. (Default: )\n  * (, then each FSDP module will broadcast module parameters and buffers from rank 0 to ensure that they are replicated across ranks (adding communication overhead to this constructor). This can help load checkpoints via in a memory efficient way. See for an example of this. (Default: )\n  * (, then FSDP prefetches the next forward-pass all-gather before the current forward computation. This is only useful for CPU-bound workloads, in which case issuing the next all-gather earlier may improve overlap. This should only be used for static-graph models since the prefetching follows the first iteration’s execution order. (Default: )\n  * (, then FSDP explicitly synchronizes the CPU thread to ensure GPU memory usage from only consecutive FSDP instances (the current instance running computation and the next instance whose all-gather is prefetched). If , then FSDP allows the CPU thread to issue all-gathers without any extra synchronization. (Default: ) We often refer to this feature as the “rate limiter”. This flag should only be set to for specific CPU-bound workloads with low memory pressure in which case the CPU thread can aggressively issue all kernels without concern for the GPU memory usage.\n  * ( has FSDP use ‘s original parameters. FSDP exposes those original parameters to the user via instead of FSDP’s internal s. This means that the optimizer step runs on the original parameters, enabling per-original-parameter hyperparameters. FSDP preserves the original parameter variables and manipulates their data between unsharded and sharded forms, where they are always views into the underlying unsharded or sharded , respectively. With the current algorithm, the sharded form is always 1D, losing the original tensor structure. An original parameter may have all, some, or none of its data present for a given rank. In the none case, its data will be like a size-0 empty tensor. Users should not author programs relying on what data is present for a given original parameter in its sharded form. is required to use . Setting this to exposes FSDP’s internal s to the user via . (Default: )\n  * () – Ignored parameters or modules that will not be managed by this FSDP instance, meaning that the parameters are not sharded and their gradients are not reduced across ranks. This argument unifies with the existing argument, and we may deprecate soon. For backward compatibility, we keep both and , but FSDP only allows one of them to be specified as not .\n  * () – DeviceMesh can be used as an altenative to process_group. When device_mesh is passed, FSDP will use the underlying process groups for all-gather and reduce-scatter collective communications. Therefore, these two args need to be mutually exclusive. For hybrid sharding strategies such as , users can pass in a 2D DeviceMesh instead of a tuple of process groups. For 2D FSDP + TP, users are required to pass in device_mesh instead of process_group. For more DeviceMesh info, please visit: \n\n    \nApply recursively to every submodule (as returned by ) as well as self.\nTypical use includes initializing the parameters of a model (see also ).\nCompared to , this version additionally gathers the full parameters before applying . It should not be called from within another context.     \n( -> None) – function to be applied to each submodule          \nThe norm is computed over all parameters’ gradients as viewed as a single vector, and the gradients are modified in-place.     \nTotal norm of the parameters (viewed as a single vector).\nIf every FSDP instance uses , meaning that no gradients are sharded across ranks, then you may directly use .\nIf at least some FSDP instance uses a sharded strategy (i.e. one other than ), then you should use this method instead of since this method handles the fact that gradients are sharded across ranks.\nThe total norm returned will have the “largest” dtype across all parameters/gradients as defined by PyTorch’s type promotion semantics. For example, if parameters/gradients use a low precision dtype, then the returned norm’s dtype will be that low precision dtype, but if there exists at least one parameter/ gradient using FP32, then the returned norm’s dtype will be FP32.\nThis needs to be called on all ranks since it uses collective communications.     \nThe API is similar to . The only difference is that the input should be returned from . Therefore, there will be all-gather calls on each rank to gather s.     \n  * () – Optimizer state dict corresponding to the unflattened parameters and holding the sharded optimizer state.\n\n    \nRun the forward pass for the wrapped module, inserting FSDP-specific pre- and post-forward sharding logic.     \nThis possibly includes itself and only includes FSDP root modules if .     \n  * () – Root module, which may or may not be an module.\n\n    \nConsolidates the full optimizer state on rank 0 and returns it as a , i.e. with keys and . The flattened parameters in modules contained in are mapped back to their unflattened parameters.\nThis needs to be called on all ranks since it uses collective communications. However, if , then the state dict is only populated on rank 0, and all other ranks return an empty \nUnlike , this method uses full parameter names as keys instead of parameter IDs.\nLike in , the tensors contained in the optimizer state dict are not cloned, so there may be aliasing surprises. For best practices, consider saving the returned optimizer state dict immediately, e.g. using .     \n  * () – Root module (which may or may not be a instance) whose parameters were passed into the optimizer .\n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n  * (, saves the populated , saves it on all ranks. (Default: )\n  * () – Model’s process group or if using the default process group. (Default: )\n\n    \nA ‘s original unflattened parameters and including keys “state” and “param_groups” following the convention of . If , then nonzero ranks return an empty      \nGet the state_dict_type and the corresponding configurations for the FSDP modules rooted at .\nThe target module does not have to be an FSDP module.     \nA containing the state_dict_type and state_dict / optim_state_dict configs that are currently set.     \nReturn an iterator over module buffers, yielding both the name of the buffer and the buffer itself.\nIntercepts buffer names and removes all occurrences of the FSDP-specific flattened buffer prefix when inside the context manager.     \nReturn an iterator over module parameters, yielding both the name of the parameter and the parameter itself.\nIntercepts parameter names and removes all occurrences of the FSDP-specific flattened parameter prefix when inside the context manager.     \nWithin this context, gradients will be accumulated in module variables, which will later be synchronized in the first forward-backward pass after exiting the context. This should only be used on the root FSDP instance and will recursively apply to all children FSDP instances.\nThis likely results in higher memory usage because FSDP will accumulate the full model gradients (instead of gradient shards) until the eventual sync.\nWhen used with CPU offloading, the gradients will not be offloaded to CPU when inside the context manager. Instead, they will only be offloaded right after the eventual sync.     \nTransform the state-dict of an optimizer corresponding to a sharded model.\nThe given state-dict can be transformed to one of three types: 1) full optimizer state_dict, 2) sharded optimizer state_dict, 3) local optimizer state_dict.\nFor full optimizer state_dict, all states are unflattened and not sharded. Rank0 only and CPU only can be specified via to avoid OOM.\nFor sharded optimizer state_dict, all states are unflattened but sharded. CPU only can be specified via to further save memory.\nFor local state_dict, no transformation will be performed. But a state will be converted from nn.Tensor to ShardedTensor to represent its sharding nature (this is not supported yet).     \n  * () – Root module (which may or may not be a instance) whose parameters were passed into the optimizer .\n  * () – the target optimizer state_dict to transform. If the value is None, optim.state_dict() will be used. ( Default: )\n  * () – Model’s process group across which parameters are sharded or if using the default process group. ( Default: )\n\n    \nA . The sharding of the optimizer state is based on .     \nConvert an optimizer state-dict so that it can be loaded into the optimizer associated with the FSDP model.\nGiven a that is transformed through , it gets converted to the flattened optimizer state_dict that can be loaded to which is the optimizer for . must be sharded by FullyShardedDataParallel.     \n  * () – Root module (which may or may not be a instance) whose parameters were passed into the optimizer .\n  * () – Model’s process group across which parameters are sharded or if using the default process group. ( Default: )\n\n    \nThis is an enhancement that provides a flexible hook to users where they can specify how FSDP aggregates gradients across multiple workers. This hook can be used to implement several algorithms like .\nFSDP communication hook should be registered before running an initial forward pass and only once.     \n  * Passed to the hook to maintain any state information during the training process. Examples include error feedback in gradient compression, peers to communicate with next in \n  * () – Callable, which has one of the following signatures: 1) : This function takes in a Python tensor, which represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). It then performs all necessary processing and returns ; 2) : This function takes in two Python tensors, the first one represents the full, flattened, unsharded gradient with respect to all variables corresponding to the model this FSDP unit is wrapping (that are not wrapped by other FSDP sub-units). The latter represents a pre-sized tensor to store a chunk of a sharded gradient after reduction. In both cases, callable performs all necessary processing and returns . Callables with signature 1 are expected to handle gradient communication for a case. Callables with signature 2 are expected to handle gradient communication for sharded cases.\n\n    \nThis can be used to achieve compatibility between optimizer state dicts from models with FSDP instances and ones without.\nTo re-key an FSDP full optimizer state dict (i.e. from ) to use parameter IDs and be loadable to a non-wrapped model:\nTo re-key a normal optimizer state dict from a non-wrapped model to be loadable to a wrapped model:     \nThe optimizer state dict re-keyed using the parameter keys specified by .     \nScatter the full optimizer state dict from rank 0 to all other ranks.\nReturns the sharded optimizer state dict on each rank. The return value is the same as , and on rank 0, the first argument should be the return value of .\n```\n     \n   \n     \n\n    \n    \n\n\n```\n\nBoth and may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.     \n  * () – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state if on rank 0; the argument is ignored on nonzero ranks.\n  * () – Root module (which may or may not be a instance) whose parameters correspond to the optimizer state in .\n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n  * () – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over . (Default: )\n  * () – Model’s process group or if using the default process group. (Default: )\n\n    \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state.     \nSet the of all the descendant FSDP modules of the target module.\nAlso takes (optional) configuration for the model’s and optimizer’s state dict. The target module does not have to be a FSDP module. If the target module is a FSDP module, its will also be changed.\nThis API should be called for only the top-level (root) module.\nThis API enables users to transparently use the conventional API to take model checkpoints in cases where the root FSDP module is wrapped by another . For example, the following will ensure is called on all non-FSDP instances, while dispatching into implementation for FSDP:     \n\n    \nA StateDictSettings that include the previous state_dict type and configuration for the module.     \nRemaps the state in to flattened parameters instead of unflattened parameters and restricts to only this rank’s part of the optimizer state. The first argument should be the return value of .\nBoth and may be used to get the sharded optimizer state dict to load. Assuming that the full optimizer state dict resides in CPU memory, the former requires each rank to have the full dict in CPU memory, where each rank individually shards the dict without any communication, while the latter requires only rank 0 to have the full dict in CPU memory, where rank 0 moves each shard to GPU memory (for NCCL) and communicates it to ranks appropriately. Hence, the former has higher aggregate CPU memory cost, while the latter has higher communication cost.     \n  * () – Optimizer state dict corresponding to the unflattened parameters and holding the full non-sharded optimizer state.\n  * () – Root module (which may or may not be a instance) whose parameters correspond to the optimizer state in .\n  * () – Input passed into the optimizer representing either a , then this method assumes the input was . This argument is deprecated, and there is no need to pass it in anymore. (Default: )\n  * () – Optimizer that will load the state dict returned by this method. This is the preferred argument to use over . (Default: )\n\n    \nThe full optimizer state dict now remapped to flattened parameters instead of unflattened parameters and restricted to only include this rank’s part of the optimizer state.     \nThe API is similar to but this API chunks all non-zero-dimension states to to save memory. This API should only be used when the model is derived with the context manager .\nThe returned state dict contains and cannot be directly used by the regular .     \nSet the of all the descendant FSDP modules of the target module.\nThis context manager has the same functions as . Read the document of for the detail.     \n\n    \nExpose full params for FSDP instances with this context manager.\nCan be useful forward/backward for a model to get the params for additional processing or checking. It can take a non-FSDP module and will summon full params for all contained FSDP modules as well as their children, depending on the argument.\nThis can be used within a forward or backward pass. Nor can forward and backward be started from within this context.\nParameters will revert to their local shards after the context manager exits, storage behavior is the same as forward.\nThe full parameters can be modified, but only the portion corresponding to the local param shard will persist after the context manager exits (unless , in which case changes will be discarded). In the case where FSDP does not shard the parameters, currently only when , or config, the modification is persisted regardless of .\nThis method works on modules which are not FSDP themselves but may contain multiple independent FSDP units. In that case, the given arguments will apply to all contained FSDP units.\nNote that in conjunction with is not currently supported and will raise an error. This is because model parameter shapes would be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\nNote that and will result in full parameters being redundantly copied to CPU memory for GPUs that reside on the same machine, which may incur the risk of CPU OOM. It is recommended to use with .     \n  * () – recursively summon all params for nested FSDP instances (default: True).\n  * () – if , modifications to params are discarded after the context manager exits; disabling this can be slightly more efficient (default: True)\n  * () – if , full parameters are materialized on only global rank 0. This means that within the context, only rank 0 will have full parameters and the other ranks will have sharded parameters. Note that setting with is not supported, as model parameter shapes will be different across ranks within the context, and writing to them can lead to inconsistency across ranks when the context is exited.\n  * () – If , full parameters are offloaded to CPU. Note that this offloading currently only occurs if the parameter is sharded (which is only not the case for world_size = 1 or config). It is recommended to use with to avoid redundant copies of model parameters being offloaded to the same CPU memory.\n  * () – If , gradients are also unsharded with the parameters. Currently, this is only supported when passing to the FSDP constructor and to this method. (Default: )\n\n    \nThis configures explicit backward prefetching, which improves throughput by enabling communication and computation overlap in the backward pass at the cost of slightly increased memory usage.\n  * : This enables the most overlap but increases memory usage the most. This prefetches the next set of parameters the current set of parameters’ gradient computation. This overlaps the and the , and at the peak, it holds the current set of parameters, next set of parameters, and current set of gradients in memory.\n  * : This enables less overlap but requires less memory usage. This prefetches the next set of parameters the current set of parameters’ gradient computation. This overlaps the and the , and it frees the current set of parameters before allocating memory for the next set of parameters, only holding the next set of parameters and current set of gradients in memory at the peak.\n  * FSDP’s argument accepts , which disables the backward prefetching altogether. This has no overlap and does not increase memory usage. In general, we do not recommend this setting since it may degrade throughput significantly.\n\n\nFor more technical context: For a single process group using NCCL backend, any collectives, even if issued from different streams, contend for the same per-device NCCL stream, which implies that the relative order in which the collectives are issued matters for overlapping. The two backward prefetching values correspond to different issue orders.     \nThis specifies the sharding strategy to be used for distributed training by .\n  * : Parameters, gradients, and optimizer states are sharded. For the parameters, this strategy unshards (via all-gather) before the forward, reshards after the forward, unshards before the backward computation, and reshards after the backward computation. For gradients, it synchronizes and shards them (via reduce-scatter) after the backward computation. The sharded optimizer states are updated locally per rank.\n  * : Gradients and optimizer states are sharded during computation, and additionally, parameters are sharded outside computation. For the parameters, this strategy unshards before the forward, does not reshard them after the forward, and only reshards them after the backward computation. The sharded optimizer states are updated locally per rank. Inside , the parameters are not resharded after the backward computation.\n  * : Parameters, gradients, and optimizer states are not sharded but instead replicated across ranks similar to PyTorch’s API. For gradients, this strategy synchronizes them (via all-reduce) after the backward computation. The unsharded optimizer states are updated locally per rank.\n  * : Apply within a node, and replicate parameters across nodes. This results in reduced communication volume as expensive all-gathers and reduce-scatters are only done within a node, which can be more performant for medium -sized models.\n  * : Apply within a node, and replicate parameters across nodes. This is like , except this may provide even higher throughput since the unsharded parameters are not freed after the forward pass, saving the all-gathers in the pre-backward.\n\n         \n  * () – This specifies the dtype for model parameters during forward and backward and thus the dtype for forward and backward computation. Outside forward and backward, the parameters are kept in full precision (e.g. for the optimizer step), and for model checkpointing, the parameters are always saved in full precision. (Default: )\n  * () – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is but is not , then this takes on the value, still running gradient reduction in low precision. This is permitted to differ from , e.g. to force gradient reduction to run in full precision. (Default: )\n  * () – This specifies the dtype for buffers. FSDP does not shard buffers. Rather, FSDP casts them to in the first forward pass and keeps them in that dtype thereafter. For model checkpointing, the buffers are saved in full precision except for . (Default: )\n  * (, then FSDP upcasts gradients to full precision after the backward pass in preparation for the optimizer step. If , then FSDP keeps the gradients in the dtype used for gradient reduction, which can save memory if using a custom optimizer that supports running in low precision. (Default: )\n  * (, then this FSDP module casts its forward args and kwargs to . This is to ensure that parameter and input dtypes match for forward computation, as required by many ops. This may need to be set to when only applying mixed precision to some but not all FSDP modules, in which case a mixed-precision FSDP submodule needs to recast its inputs. (Default: )\n  * (, then the root FSDP module casts its forward args and kwargs to , overriding the value of . For non-root FSDP modules, this does not do anything. (Default: )\n  * () – (Sequence[Type[nn.Module]]): This specifies module classes to ignore for mixed precision when using an : Modules of these classes will have FSDP applied to them separately with mixed precision disabled (meaning that the final FSDP construction would deviate from the specified policy). If is not specified, then this does not do anything. This API is experimental and subject to change. (Default: )\n\n\nOnly floating point tensors are cast to their specified dtypes.\nIn , parameters are forced to full precision, but buffers are not.\nLayer norm and batch norm accumulate in even when their inputs are in a low precision like or . Disabling FSDP’s mixed precision for those norm modules only means that the affine parameters are kept in . However, this incurs separate all-gathers and reduce-scatters for those norm modules, which may be inefficient, so if the workload permits, the user should prefer to still apply mixed precision to those modules.\nBy default, if the user passes a model with any modules and specifies an , then the batch norm modules will have FSDP applied to them separately with mixed precision disabled. See the argument.\nhas and by default. For the root FSDP instance, its takes precedence over its . For non-root FSDP instances, their values are ignored. The default setting is sufficient for the typical case where each FSDP instance has the same configuration and only needs to cast inputs to the at the beginning of the model’s forward pass.\nFor nested FSDP instances with different configurations, we recommend setting individual values to configure casting inputs or not before each instance’s forward. In such a case, since the casts happen before each FSDP instance’s forward, a parent FSDP instance should have its non-FSDP submodules run before its FSDP submodules to avoid the activation dtype being changed due to a different configuration.\nThe above shows a working example. On the other hand, if were replaced with , meaning that the submodule using different ran its forward first, then would incorrectly see activations instead of ones.          \n(, then this offloads gradients to CPU as well, meaning that the optimizer step runs on CPU.     \nis the base class for all configuration classes. Users should instantiate a child class (e.g. ) in order to configure settings for the corresponding type supported by FSDP.     \n(, then FSDP offloads the state dict values to CPU, and if , then FSDP keeps them on GPU. (Default: )     \nis a config class meant to be used with . We recommend enabling both and when saving full state dicts to save GPU memory and CPU memory, respectively. This config class is meant to be used via the context manager as follows:\n```\n     \n   \n   \n   \n      \n# `state` will be empty on non rank 0 and contain CPU tensors on rank 0.\n# To reload checkpoint for inference, finetuning, transfer learning, etc:\n    \n   \n# Load checkpoint only on rank 0 to avoid memory redundancy\n      \n    \n# All ranks initialize FSDP module as usual. `sync_module_states` argument\n# communicates loaded checkpoint states from rank 0 to rest of the world.\n  \n    \n    \n    \n    \n\n# After this point, all ranks have FSDP model with loaded checkpoint.\n\n```\n    \n(, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If , then all ranks save the full state dict. (Default: )          \n(, then FSDP saves the state dict values as , and if , then FSDP saves them as . (Default: )\nis a private field of and it is used by FSDP to determine the type of state dict values. Users should not manually modify .     \nis the base class for all configuration classes. Users should instantiate a child class (e.g. ) in order to configure settings for the corresponding type supported by FSDP.     \n(, then FSDP offloads the state dict’s tensor values to CPU, and if , then FSDP keeps them on the original device (which is GPU unless parameter CPU offloading is enabled). (Default: )          \n(, then only rank 0 saves the full state dict, and nonzero ranks save an empty dict. If , then all ranks save the full state dict. (Default: )          \n(, then FSDP saves the state dict values as , and if , then FSDP saves them as . (Default: )\nis a private field of and it is used by FSDP to determine the type of state dict values. Users should not manually modify .\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/fx.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nFX is a toolkit for developers to use to transform instances. FX consists of three main components: a an , and . A demonstration of these components in action:\n```\n \n\n\n\n \n       \n        \n           \n           \n\n      \n            \n\n\n  \n\n   \n\n# Symbolic tracing frontend - captures the semantics of the module\n   \n\n\n\n\n\n\n\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %param), kwargs = {})\n    %linear : [num_users=1] = call_module[target=linear](args = (%add,), kwargs = {})\n    %clamp : [num_users=1] = call_method[target=clamp](args = (%linear,), kwargs = {min: 0.0, max: 1.0})\n\n\n\n\n\n\n\n\n    add = x + param;  x = param = None\n\n    clamp = linear.clamp(min = 0.0, max = 1.0);  linear = None\n\n\n\n```\n\nThe performs “symbolic execution” of the Python code. It feeds fake values, called Proxies, through the code. Operations on theses Proxies are recorded. More information about symbolic tracing can be found in the and documentation.\nThe is the container for the operations that were recorded during symbolic tracing. It consists of a list of Nodes that represent function inputs, callsites (to functions, methods, or instances), and return values. More information about the IR can be found in the documentation for . The IR is the format on which transformations are applied.\nis what makes FX a Python-to-Python (or Module-to-Module) transformation toolkit. For each Graph IR, we can create valid Python code matching the Graph’s semantics. This functionality is wrapped up in , which is a instance that holds a as well as a method generated from the Graph.\nTaken together, this pipeline of components (symbolic tracing -> intermediate representation -> transforms -> Python code generation) constitutes the Python-to-Python transformation pipeline of FX. In addition, these components can be used separately. For example, symbolic tracing can be used in isolation to capture a form of the code for analysis (and not transformation) purposes. Code generation can be used for programmatically generating models, for example from a config file. There are many uses for FX!\nWhat is an FX transform? Essentially, it’s a function that looks like this.\n```\n \n \n\n  \n                    \n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    \n    # split that out in our transform to allow the caller to\n    \n        \n\n    # Step 2: Modify this Graph or create a new one\n      \n\n    \n      \n\n```\n\nYour transform will take in a , acquire a from it, do some modifications, and return a new . You should think of the that your FX transform returns as identical to a regular – you can pass it to another FX transform, you can pass it to TorchScript, or you can run it. Ensuring that the inputs and outputs of your FX transform are a will allow for composability.\nIt is also possible to modify an existing instead of creating a new one, like so:\n```\n \n \n\n     \n        \n\n    \n    \n\n    # Recompile the forward() method of `gm` from its Graph\n    \n\n     \n\n```\n\nNote that you MUST call to bring the generated method on the in sync with the modified .\nGiven that you’ve passed in a that has been traced into a , there are now two primary approaches you can take to building a new .\nFull treatment of the semantics of graphs can be found in the documentation, but we are going to cover the basics here. A is a data structure that represents a method on a . The information that this requires is:\n  * What is the output (i.e. return) value from the method?\n\n\nAll three of these concepts are represented with instances. Let’s see what we mean by that with a short example:\nHere we define a module for demonstration purposes, instantiate it, symbolically trace it, then call the method to print out a table showing the nodes of this :\n> We can use this information to answer the questions we posed above.\n  * What are the inputs to the method? In FX, method inputs are specified via special nodes. In this case, we have a single node with a of , meaning we have a single (non-self) argument named x.\n  * What are the operations within the method? The , , , and nodes represent the operations in the method. A full treatment of the semantics of all of these can be found in the documentation.\n  * What is the return value of the method? The return value in a is specified by a special node.\n\n\nGiven that we now know the basics of how code is represented in FX, we can now explore how we would edit a .\nOne approach to building this new is to directly manipulate your old one. To aid in this, we can simply take the we obtain from symbolic tracing and modify it. For example, let’s say we desire to replace calls with calls.\n```\n \n \n\n\n \n       \n          \n\n  \n                    \n        \n    # FX represents its Graph as an ordered list of\n    \n       \n        \n        \n           \n            \n            \n               \n                  \n\n     \n                 \n\n      \n\n```\n\nWe can also do more involved rewrites, such as deleting or appending nodes. To aid in these transformations, FX has utility functions for transforming the graph that can be found in the documentation. An example of using these APIs to append a call can be found below.\n```\n# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\n \n    \n      \n         \n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    \n    \n\n```\n\nFor simple transformations that only consist of substitutions, you can also make use of the \nFX also provides another level of automation on top of direct graph manipulation. The API is essentially a “find/replace” tool for editing s. It allows you to specify a and function and it will trace through those functions, find instances of the group of operations in the graph, and replace those instances with copies of the graph. This can help to greatly automate tedious graph manipulation code, which can get unwieldy as the transformations get more complex.\nAnother way of manipulating s is by reusing the machinery used in symbolic tracing. For example, let’s imagine that we wanted to write a transformation that decomposed PyTorch functions into smaller operations. It would transform every call into . One possibility would be to perform the requisite graph rewriting to insert the comparison and multiplication after the , and then clean up the original . However, we can automate this process by using objects to automatically record operations into the .\nTo use this method, we write the operations that we want inserted as regular PyTorch code and invoke that code with objects as arguments. These objects will capture the operations that are performed on them and append them to the .\n```\n# Note that this decomposition rule can be read as regular Python\n \n         \n\n  \n  \n\n  \n                    \n\n\n\n\n\n        \n      \n      \n      \n       \n               \n            \n            \n            \n            \n              \n                          \n              \n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n              \n              \n        \n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n                 \n              \n      \n\n```\n\nIn addition to avoiding explicit graph manipulation, using s also allows you to specify your rewrite rules as native Python code. For transformations that require a large amount of rewrite rules (such as vmap or grad), this can often improve readability and maintainability of the rules. Note that while calling we also passed a tracer pointing to the underlying variable . This is done so if in case the operations in graph are n-ary (e.g. add is a binary operator) the call to does not create multiple instances of a graph tracer which can lead to unexpected runtime errors. We recommend this method of using especially when the underlying operators can not be safely assumed to be unary.\nA useful code organizational pattern in FX is to loop over all the s in a and execute them. This can be used for several things including runtime analysis of values flowing through the graph or transformation of the code via retracing with s. For example, suppose we want to run a and record the shape and dtype properties on the nodes as we see them at runtime. That might look like:\n```\n \n \n   \n\n   \n\n \n\n\n\n\n\n    element type for the output values of each operation on\n\n\n\n      \n          \n          \n          \n\n      \n          \n             \n\n         \n                \n\n           \n              \n              \n                \n                   \n                     \n                   \n             \n\n           \n               \n                  \n               \n                  \n               \n                   \n               \n                   \n                  \n                    \n               \n                   \n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            \n              \n                  \n                  \n\n              \n\n         \n\n```\n\nAs you can see, a full interpreter for FX is not that complicated but it can be very useful. To ease using this pattern, we provide the class, which encompasses the above logic in a way that certain aspects of the interpreter’s execution can be overridden via method overrides.\nIn addition to executing operations, we can also generate a new by feeding values through an interpreter. Similarly, we provide the class to encompass this pattern. behaves similarly to , but instead of calling the method to get a concrete output value from the Module, you would call the method to return a new which was subject to any transformation rules you installed as overridden methods.\nOften in the course of authoring transformations, our code will not be quite right. In this case, we may need to do some debugging. The key is to work backwards: first, check the results of invoking the generated module to prove or disprove correctness. Then, inspect and debug the generated code. Then, debug the process of transformations that led to the generated code.\nIf you’re not familiar with debuggers, please see the auxiliary section .\n  * Nondeterministic iteration order. In Python, the datatype is unordered. Using to contain collections of objects like s, for example, can cause unexpected nondeterminism. An example is iterating over a set of s to insert them into a . Because the data type is unordered, the ordering of the operations in the output program will be nondeterministic and can change across program invocations. The recommended alternative is to use a data type, which is can be used equivalently to a set by storing values to be deduplicated in the keys of the .\n\n\nBecause the output of most deep learning modules consists of floating point instances, checking for equivalence between the results of two is not as straightforward as doing a simple equality check. To motivate this, let’s use an example:\n```\n \n \n   \n\n     \n      \n\n    \n    \n\n    \n\n     \n\n  \n  \n\n     \n\n   \n\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\n\n```\n\nHere, we’ve tried to check equality of the values of two deep learning models with the equality operator. However, this is not well- defined both due to the issue of that operator returning a tensor and not a bool, but also because comparison of floating point values should use a margin of error (or epsilon) to account for the non-commutativity of floating point operations (see instead, which will give us an approximate comparison taking into account a relative and absolute tolerance threshold:\nThis is the first tool in our toolbox to check if transformed modules are behaving as we expect compared to a reference implementation.\nBecause FX generates the function on s, using traditional debugging techniques like statements or is not as straightforward. Luckily, we have several techniques we can use for debugging the generated code.\nInvoke to step into the running program. Although the code that represents the is not in any source file, we can still step into it manually using when the forward pass is invoked.\n```\n \n \n   \n\n         \n      \n    \n    \n\n    \n      \n\n  \n  \n\n     \n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n\n  \n\n\n\n```\n\nIf you’d like to run the same code multiple times, then it can be a bit tedious to step to the right code with . In that case, one approach is to simply copy-paste the generated pass into your code and examine it from there.\n```\n# Assume that `traced` is a GraphModule that has undergone some\n\n\n\n\n# Print the code generated from symbolic tracing. This outputs:\n\n\n\n    add_1 = x + y;  x = y = None\n\n\n\n\n \n     \n        \n\n    # Paste the generated `forward` function (the one we printed and\n    \n      \n          \n                  \n         \n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\n  \n  \n\n```\n\nis a method in that allows you to dump out the generated FX code to a folder. Although copying the forward pass into the code often suffices as in , it may be easier to examine modules and parameters using .\nAfter running the above example, we can then look at the code within and modify it as desired (e.g. adding statements or using ) to debug the generated code.\nNow that we’ve identified that a transformation is creating incorrect code, it’s time to debug the transformation itself. First, we’ll check the section in the documentation. Once we verify that tracing is working as expected, the goal becomes figuring out what went wrong during our transformation. There may be a quick answer in , but, if not, there are several ways to examine our traced module:\n```\n\n \n       \n           \n\n\n  \n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n\n  \n\n\n\n\n\n\n    add = x + y;  x = y = None\n\n\n\n\n\n\n\n\n\n\n    %add : [num_users=1] = call_function[target=operator.add](args = (%x, %y), kwargs = {})\n\n\n\n\n\n\n\nopcode         name    target                   args    kwargs\n\nplaceholder    x       x                        ()      {}\nplaceholder    y       y                        ()      {}\ncall_function  add     <built-in function add>  (x, y)  {}\noutput         output  output                   (add,)  {}\n\n\n```\n\nUsing the utility functions above, we can compare our traced Module before and after we’ve applied our transformations. Sometimes, a simple visual comparison is enough to trace down a bug. If it’s still not clear what’s going wrong, a debugger like can be a good next step.\nGoing off of the example above, consider the following code:\n```\n\n         \n    \n      \n\n\n\n\n\n      \n\n\n  \n\n# Print the new code after our transforms. Check to see if it was\n\n\n\n```\n\nUsing the above example, let’s say that the call to showed us that there was an error in our transforms. We want to find what goes wrong using a debugger. We start a session. We can see what’s happening during the transform by breaking on , then pressing to “step into” the call to .\nWe may also have good luck by editing the method to print different attributes of the Nodes in the Graph. (For example, we might want to see the Node’s and .)\nThe most common Python debugger is by typing into the command line, where is the name of the file you want to debug. After that, you can use the ) when you start , then call to run the program until that point. This prevents you from having to step through each line of execution (using or ) to get to the part of the code you want to examine. Alternatively, you can write before the line you want to break at. If you add , your program will automatically start in debug mode when you run it. (In other words, you can just type into the command line instead of .) Once you’re running your file in debug mode, you can step through the code and examine your program’s internal state using certain commands. There are many excellent tutorials on online, including RealPython’s \nIDEs like PyCharm or VSCode usually have a debugger built in. In your IDE, you can choose to either a) use by pulling up a terminal window in your IDE (e.g. View → Terminal in VSCode), or b) use the built-in debugger (usually a graphical wrapper around ).\nFX uses a system of (a.k.a in that it executes the program (really a or function) to record operations. It is in that the data flowing through the program during this execution is not real data, but rather symbols ( in FX parlance).\nAlthough symbolic tracing works for most neural net code, it has some limitations.\nThe main limitation of symbolic tracing is it does not currently support . That is, loops or statements where the condition may depend on the input values of the program.\n```\n \n       \n         \n    \n         \n\n  \n\n\n\n\n\n\n\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\n\n```\n\nThe condition to the statement relies on the value of , which relies on the value of , a function input. Since can change (i.e. if you pass a new input tensor to the traced function), this is . The traceback walks back up through your code to show you where this situation happens.\nOn the other hand, so-called is supported. Static control flow is loops or statements whose value cannot change across invocations. Typically, in PyTorch programs, this control flow arises for code making decisions about a model’s architecture based on hyper-parameters. As a concrete example:\n```\n \n \n\n \n          \n        \n          \n           \n\n      \n          \n        \n        # Its condition does not depend on any input values\n         \n              \n         \n\n  \n  \n\n  \n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n```\n\nThe if-statement does not depend on any function inputs, thus it is static. can be considered to be a hyper-parameter, and the traces of different instances of with different values for that parameter have different code. This is a valid pattern that is supported by symbolic tracing.\nMany instances of dynamic control flow are semantically static control flow. These instances can be made to support symbolic tracing by removing the data dependencies on input values, for example by moving values to attributes or by binding concrete values to arguments during symbolic tracing:\nIn the case of truly dynamic control flow, the sections of the program that contain this code can be traced as calls to the Method (see ) or function (see ) rather than tracing through them.\nFX uses as the mechanism by which it intercepts calls (see the module, are not covered by , but we would still like to capture them in symbolic tracing. For example:\n```\n \n \n   \n\n \n\n\n\n       \n\n\n \n\n  \n\n\n\n\n\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\n\n```\n\nThe error tells us that the built-in function is not supported. We can make it so that functions like this are recorded in the trace as direct calls using the API:\n```\n\n\n\n  \n\n\n\n\n\n\n\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n\n\n\n```\n\nThe class is the class that underlies the implementation of . The behavior of tracing can be customized by subclassing Tracer, like so:\n```\n \n    \n    \n    \n    \n\n\n# Let's use this custom tracer to trace through this module\n \n      \n            \n\n  \n\n  \n# trace() returns a Graph. Let's wrap it up in a\n\n   \n\n```\n\nLeaf Modules are the modules that appear as calls in the symbolic trace rather than being traced through. The default set of leaf modules is the set of standard module instances. For example:\n```\n \n      \n         \n\n \n     \n        \n           \n          \n\n      \n         \n\n  \n\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n\n\n\n\n\n\n\n\n\n```\n\nThe set of leaf modules can be customized by overriding .\n  *     * The deterministic constructors (, ) can be used and the value they produce will be embedded in the trace as a constant. This is only problematic if the arguments to these constructors refers to dynamic input sizes. In this case, or may be a viable substitute.\n    * Nondeterministic constructors (, ) will have a single random value embedded in the trace. This is likely not the intended behavior. One workaround is to wrap in a function and call that instead.\n  *     * Python 3-style type annotations (e.g. ) are supported and will be preserved by symbolic tracing.\n    * Annotations on local names within a function are not currently supported.\n  *     * When using functionals like , it will be common for the training argument to be passed in as . During FX tracing, this will likely be baked in as a constant value.\n> ```\n \n \n\n \n    \n      \n\n\n  \n\n\n\n  dropout = torch.nn.functional.dropout(x, p = 0.5, training = True, inplace = False);  x = None\n\n\n\n\n\n   \n \n\n\n\n\nGreatest absolute difference: 1.6207983493804932 at index (0, 2) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (0, 0) (up to 0.0001 allowed)\n\n\n```\n\n    * However, when the standard submodule is used, the training flag is encapsulated and–because of the preservation of the object model–can be changed.\n> ```\n \n   \n    \n      \n\n    \n     \n\n  \n\n\n\n\n\n\n\n\n\n   \n \n\n```\n\n\n\n>   * Because of this difference, consider marking modules that interact with the flag dynamically as leaf modules.\n> \n    \nGiven an or function instance , this function will return a constructed by recording operations seen while tracing through .\nallows you to partially specialize your function, whether it’s to remove control flow or data structures.\nFX can typically not trace through this due to the presence of control flow. However, we can use to specialize on the value of to trace through this:\nNote that although you can still pass in different values of , they will be ignored.\nWe can also use to eliminate data-structure handling from our function. This will use pytrees to flatten your input. To avoid overspecializing, pass in for values that shouldn’t be specialized. For example:     \n  * () – Module or function to be traced and converted into a Graph representation.\n\n    \nThis function can be called at module-level scope to register fn_or_name as a “leaf function”. A “leaf function” will be preserved as a CallFunction node in the FX trace instead of being traced through:\n```\n\n  \n           \n\n\n\n\n\n  \n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    \n      \n\n```\n\nThis function can also equivalently be used as a decorator:\nA wrapped function can be thought of a “leaf function”, analogous to the concept of “leaf modules”, that is, they are functions that are left as calls in the FX trace rather than traced through.     \n() – The function or name of the global function to insert into the graph when it’s called     \nGraphModule is an nn.Module generated from an fx.Graph. Graphmodule has a attribute, as well as and attributes generated from that .\nWhen is reassigned, and will be automatically regenerated. However, if you edit the contents of the without reassigning the attribute itself, you must call to update the generated code.          \n  * () – can either be an nn.Module instance or a Dict mapping strings to any attribute type. In the case that is a Module, any references to Module-based objects (via qualified name) in the Graph’s Nodes’ field will be copied over from the respective place within ’s Module hierarchy into the GraphModule’s module hierarchy. In the case that is a dict, the qualified name found in a Node’s will be looked up directly in the dict’s keys. The object mapped to by the Dict will be copied over into the appropriate place within the GraphModule’s module hierarchy.\n  * () – contains the nodes this GraphModule should use for code generation\n  * ( denotes the name of this GraphModule for debugging purposes. If it’s unset, all error messages will report as originating from . It may be helpful to set this to ’s original name or a name that makes sense within the context of your transform.\n\n    \nThis installs empty Modules where none exist yet if they are subpaths of .     \n  * () – The submodule itself; the actual object we want to install in the current Module\n\n         \nthis method to return True, each object in the chain denoted by must either a) not exist yet, or b) reference an (not a parameter or other attribute)     \nA Module is considered “used” if any one of the following is true: 1. It has children that are used 2. Its forward is called directly via a node 3. It has a non-Module attribute that is used from a node\nThis method can be called to clean up an without manually calling on each unused submodule.     \nThe module will not be deleted if is not a valid target.          \nsubmodule we want to delete. A return value of means that the was not a valid reference to a submodule.     \nReturn the Python code generated for current GraphModule and its children GraphModules     \nRecompile this GraphModule from its attribute. This should be called after editing the contained , otherwise the generated code of this will be out of date.          \n> folder (Union[str, os.PathLike]): The folder to write the code out to     \nis the main data structure used in the FX Intermediate Representation. It consists of a series of s, each representing callsites (or other syntactic constructs). The list of s, taken together, constitute a valid Python function.\nFor the semantics of operations represented in the , please see .     \nInsert a into the . A node represents a call to a Python callable, specified by .     \n  * () – The function to be called. Can be any PyTorch operator, Python function, or member of the or namespaces.\n  * () – The positional arguments to be passed to the called function.\n  * () – The keyword arguments to be passed to the called function\n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n\nThe same insertion point and type expression rules apply for this method as .     \nInsert a into the . A node represents a call to a given method on the 0th element of .     \n  * () – The positional arguments to be passed to the called method. Note that this include a argument.\n  * () – The keyword arguments to be passed to the called method\n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n\nThe same insertion point and type expression rules apply for this method as .     \nInsert a into the . A node represents a call to the forward() function of a in the hierarchy.     \n  * ( in the hierarchy to be called. For example, if the traced has a submodule named , which has a submodule named , the qualified name should be passed as to call that module.\n  * () – The positional arguments to be passed to the called method. Note that this should include a argument.\n  * () – The keyword arguments to be passed to the called method\n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n\nThe same insertion point and type expression rules apply for this method as .     \nCreate a and add it to the at the current insert-point. Note that the current insert-point can be set via and .     \n  * () – is a tuple of arguments to this node.\n  * () – an optional string name for the . This will influence the name of the value assigned to in the Python generated code.\n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n    \nRemove all dead code from the graph, based on each node’s number of users, and whether the nodes have any side effects. The graph must be topologically sorted before calling.     \n\n    \nWhether the graph was changed as a result of the pass.\nBefore dead code is eliminated, from below has no users and thus can be eliminated from the graph without having an effect.\nAfter dead code is eliminated, has been removed, and the rest of remains.\nDead code elimination has some heuristics to avoid removing side-effectful nodes (see Node.is_impure) but in general coverage is very bad, so you should assume that this method is not sound to call unless you know that your FX graph consists entirely of functional operations or you supply your own custom function for detecting side-effectful nodes.     \nErases a from the . Throws an exception if there are still users of that node in the .          \n  * () – the target of the node. For call_function, the target is required. For other ops, the target is optional.\n\n    \nInsert a node into the Graph. A represents the fetch of an attribute from the hierarchy.     \n  * (, which has a submodule named , which has an attribute named , the qualified name should be passed as .\n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n\nThe same insertion point and type expression rules apply for this method as .          \n  * () – The source graph from which to copy Nodes.\n  * () – a dictionary that will be populated with a mapping from nodes in to nodes in . Note that can be passed in with values in it already to override copying of certain values.\n\n    \nThe value in that is now equivalent to the output value in , if had an node. otherwise.      \n\nSet the point at which create_node and companion methods will insert into the graph.\n    \nWhen used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:\n```\n \n      \n  \n  \n\n```\n\n> \n\nn (Optional[Node]): The node before which to insert. If None this will insert after\n    \nA resource manager that will restore the insert point on .      \n\nSet the point at which create_node and companion methods will insert into the graph.\n    \nWhen used within a ‘with’ statement, this will temporary set the insert point and then restore it when the with statement exits:\n```\n \n      \n  \n  \n\n```\n\n> \n\nn (Optional[Node]): The node before which to insert. If None this will insert before\n    \nA resource manager that will restore the insert point on .     \nRuns various checks on this Graph to make sure it is well-formed. In particular: - Checks Nodes have correct ownership (owned by this graph) - Checks Nodes appear in topological order - If this Graph has an owning GraphModule, checks that targets exist in that GraphModule     \nCopy a node from one graph into another. needs to transform arguments from the graph of node to the graph of self. Example:     \n  * () – A function that transforms arguments in node’s and into the equivalent argument in . In the simplest case, this should retrieve a value out of a table mapping Nodes in the original graph to .\n\n    \nNote that this list representation is a doubly-linked list. Mutations during iteration (e.g. delete a Node, add a Node) are safe.     \nA doubly-linked list of Nodes. Note that can be called on this list to switch iteration order.     \n>          \n> a function that returns a code transformer to be registered. This function is called by to obtain the code transformer.\n> This function is also given as its input the currently registered code transformer (or None if nothing is registered), in case it is not desirable to overwrite it. This is useful to chain code transformers together.     \n> a context manager that when used in a statement, to automatically restore the previously registered code transformer.\n> ```\n   \n\n\n# This is a code transformer we want to register. This code\n# transformer prepends a pdb import and trace statement at the very\n# beginning of the generated torch.fx code to allow for manual\n\n \n      \n\n\n\n\n  \n\n\n\n\n\n      \n              \n    \n\n\n\n  \n\n```\n\n> This function can also be used as a context manager, with the benefit to automatically restores the previously registered code transformer:\n> ```\n\n\n   \n    \n    \n      \n\n# now previous code transformer is restored (but `gm`'s code with pdb\n# remains - that means you can run `gm` with pdb here too, until you\n\n\n```\n    \nInsert an into the . An node represents a statement in Python code. is the value that should be returned.     \n  * () – an optional type annotation representing the Python type the output of this node will have.\n\n\nThe same insertion point and type expression rules apply for this method as .          \n  * () – an optional type annotation representing the Python type the output of this node will have. This is needed in some cases for proper code generation (e.g. when the function is used subsequently in TorchScript compilation).\n  * () – The default value this function argument should take on. NOTE: to allow for as a default value, should be passed as this argument to specify that the parameter does _not_ have a default value.\n\n\nThe same insertion point and type expression rules apply for this method as .     \nPrints the intermediate representation of the graph in tabular format. Note that this API requires the module to be installed.     \nProcesses args so that they can be passed to the FX graph.          \nsrc: the Python source code representing the object globals: a dictionary of global names in -> the objects that they reference.     \nis the data structure that represents individual operations within a . For the most part, Nodes represent callsites to various entities, such as operators, methods, and Modules (some exceptions include nodes that specify function inputs and outputs). Each has a function specified by its property. The semantics for each value of are as follows:\n  * represents a function input. The attribute specifies the name this value will take on. is similarly the name of the argument. holds either: 1) nothing, or 2) a single argument denoting the default parameter of the function input. is don’t-care. Placeholders correspond to the function parameters (e.g. ) in the graph printout.\n  * retrieves a parameter from the module hierarchy. is similarly the name the result of the fetch is assigned to. is the fully-qualified name of the parameter’s position in the module hierarchy. and are don’t-care\n  * applies a free function to some values. is similarly the name of the value to assign to. is the function to be applied. and represent the arguments to the function, following the Python calling convention\n  * applies a module in the module hierarchy’s method to given arguments. is as previous. is the fully-qualified name of the module in the module hierarchy to call. and represent the arguments to invoke the module on, .\n  * calls a method on a value. is as similar. is the string name of the method to apply to the argument. and represent the arguments to invoke the module on, \n  * contains the output of the traced function in its attribute. This corresponds to the “return” statement in the Graph printout.\n\n    \nReturn all Nodes that are inputs to this Node. This is equivalent to iterating over and and only collecting the values that are Nodes.     \nInsert after this node in the list of nodes in the graph. Equivalent to      \n() – The node to put after this node. Must be a member of the same graph.     \nThe tuple of arguments to this . The interpretation of arguments depends on the node’s opcode. See the docstring for more information.\nAssignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.     \nThis method can be used with no arguments as a debugging utility.\nThis function is also used internally in the method of . Together, the strings in and make up the signature of the autogenerated function in this Graph’s surrounding GraphModule. and should not be used otherwise.     \n  * () – A list that will store formatted strings representing the placeholders in the generated function. Internal use only.\n  * () – A single-element list that will store a formatted string representing the output of the generated function. Internal use only.\n\n         \nin the method of , and 2) is a placeholder Node, return . Otherwise, return a descriptive string representation of the current Node.     \nInsert an positional argument to the argument list with given index.     \n\n    \nReturns whether this op is impure, i.e. if its op is a placeholder or output, or if a call_function or call_module which is impure.     \nThe dict of keyword arguments to this . The interpretation of arguments depends on the node’s opcode. See the docstring for more information.\nAssignment to this property is allowed. All accounting of uses and users is updated automatically on assignment.          \nReturns normalized arguments to Python targets. This means that will be matched up to the module/functional’s signature and return exclusively kwargs in positional order if is true. Also populates default values. Does not support positional-only parameters or varargs parameters.     \n\n    \nInsert x before this node in the list of nodes in the graph. Example:     \n() – The node to put before this node. Must be a member of the same graph.               \n  * () – Callback that is called to determine whether a given user of the self node should be removed.\n\n    \nThe list of Nodes on which this change was made.     \nLoop through input nodes of , and replace all instances of with .     \n\n    \nReturn the Python stack trace that was recorded during tracing, if any. When traced with fx.Tracer, this property is usually populated by . To record stack traces during tracing for debug purposes, set on the instance. When traced with dynamo, this property will be populated by default by .\nstack_trace would have the innermost frame at the end of the string.     \nUpdate an existing positional argument to contain the new value . After calling, .     \n\n    \nUpdate an existing keyword argument to contain the new value . After calling, .     \n\n    \n> is the class that implements the symbolic tracing functionality of . A call to is equivalent to .\n> Tracer can be subclassed to override various behaviors of the tracing process. The different behaviors that can be overridden are described in the docstrings of the methods on this class.     \nMethod that specifies the behavior of this when it encounters a call to an instance.\nBy default, the behavior is to check if the called module is a leaf module via . If it is, emit a node referring to in the . Otherwise, call the normally, tracing through the operations in its function.\nThis method can be overridden to–for example–create nested traced GraphModules, or any other behavior you would want while tracing across boundaries.     \n  * () – The module for which a call is being emitted\n\n    \nThe return value from the Module call. In the case that a node was emitted, this is a value. Otherwise, it is whatever value was returned from the invocation.     \nA method to specify the behavior of tracing when preparing values to be used as arguments to nodes in the .\n  1. Iterate through collection types (e.g. tuple, list, dict) and recursively call on the elements.\n  2. Given a Proxy object, return a reference to the underlying IR \n  3. Given a non-Proxy Tensor object, emit IR for various cases:\n>      * For a non-Parameter Tensor, store the Tensor away in a special attribute referring to that attribute.\n\n    \n() – The value to be emitted as an in the .     \nCreate nodes corresponding to the signature of the Module. This method introspects root’s signature and emits those nodes accordingly, also supporting and .     \nInserts a graph node given target, args, kwargs, and name.\nThis method can be overridden to do extra checking, validation, or modification of values used in node creation. For example, one might want to disallow in-place operations from being recorded.     \nCreate a Node from the given arguments, then return the Node wrapped in a Proxy object.\nIf kind = ‘placeholder’, then we’re creating a Node that represents the parameter of a function. If we need to encode a default parameter, we use the tuple. is otherwise empty for Nodes.     \nGets a fresh name for a prefix and returns it. This function ensures that it will not clash with an existing attribute on the graph.     \nMethod that specifies the behavior of this when we call getattr on a call to an instance.\nBy default, the behavior is to return a proxy value for the attribute. It also stores the proxy value in the , so that future calls will reuse the proxy rather than creating a new one.\nThis method can be overridden to –for example– not return proxies when querying parameters.     \n\n    \nA method to specify whether a given is a “leaf” module.\nLeaf modules are the atomic units that appear in the IR, referenced by calls. By default, Modules in the PyTorch standard library namespace (torch.nn) are leaf modules. All other modules are traced through and their constituent ops are recorded, unless specified otherwise via this parameter.     \n  * ( contains submodule , which contains submodule , that module will appear with the qualified name here.\n\n     \n\nCalled when a proxy object is being iterated over, such as\n    \nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return an iterator.      \n\nCalled when a proxy object is has the keys() method called.\n    \nThis is what happens when ** is called on a proxy. This should return an iterator it ** is suppose to work in your custom tracer.     \nHelper method to find the qualified name of in the Module hierarchy of . For example, if has a submodule named , which has a submodule named , passing into this function will return the string “foo.bar”.      \n\nCalled when a proxy object is being converted to a boolean, such as\n    \nwhen used in control flow. Normally we don’t know what to do because we don’t know the value of the proxy, but a custom tracer can attach more information to the graph node using create_node and can choose to return a value.     \nTrace and return the corresponding FX representation. can either be an instance or a Python callable.\nNote that after this call, may be different from the passed in here. For example, when a free function is passed to , we will create an instance to use as the root and add embedded constants to.     \n  * () – Either a or a function to be traced through. Backwards-compatibility for this parameter is guaranteed.\n  * () – Concrete arguments that should not be treated as Proxies. This parameter is experimental and its backwards-compatibility is guaranteed.\n\n    \nobjects are wrappers that flow through the program during symbolic tracing and record all the operations ( function calls, method calls, operators) that they touch into the growing FX Graph.\nIf you’re doing graph transforms, you can wrap your own method around a raw so that you can use the overloaded operators to add additional things to a .\nobjects cannot be iterated. In other words, the symbolic tracer will throw an error if a is used in a loop or as an / function argument.\nThere are two main ways around this: 1. Factor out the untraceable logic into a top-level function and use on it. 2. If the control flow is static (i.e. the loop trip count is based on some hyperparameter), the code can be kept in its original position and refactored into something like:\nFor a more detailed description into the Proxy internals, check out the “Proxy” section in      \nAn Interpreter executes an FX graph Node-by-Node. This pattern can be useful for many things, including writing code transformations as well as analysis passes.\nMethods in the Interpreter class can be overridden to customize the behavior of execution. The map of overrideable methods in terms of call hierarchy:\nSuppose we want to swap all instances of with and vice versa (including their method equivalents). We could subclass Interpreter like so:     \n  * () – If passed, the interpreter will execute this graph instead of , using the provided argument to satisfy any requests for state.\n\n    \nRun via interpretation and return the result. This uses the “boxed” calling convention, where you pass a list of arguments, which will be cleared by the interpreter. This ensures that input tensors are promptly deallocated.          \n  * () – The call target for this node. See for details on semantics\n\n         \n  * () – The call target for this node. See for details on semantics\n\n         \n  * () – The call target for this node. See for details on semantics\n\n    \nFetch the concrete values of and of node from the current execution environment.               \n  * () – The call target for this node. See for details on semantics\n\n    \nRecursively descend through and look up the concrete value for each in the current execution environment.     \n  * () – Data structure within which to look up concrete values\n  * () – Node to which belongs. This is only used for error reporting.\n\n    \nExecute an node. This really just retrieves the value referenced by the node and returns it.     \n  * () – The call target for this node. See for details on semantics\n\n    \nExecute a node. Note that this is stateful: maintains an internal iterator over arguments passed to and this method returns next() on that iterator.     \n  * () – The call target for this node. See for details on semantics\n\n         \n  * – The arguments to the Module to run, in positional order\n  * () – An optional starting environment for execution. This is a dict mapping to any value. This can be used, for example, to pre-populate results for certain so as to do only partial evaluation within the interpreter.\n\n    \nRun a specific node and return the result. Calls into placeholder, get_attr, call_function, call_method, call_module, or output depending on      \nis a special type of interpreter that produces a new . It exposes a method that returns the transformed . does not require arguments to run, as does. works entirely symbolically.\nSuppose we want to swap all instances of with and vice versa (including their method equivalents). We could subclass like so:     \nExecute a node. In , this is overridden to insert a new node into the output graph.     \n  * () – The call target for this node. See for details on semantics\n\n    \nExecute a node. In , this is overridden to insert a new into the output graph.     \n  * () – The call target for this node. See for details on semantics\n\n         \nMatches all possible non-overlapping sets of operators and their data dependencies () in the Graph of a GraphModule (), then replaces each of these matched subgraphs with another subgraph ().     \n  * () – The GraphModule that wraps the Graph to operate on\n\n    \nA list of objects representing the places in the original graph that was matched to. The list is empty if there are no matches. is defined as:\n```\n \n    \n     \n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n      \n\n```\n\nThe above code will first match in the method of . Pattern-matching is done based on use-def relationships, not node names. For example, if you had in , you could match in the original function, despite the variable names being different ( vs ).\nThe statement in is matched based on its value only; it may or may not match to the statement in the larger graph. In other words, the pattern doesn’t have to extend to the end of the larger graph.\nWhen the pattern is matched, it will be removed from the larger function and replaced by . If there are multiple matches for in the larger function, each non-overlapping match will be replaced. In the case of a match overlap, the first found match in the set of overlapping matches will be replaced. (“First” here being defined as the first in a topological ordering of the Nodes’ use-def relationships. In most cases, the first Node is the parameter that appears directly after , while the last Node is whatever the function returns.)\nOne important thing to note is that the parameters of the Callable must be used in the Callable itself, and the parameters of the Callable must match the pattern. The first rule is why, in the above code block, the function has parameters , but the function only has parameters . doesn’t use , so it shouldn’t specify as a parameter. As an example of the second rule, consider replacing\nIn this case, needs the same number of parameters as (both and ), even though the parameter isn’t used in .\n  *     *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/genindex.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/library.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\ntorch.library is a collection of APIs for extending PyTorch’s core library of operators. It contains utilities for testing custom operators, creating new custom operators, and extending operators defined with PyTorch’s C++ operator registration APIs (e.g. aten operators).\nFor a detailed guide on effectively using these APIs, please see for more details on how to effectively use these APIs.\nUse to test custom ops for incorrect usage of the Python torch.library and/or C++ TORCH_LIBRARY APIs. Also, if your operator supports training, use to test that the gradients are mathematically correct.     \nGiven an operator and some sample arguments, tests if the operator is registered correctly.\nThat is, when you use the torch.library/TORCH_LIBRARY APIs to create a custom op, you specified metadata (e.g. mutability info) about the custom op and these APIs require that the functions you pass them satisfy certain properties (e.g. no data pointer access in the fake/meta/abstract kernel) tests these metadata and properties.\n  * test_schema: If the schema matches the implementation of the operator. For example: if the schema specifies a Tensor is mutated, then we check the implementation mutates the Tensor. If the schema specifies that we return a new Tensor, then we check that the implementation returns a new Tensor (instead of an existing one or a view of an existing one).\n  * test_autograd_registration: If the operator supports training (autograd): we check that its autograd formula is registered via torch.library.register_autograd or a manual registration to one or more DispatchKey::Autograd keys. Any other DispatchKey-based registrations may lead to undefined behavior.\n  * test_faketensor: If the operator has a FakeTensor kernel (and if it is correct). The FakeTensor kernel is necessary ( but not sufficient) for the operator to work with PyTorch compilation APIs (torch.compile/export/FX). We check that a FakeTensor kernel (also sometimes known as a meta kernel) was registered for the operator and that it is correct. This test takes the result of running the operator on real tensors and the result of running the operator on FakeTensors and checks that they have the same Tensor metadata (sizes/strides/dtype/device/etc).\n  * test_aot_dispatch_dynamic: If the operator has correct behavior with PyTorch compilation APIs (torch.compile/export/FX). This checks that the outputs (and gradients, if applicable) are the same under eager-mode PyTorch and torch.compile. This test is a superset of and is an e2e test; other things it tests are that the operator supports functionalization and that the backward pass (if it exists) also supports FakeTensor and functionalization.\n\n\nFor best results, please call multiple times with a representative set of inputs. If your operator supports autograd, please use with inputs with ; if your operator supports multiple devices (e.g. CPU and CUDA), please use with inputs on all supported devices.     \n  * () – The operator. Must either be a function decorated with or an OpOverload/OpOverloadPacket found in torch.ops.* (e.g. torch.ops.aten.sin, torch.ops.mylib.foo)\n  * () – Tests that we should run. Default: all of them. Example: (“test_schema”, “test_faketensor”)\n  * () – Relative tolerance for floating point comparisons. If specified must also be specified. If omitted, default values based on the are selected (see the table in ).\n  * () – Absolute tolerance for floating point comparisons. If specified must also be specified. If omitted, default values based on the are selected (see the table in ).\n\n\nopcheck and test different things; opcheck tests if your usage of torch.library APIs is correct while tests if your autograd formula is mathematically correct. Use both to test custom ops that support gradient computation.     \nReasons why you may want to create a custom op include: - Wrapping a third-party library or custom kernel to work with PyTorch subsystems like Autograd. - Preventing torch.compile/export/FX tracing from peeking inside your function.\nThis API is used as a decorator around a function (please see examples). The provided function must have type hints; these are needed to interface with PyTorch’s various subsystems.     \n  * () – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.\n  * () – The device type(s) the function is valid for. If no device type is provided, then the function is used as the default implementation for all device types. Examples: “cpu”, “cuda”. When registering a device-specific implementation for an operator that accepts no Tensors, we require the operator to have a “device: torch.device argument”.\n\n\nWe recommend not passing in a arg and instead letting us infer it from the type annotations. It is error-prone to write your own schema. You may wish to provide your own schema if our interpretation of the type annotation is not what you want. For more info on how to write a schema string, see      \n```\n \n   \n   \n   \n\n \n    \n      \n      \n     \n\n  \n  \n  \n\n# Example of a custom op that only works for one device type.\n  \n    \n      \n      \n     \n\n  \n  \n  \n\n# Example of a custom op that mutates an input\n  \n    \n      \n     \n\n  \n  \n\n  \n\n\n  \n    \n     \n\n\n\n```\n    \nCreate a custom operator whose implementation is backed by 1+ triton kernels.\nThis is a more structured way of using triton kernels with PyTorch. Prefer using triton kernels with no custom operator wrappers (like , ) because that is simpler; only use / if you want to create an operator that behaves like PyTorch built-in operators. For example, you may use a wrapper API to define the behavior of the triton kernel when passed a tensor subclass or under a TorchDispatchMode.\nUse instead of when the implementation consists of 1+ triton kernels. treats custom operators as opaque ( and will never trace into them), but makes the implementation visible to these subsystems, allowing them to optimize the triton kernel(s).\nNote that must only consist of calls to PyTorch-understood operators and triton kernels. Any triton kernels called inside must be wrapped in a call to .     \n  * () – The names of args that the function mutates. This MUST be accurate, otherwise, the behavior is undefined. If “unknown”, it pessimistically assumes that all inputs to the operator are being mutated.\n\n\n```\n \n    \n\n \n     \n\n\n \n    \n    \n    \n    \n     \n\n      \n        \n         \n        \n         \n         \n        \n        \n\n \n      \n      \n      \n\n     \n          \n\n    # NB: we need to wrap the triton kernel in a call to wrap_triton\n        \n     \n\n\n  \n      \n\n   \n   \n\n   \n    \n\n```\n    \nAllows capture of a triton kernel into a graph via make_fx or non-strict .\nThese technologies perform Dispatcher-based tracing (via ) and cannot see calls to raw triton kernels. The API wraps a triton kernel into a callable that can actually be traced into a graph.\n```\n \n \n     \n   \n   \n\n\n \n    \n    \n    \n    \n     \n\n      \n        \n         \n        \n         \n         \n        \n        \n\n  \n      \n      \n\n     \n          \n\n        \n     \n\n   \n   \n   \n\n\n#     empty_like = torch.ops.aten.empty_like.default(x_1, pin_memory = False)\n\n#         kernel_idx = 0, constant_args_idx = 0,\n#         grid = [(1, 1, 1)], kwargs = {\n#             'in_ptr0': x_1, 'in_ptr1': y_1, 'out_ptr': empty_like,\n#             'n_elements': 3, 'BLOCK_SIZE': 16\n#         })\n\n\n```\n\nUse the register.* methods, such as and , to add implementations for any operators (they may have been created using or via PyTorch’s C++ operator registration APIs).     \nRegister an implementation for a device type for this operator.\nSome valid device_types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”. This API may be used as a decorator.     \n  * () – The device_types to register an impl to. If None, we will register to all device types – please only use this option if your implementation is truly device-type-agnostic.\n  * () – The function to register as the implementation for the given device types.\n\n    \n```\n \n   \n   \n   \n\n\n  \n    \n      \n      \n     \n\n\n \n \n      \n      \n     \n\n  \n  \n  \n  \n\n```\n         \n  * () – The operator to register an autocast dispatch rule to.\n  * ( attribute of a . Thus, you may obtain the device type of a tensor using .\n  * () – When custom op runs in an autocast-enabled region, casts incoming floating-point Tensors to the target dtype (non-floating-point Tensors are not affected), then executes custom op with autocast disabled.\n\n    \n```\n \n   \n   \n\n\n \n    \n     \n\n\n  \n\n    \n  \n      \n   \n\n```\n    \nIn order for an operator to work with autograd, you need to register a backward formula: 1. You must tell us how to compute gradients during the backward pass by providing us a “backward” function. 2. If you need any values from the forward to compute gradients, you can use to save values for backward.\nruns during the backward pass. It accepts : - is one or more gradients. The number of gradients matches the number of outputs of the operator. The object is used by . The semantics of are the same as .\nruns during the forward pass. Please save quantities needed for backward onto the object via either or assigning them as attributes of . If your custom op has kwarg-only arguments, we expect the signature of to be .\nBoth and must be traceable. That is, they may not directly access and they must not depend on or mutate global state. If you need a non-traceable backward, you can make it a separate custom_op that you call inside .\nIf you need different autograd behavior on different devices, then we recommend creating two different custom operators, one for each device that needs different behavior, and switching between them at runtime.     \nAn “FakeTensor implementation” specifies the behavior of this operator on Tensors that carry no data (“FakeTensor”). Given some input Tensors with certain properties (sizes/strides/storage_offset/device), it specifies what the properties of the output Tensors are.\nThe FakeTensor implementation has the same signature as the operator. It is run for both FakeTensors and meta tensors. To write a FakeTensor implementation, assume that all Tensor inputs to the operator are regular CPU/CUDA/Meta tensors, but they do not have storage, and you are trying to return regular CPU/CUDA/Meta tensor(s) as output. The FakeTensor implementation must consist of only PyTorch operations (and may not directly access the storage or data of any input or intermediate Tensors).\nThis API may be used as a decorator (see examples).\n```\n \n   \n   \n\n\n \n        \n     \n\n\n   \n       \n       \n       \n       \n       \n       \n\n         \n\n \n       \n       \n      \n        \n\n    \n\n\n \n    \n      \n       \n      \n\n\n \n\n# Since we cannot peek at the data in an fake impl,\n# we use the ctx object to construct a new symint that\n\n      \n      \n       \n       \n     \n\n   \n\n       \n   \n\n\n  \n\n```\n    \nThis API may be used as a decorator (see examples).\nIn order for an operator to work with , you may need to register a vmap implementation in the following signature:\nwhere and are the arguments and kwargs for . We do not support kwarg-only Tensor args.\nIt specifies how do we compute the batched version of given inputs with an additional dimension (specified by ).\nFor each arg in , has a corresponding . It is if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.\nis a collection of additional metadata that may be helpful: specifies the size of the dimension being vmapped over, while is the option that was passed to .\nThe return of the function is a tuple of . Similar to , should be of the same structure as and contain one per output that specifies if the output has the vmapped dimension and what index it is in.\nThe vmap function should aim to preserve the semantics of the entire custom operator. That is, should be replaceable with a .\nIf your custom operator has any custom behavior in the backward pass, please keep this in mind.     \nThis API was renamed to in PyTorch 2.4. Please use that instead.     \nCalling is only valid inside of an fake impl (see for more usage details.     \nThis allows for open registration to specify the behavior between the operator and the without needing to modify the or the operator directly.\nIf it is a Tensor subclass, we expect to have the following signature: \nIf it is a TorchDispatchMode, we expect to have the following signature: \nand will have been normalized the same way they are in (see ).     \nParses the schema of a given function with type hints. The schema is inferred from the function’s type hints, and can be used to define a new operator.\n  * None of the outputs alias any of the inputs or each other.\n  * String type annotations “device, dtype, Tensor, types” without library specification are\nassumed to be torch.*. Similarly, string type annotations “Optional, List, Sequence, Union”\n  * it assumes that all inputs to the operator are being mutates.\n\n\nCallers (e.g. the custom ops API) are responsible for checking these assumptions.     \n  * () – The name of the operator in the schema. If is None, then the name is not included in the inferred schema. Note that the input schema to requires a operator name.\n  * () – The arguments that are mutated in the function.\n\n    \nCustomOpDef is a wrapper around a function that turns it into a custom op.\nIt has various methods for registering additional behavior for this custom op.     \nDisable or re-enable an already registered kernel for this custom operator.\nIf the kernel is already disabled/enabled, this is a no-op.\nIf a kernel is first disabled and then registered, it is disabled until enabled again.\n```\n  \n\n\n \n    \n     \n\n  \n\n\n \n     \n\n  \n\n\n    \n      \n\n```\n\nThe following APIs are direct bindings to PyTorch’s C++ low-level operator registration APIs.\nThe low-level operator registration APIs and the PyTorch Dispatcher are a complicated PyTorch concept. We recommend you use the higher level APIs above (that do not require a torch.library.Library object) when possible. This blog post <\nA tutorial that walks you through some examples on how to use this API is available on      \nA class to create libraries that can be used to register new operators or override operators in existing libraries from Python. A user can optionally pass in a dispatch keyname if they only want to register kernels corresponding to only one specific dispatch key.\nTo create a library to override operators in an existing library (with name ns), set the kind to “IMPL”. To create a new library (with name ns) to register new operators, set the kind to “DEF”. To create a fragment of a possibly existing library to register operators (and bypass the limitation that there is only one library for a given namespace), set the kind to “FRAGMENT”.     \n\n    \nDefines a new operator and its semantics in the ns namespace.     \n  * () – Indicates if the aliasing properties of the operator arguments can be inferred from the schema (default behavior) or not (“CONSERVATIVE”).\n  * () – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.\n\n    \nRegisters the function implementation as the fallback for the given key.\nThis function only works for a library with global namespace (“_”).     \n  * – function used as fallback for the given dispatch key or to register a fallthrough.\n  * – dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.\n  * – flag controlling if the current dispatcher call keyset should be passed as the first argument to when calling. This should be used to create the appropriate keyset for redispatch calls.\n\n    \nRegisters the function implementation for an operator defined in the library.     \n  * – operator name (along with the overload) or OpOverload object.\n  * – function that’s the operator implementation for the input dispatch key or to register a fallthrough.\n  * – dispatch key that the input function should be registered for. By default, it uses the dispatch key that the library was created with.\n  * – flag controlling if the current dispatcher call keyset should be passed as the first argument to when calling. This should be used to create the appropriate keyset for redispatch calls.\n\n    \nA dummy function to pass to in order to register a fallthrough.     \nIn PyTorch, defining an op (short for “operator”) is a two step-process: - we need to define the op (by providing an operator name and schema) - we need to implement behavior for how the operator interacts with various PyTorch subsystems, like CPU/CUDA Tensors, Autograd, etc.\nThis entrypoint defines the custom operator (the first step) you must then perform the second step by calling various APIs, like or .     \n  * () – If provided, the lifetime of this operator will be tied to the lifetime of the Library object.\n  * () – one or more torch.Tag to apply to this operator. Tagging an operator changes the operator’s behavior under various PyTorch subsystems; please read the docs for the torch.Tag carefully before applying it.\n\n    \n```\n \n   \n\n\n \n\n\n \n \n     \n\n\n  \n  \n  \n\n```\n    \nRegister an implementation for a device type for this operator.\nYou may pass “default” for to register this implementation as the default implementation for ALL device types. Please only use this if the implementation truly supports all device types; for example, this is true if it is a composition of built-in PyTorch operators.\nThis API may be used as a decorator. You can use nested decorators with this API provided they return a function and are placed inside this API (see Example 2).\nSome valid types are: “cpu”, “cuda”, “xla”, “mps”, “ipu”, “xpu”.     \n  * () – The device types to register an impl to.\n  * () – If provided, the lifetime of this registration will be tied to the lifetime of the Library object.\n\n\n```\n \n   \n\n\n \n\n\n \n \n     \n\n  \n  \n  \n\n\n \n      \n            \n     \n\n\n \n\n\n \n\n \n     \n\n\n  \n\n  \n    \n  \n\n```\n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mps.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis package enables an interface for accessing MPS (Metal Performance Shaders) backend in Python. Metal is Apple’s API for programming metal GPU (graphics processor unit). Using MPS means that increased performance can be achieved, by running work on the metal GPU(s). See \nWaits for all kernels in all streams on a MPS device to complete.  \n---  \nSets the seed for generating random numbers to a random number.  \nReleases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU applications.  \nSet memory fraction for limiting process's memory allocation on MPS device.  \nReturns the current GPU memory occupied by tensors in bytes.  \nReturns total GPU memory allocated by Metal driver for the process in bytes.  \nReturns recommended max Working set size for GPU memory in bytes.  \nCompiles compute shader from source and allows one to invoke kernels defined there from the comfort of Python runtime Example.  \nContext Manager to enabling generating OS Signpost tracing from MPS backend.  \n---  \nChecks if context manager is usable To enable metal capture, set MTL_CAPTURE_ENABLED envvar  \nConext manager that enables capturing of Metal calls into gputrace  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mtia.memory.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\nThis package adds support for device memory management implemented in MTIA.\nReturn a dictionary of MTIA memory allocator statistics for a given device.  \n---  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/meta.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe “meta” device is an abstract device which denotes a tensor which records only metadata, but no actual data. Meta tensors have two primary use cases:\n  * Models can be loaded on the meta device, allowing you to load a representation of the model without actually loading the actual parameters into memory. This can be helpful if you need to make transformations on the model before you load the actual data.\n  * Most operations can be performed on meta tensors, producing new meta tensors that describe what the result would have been if you performed the operation on a real tensor. You can use this to perform abstract analysis without needing to spend time on compute or space to represent the actual tensors. Because meta tensors do not have real data, you cannot perform data-dependent operations like or . In some cases, not all device types (e.g., CPU and CUDA) have exactly the same output metadata for an operation; we typically prefer representing the CUDA behavior faithfully in this situation.\n\n\nAlthough in principle meta tensor computation should always be faster than an equivalent CPU/CUDA computation, many meta tensor implementations are implemented in Python and have not been ported to C++ for speed, so you may find that you get lower absolute framework latency with small CPU tensors.\nAn object can be loaded with onto meta device by specifying :\nIf you have some arbitrary code which performs some tensor construction without explicitly specifying a device, you can override it to instead construct on meta device by using the context manager:\nThis is especially helpful NN module construction, where you often are not able to explicitly pass in a device for initialization:\nYou cannot convert a meta tensor directly to a CPU/CUDA tensor, because the meta tensor stores no data and we do not know what the correct data values for your new tensor are:\n```\n \n\n  File , line , in \n: \n\n```\n\nUse a factory function like to explicitly specify how you would like the missing data to be filled in.\nNN modules have a convenience method that allow you to the module to another device, leaving all parameters uninitialized. You are expected to explicitly reinitialize the parameters manually:\ncontains undocumented utilities for taking an arbitrary Tensor and constructing an equivalent meta Tensor with high fidelity. These APIs are experimental and may be changed in a BC breaking way at any time.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mtia.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe MTIA backend is implemented out of the tree, only interfaces are be defined here.\nThis package enables an interface for accessing MTIA backend in python\nReturn a dictionary of MTIA memory allocator statistics for a given device.  \n---  \nReturn capability of a given device as a tuple of (major version, minor version).  \nSet the current stream.This is a wrapper API to set the stream.  \nWrap around the Context-manager StreamContext that selects a given stream.  \nWaits for all jobs in all streams on a MTIA device to complete.  \nQuery and record Stream status to identify or control dependencies across Stream and measure timing.  \n---  \nAn in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/monitor.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis module is a prototype release, and its interfaces and functionality may change without warning in future PyTorch releases.\nThe stat interfaces are designed to be used for tracking high level metrics that are periodically logged out to be used for monitoring system performance. Since the stats aggregate with a specific window size you can log to them from critical loops with minimal performance impact.\nFor more infrequent events or values such as loss, accuracy, usage tracking the event interface can be directly used.\nEvent handlers can be registered to handle the events and pass them to an external event sink.     \n> These are types of aggregations that can be used to accumulate stats.\n>     \n> MEAN computes the arithmetic mean of all the added values.     \nStat is used to compute summary statistics in a performant way over fixed intervals. Stat logs the statistics as an Event once every duration. When the window closes the stats are logged via the event handlers as a event.\nshould be set to something relatively high to avoid a huge number of events being logged. Ex: 60s. Stat uses millisecond precision.\nIf is set, the stat will cap the number of samples per window by discarding calls once adds have occurred. If it’s not set, all calls during the window will be included. This is an optional field to make aggregations more directly comparable across windows when the number of samples might vary.\nWhen the Stat is destructed it will log any remaining data even if the window hasn’t elapsed.     \nAdds a value to the stat to be aggregated according to the configured stat type and aggregations.     \nNumber of data points that have currently been collected. Resets once the event has been logged.     \nReturns the current value of the stat, primarily for testing purposes. If the stat has logged and no additional values have been added this will be zero.     \nThe name of the stat that was set during creation.     \nEvent represents a specific typed event to be logged. This can represent high-level data points such as loss or accuracy per epoch or more low-level aggregations such as through the Stats provided through this library.\nAll Events of the same type should have the same name so downstream handlers can correctly process them.     \nEventHandlerHandle is a wrapper type returned by used to unregister the handler via . This cannot be directly initialized.     \nlog_event logs the specified event to all of the registered event handlers. It’s up to the event handlers to log the event out to the corresponding event sink.\nIf there are no event handlers registered this method is a no-op.     \nregister_event_handler registers a callback to be called whenever an event is logged via . These handlers should avoid blocking the main thread since that may interfere with training as they run during the call.     \nunregister_event_handler unregisters the returned after calling . After this returns the event handler will no longer receive events.     \nTensorboardEventHandler is an event handler that will write known events to the provided SummaryWriter.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nested.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe PyTorch API of nested tensors is in prototype stage and will change in the near future.\nNested tensors allow for ragged-shaped data to be contained within and operated upon as a single tensor. Such data is stored underneath in an efficient packed representation, while exposing a standard PyTorch tensor interface for applying operations.\nA common application of nested tensors is for expressing batches of variable-length sequential data present in various domains, such as varying sentence lengths, image sizes, and audio / video clip lengths. Traditionally, such data has been handled by padding sequences to that of the max length within a batch, performing computation on the padded form, and subsequently masking to remove padding. This is inefficient and error-prone, and nested tensors exist to address these problems.\nThe API for calling operations on a nested tensor is no different from that of a regular , allowing for seamless integration with existing models, with the main difference being .\nAs this is a prototype feature, the set of is limited, but growing. We welcome issues, feature requests, and contributions. More information on contributing can be found \nThere are two forms of nested tensors present within PyTorch, distinguished by layout as specified during construction. Layout can be one of or . We recommend utilizing the layout whenever possible. While it currently only supports a single ragged dimension, it has better op coverage, receives active development, and integrates well with . These docs adhere to this recommendation and refer to nested tensors with the layout as “NJTs” for brevity throughout.\nConstruction is straightforward and involves passing a list of tensors to the constructor. A nested tensor with the layout (AKA an “NJT”) supports a single ragged dimension. This constructor will copy the input tensors into a packed, contiguous block of memory according to the layout described in the section below.\n```\n      \n\n\n\n\n    \n    \n\n\n```\n\nEach tensor in the list must have the same number of dimensions, but the shapes can otherwise vary along a single dimension. If the dimensionalities of the input components don’t match, the constructor throws an error.\n```\n    \n     \n    \n\nRuntimeError: When constructing a nested tensor, all tensors in list must have the same dim\n\n```\n\nDuring construction, dtype, device, and whether gradients are required can be chosen via the usual keyword arguments.\n```\n       \n    \n\n\n\n\n```\n\ncan be used to preserve autograd history from the tensors passed to the constructor. When this constructor is utilized, gradients will flow through the nested tensor back into the original components. Note that this constructor still copies the input components into a packed, contiguous block of memory.\n```\n    \n    \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThe above functions all create contiguous NJTs, where a chunk of memory is allocated to store a packed form of the underlying components (see the section below for more details).\nIt is also possible to create a non-contiguous NJT view over a pre-existing dense tensor with padding, avoiding the memory allocation and copying. is the tool for accomplishing this.\nNote that the nested tensor acts as a view over the original padded dense tensor, referencing the same memory without copying / allocation. Operation support for non-contiguous NJTs is somewhat more limited, so if you run into support gaps, it’s always possible to convert to a contiguous NJT using .\nFor efficiency, nested tensors generally pack their tensor components into a contiguous chunk of memory and maintain additional metadata to specify batch item boundaries. For the layout, the contiguous chunk of memory is stored in the component, with the component delineating batch item boundaries for the ragged dimension.\nIt’s possible to directly access the underlying NJT components when necessary.\n```\n    \n    \n     \n  # note the \"packing\" of the ragged dimension; no padding needed\n\n\n\n\n```\n\nIt can also be useful to construct an NJT from the jagged and constituents directly; the constructor serves this purpose.\nAn NJT has a well-defined shape with dimensionality 1 greater than that of its components. The underlying structure of the ragged dimension is represented by a symbolic value ( in the example below).\nNJTs must have the same ragged structure to be compatible with each other. For example, to run a binary operation involving two NJTs, the ragged structures must match (i.e. they must have the same ragged shape symbol in their shapes). In the details, each symbol corresponds with an exact tensor, so both NJTs must have the same tensor to be compatible with each other.\n```\n   \n   \n     \n     \n  \n\n    \nRuntimeError: cannot call binary pointwise function add.Tensor with inputs of shapes (2, j2, 128) and (2, j3, 128)\n\n```\n\nIn the above example, even though the conceptual shapes of the two NJTs are the same, they don’t share a reference to the same tensor, so their shapes differ, and they are not compatible. We recognize that this behavior is unintuitive and are working hard to relax this restriction for the beta release of nested tensors. For a workaround, see the section of this document.\nIn addition to the metadata, NJTs can also compute and cache the minimum and maximum sequence lengths for its components, which can be useful for invoking particular kernels (e.g. SDPA). There are currently no public APIs for accessing these, but this will change for the beta release.\nThis section contains a list of common operations over nested tensors that you may find useful. It is not comprehensive, as there are on the order of a couple thousand ops within PyTorch. While a sizeable subset of these are supported for nested tensors today, full support is a large task. The ideal state for nested tensors is full support of all PyTorch operations that are available for non-nested tensors. To help us accomplish this, please consider:\n  * Contributing! It’s not too hard to add nested tensor support for a given PyTorch op; see the section below for details.\n\n\nallows you to retrieve a view of the nested tensor’s constituents.\n```\n \n   \n   \n    \n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n```\n\nNote that is not a copy, but rather a slice of the underlying memory, which represents the first entry or constituent of the nested tensor.\nconverts an NJT to a padded dense tensor with the specified padding value. The ragged dimension will be padded out to the size of the maximum sequence length.\n```\n \n   \n   \n    \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThis can be useful as an escape hatch to work around NJT support gaps, but ideally such conversions should be avoided when possible for optimal memory usage and performance, as the more efficient nested tensor layout does not materialize padding.\nThe reverse conversion can be accomplished using , which applies ragged structure to a given dense tensor to produce an NJT. Note that by default, this operation does not copy the underlying data, and thus the output NJT is generally non-contiguous. It may be useful to explicitly call here if a contiguous NJT is desired.\nNested tensors support a wide array of operations for shape manipulation, including views.\n```\n   \n   \n    \n\n\n\n\n  \n\n  \n\n  \n\n \n\n\n```\n\nAs variable-length sequences are common inputs to attention mechanisms, nested tensors support important attention operators and . See for usage examples of NJT with SDPA and for usage examples of NJT with FlexAttention.\nNJTs are designed to be used with for optimal performance, and we always recommend utilizing with NJTs when possible. NJTs work out-of-the-box and graph-break-free both when passed as inputs to a compiled function or module OR when instantiated in-line within the function.\nIf you’re not able to utilize for your use case, performance and memory usage may still benefit from the use of NJTs, but it’s not as clear-cut whether this will be the case. It is important that the tensors being operated on are large enough so the performance gains are not outweighed by the overhead of python tensor subclasses.\nNote that NJTs support to avoid unnecessary recompiles with changing ragged structure.\n```\n   \n   \n   \n   \n    \n    \n     \n\n   \n  \n    # NB: No recompile needed even though ragged structure differs\n\n```\n\nIf you run into problems or arcane errors when utilizing NJT + , please file a PyTorch issue. Full subclass support within is a long-term effort and there may be some rough edges at this time.\nThis section contains common errors that you may run into when utilizing nested tensors, alongside the reason for these errors and suggestions for how to address them.\nThis error is becoming rarer as nested tensor op support grows, but it’s still possible to hit it today given that there are a couple thousand ops within PyTorch.\nThe error is straightforward; we haven’t gotten around to adding op support for this particular op yet. If you’d like, you can an implementation yourself OR simply \nThis error occurs when calling an op that operates over multiple NJTs with incompatible ragged structures. Currently, it is required that input NJTs have the exact same constituent in order to have the same symbolic ragged structure symbol (e.g. ).\nAs a workaround for this situation, it is possible to construct NJTs from the and components directly. With both NJTs referencing the same components, they are considered to have the same ragged structure and are thus compatible.\nThis error occurs when calling an op that does data-dependent operation within torch.compile; this commonly occurs for ops that need to examine the values of the NJT’s to determine the output shape. For example:\nIn this example, calling on the batch dimension of the NJT requires examination of the NJT’s data to delineate batch item boundaries within the packed ragged dimension. As a workaround, there are a couple torch.compile flags that can be set:\nIf, after setting these, you still see data-dependent operator errors, please file an issue with PyTorch. This area of is still in heavy development and certain aspects of NJT support may be incomplete.\nIf you’d like to contribute to nested tensor development, one of the most impactful ways to do so is to add nested tensor support for a currently-unsupported PyTorch op. This process generally consists of a couple simple steps:\n  1. Determine the name of the op to add; this should be something like . The signature for this op can be found in .\n  2. Register an op implementation in , following the pattern established there for other ops. Use the signature from for schema validation.\n\n\nThe most common way to implement an op is to unwrap the NJT into its constituents, redispatch the op on the underlying buffer, and propagate the relevant NJT metadata (including ) to a new output NJT. If the output of the op is expected to have a different shape from the input, new , etc. metadata must be computed.\nWhen an op is applied over the batch or ragged dimension, these tricks can help quickly get a working implementation:\n  * For operation on the ragged dimension, consider converting to padded dense with a properly-selected padding value that won’t negatively bias the output, running the op, and converting back to NJT. Within , these conversions can be fused to avoid materializing the padded intermediate.\n\n    \nConstructs a nested tensor with no autograd history (also known as a “leaf tensor”, see ) from a list of tensors.     \n  * () – a list of tensors, or anything that can be passed to torch.tensor,\n\n    \n  * (, optional) – the desired type of returned nested tensor. Default: if None, same as leftmost tensor in the list.\n  * (, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout.\n  * (, optional) – the desired device of returned nested tensor. Default: if None, same as leftmost tensor in the list\n  * () – If autograd should record operations on the returned nested tensor. Default: .\n  * () – If set, returned nested tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: .\n\n    \nConstructs a jagged layout nested tensor from the given jagged components. The jagged layout consists of a required values buffer with the jagged dimension packed into a single dimension. The offsets / lengths metadata determines how this dimension is split into batch elements and are expected to be allocated on the same device as the values buffer.     \n  * offsets: Indices within the packed dimension splitting it into heterogeneously-sized batch elements. Example: [0, 2, 3, 6] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3]. Note that both the beginning and ending offsets are required for kernel convenience (i.e. shape batch_size + 1).\n  * lengths: Lengths of the individual batch elements; shape == batch_size. Example: [2, 1, 3] indicates that a packed jagged dim of size 6 should be conceptually split into batch elements of length [2, 1, 3].\n\n\nNote that it can be useful to provide both offsets and lengths. This describes a nested tensor with “holes”, where the offsets indicate the start position of each batch item and the length specifies the total number of elements (see example below).\nThe returned jagged layout nested tensor will be a view of the input values tensor.     \n  * () – The underlying buffer in the shape of (sum_B(*), D_1, …, D_N). The jagged dimension is packed into a single dimension, with the offsets / lengths metadata used to distinguish batch elements.\n  * (optional ) – Offsets into the jagged dimension of shape B + 1.\n  * (optional ) – Lengths of the batch elements of shape B.\n  * () – Indicates which dimension in values is the packed jagged dimension. If None, this is set to dim=1 (i.e. the dimension immediately following the batch dimension). Default: None\n  * () – If set, uses the specified value as the cached minimum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n  * () – If set, uses the specified value as the cached maximum sequence length for the returned nested tensor. This can be a useful alternative to computing this value on-demand, possibly avoiding a GPU -> CPU sync. Default: None\n\n\n```\n   \n       \n   \n\n\n\n\n\n\n\n   \n     \n    \n\n    \n    \n\n  \n\n\n  \n\n\n  \n\n\n```\n    \nConstructs a nested tensor preserving autograd history from a tensor or a list / tuple of tensors.\nIf a nested tensor is passed, it will be returned directly unless the device / dtype / layout differ. Note that converting device / dtype will result in a copy, while converting layout is not currently supported by this function.\nIf a non-nested tensor is passed, it is treated as a batch of constituents of consistent size. A copy will be incurred if the passed device / dtype differ from those of the input OR if the input is non-contiguous. Otherwise, the input’s storage will be used directly.\nIf a tensor list is provided, tensors in the list are always copied during construction of the nested tensor.     \n() – a tensor to treat as a nested tensor OR a list / tuple of tensors with the same ndim     \n  * (, optional) – the desired type of returned nested tensor. Default: if None, same as leftmost tensor in the list.\n  * (, optional) – the desired device of returned nested tensor. Default: if None, same as leftmost tensor in the list\n  * (, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout.\n\n    \nReturns a new (non-nested) Tensor by padding the nested tensor. The leading entries will be filled with the nested data, while the trailing entries will be padded.\nalways copies the underlying data, since the nested and the non-nested tensors differ in memory layout.     \n  * () – The size of the output tensor. If given, it must be large enough to contain all nested data; else, will infer by taking the max size of each nested sub-tensor along each dimension.\n\n\n```\n     \n\n\n\n\n\n\n\n   \n\n\n         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n\n\n\n      \ntensor([[[ 1.6862, -1.1282,  1.1031,  0.0464, -1.3276,  1.0000],\n\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]],\n\n         [ 0.2773,  0.8793, -0.5183, -0.6447,  1.0000,  1.0000],\n         [ 1.8009,  1.8468, -0.9832, -1.5272,  1.0000,  1.0000],\n         [ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000]]])\n      \nRuntimeError: Value in output_size is less than NestedTensor padded size. Truncation is not supported.\n\n```\n    \nConstructs a nested tensor given a strided tensor input and a strided mask, the resulting jagged layout nested tensor will have values retain values where the mask is equal to True. The dimensionality of the mask is preserved and is represented with the offsets, this is unlike where the output is collapsed to a 1D tensor.\nArgs: tensor (): a strided tensor from which the jagged layout nested tensor is constructed from. mask (): a strided mask tensor which is applied to the tensor input\n```\n   \n          \n   \n\n\n\n\n\n\n   \n  \n   \n\n\n\n\n\n\n```\n    \nConstructs a nested tensor (which might be a view) from , a strided tensor. This follows similar semantics to torch.Tensor.narrow, where in the -th dimension the new nested tensor shows only the elements in the interval . As nested representations allow for a different and at each ‘row’ of that dimension, and can also be tensors of shape .\nThere’s some differences depending on the layout you use for the nested tensor. If using strided layout, torch.narrow will do a copy of the narrowed data into a contiguous NT with strided layout, while jagged layout narrow() will create a non-contiguous view of your original strided tensor. This particular representation is really useful for representing kv-caches in Transformer models, as specialized SDPA kernels can deal with format easily, resulting in performance improvements.     \n  * () – a strided tensor, which will be used as the underlying data for the nested tensor if using the jagged layout or will be copied for the strided layout.\n  * ( is supported for the jagged layout, while strided supports all dim\n  * (Union[int, ]) – number of elements taken during the narrow op\n\n    \n(, optional) – the desired layout of returned nested tensor. Only strided and jagged layouts are supported. Default: if None, the strided layout.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/logging.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch has a configurable logging system, where different components can be given different log level settings. For instance, one component’s log messages can be completely disabled, while another component’s log messages can be set to maximum verbosity.\nThis feature is in beta and may have compatibility breaking changes in the future.\nThis feature has not been expanded to control the log messages of all components in PyTorch yet.\nThere are two ways to configure the logging system: through the environment variable or the python API torch._logging.set_logs.\nSets the log level for individual components and toggles individual log artifact types.  \n---  \nThe environment variable is a comma-separated list of pairs, where is a component specified below. The prefix will decrease the log level of the component, displaying more log messages while the prefix will increase the log level of the component and display fewer log messages. The default setting is the behavior when a component is not specified in . In addition to components, there are also artifacts. Artifacts are specific pieces of debug information associated with a component that are either displayed or not displayed, so prefixing an artifact with or will be a no-op. Since they are associated with a component, enabling that component will typically also enable that artifact, unless that artifact was specified to be . This option is specified in _registrations.py for artifacts that are so spammy they should only be displayed when explicitly enabled. The following components and artifacts are configurable through the environment variable (see torch._logging.set_logs for the python API):          \nSpecial component which configures the default log level of all components. Default:      \nThe log level for an arbitrary unregistered module. Provide the fully qualified name and the module will be enabled. Default:           \nWhether to emit the original and generated bytecode from TorchDynamo. Default:      \nWhether to emit the joint forward-backward graph generated by AOTAutograd. Default:      \nWhether to emit the graph captured by TorchDynamo in tabular format. Default:      \nWhether to emit the python source of the graph captured by TorchDynamo. Default:      \nWhether to emit a message when a unique graph break is encountered during TorchDynamo tracing. Default:      \nWhether to emit the guards generated by TorchDynamo for each compiled function. Default:      \nWhether to emit a guard failure reason and message every time TorchDynamo recompiles a function. Default:      \nwill set the log level of TorchDynamo to and AOT to \nwill set the log level of TorchDynamo to and TorchInductor to \nwill enable set the log level of TorchDynamo to and enable the artifact\nwill set the log level of some.random.module to and enable the artifact\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/linalg.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nComputes the sign and natural logarithm of the absolute value of the determinant of a square matrix.  \n---  \nComputes the condition number of a matrix with respect to a matrix norm.  \nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.  \n---  \nComputes the LU decomposition with partial pivoting of a matrix.  \nComputes a compact representation of the LU factorization with partial pivoting of a matrix.  \nComputes the eigenvalue decomposition of a square matrix if it exists.  \nComputes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.  \nComputes the eigenvalues of a complex Hermitian or real symmetric matrix.  \nComputes the solution of a square system of linear equations with a unique solution.  \n---  \nComputes the solution of a triangular system of linear equations with a unique solution.  \nComputes the solution of a square system of linear equations with a unique solution given an LU decomposition.  \nComputes a solution to the least squares problem of a system of linear equations.  \nComputes the inverse of a square matrix if it exists.  \n---  \nComputes the -th power of a square matrix for an integer .  \n---  \nComputes the dot product of two batches of vectors along a dimension.  \n---  \nEfficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.  \nComputes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.  \n---  \nComputes the inverse of a square matrix if it is invertible.  \nA version of that does not perform error checks unless .  \nThis is a version of that does not perform error checks unless .  \nComputes a compact representation of the LDL factorization of a Hermitian or symmetric (possibly indefinite) matrix.  \nThis is a version of that does not perform error checks unless .  \nComputes the solution of a system of linear equations using the LDL factorization.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/masked.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.\nMaskedTensor serves as an extension to that provides the user with the ability to:\n  * use any masked semantics (e.g. variable length tensors, nan* operators, etc.)\n\n\n“Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.\nA MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored.\nBy way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray) and take the max:\nOn top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they’d like during computation.\nThere are already a number of existing tutorials that we’ve written to help users onboard, such as:\n  * [Overview - the place to start for new users, discusses how to use MaskedTensors and why they’re useful](https://pytorch.org/tutorials/prototype/maskedtensor_overview)\n  * [Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors](https://pytorch.org/tutorials/prototype/maskedtensor_sparsity)\n  * [Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations](https://pytorch.org/tutorials/prototype/maskedtensor_adagrad)\n  * [Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy’s MaskedArray, and reduction semantics](https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics)\n\n\nUnary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we’ll continue to mask out the data.\nReturns a new tensor with the inverse hyperbolic cosine of the elements of .  \n---  \nReturns a new tensor with the arcsine of the elements of .  \nReturns a new tensor with the inverse hyperbolic sine of the elements of .  \nReturns a new tensor with the arctangent of the elements of .  \nReturns a new tensor with the inverse hyperbolic tangent of the elements of .  \nReturns a new tensor with the ceil of the elements of , the smallest integer greater than or equal to each element.  \nReturns a new tensor with the cosine of the elements of .  \nReturns a new tensor with the hyperbolic cosine of the elements of .  \nReturns a new tensor with each of the elements of converted from angles in degrees to radians.  \nReturns a new tensor with the exponential of the elements of the input tensor .  \nReturns a new tensor with the floor of the elements of , the largest integer less than or equal to each element.  \nComputes the natural logarithm of the absolute value of the gamma function on .  \nReturns a new tensor with the natural logarithm of the elements of .  \nReturns a new tensor with the logarithm to the base 10 of the elements of .  \nReturns a new tensor with the natural logarithm of (1 + ).  \nReturns a new tensor with the logarithm to the base 2 of the elements of .  \nReturns a new tensor with boolean elements representing if each element of is NaN or not.  \nReplaces , positive infinity, and negative infinity values in with the values specified by , , and , respectively.  \nReturns a new tensor with the negative of the elements of .  \nTakes the power of each element in with and returns a tensor with the result.  \nReturns a new tensor with each of the elements of converted from angles in radians to degrees.  \nReturns a new tensor with the reciprocal of the elements of   \nReturns a new tensor with the reciprocal of the square-root of each of the elements of .  \nReturns a new tensor with the signs of the elements of .  \nThis function is an extension of torch.sign() to complex tensors.  \nTests if each element of has its sign bit set or not.  \nReturns a new tensor with the sine of the elements of .  \nReturns a new tensor with the hyperbolic sine of the elements of .  \nReturns a new tensor with the square-root of the elements of .  \nReturns a new tensor with the square of the elements of .  \nReturns a new tensor with the tangent of the elements of .  \nReturns a new tensor with the hyperbolic tangent of the elements of .  \nReturns a new tensor with the truncated integer values of the elements of .  \nThe available inplace unary operators are all of the above :\nTests if each element of has its sign bit set or not.  \n---  \nReturns a new tensor with boolean elements representing if each element of is NaN or not.  \nAs you may have seen in the tutorial, also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics.\nDivides each element of the input by the corresponding element of .  \n---  \nLogarithm of the sum of exponentiations of the inputs in base-2.  \nThe available inplace binary operators are all of the above :\nLogarithm of the sum of exponentiations of the inputs in base-2.  \n---  \nThe following reductions are available (with autograd support). For more information, the tutorial details some examples of reductions, while the tutorial has some further in-depth discussions about how we decided on certain reduction semantics.\nReturns the minimum value of each slice of the tensor in the given dimension(s) .  \n---  \nReturns the maximum value of each slice of the tensor in the given dimension(s) .  \nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension  \nReturns the indices of the maximum value of all elements in the tensor.  \nReturns the matrix norm or vector norm of a given tensor.  \nWe’ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a . For a quick example, consider :\n```\n    \n\n\n\n\n             \n   \n \n\n \n\n \n\n  [      --,   5.0000,       --,       --]\n\n\n```\n\nReturns a 1-dimensional view of each input tensor with zero dimensions.  \n---  \nConcatenates the given sequence of tensors in in the given dimension.  \nAttempts to split a tensor into the specified number of chunks.  \nCreates a new tensor by horizontally stacking the tensors in .  \nSplits , a tensor with three or more dimensions, into multiple tensors depthwise according to .  \nSplits , a tensor with one or more dimensions, into multiple tensors horizontally according to .  \nCreates grids of coordinates specified by the 1D inputs in :tensors.  \nReturns a new tensor that is a narrowed version of tensor.  \nSlices the tensor along the selected dimension at the given index.  \nExpects to be <= 2-D tensor and transposes dimensions 0 and 1.  \nSplits , a tensor with two or more dimensions, into multiple tensors vertically according to .  \nReturns a new view of the tensor with singleton dimensions expanded to a larger size.  \nReturns a tensor with the same data and number of elements as but with the specified shape.  \nReturns a view of the original tensor which contains all slices of size from tensor in the dimension .  \nReturns a new tensor with the same data as the tensor but of a different .  \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.attention.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis module contains functions and classes that alter the behavior of torch.nn.functional.scaled_dot_product_attention\nContext manager to select which backend to use for scaled dot product attention.  \n---  \nAn enum-like class that contains the different backends for scaled dot product attention.  \nThis module implements the user facing API for flex_attention in PyTorch.  \n---  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/custom_operators.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nCreated On: Jun 18, 2024 | Last Updated: Jan 06, 2025 | Last Verified: Nov 05, 2024\nPyTorch offers a large library of operators that work on Tensors (e.g. , , etc). However, you may wish to bring a new custom operation to PyTorch and get it to work with subsystems like , autograd, and . In order to do so, you must register the custom operation with PyTorch via the Python or C++ APIs.\nYou may wish to author a custom operator from Python (as opposed to C++) if:\n  * you have a Python function you want PyTorch to treat as an opaque callable, especially with respect to and .\n  * you have some Python bindings to C++/CUDA kernels and want those to compose with PyTorch subsystems (like or )\n  * you are using Python (and not a C++-only environment like AOTInductor).\n\n\nYou may wish to author a custom operator from C++ (as opposed to Python) if:\n  * you plan to use this code with to do Python-less inference.\n\n\nFor information not covered in the tutorials and this page, please see \nIf your operation is expressible as a composition of built-in PyTorch operators then please write it as a Python function and call it instead of creating a custom operator. Use the operator registration APIs to create a custom operator if you are calling into some library that PyTorch doesn’t understand (e.g. custom C/C++ code, a custom CUDA kernel, or Python bindings to C/C++/CUDA extensions).\nIt is possible to use a C/C++/CUDA kernel by grabbing a Tensor’s data pointer and passing it to a pybind’ed kernel. However, this approach doesn’t compose with PyTorch subsystems like autograd, torch.compile, vmap, and more. In order for an operation to compose with PyTorch subsystems, it must be registered via the operator registration APIs.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/extending.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIn this note we’ll cover ways of extending , , , and writing custom C++ extensions.\nPyTorch offers a large library of operators that work on Tensors (e.g. , , etc). However, you may wish to bring a new custom operation to PyTorch and have it behave like PyTorch’s built-in operators. In order to do so, you must register the custom operation with PyTorch via the Python or C++ TORCH_LIBRARY APIs.\nAdding operations to requires implementing a new subclass for each operation. Recall that Functions are what uses to encode the operation history and compute gradients.\nThe first part of this doc is focused on backward mode AD as it is the most widely used feature. A section at the end discusses the extensions for forward mode AD.\nIn general, implement a custom function if you want to perform computations in your model that are not differentiable or rely on non-PyTorch libraries (e.g., NumPy), but still wish for your operation to chain with other ops and work with the autograd engine.\nIn some situations, custom functions can also be used to improve performance and memory usage: If you implemented your forward and backward passes using a , you can wrap them in to interface with the autograd engine. If you’d like to reduce the number of buffers saved for the backward pass, custom functions can be used to combine ops together.\nIf you can already write your function in terms of PyTorch’s built-in ops, its backward graph is (most likely) already able to be recorded by autograd. In this case, you do not need to implement the backward function yourself. Consider using a plain old Python function.\nIf you need to maintain state, i.e., trainable parameters, you should (also) use a custom module. See the section below for more information on extending .\nIf you’d like to alter the gradients during the backward pass or perform a side effect, consider registering a or hook.\nTake the following steps: 1. Subclass and implement the , (optional) and methods. 2. Call the proper methods on the argument. 3. Declare whether your function supports . 4. Validate whether your gradients are correct using gradcheck.\n  * is the code that performs the operation. It can take as many arguments as you want, with some of them being optional, if you specify the default values. All kinds of Python objects are accepted here. arguments that track history (i.e., with ) will be converted to ones that don’t track history before the call, and their use will be registered in the graph. Note that this logic won’t traverse lists/dicts/any other data structures and will only consider tensors that are direct arguments to the call. You can return either a single output, or a to find descriptions of useful methods that can be called only from .\n  * (optional). One can either write a “combined” that accepts a object or (as of PyTorch 2.0) a separate that does not accept and a method where the modification happens. The should have the compute and should only be responsible for the modification (and not have any compute). In general the separate and is closer to how PyTorch native operations work and therefore more composable with various PyTorch subsystems. See for more details.\n  * (or ) defines the gradient formula. It will be given as many arguments as there were outputs, with each of them representing gradient w.r.t. that output. It is important NEVER to modify these in-place. It should return as many tensors as there were inputs, with each of them containing the gradient w.r.t. its corresponding input. If your inputs didn’t require gradient ( is a tuple of booleans indicating whether each input needs gradient computation), or were non- objects, you can return . Also, if you have optional arguments to you can return more gradients than there were inputs, as long as they’re all \n\n\nIt is your responsibility to use the functions in properly in order to ensure that the new works properly with the autograd engine.\n  * must be used to save any tensors to be used in the backward pass. Non-tensors should be stored directly on . If tensors that are neither input nor output are saved for backward your may not support double backward (see step 3).\n  * must be used to mark any input that is modified inplace by the forward function.\n  * must be used to tell the engine if an output is not differentiable. By default all output tensors that are of differentiable type will be set to require gradient. Tensors of non-differentiable type (i.e., integral types) are never marked as requiring gradients.\n  * can be used to tell the autograd engine to optimize gradient computations in the cases where the output does not depend on the input by not materializing grad tensors given to backward function. That is, if set to False, None object in Python or “undefined tensor” (tensor x for which x.defined() is False) in C++ will not be converted to a tensor filled with zeros prior to calling backward, and so your code will need to handle such objects as if they were tensors filled with zeros. The default value of this setting is True.\n\n\nIf your does not support double backward you should explicitly declare this by decorating backward with the . With this decorator, attempts to perform double backward through your function will produce an error. See our double backward tutorial for more information on double backward.\nIt is recommended that you use to check whether your backward function correctly computes gradients of the forward by computing the Jacobian matrix using your backward function and comparing the value element-wise with the Jacobian computed numerically using finite-differencing.\n```\n\n \n\n    \n    \n       \n          \n            \n              \n         \n\n    \n    # inputs is a Tuple of all of the inputs passed to forward.\n    \n       \n            \n          \n\n    # This function has only a single output, so it gets only one gradient\n    \n      \n        # This is a pattern that is very convenient - at the top of backward\n        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n        # None. Thanks to the fact that additional trailing Nones are\n        # ignored, the return statement is simple even when the function has\n        \n            \n              \n\n        # These needs_input_grad checks are optional and there only to\n        # improve efficiency. If you want to make your code simpler, you can\n        # skip them. Returning gradients for inputs that don't require it is\n        \n         \n              \n         \n              \n              \n              \n\n           \n\n```\n\nNow, to make it easier to use these custom ops, we recommend either aliasing them or wrapping them in a function. Wrapping in a function lets us support default arguments and keyword arguments:\n```\n\n  \n\n# Option 2: wrap in a function, to support default args and keyword args.\n   \n       \n\n```\n\nHere, we give an additional example of a function that is parametrized by non-Tensor arguments:\n```\n \n    \n      \n           \n\n    \n       \n        # ctx is a context object that can be used to stash information\n        \n           \n          \n\n    \n      \n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n            \n\n```\n\nAnd here, we optimize the above example by calling set_materialize_grads(False):\n```\n \n    \n      \n           \n\n    \n       \n           \n        \n          \n\n    \n      \n        # Here we must handle None grad_output tensor. In this case we\n        \n           \n              \n\n        # We return as many input gradients as there were arguments.\n        # Gradients of non-Tensor arguments to forward must be None.\n            \n\n```\n\nIf you need any “intermediate” Tensors computed in to be saved, either they must be returned as outputs, or combine and (see ). Note that this means if you want gradients to flow through those intermediate values, you need to define the gradient formula for them (see also ):\n```\n \n    \n     \n        # We wish to save dx for backward. In order to do so, it must\n        \n              \n            \n          \n\n    \n       \n          \n           \n         \n\n    \n       \n           \n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`,\n        \n                  \n         \n\n# Wrap MyCube in a function so that it is clearer what the output is\n \n       \n     \n\n```\n\nInputs to , i.e., , can also be tensors that track history. So if is implemented with differentiable operations, (e.g., invocation of another custom ), higher order derivatives will work. In this case, the tensors saved with can also be used in the backward and have gradients flowing back but tensors saved in the won’t have gradients flowing back for them. If you need gradients to flow back for a Tensor saved in the , you should make it an output of the custom and save it with .\nYou probably want to check if the backward method you implemented actually computes the derivatives of your function. It is possible by comparing with numerical approximations using small finite differences:\n```\n   \n\n# gradcheck takes a tuple of tensors as input, check if your gradient\n# evaluated with these tensors are close enough to numerical\n# approximations and returns True if they all verify this condition.\n   \n     \n\n\n```\n\nSee for more details on finite-difference gradient comparisons. If your function is used in higher order derivatives (differentiating the backward pass) you can use the function from the same package to check higher order derivatives.\n\n\nWe recommend the second option (separate and ) because that is closer to how PyTorch native operations are implemented and it composes with transforms. However, we plan to support both approaches going forward; combining with : leads to more flexibility since you are able to save intermediates without returning them as output.\nPlease see the previous section for how to define with separate and .\nHere is an example of how to define a with combined and :\n```\n \n    \n    \n        \n        \n          \n          \n            \n              \n         \n\n    \n      \n            \n              \n\n         \n              \n         \n              \n              \n              \n\n           \n\n```\n\nOverriding the forward mode AD formula has a very similar API with some different subtleties. You can implement the function.\nIt will be given as many arguments as there were inputs, with each of them representing gradient w.r.t. that input. It should return as many tensors as there were outputs, with each of them containing the gradient w.r.t. its corresponding output. The will be called just after the method, before the returns.\n  * You can use the to pass any data from the to the function. If that state will not be needed for the , you can explicitly free it by doing at the end of the function.\n  * The implementation of must be backward differentiable or explicitly check that none of the given forward mode gradient has set.\n  * The function must match the view/inplace behavior of . For example, if the th input is modified inplace, then the th gradient must be updated inplace. Similarly, if the th output is a view of the th input. Then the returned th output gradient must be a view of the given th input gradient.\n  * Because the user cannot specify which gradient needs to be computed, the function should always compute gradients for all the outputs.\n  * The forward mode gradients do respect the flag set by and you can get input gradients when this is disabled.\n\n\nexports two kinds of interfaces - modules and their functional versions. You can extend it in both ways, but we recommend using modules for all kinds of layers, that hold any parameters or buffers, and recommend using a functional form parameter-less operations like activation functions, pooling, etc.\nAdding a functional version of an operation is already fully covered in the section above.\nSince heavily utilizes , adding a new requires implementing a that performs the operation and can compute the gradient. From now on let’s assume that we want to implement a module and we have the function implemented as in the listing above. There’s very little code required to add this. Now, there are two functions that need to be implemented:\n  * () - takes in arguments such as kernel sizes, numbers of features, etc. and initializes parameters and buffers.\n  * - instantiates a and uses it to perform the operation. It’s very similar to a functional wrapper shown above.\n\n\n```\n \n        \n        \n          \n          \n\n        # nn.Parameter is a special kind of Tensor, that will get\n        \n        # as an attribute. Parameters and buffers need to be registered, or\n        # they won't appear in .parameters() (doesn't apply to buffers), and\n        # won't be converted when e.g. .cuda() is called. You can use\n        \n        \n           \n         \n              \n        \n            # You should always register all possible parameters, but the\n            \n             \n\n        \n          \n            \n              \n\n      \n        # See the autograd section for explanation of what happens here.\n           \n\n     \n        # (Optional)Set the extra information about this module. You can test\n        \n         \n                 \n        \n\n```\n\nYou can create custom types that emulate by defining a custom class with methods that match . But what if you want to be able to pass these types to functions like in the top-level namespace that accept operands?\nIf your custom Python type defines a method named , PyTorch will invoke your implementation when an instance of your custom class is passed to a function in the namespace. This makes it possible to define custom implementations for any of the functions in the namespace which your implementation can call, allowing your users to make use of your custom type with existing PyTorch workflows that they have already written for . This works with “duck” types that are unrelated to as well as user-defined subclasses of .\nTo make this concrete, let’s begin with a simple example that illustrates the API dispatch mechanism. We’ll create a custom type that represents a 2D scalar tensor, parametrized by the order and value along the diagonal entries, :\nThis first iteration of the design isn’t very useful. The main functionality of is to provide a more compact string representation of a scalar tensor than in the base tensor class:\n```\n   \n\n\n\n\n\n\n\n\n\n```\n\nIf we try to use this object with the API, we will run into issues:\n```\n \n\nTypeError: mean(): argument 'input' (position 1) must be Tensor, not ScalarTensor\n\n```\n\nAdding a implementation to makes it possible for the above operation to succeed. Let’s re-do our implementation, this time adding a implementation:\nThe method takes four arguments: , a reference to the torch API function that is being overridden, , the list of types of Tensor-likes that implement , , the tuple of arguments passed to the function, and , the dict of keyword arguments passed to the function. It uses a global dispatch table named to store custom implementations. The keys of this dictionary are functions in the namespace and the values are implementations for .\nUsing a global dispatch table is not a mandated part of the API, it is just a useful design pattern for structuring your override implementations.\nThis class definition isn’t quite enough to make do the right thing when we pass it a – we also need to define an implementation for for operands and add the implementation to the dispatch table dictionary. One way of doing this is to define a decorator:\nwhich can be applied to the implementation of our override:\nOf course is an example of the simplest kind of function to override since it only takes one operand. We can use the same machinery to override a function that takes more than one operand, any one of which might be a tensor or tensor-like that defines , for example for :\nThis version has a fast path for when both operands are instances and also a slower path which degrades to converting the data to tensors when either operand is not a . That makes the override function correctly when either operand is a or a regular :\nFor speed and flexibility the dispatch mechanism does not check that the signature of an override function matches the signature of the function being overridden in the API. For some applications ignoring optional arguments would be fine but to ensure full compatibility with , user implementations of torch API functions should take care to exactly emulate the API of the function that is being overridden.\nFunctions in the API that do not have explicit overrides will return from . If all operands with defined on them return , PyTorch will raise a . This means that most of the time operations that do not have explicit overrides for a type will raise a when an instance of such a type is passed:\n```\n \n\n\n\n```\n\nIn practice this means that if you would like to implement your overrides using a implementation along these lines, you will need to explicitly implement the full API or the entire subset of the API that you care about for your use case. This may be a tall order as the full API is quite extensive.\nAnother option is to not return for operations that are not handled but to instead pass a to the original function when no override is available. For example, if we change our implementation of for to the one below:\nThen will work correctly, although the return type will always be a rather than a , even if both operands are instances:\nAlso see the example below for another variation on this pattern but instead always returns a to propagate metadata through operations in the API.\nThe protocol is designed for full coverage of the API, partial coverage may lead to undesirable results, in particular, certain functions raising a . This is especially true for subclasses, where all three of , and must be covered, even if they return exactly the same result. Failing to do this may also lead to infinite recursion. If one requires the implementation of a function from subclasses, they must use inside their implementation.\nAs of version 1.7.0, methods on and functions in public namespaces applied on subclasses will return subclass instances instead of instances:\nIf multiple subclasses exist, the lowest one in the hierarchy will be chosen by default. If there is no unique way to determine such a case, then a is raised:\n```\n \n\n \n\n \n\n  File , line , in \n: no implementation found for 'torch.add' on types that implement __torch_function__: [SubTensor, OtherSubTensor]\n\n```\n\nIf one wishes to have a global override for all tensor methods, one can use . Here is an example that logs all function/method calls:\n```\n \n    \n         \n        # NOTE: Logging calls Tensor.__repr__, so we can't log __repr__ without infinite recursion\n            \n            \n           \n              \n            \n\n```\n\nHowever, if one instead wishes to override a method on the Tensor subclass, there one can do so either by directly overriding the method (by defining it for a subclass), or by using and matching with .\nOne should be careful within for subclasses to always call instead of directly, as was the case before version 1.7.0. Failing to do this may cause to recurse back into and therefore cause infinite recursion.\nAnother useful case is a type that wraps a , either as an attribute or via subclassing. Below we implement a special case of this sort of type, a that attaches a dictionary of metadata to a that is propagated through operations. Since this is a generic sort of wrapping for the full API, we do not need to individually implement each override so we can make the implementation more permissive about what operations are allowed:\nThis simple implementation won’t necessarily work with every function in the API but it is good enough to capture most common operations:\n```\n   \n      \n     \n \n\n\n\n\n\n\n \n\n\n\n\n\n\n\n```\n\nIt is possible to use the torch API with multiple distinct types that each have a implementation, but special care must be taken. In such a case the rules are:\n  * The dispatch operation gathers all distinct implementations of for each operand and calls them in order: subclasses before superclasses, and otherwise left to right in the operator expression.\n  * If any value other than is returned, that value is returned as the result. Implementations can register that they do not implement an operation by returning .\n\n\nOne troublesome aspect of implementing is that if some operations do and others do not have overrides, users will at best see an inconsistent experience, or at worst will see errors raised at runtime when they use a function that does not have an override. To ease this process, PyTorch provides a developer-facing API for ensuring full support for overrides. This API is private and may be subject to changes without warning in the future.\nFirst, to get a listing of all overridable functions, use . This returns a dictionary whose keys are namespaces in the Python API and whose values are a list of functions in that namespace that can be overridden. For example, let’s print the names of the first 5 functions in that can be overridden:\nThis listing of functions makes it possible to iterate over all overridable functions, however in practice this is not enough to write tests for all of these functions without laboriously and manually copying the signature of each function for each test. To ease this process, the function returns a dictionary mapping overridable functions in the API to dummy lambda functions that have the same signature as the original function but unconditionally return -1. These functions are most useful to use with to analyze the function signature of the original function:\nFinally, returns a tuple of functions that explicitly cannot be overridden by . This list can be useful to confirm that a function that isn’t present in the dictionary returned by cannot be overridden.\nWhile allows one to effectively extend PyTorch’s pure Python components’ behavior, it does not allow one to extend the parts of PyTorch implemented in C++. To that end, a subclass can also define which will be able to override the behavior at the C++ level.\nTo effectively use this feature, it is important to know how the native part of PyTorch is implemented. The most important component there is what we call the “dispatcher” (the best description can be found in this , the dispatcher will inspect both arguments, figure out which “feature” (autograd, autocast, functionalization, etc) and which “backend” (CPU, CUDA, MPS, etc) should be used for this specific call and finally call all the right kernels. A very common thing done by a kernel is to “redispatch”. For example, when running your neural network on GPU with autocast, the first call will be the autocast kernel that will handle any potential autocast logic and redispatch down. The next feature in line will be autograd that will properly create the autograd graph and then redispatch down. Finally, we reach the backend kernel for CUDA which will launch the right CUDA kernel and return the final result. On the way out, autograd will attach the graph to the output and, finally, autocast will have a chance to do any update it needs on exit.\nOne configuration of the dispatcher is the order in which all these feature and backend keys are called. The latest list and their order can be found in inside the enum. For the purpose of extending torch, the important subset of the ordering for this discussion is:\nvmap -> Autocast -> Autograd -> ZeroTensor -> Neg/Conj -> Functionalize -> Python -> Backends\nThe most important key for the purpose of this discussion is as every Tensor subclass with the method defined will call into this feature. It is from there that the user-defined method is called and where the behavior can be overwritten arbitrarily. From there, calling the provided again will perform a “redispatch”.\n  * This code runs “below all features”. It is thus only responsible, like a regular backend, for generating the output value of each Tensor (and can, and should, ignore all advanced features like autograd, autocast, etc).\n  * If any high level feature implements a given function without redispatching, it will never reach the key and so the callback will never be triggered. This happens in particular for CompositeImplicitAutograd functions which are evaluated at the Autograd level without redispatching. This is because a CompositeImplicitAutograd function specifies its autograd formula by implicitly calling other native ops, so at the Autograd level, the function is decomposed into its native ops and those are evaluated instead.\n  * When calling back to Python and when wrapping the results, the same conversions are used as the regular PyTorch Python/C++ binding. In particular, some objects cannot be represented in Python and need special handling (undefined Tensors for example become None).\n  * Our native functions are lazily populated as as callable Python objects to enable easily interacting with them from Python. The object given to is always an entry from this namespace. This namespace can be used to directly call native ops and bypass the usual Python API and binding code.\n\n\nIn a similar way where is able to interpose on all of torch’s Python API and Tensor methods, is able to intercept all calls into the aten native API. Note that all methods on Tensors are converted into function calls before entering the dispatcher and thus will appear as function calls here: and will lead to exactly the same aten call. Most of these functions are defined in which specifies the properties of these functions as well as their backend implementation. Their implementation alongside specified features are then automatically registered via codegen. Some more exotic functions or features are also registered in other places in the C++ codebase or in user-defined C++ extensions.\nIt is also possible to add native functions using . This Python feature allows defining and/or adding new implementations to native functions. This can be used to add missing kernels, replace existing ones or define brand new native functions.\nWhen a user calls an operator with inputs that have , that call may be forwarded to the . args and kwargs get normalized before the call to , that is:\n  * the consist of keyword-only arguments in the operator’s schema. If a kwarg is equal to its default value (in the schema), it will not be passed.\n  * the consists of all other arguments, no matter how they were passed to the operator (positional vs keyword). If an arg is equal to its default value, and it is the right-most positional arg or all the args to the right of it are not passed, it will not be passed.\n\n\nUnfortunately, there are functions that do not take Tensor inputs. This means that the subclass approach described above cannot be used to override the behavior of all of PyTorch’s functions. Also, if the use case requires to intercept every function call, changing every Tensor to be a subclass can be overly intrusive.\nTo address this use case, we introduced the concept of “Mode”. These exist for and overrides, are created by subclassing respectively and , and are used as a context manager.\nTo simplify the description of how it interacts with subclasses and other modes, whenever the context manager for a mode is entered, every function behaves as if there was an extra Tensor argument at the beginning of the argument list with the mode as a subclass. This means in particular that all modes handlers will be called before any subclass handler and that modes corresponding to the inner context manager will always run first.\nIt is also important to note that within a given mode handler, this specific mode is disabled and can be re-enabled manually by doing .\nHere is an example that shows logging modes of each type:\n```\n \n    \n          \n           \n          \n          \n# Note that at the python level, we only see the call to backward but not what happens in the autograd engine.\n           \n\n \n# Here the requires_grad flag from autograd is removed while default arguments were populated.\n      \n          \n           \n          \n          \n# Here we don't see the call to backward itself, but its constituents. Starting here with the factory function that creates the initial gradient.\n       \n\n    \n             \n            \n            \n\n```\n\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/ddp.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe implementation of evolves over time. This design note is written based on the state as of v1.4.\n(DDP) transparently performs distributed data parallel training. This page describes how it works and reveals implementation details.\nLet us start with a simple example. This example uses a as the local model, wraps it with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.\n```\n \n   \n   \n   \n   \n \n     \n\n\n  \n    \n      \n    \n       \n    \n       \n    \n      \n       \n\n    \n       \n       \n    \n     \n    \n    \n\n \n      \n    \n        \n        \n        \n\n \n    \n    \n    \n      \n      \n    \n\n```\n\nDDP works with TorchDynamo. When used with TorchDynamo, apply the DDP model wrapper before compiling the model, such that torchdynamo can apply (graph-break optimizations) based on DDP bucket sizes. (See for more information.)\nThis section reveals how it works under the hood of by diving into details of every step in one iteration.\n  * : DDP relies on c10d for communications. Hence, applications must create instances before constructing DDP.\n  * : The DDP constructor takes a reference to the local module, and broadcasts from the process with rank 0 to all other processes in the group to make sure that all model replicas start from the exact same state. Then, each DDP process creates a local , which later will take care of the gradients synchronization during the backward pass. To improve communication efficiency, the organizes parameter gradients into buckets, and reduces one bucket at a time. Bucket size can be configured by setting the argument in DDP constructor. The mapping from parameter gradients to buckets is determined at the construction time, based on the bucket size limit and parameter sizes. Model parameters are allocated into buckets in (roughly) the reverse order of from the given model. The reason for using the reverse order is because DDP expects gradients to become ready during the backward pass in approximately that order. The figure below shows an example. Note that, the and are in , and the other two gradients are in . Of course, this assumption might not always be true, and when that happens it could hurt DDP backward speed as the cannot kick off the communication at the earliest possible time. Besides bucketing, the also registers autograd hooks during construction, one hook per parameter. These hooks will be triggered during the backward pass when the gradient becomes ready.\n  * : The DDP takes the input and passes it to the local model, and then analyzes the output from the local model if is set to . This mode allows running backward on a subgraph of the model, and DDP finds out which parameters are involved in the backward pass by traversing the autograd graph from the model output and marking all unused parameters as ready for reduction. During the backward pass, the would only wait for unready parameters, but it would still reduce all buckets. Marking a parameter gradient as ready does not help DDP skip buckets as for now, but it will prevent DDP from waiting for absent gradients forever during the backward pass. Note that traversing the autograd graph introduces extra overheads, so applications should only set to when necessary.\n  * : The function is directly invoked on the loss , which is out of DDP’s control, and DDP uses autograd hooks registered at construction time to trigger gradients synchronizations. When one gradient becomes ready, its corresponding DDP hook on that grad accumulator will fire, and DDP will then mark that parameter gradient as ready for reduction. When gradients in one bucket are all ready, the kicks off an asynchronous on that bucket to calculate mean of gradients across all processes. When all buckets are ready, the will block waiting for all operations to finish. When this is done, averaged gradients are written to the field of all parameters. So after the backward pass, the field on the same corresponding parameter across different DDP processes should be the same.\n  * : From the optimizer’s perspective, it is optimizing a local model. Model replicas on all DDP processes can keep in sync because they all start from the same state and they have the same averaged gradients in every iteration.\n\n\nDDP requires instances on all processes to invoke in exactly the same order, which is done by always running in the bucket index order instead of actual bucket ready order. Mismatched order across processes can lead to wrong results or DDP backward hang.\nBelow are pointers to the DDP implementation components. The stacked graph shows the structure of the code.\n  * library provides 3 implementations out of the box, namely, , , and . uses to send model states from the process with rank 0 to others during initialization and to sum gradients.\n\n\n  * function for the module which call into C++ libraries. Its function performs intra-process parameter synchronization when one DDP process works on multiple devices, and it also broadcasts model buffers from the process with rank 0 to all other processes. The inter-process parameter synchronization happens in .\n  *     * function will be invoked by the autograd engine when a gradient becomes ready.\n    * is called at the end of DDP forward pass in . It traverses the autograd graph to find unused parameters when is set to in DDP constructor.\n\n\nDDP’s performance advantage comes from overlapping allreduce collectives with computations during backwards. AotAutograd prevents this overlap when used with TorchDynamo for compiling a whole forward and whole backward graph, because allreduce ops are launched by autograd hooks _after_ the whole optimized backwards computation finishes.\nTorchDynamo’s DDPOptimizer helps by breaking the forward graph at the logical boundaries of DDP’s allreduce buckets during backwards. Note: the goal is to break the graph during backwards, and the simplest implementation is to break the forward graphs and then call AotAutograd and compilation on each section. This allows DDP’s allreduce hooks to fire in-between sections of backwards, and schedule communications to overlap with compute.\nSee for a more in-depth explanation and experimental results, or read the docs and code at \nTo Debug DDPOptimizer, set for full graph dumps. For logs without graphs, add any of ‘dynamo’, ‘distributed’, or ‘dist_ddp’ to (for basic info about bucket boundaries). To disable DDPOptimizer, set . DDP and TorchDynamo should still work correctly without DDPOptimizer, but with performance degradation.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/broadcasting.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIn short, if a PyTorch operation supports broadcast, then its Tensor arguments can be automatically expanded to be of equal sizes (without making copies of the data).\n  * When iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\n```\n\n\n# same shapes are always broadcastable (i.e. the above rules always hold)\n\n\n\n# x and y are not broadcastable, because x does not have at least 1 dimension\n\n\n\n  \n\n\n\n\n\n\n\n\n  \n# x and y are not broadcastable, because in the 3rd trailing dimension 2 != 3\n\n```\n\nIf two tensors , are “broadcastable”, the resulting tensor size is calculated as follows:\n  * If the number of dimensions of and are not equal, prepend 1 to the dimensions of the tensor with fewer dimensions to make them equal length.\n  * Then, for each dimension size, the resulting dimension size is the max of the sizes of and along that dimension.\n\n\n```\n# can line up trailing dimensions to make reading easier\n \n   \n \n   \n\n\n \n \n \n  \n\n \n \n \n                  \n\n```\n\nOne complication is that in-place operations do not allow the in-place tensor to change shape as a result of the broadcast.\n```\n\n\n\n\n\n\n\n\n\nRuntimeError: The expanded size of the tensor (1) must match the existing size (7) at non-singleton dimension 2.\n\n```\n\nPrior versions of PyTorch allowed certain pointwise functions to execute on tensors with different shapes, as long as the number of elements in each tensor was equal. The pointwise operation would then be carried out by viewing each tensor as 1-dimensional. PyTorch now supports broadcasting and the “1-dimensional” pointwise behavior is considered deprecated and will generate a Python warning in cases where tensors are not broadcastable, but have the same number of elements.\nNote that the introduction of broadcasting can cause backwards incompatible changes in the case where two tensors do not have the same shape, but are broadcastable and have the same number of elements. For Example:\nwould previously produce a Tensor with size: torch.Size([4,1]), but now produces a Tensor with size: torch.Size([4,4]). In order to help identify cases in your code where backwards incompatibilities introduced by broadcasting may exist, you may set to , which will generate a python warning in such cases.\n```\n\n \n__main__:1: UserWarning: self and other do not have the same shape, but are broadcastable, and have the same number of elements.\nChanging behavior in a backwards incompatible manner to broadcasting rather than viewing as 1-dimensional.\n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/amp_examples.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nInstances of enable autocasting for chosen regions. Autocasting automatically chooses the precision for operations to improve performance while maintaining accuracy.\nInstances of help perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with (by default on CUDA and XPU) gradients by minimizing gradient underflow, as explained .\nand are modular. In the samples below, each is used as its individual documentation suggests.\n(Samples here are illustrative. See the for a runnable walkthrough.)\n  *   * \n\n```\n\n  \n   \n\n# Creates a GradScaler once at the beginning of training.\n  \n\n   \n        \n        \n\n        \n          \n              \n               \n\n        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n        \n        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n        \n\n        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n        \n        \n\n        \n        \n\n```\n\nAll gradients produced by are scaled. If you wish to modify or inspect the parameters’ attributes between and , you should unscale them first. For example, gradient clipping manipulates a set of gradients such that their global norm (see ) or maximum magnitude (see ) is some user-imposed threshold. If you attempted to clip unscaling, the gradients’ norm/maximum magnitude would also be scaled, so your requested threshold (which was meant to be the threshold for gradients) would be invalid.\nunscales gradients held by ’s assigned parameters. If your model or models contain other parameters that were assigned to another optimizer (say ), you may call separately to unscale those parameters’ gradients as well.\nCalling before clipping enables you to clip unscaled gradients as usual:\n```\n  \n\n   \n        \n        \n          \n              \n               \n        \n\n        \n        \n\n        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:\n         \n\n        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n        # although it still skips optimizer.step() if the gradients contain infs or NaNs.\n        \n\n        \n        \n\n```\n\nrecords that was already called for this optimizer this iteration, so knows not to redundantly unscale gradients before (internally) calling .\nshould only be called once per optimizer per call, and only after all gradients for that optimizer’s assigned parameters have been accumulated. Calling twice for a given optimizer between each triggers a RuntimeError.\nGradient accumulation adds gradients over an effective batch of size ( if distributed). The scale should be calibrated for the effective batch, which means inf/NaN checking, step skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity. Also, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective batch are accumulated. If grads are unscaled (or the scale factor changes) before accumulation is complete, the next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor) after which it’s impossible to recover the accumulated unscaled grads must apply.\nTherefore, if you want to grads (e.g., to allow clipping unscaled grads), call just before , after all (scaled) grads for the upcoming have been accumulated. Also, only call at the end of iterations where you called for a full effective batch:\n```\n  \n\n   \n         \n          \n              \n               \n                \n\n        \n        \n\n               \n            # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n            \n            \n            \n\n```\n\nA gradient penalty implementation commonly creates gradients using , combines them to create the penalty value, and adds the penalty value to the loss.\nHere’s an ordinary example of an L2 penalty without gradient scaling or autocasting:\n```\n   \n        \n        \n          \n           \n\n        \n          \n                                          \n                                          \n\n        # Computes the penalty term and adds it to the loss\n          \n           \n              \n          \n            \n\n        \n\n        \n\n        \n\n```\n\nTo implement a gradient penalty gradient scaling, the Tensor(s) passed to should be scaled. The resulting gradients will therefore be scaled, and should be unscaled before being combined to create the penalty value.\nAlso, the penalty term computation is part of the forward pass, and therefore should be inside an context.\n```\n  \n\n   \n        \n        \n          \n              \n               \n\n        # Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\n          \n                                                 \n                                                 \n\n        # Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n        # not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\n          \n                \n\n        # Computes the penalty term and adds it to the loss\n          \n              \n               \n                  \n              \n                \n\n        \n        \n        \n\n        # may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n\n        \n        \n        \n\n```\n\nIf your network has multiple losses, you must call on each of them individually. If your network has multiple optimizers, you may call on any of them individually, and you must call on each of them individually.\nHowever, should only be called once, after all optimizers used this iteration have been stepped:\n```\n  \n\n   \n        \n        \n        \n          \n              \n              \n                     \n                     \n\n        # (retain_graph here is unrelated to amp, it's present because in this\n        # example, both backward() calls share some sections of graph.)\n        \n        \n\n        # You can choose which optimizers receive explicit unscaling, if you\n        # want to inspect or modify the gradients of the params they own.\n        \n\n        \n        \n\n        \n\n```\n\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision whether or not to skip the step. This may result in one optimizer skipping the step while the other one does not. Since step skipping occurs rarely (every several hundred iterations) this should not impede convergence. If you observe poor convergence after adding gradient scaling to a multiple-optimizer model, please report a bug.\nEven if spawns threads to run the forward pass on each device. The autocast state is propagated in each one and the following will work:\n```\n  \n  \n\n\n  \n    \n      \n    \n      \n\n```\n\n’s documentation recommends one GPU per process for best performance. In this case, does not spawn threads internally, so usages of and are not affected.\nHere may spawn a side thread to run the forward pass on each device, like . : apply autocast as part of your model’s method to ensure it’s enabled in side threads.\nIf your network uses (subclasses of ), changes are required for autocast compatibility if any function\n  * requires a particular (for example, if it wraps that were only compiled for ).\n\n\nIn all cases, if you’re importing the function and can’t alter its definition, a safe fallback is to disable autocast and force execution in ( or ) at any points of use where errors occur:\nIf you’re the function’s author (or can alter its definition) a better solution is to use the and decorators as shown in the relevant case below.\nApply and (with no arguments) to and respectively. These ensure executes with the current autocast state and executes with the same autocast state as (which can prevent type mismatch errors):\nNow can be invoked anywhere, without disabling autocast or manually casting inputs:\nConsider a custom function that requires inputs. Apply to and to . If runs in an autocast-enabled region, the decorators cast floating-point Tensor inputs to on designated device assigned by the argument , in this example, and locally disable autocast during and :\nNow can be invoked anywhere, without manually disabling autocast or casting inputs:\n```\n  \n\n  \n    # func will run in float32, regardless of the surrounding autocast state\n      \n\n```\n\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n\n\nA kind of Tensor that should not be considered a model parameter.  \n---  \nA kind of Tensor that is to be considered a module parameter.  \nApplies a 1D convolution over an input signal composed of several input planes.  \n---  \nApplies a 2D convolution over an input signal composed of several input planes.  \nApplies a 3D convolution over an input signal composed of several input planes.  \nApplies a 1D transposed convolution operator over an input image composed of several input planes.  \nApplies a 2D transposed convolution operator over an input image composed of several input planes.  \nApplies a 3D transposed convolution operator over an input image composed of several input planes.  \nCombines an array of sliding local blocks into a large containing tensor.  \nApplies a 1D max pooling over an input signal composed of several input planes.  \n---  \nApplies a 2D max pooling over an input signal composed of several input planes.  \nApplies a 3D max pooling over an input signal composed of several input planes.  \nApplies a 1D average pooling over an input signal composed of several input planes.  \nApplies a 2D average pooling over an input signal composed of several input planes.  \nApplies a 3D average pooling over an input signal composed of several input planes.  \nApplies a 2D fractional max pooling over an input signal composed of several input planes.  \nApplies a 3D fractional max pooling over an input signal composed of several input planes.  \nApplies a 1D power-average pooling over an input signal composed of several input planes.  \nApplies a 2D power-average pooling over an input signal composed of several input planes.  \nApplies a 3D power-average pooling over an input signal composed of several input planes.  \nApplies a 1D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 2D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 3D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 1D adaptive average pooling over an input signal composed of several input planes.  \nApplies a 2D adaptive average pooling over an input signal composed of several input planes.  \nApplies a 3D adaptive average pooling over an input signal composed of several input planes.  \nPads the input tensor using the reflection of the input boundary.  \n---  \nPads the input tensor using the reflection of the input boundary.  \nPads the input tensor using the reflection of the input boundary.  \nPads the input tensor using replication of the input boundary.  \nPads the input tensor using replication of the input boundary.  \nPads the input tensor using replication of the input boundary.  \nPads the input tensor using circular padding of the input boundary.  \nPads the input tensor using circular padding of the input boundary.  \nPads the input tensor using circular padding of the input boundary.  \nAllows the model to jointly attend to information from different representation subspaces.  \n---  \nApplies Root Mean Square Layer Normalization over a mini-batch of inputs.  \n---  \nApply a multi-layer Elman RNN with or non-linearity to an input sequence.  \n---  \nApply a multi-layer long short-term memory (LSTM) RNN to an input sequence.  \nApply a multi-layer gated recurrent unit (GRU) RNN to an input sequence.  \nTransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.  \n---  \nApplies an affine linear transformation to the incoming data: .  \n---  \nApplies a bilinear transformation to the incoming data: .  \nDuring training, randomly zeroes some of the elements of the input tensor with probability .  \n---  \nA simple lookup table that stores embeddings of a fixed dictionary and size.  \n---  \nCompute sums or means of 'bags' of embeddings, without instantiating the intermediate embeddings.  \nComputes the pairwise distance between input vectors, or between columns of input matrices.  \n---  \nCreates a criterion that measures the mean absolute error (MAE) between each element in the input and target .  \n---  \nCreates a criterion that measures the mean squared error (squared L2 norm) between each element in the input and target .  \nThis criterion computes the cross entropy loss between input logits and target.  \nCreates a criterion that measures the Binary Cross Entropy between the target and the input probabilities:  \nCreates a criterion that measures the loss given inputs , , two 1D mini-batch or 0D , and a label 1D mini-batch or 0D (containing 1 or -1).  \nMeasures the loss given an input tensor and a labels tensor (containing 1 or -1).  \nCreates a criterion that optimizes a multi-class multi-classification hinge loss (margin-based loss) between input (a 2D mini-batch ) and output (which is a 2D of target class indices).  \nCreates a criterion that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.  \nCreates a criterion that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.  \nCreates a criterion that optimizes a two-class classification logistic loss between input tensor and target tensor (containing 1 or -1).  \nCreates a criterion that optimizes a multi-label one-versus-all loss based on max-entropy, between input and target of size .  \nCreates a criterion that measures the loss given input tensors , and a label with values 1 or -1.  \nCreates a criterion that optimizes a multi-class classification hinge loss (margin-based loss) between input (a 2D mini-batch ) and output (which is a 1D tensor of target class indices, ):  \nCreates a criterion that measures the triplet loss given an input tensors , , and a margin with a value greater than .  \nCreates a criterion that measures the triplet loss given input tensors , , and (representing anchor, positive, and negative examples, respectively), and a nonnegative, real-valued function (\"distance function\") used to compute the relationship between the anchor and positive example (\"positive distance\") and the anchor and negative example (\"negative distance\").  \nRearrange elements in a tensor according to an upscaling factor.  \n---  \nUpsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.  \nApplies a 2D nearest neighbor upsampling to an input signal composed of several input channels.  \nApplies a 2D bilinear upsampling to an input signal composed of several input channels.  \nClip the gradients of an iterable of parameters at specified value.  \n---  \nScale the gradients of an iterable of parameters given a pre-calculated total norm and desired max norm.  \nUtility functions to flatten and unflatten Module parameters to and from a single vector.\nCopy slices of a vector into an iterable of parameters.  \n---  \nFuse a convolutional module and a BatchNorm module into a single, new convolutional module.  \n---  \nFuse convolutional module parameters and BatchNorm module parameters into new convolutional module parameters.  \nFuse a linear module and a BatchNorm module into a single, new linear module.  \nFuse linear module parameters and BatchNorm module parameters into new linear module parameters.  \nUtility functions to apply and remove weight normalization from Module parameters.\nApply weight normalization to a parameter in the given module.  \n---  \nApply spectral normalization to a parameter in the given module.  \nGiven a module class object and args / kwargs, instantiate the module without initializing parameters / buffers.  \n---  \nContainer holding a sequence of pruning methods for iterative pruning.  \n---  \nUtility pruning method that does not prune any units but generates the pruning parametrization with a mask of ones.  \nPrune (currently unpruned) units in a tensor by zeroing out the ones with the lowest L1-norm.  \nPrune entire (currently unpruned) channels in a tensor at random.  \nPrune entire (currently unpruned) channels in a tensor based on their L-norm.  \nPrune tensor by removing random channels along the specified dimension.  \nPrune tensor by removing channels with the lowest L-norm along the specified dimension.  \nGlobally prunes tensors corresponding to all parameters in by applying the specified .  \nPrune tensor corresponding to parameter called in by applying the pre-computed mask in .  \nRemove the pruning reparameterization from a module and the pruning method from the forward hook.  \nCheck if a module is pruned by looking for pruning pre-hooks.  \nApply an orthogonal or unitary parametrization to a matrix or a batch of matrices.  \n---  \nApply weight normalization to a parameter in the given module.  \nApply spectral normalization to a parameter in the given module.  \nUtility functions to parametrize Tensors on existing Modules. Note that these functions can be used to parametrize a given Parameter or Buffer given a specific function that maps from an input space to the parametrized space. They are not parameterizations that would transform an object into a parameter. See the for more information on how to implement your own parametrizations.\nContext manager that enables the caching system within parametrizations registered with .  \n---  \nA sequential container that holds and manages the original parameters or buffers of a parametrized .  \n---  \nUtility functions to call a given Module in a stateless manner.\nPerform a functional call on the module by replacing the module parameters and buffers with the provided ones.  \n---  \nUnpad padded Tensor into a list of variable length Tensors.  \n---  \nUnflattens a tensor dim expanding it to a desired shape.  \n---  \nQuantization refers to techniques for performing computations and storing tensors at lower bitwidths than floating point precision. PyTorch supports both per tensor and per channel asymmetric linear quantization. To learn more how to use quantized functions in PyTorch, please refer to the documentation.\nA mixin for modules that lazily initialize parameters, also known as \"lazy modules\".  \n---  \nApplies Root Mean Square Layer Normalization over a mini-batch of inputs.  \n---  \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.init.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nAll the functions in this module are intended to be used to initialize neural network parameters, so they all run in mode and will not be taken into account by autograd.     \nReturn the recommended gain value for the given nonlinearity function.\nIn order to implement instead of . This gives the initial weights a variance of , which is necessary to induce a stable fixed point in the forward pass. In contrast, the default gain for sacrifices the normalization effect for more stable gradient flow in rectangular layers.     \n\n    \nFill the input Tensor with values drawn from the uniform distribution.     \n  * () – the torch Generator to sample from (default: None)\n\n    \nFill the input Tensor with values drawn from the normal distribution.     \n  * () – the torch Generator to sample from (default: None)\n\n                   \nPreserves the identity of the inputs in layers, where as many inputs are preserved as possible.     \nFill the {3, 4, 5}-dimensional input with the Dirac delta function.\nPreserves the identity of the inputs in layers, where as many input channels are preserved as possible. In case of groups>1, each group of channels preserves identity     \n  * () – number of groups in the conv layer (default: 1)\n\n    \nThe method is described in - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have values sampled from where     \n  * () – the torch Generator to sample from (default: None)\n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nThe method is described in - Glorot, X. & Bengio, Y. (2010). The resulting tensor will have values sampled from where     \n  * () – the torch Generator to sample from (default: None)\n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nThe method is described in - He, K. et al. (2015). The resulting tensor will have values sampled from where     \n  * ( (default) or . Choosing preserves the magnitude of the variance of the weights in the forward pass. Choosing preserves the magnitudes in the backwards pass.\n  * () – the torch Generator to sample from (default: None)\n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nThe method is described in - He, K. et al. (2015). The resulting tensor will have values sampled from where     \n  * ( (default) or . Choosing preserves the magnitude of the variance of the weights in the forward pass. Choosing preserves the magnitudes in the backwards pass.\n  * () – the torch Generator to sample from (default: None)\n\n\nBe aware that and are calculated assuming that the weight matrix is used in a transposed manner, (i.e., in layers, where ). This is important for correct initialization. If you plan to use , where , pass in a transposed weight matrix, i.e. .     \nFill the input Tensor with values drawn from a truncated normal distribution.\nThe values are effectively drawn from the normal distribution with values outside redrawn until they are within the bounds. The method used for generating the random values works best when .     \n  * () – the torch Generator to sample from (default: None)\n\n    \nDescribed in Exact solutions to the nonlinear dynamics of learning in deep linear neural networks - Saxe, A. et al. (2013). The input tensor must have at least 2 dimensions, and for tensors with more than 2 dimensions the trailing dimensions are flattened.     \n  * () – the torch Generator to sample from (default: None)\n\n    \nThe non-zero elements will be drawn from the normal distribution , as described in - Martens, J. (2010).     \n  * – The fraction of elements in each column to be set to zero\n  * – the standard deviation of the normal distribution used to generate the non-zero values\n  * () – the torch Generator to sample from (default: None)\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/extending.func.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n  * you wish to call code that does not contain PyTorch operations and have it work with function transforms. That is, the ’s forward/backward/etc calls into functions from other systems like C++, CUDA, numpy.\n\n\nThis guide assumes you are familiar with , which explains how to use .\ncan either have a that accepts a ctx object, or it can have separate (that does not accept ) and a staticmethod that modifies the object.\n  * is the code that performs the operation and it should not accept a object.\n  * is the code where you can call methods on . Here is where you should save Tensors for backward (by calling ), or save non-Tensors (by assigning them to the object).\n\n\nBecause accepts only and , the only quantities that can be saved are either objects (such as Tensors) in the inputs or outputs or quantities (like ) derived from them. If you wish to save a non-input intermediate activation from for backward, then you’ll need to return it as an output from so that it gets passed to .\n  * to support compositions of transforms (like , , ) – you may need multiple of the above.\n\n\nIn order for the to be arbitrarily composable with function transforms, we recommend that all other staticmethods other than and must be transformable: that is, they must consist of only PyTorch operators or call other (that may call into C++/CUDA/etc).\nA common case is a with both forward() and backward() calling into another system (like C++, CUDA, numpy, triton).\n```\n \n   \n\n \n     \n\n \n    \n    \n      \n          \n          \n           \n           \n            \n        # Any intermediates to be saved in backward must be returned as\n        \n         \n            \n             \n            \n             \n            \n             \n        \n\n    # setup_context is responsible for calling methods and/or assigning to\n    # the ctx object. Please do not do additional compute (e.g. add\n    \n    \n       \n           \n        # Note that output is whatever you returned from forward.\n        # If you returned multiple values, then output is a Tuple of multiple values.\n        # If you returned a single Tensor, then output is a Tensor.\n        # If you returned a Tuple with a single Tensor, then output is a\n        \n            \n         \n        # Tensors must be saved via ctx.save_for_backward. Please do not\n        \n         \n        # Non-tensors may be saved by assigning them as attributes on the ctx object.\n          \n\n    \n        \n        # For the autograd.Function to be arbitrarily composable with function\n        \n        # must be implemented in a \"transformable\" way; that is, they must\n        \n        \n        # For example, this allows us to do double backwards and/or compute\n        \n        \n        # We've written the backward pass of NumpySort in terms of another\n        \n           \n             \n\n \n    \n        \n          \n          \n          \n            \n\n    \n       \n             \n         \n          \n\n    \n      \n           \n             \n            \n\n```\n\nNow, to make it easier to use (to hide away the intermediates we returned as outputs, as well as allow default args and kwargs), we create a new function that invokes it:\nAnother common case is an that is implemented with PyTorch operations. PyTorch is able to compute gradients for PyTorch operations automatically, but perhaps we wish to customize how the gradients are computed. Some reasons why we may want a custom backward different from the one PyTorch gives us are:\n\n\nHere’s an example of an for the function where we change the performance characteristics (some computation that would normally happen during the backward pass, computing dx, happens in the forward pass).\n```\n \n    \n     \n            \n        # In regular PyTorch, if we had just run y = x ** 3, then the backward\n        # pass computes dx = 3 * x ** 2. In this autograd.Function, we've done\n        \n              \n          \n\n    \n       \n          \n           \n         \n\n    \n       \n           \n        # In order for the autograd.Function to work with higher-order\n        # gradients, we must add the gradient contribution of `dx`.\n                  \n         \n\n```\n\nNow, to make it easier to use (and hide away the intermediates we returned as outputs) we create a new function that invokes it:\nPlease read these limitations of with torch.func transforms carefully. We are not able to catch many of these situations and error out gracefully so they will lead to undefined behavior.\nPlease do not capture Tensors that are being transformed over, have requires_grad=True, or are dual tensors, into the methods of the . The way to be completely safe is to ensure that the only Tensors being used inside any method of the must be directly passed as inputs (or via the ctx object) rather than come from outside the .\ndoes not handle Tensors in pytrees (arbitrary nested Python data structures that may or may not contain Tensors). For those Tensors to be tracked by autograd, they must be passed directly as an argument to . This is in contrast to jax.{custom_vjp, custom_jvp}, which do accept pytrees.\nPlease only use or to save Tensors. Please do not assign Tensors or collections of Tensors directly onto the ctx object - these Tensors will not get tracked\n\n\nIf your fulfills the following additional constraints, then we are able to generate a vmap rule for it. If it doesn’t fulfill the constraints or if you want custom behavior under vmap, please manually define a vmap staticmethod (see next section).\nWe are not easily able to check for the following constraints and error out gracefully. Violation of the constraints may lead to undefined behavior.\n  * The ’s , (if it exists) and (if it exists) staticmethods must be transformable via . That is, they must consist of only PyTorch operations (as opposed to e.g. NumPy or custom CUDA kernels).\n\n\n```\n \n    # Set generate_vmap_rule to True to ask PyTorch to automatically generate\n    \n      \n\n    \n     \n            \n              \n          \n\n    \n       \n          \n           \n         \n\n    \n       \n           \n                  \n         \n\n \n       \n     \n\n  \n  \n    \n\n```\n\nIf your calls into another system (like NumPy, C++, CUDA, triton), then to get it to work with or transforms that use it, you’ll need to manually define a staticmethod.\nDepending on what transforms you want to use and your use case, you may not need to add a staticmethod to all of your :\n  * For example, performs over the backward pass. So if you’re only interested in using , only the staticmethod needs to be vmappable.\n\n\nWe do recommend ensuring all of your have support for though, especially if you are writing a third-party library and you want your to work with all combinations of transforms.\nConceptually, the vmap staticmethod is responsible for defining how the should behave under . That is, it defines how to transform the to run over inputs with an additional dimension (the dimension being vmapped over). This is similar to how is implemented over PyTorch operations: for each operation, we define a vmap rule (sometimes also referred to as a “batching rule”).\n  * the signature is , where is the same as the args to .\n  * The vmap staticmethod is responsible for defining how the should behave under . That is, given inputs with an additional dimension (specified by ), how do we compute the batched version of ?\n  * For each arg in , has a corresponding . It is if the arg is not a Tensor or if the arg is not being vmapped over, otherwise, it is an integer specifying what dimension of the Tensor is being vmapped over.\n  * is a collection of additional metadata that may be helpful: specifies the size of the dimension being vmapped over, while is the option that was passed to .\n  * The return of the vmap staticmethod is a tuple of . Similar to , should be of the same structure as and contain one per output that specifies if the output has the vmapped dimension and what index it is in.\n\n\n```\n \n     \n\n \n    \n      \n          \n          \n           \n           \n            \n         \n             \n             \n             \n        \n\n    \n       \n           \n            \n         \n         \n          \n\n    \n        \n           \n             \n\n    \n    \n    # where *args is the same as the arguments to `forward`.\n    \n        \n        # For every input (x and dim), in_dims stores an Optional[int]\n        \n        # - None if the input is not being vmapped over or if the input\n        \n        # - an integer if the input is being vmapped over that represents\n        #   the index of the dimension being vmapped over.\n           \n\n        # A \"vmap rule\" is the logic of how to perform the operation given\n        # inputs with one additional dimension. In NumpySort, x has an\n        \n        # to call NumpySort again but pass it a different `dim`.\n           \n        \n                    \n             \n\n        # The vmap rule must return a tuple of two things\n        # 1. the output. Should be the same amount of things\n        \n        # 2. one Optional[int] for each output specifying if each output\n        # is being vmapped over, and if so, the index of the\n        \n        \n        # NumpySort.forward returns a Tuple of 3 Tensors. Since we moved the\n        # dimension being vmapped over to the front of `x`, that appears at\n        \n        # The return is (output, out_dims) -- output is a tuple of 3 Tensors\n        \n               \n\n \n    \n        \n          \n          \n          \n            \n\n    \n       \n             \n         \n          \n\n    \n      \n           \n             \n            \n\n    \n          \n             \n\n        # The strategy is: expand {x, ind, ind_inv} to all have the dimension\n        \n        \n\n        # Handle negative dims by wrapping them to be positive\n                  \n                  \n\n          \n               \n                  \n              \n\n        # If the Tensor doesn't have the dimension being vmapped over,\n        # expand it out. Otherwise, move it to the front of the Tensor\n           \n           \n           \n\n        # The return is a tuple (output, out_dims). Since output is a Tensor,\n        # then out_dims is an Optional[int] (instead of being a Tuple).\n               \n\n  \n         \n     \n\n   \n  \n   \n\n```\n\nThe vmap staticmethod should aim to preserve the semantics of the entire . That is, (pseudocode) should be replaceable with a .\nIf your autograd.Function has any custom behavior in the backward pass, please keep this in mind.\nIt is a legitimate use case to write a custom vmap staticmethod for a that PyTorch is able to generate a vmap rule for via . You may wish to do this if the generated vmap rule doesn’t have the semantics you’re looking for.\nTo support forward-mode AD, a must have a staticmethod. Please see for details.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/nn.functional.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nApplies a 1D convolution over an input signal composed of several input planes.  \n---  \nApplies a 2D convolution over an input image composed of several input planes.  \nApplies a 3D convolution over an input image composed of several input planes.  \nApplies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \"deconvolution\".  \nApplies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\".  \nApplies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \"deconvolution\"  \nCombine an array of sliding local blocks into a large containing tensor.  \nApplies a 1D average pooling over an input signal composed of several input planes.  \n---  \nApplies 2D average-pooling operation in regions by step size steps.  \nApplies 3D average-pooling operation in regions by step size steps.  \nApplies a 1D max pooling over an input signal composed of several input planes.  \nApplies a 2D max pooling over an input signal composed of several input planes.  \nApplies a 3D max pooling over an input signal composed of several input planes.  \nApply a 1D power-average pooling over an input signal composed of several input planes.  \nApply a 2D power-average pooling over an input signal composed of several input planes.  \nApply a 3D power-average pooling over an input signal composed of several input planes.  \nApplies a 1D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 2D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 3D adaptive max pooling over an input signal composed of several input planes.  \nApplies a 1D adaptive average pooling over an input signal composed of several input planes.  \nApply a 2D adaptive average pooling over an input signal composed of several input planes.  \nApply a 3D adaptive average pooling over an input signal composed of several input planes.  \nApplies 2D fractional max pooling over an input signal composed of several input planes.  \nApplies 3D fractional max pooling over an input signal composed of several input planes.  \nThe module contains attention_biases that are designed to be used with scaled_dot_product_attention.\nApply a threshold to each element of the input Tensor.  \n---  \nApplies element-wise, \\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))), with and .  \nApplies element-wise, \\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1)).  \nApplies element-wise,   \nApplies element-wise the function where weight is a learnable parameter.  \nWhen the approximate argument is 'none', it applies element-wise the function   \nApplies element-wise, the function .  \nApplies element-wise,   \nApply Batch Normalization for each channel across a batch of data.  \nApply Instance Normalization independently for each channel in every data sample within a batch.  \nApplies a linear transformation to the incoming data: .  \n---  \nApplies a bilinear transformation to the incoming data:   \nDuring training, randomly zeroes some elements of the input tensor with probability .  \n---  \nRandomly masks out entire channels (a channel is a feature map).  \nRandomly zero out entire channels (a channel is a 1D feature map).  \nRandomly zero out entire channels (a channel is a 2D feature map).  \nRandomly zero out entire channels (a channel is a 3D feature map).  \nGenerate a simple lookup table that looks up embeddings in a fixed dictionary and size.  \n---  \nTakes LongTensor with index values of shape and returns a tensor of shape that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.  \nComputes the p-norm distance between every pair of row vectors in the input.  \n---  \nMeasure Binary Cross Entropy between the target and input probabilities.  \n---  \nCompute the cross entropy loss between input logits and target.  \nCompute the triplet loss between given input tensors and a margin greater than 0.  \nCompute the triplet margin loss for input tensors using a custom distance function.  \nRearranges elements in a tensor of shape to a tensor of shape , where r is the .  \n---  \nReverses the operation by rearranging elements in a tensor of shape to a tensor of shape , where r is the .  \nGenerate 2D or 3D flow field (sampling grid), given a batch of affine matrices .  \nEvaluate module(input) in parallel across the GPUs given in device_ids.  \n---  \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/faq.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nAs the error message suggests, you have run out of memory on your GPU. Since we often deal with large amounts of data in PyTorch, small mistakes can rapidly cause your program to use up all of your GPU; fortunately, the fixes in these cases are often simple. Here are a few common things to check:\nBy default, computations involving variables that require gradients will keep history. This means that you should avoid using such variables in computations which will live beyond your training loops, e.g., when tracking statistics. Instead, you should detach the variable or access its underlying data.\nSometimes, it can be non-obvious when differentiable variables can occur. Consider the following training loop (abridged from ):\nHere, is accumulating history across your training loop, since is a differentiable variable with autograd history. You can fix this by writing instead.\nIf you assign a Tensor or Variable to a local, Python will not deallocate until the local goes out of scope. You can free this reference by using . Similarly, if you assign a Tensor or Variable to a member variable of an object, it will not deallocate until the object goes out of scope. You will get the best memory usage if you don’t hold onto temporaries you don’t need.\nThe scopes of locals can be larger than you expect. For example:\nHere, remains live even while is executing, because its scope extrudes past the end of the loop. To free it earlier, you should when you are done with it.\nThe amount of memory required to backpropagate through an RNN scales linearly with the length of the RNN input; thus, you will run out of memory if you try to feed an RNN a sequence that is too long.\nThe technical term for this phenomenon is function as described in .\nA linear layer uses memory: that is to say, the memory requirements of the weights scales quadratically with the number of features. It is very easy to \nPyTorch uses a caching memory allocator to speed up memory allocations. As a result, the values shown in usually don’t reflect the true memory usage. See for more details about GPU memory management.\nIf your GPU memory isn’t freed even after Python quits, it is very likely that some Python subprocesses are still alive. You may find them via and manually kill them with .\nYou may have some code that tries to recover from out of memory errors.\nBut find that when you do run out of memory, your recovery code can’t allocate either. That’s because the python exception object holds a reference to the stack frame where the error was raised. Which prevents the original tensor objects from being freed. The solution is to move you OOM recovery code outside of the clause.\nYou are likely using other libraries to generate random numbers in the dataset and worker subprocesses are started via . See ’s documentation for how to properly set up random seeds in workers with its option.\nThere is a subtlety in using the pattern in a with or . Input to each the on each device will only be part of the entire input. Because the unpack operation by default only pads up to the longest input it sees, i.e., the longest on that particular device, size mismatches will happen when results are gathered together. Therefore, you can instead take advantage of the argument of to make sure that the calls return sequences of same length. For example, you can write:\n```\n    \n\n \n    \n\n    # padded_input is of shape [B x T x *] (batch_first mode) and contains\n    \n    \n    \n       \n            \n           \n                                            \n           \n            \n                                        \n         \n\n\n  \n  \n\n```\n\nAdditionally, extra care needs to be taken when batch dimension is dim (i.e., ) with data parallelism. In this case, the first argument of pack_padded_sequence will be of shape and should be scattered along dim , but the second argument will be of shape and should be scattered along dim . Extra code to manipulate the tensor shapes will be needed.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/mps.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\ndevice enables high-performance training on GPU for MacOS devices with Metal programming framework. It introduces a new device to map Machine Learning computational graphs and primitives on highly efficient Metal Performance Shaders Graph framework and tuned kernels provided by Metal Performance Shaders framework respectively.\nThe new MPS backend extends the PyTorch ecosystem and provides existing scripts capabilities to setup and run operations on GPU.\nTo get started, simply move your Tensor and Module to the device:\n```\n\n  \n      \n        \"MPS not available because the current PyTorch install was not \"\n              \n    \n        \"MPS not available because the current MacOS version is not 12.3+ \"\n              \"and/or you do not have an MPS-enabled device on this machine.\"\n\n\n      \n\n    \n       \n    \n       \n\n    \n        \n\n    # Move your model to mps just like any other device\n      \n    \n\n    \n      \n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIn modern computers, floating point numbers are represented using IEEE 754 standard. For more details on floating point arithmetic and IEEE 754 standard, please see \nMany operations in PyTorch support batched computation, where the same operation is performed for the elements of the batches of inputs. An example of this is and . It is possible to implement batched computation as a loop over batch elements, and apply the necessary math operations to the individual batch elements, for efficiency reasons we are not doing that, and typically perform computation for the whole batch. The mathematical libraries that we are calling, and PyTorch internal implementations of operations can produces slightly different results in this case, compared to non-batched computations. In particular, let and be 3D tensors with the dimensions suitable for batched matrix multiplication. Then (the first element of the batched result) is not guaranteed to be bitwise identical to (the matrix product of the first elements of the input batches) even though mathematically it’s an identical computation.\nSimilarly, an operation applied to a tensor slice is not guaranteed to produce results that are identical to the slice of the result of the same operation applied to the full tensor. E.g. let be a 2-dimensional tensor. is not guaranteed to be bitwise equal to .\nWhen inputs contain large values such that intermediate results may overflow the range of the used datatype, the end result may overflow too, even though it is representable in the original datatype. E.g.:\n```\n \n  \n \n \n\n```\n\nThe external libraries (backends) that uses provide no guarantees on their behaviour when the inputs have non-finite values like or . As such, neither does PyTorch. The operations may return a tensor with non-finite values, or raise an exception, or even segfault.\nand assume that the input matrix is invertible. If it is close to being non-invertible (for example, if it has a very small singular value), then these algorithms may silently return incorrect results. These matrices are said to be .\nSpectral operations like , , and may also return incorrect results (and their gradients may be infinite) when their inputs have singular values that are close to each other. This is because the algorithms used to compute these decompositions struggle to converge for these inputs.\nRunning the computation in (as NumPy does by default) often helps, but it does not solve these issues in all cases. Analyzing the spectrum of the inputs via or their condition number via may help to detect these issues.\nOn Ampere (and later) Nvidia GPUs, PyTorch can use TensorFloat32 (TF32) to speed up mathematically intensive operations, in particular matrix multiplications and convolutions. When an operation is performed using TF32 tensor cores, only the first 10 bits of the input mantissa are read. This may reduce accuracy and produce surprising results (e.g., multiplying a matrix by the identity matrix may produce results that are different from the input). By default, TF32 tensor cores are disabled for matrix multiplications and enabled for convolutions, although most neural network workloads have the same convergence behavior when using TF32 as they have with fp32. We recommend enabling TF32 tensor cores for matrix multiplications with if your network does not need full float32 precision. If your network needs full float32 precision for both matrix multiplications and convolutions, then TF32 tensor cores can also be disabled for convolutions with .\nHalf-precision GEMM operations are typically done with intermediate accumulations (reduction) in single-precision for numerical accuracy and improved resilience to overflow. For performance, certain GPU architectures, especially more recent ones, allow a few truncations of the intermediate accumulation results to the reduced precision (e.g., half-precision). This change is often benign from the perspective of model convergence, though it may lead to unexpected results (e.g., values when the final result should be be representable in half-precision). If reduced-precision reductions are problematic, they can be turned off with \nA similar flag exists for BF16 GEMM operations and is turned on by default. If BF16 reduced-precision reductions are problematic, they can be turned off with \n## Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA) \nA naive SDPA math backend, when using FP16/BF16 inputs, can accumulate significant numerical errors due to the usage of low-precision intermediate buffers. To mitigate this issue, the default behavior now involves upcasting FP16/BF16 inputs to FP32. Computations are performed in FP32/TF32, and the final FP32 results are then downcasted back to FP16/BF16. This will improve numerical accuracy of the final output for the math backend with FP16/BF16 inputs, but increases memory usages and may cause the performance regressions in the math backend as computations shift from FP16/BF16 BMM to FP32/TF32 BMM/Matmul.\nFor scenarios where reduced-precision reductions are preferred for speed, they can be enabled with the following setting: \n## Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices \nOn AMD Instinct MI200 GPUs, the FP16 and BF16 V_DOT2 and MFMA matrix instructions flush input and output denormal values to zero. FP32 and FP64 MFMA matrix instructions do not flush input and output denormal values to zero. The affected instructions are only used by rocBLAS (GEMM) and MIOpen (convolution) kernels; all other PyTorch operations will not encounter this behavior. All other supported AMD GPUs will not encounter this behavior.\nrocBLAS and MIOpen provide alternate implementations for affected FP16 operations. Alternate implementations for BF16 operations are not provided; BF16 numbers have a larger dynamic range than FP16 numbers and are less likely to encounter denormal values. For the FP16 alternate implementations, FP16 input values are cast to an intermediate BF16 value and then cast back to FP16 output after the accumulate FP32 operations. In this way, the input and output types are unchanged.\nWhen training using FP16 precision, some models may fail to converge with FP16 denorms flushed to zero. Denormal values more frequently occur in the backward pass of training during gradient calculation. PyTorch by default will use the rocBLAS and MIOpen alternate implementations during the backward pass. The default behavior can be overridden using environment variables, ROCBLAS_INTERNAL_FP16_ALT_IMPL and MIOPEN_DEBUG_CONVOLUTION_ATTRIB_FP16_ALT_IMPL. The behavior of these environment variables is as follows:\nThe following is the list of operations where rocBLAS may be used:\nThe following is the list of operations where MIOpen may be used:\n  *     * [Reduced Precision Reduction for FP16 and BF16 in Scaled Dot Product Attention (SDPA)](https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-reduction-for-fp16-and-bf16-in-scaled-dot-product-attention-sdpa)\n    * [Reduced Precision FP16 and BF16 GEMMs and Convolutions on AMD Instinct MI200 devices](https://docs.pytorch.org/docs/stable/notes/numerical_accuracy.html#reduced-precision-fp16-and-bf16-gemms-and-convolutions-on-amd-instinct-mi200-devices)\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/fsdp.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nImplicit prefetching refers to relying on issuing the all-gathers from a separate CUDA stream to allow for overlapping an all-gather with compute issued before it (from the CPU perspective). For example, if we have layer 0 all-gather -> layer 0 compute -> layer 1 all-gather -> …, then layer 1 all-gather can overlap with layer 0 compute even though the CPU thread issued it afterwards. (The 1st all-gather will not be able to overlap with anything.)\nExplicit prefetching refers to changing the CPU thread’s issue order: e.g. layer 0 all-gather -> layer 1 all-gather -> layer 0 compute -> …. In eager mode, there is no way to know in general which layer is the next layer (e.g. layer 1 in the example) when still executing on layer 0. Therefore, explicit prefetching should only be used for models whose execution order is fixed from iteration to iteration (which we sometimes call “static graph”). An example of a model that does not satisfy this constraint is ).\nExplicit prefetching only saves the time taken to issue a layer’s compute kernels at the cost that the next all-gather’s output tensor must be allocated while the current one is still in use. By issuing the next all- gather before the current compute kernels, the next all-gather can start sooner on GPU. For most LLM workloads, this is not the case, so there is no motivation for enabling .\nIn contrast, for , we must use explicit prefetching or else there will be 0 overlap of communication and computation. The reason is because we use a single NCCL process group for both all-gather and reduce-scatter (partially because in earlier NCCL versions, it was not safe to use multiple concurrently on the same device over the same ranks). A single NCCL process group means a single internal NCCL stream on which reduce-scatters and all-gathers run serially. As such, unless we explicitly reorder the CPU issue order to be next all-gather -> current reduce-scatter, then the current reduce-scatter would block the next all-gather and hence the next computation, preventing the current reduce-scatter from overlapping.\n\n\nIf activation checkpointing () is used there is no additional communication since the parameters are prefetched anyway during .\nIn the FSDP design, the communication payload per rank is determined as follows: Each call to creates one communication group consisting of the parameters in except any already assigned to a nested instance. For example, for Llama, if you apply to every transformer block and also to the root module, then there is one communication group for each transformer block and finally one communication group with the initial embedding and final linear. Each communication group corresponds to a single all-gather call and single reduce-scatter call. In that way, how you apply determines the communication size. In general, applying FSDP to each transformer block is a good heuristic for LLMs, and it is hard to do better than that given the current design.\nLet’s consider an example where we have a Transformer-based model sharded over 8 GPUs, where the sharding happens at the transformer block-level only, and each transformer block contains 1.6B parameters and the parameters are in fp32 (4 bytes each). Which means that once sharded, each transformer block will contain 0.2B parameters on each rank.\n  * The pass will communicate 2 times each (1x all-gather and 1x reduce-scatter)\n\n\nIn other words there will be 3 communications with a payload of each. If the model was comprised of 10 transformer blocks there would be a total of 30 communications for a total of .\nTo formalize the payload size per communication per rank is (GBs).\nPlease note that in this example we didn’t include the additional communications required for the embedding, which should be accounted for as well. And the math would depend on whether the input and output embeddings are tied or not. If they aren’t tied there will be 2x more communications.\nAs explained in in the case of explicit prefetching ( while the other is used to do the prefetching.\nWhile the implicit prefetching (, default) case of the same sequence in theory should need only 1 buffer, in reality it’s still 2x all-gather-sized buffers. The reason is that in the flat-parameter FSDP design, we do not copy-out of the all-gather buffer. The parameters used for compute are directly viewed into the all-gather buffer (in fact, the main benefit of the “flat parameter” is exactly this reason). In that case, while ‘layer 1 all-gather’ is overlapping with ‘layer 0 forward compute’, the ‘layer 0 forward compute’ is using the parameters viewed into the ‘layer 0 all-gather’ buffer.\nA natural question then is, when would you want ? For static-graph models (like most LLMs), there is a major technical reason. It is more that, practically, we added this option quickly for some CPU-bound internal models and have not tested every code path with it in unit testing, so we are less confident in it. can be slightly easier to reason about since we do not have to check the recorded forward order as a possible ‘failure mode’; a module’s all-gather can always be found under its own label in its profiler trace.\ncurrently requires at least 2x all-gather buffer size and potentially a bit more. Here is why:\nThe current FSDP design uses to manage allocations produced in one stream consumed in another, which can lead to more memory usage than expected. How much more can be “non-deterministic” in that it depends on GPU kernel timing relative to the CPU. The argument is a mitigation to that - for more details refer to this discussion is .\n  * It calls to get 1D views into the corresponding to its constituent original parameters.\n  * It calls on each 1D split to view back to ND.\n  * This means that in , we end up with (ND -> 1D) and (which is a concat). In particular, each individual gradient is computed as a separate allocation, and an explicit concat happens to construct the reduce-scatter input buffer. This implies actually a 2x buffer size for reduce-scatter at that peak memory point.\n\n\nIn summary, for , it is about 2x buffer size for reduce-scatter plus any effects.\nOnce the sharded parameters are gathered from all ranks, they require an additional buffer of for the full parameters - so continuing the example from earlier if each transformer block is 1.6B parameters and the parameters are in fp32, then it’d be buffer.\nAnd there is a need for 2 of those buffers, since there is one currently being used and another being prefetched.\n\n\nNow let’s briefly discuss what happens to the embeddings as we have left those out from the calculations:\nGiven the rule we discussed that you included in the note starting with “the communication buffer size is determined as follows”, we can analyze as follows:\n  * Suppose we apply FSDP to the root module (e.g. the class). Suppose we further apply FSDP to each transformer block (e.g. the class).\n  * Most commonly, the embedding and final linear projection are direct children of the root class.\n  * Following our rule, that means that the embedding and final linear projection are assigned to the root ’s flat parameter.\n  * We have _another_ special rule, which is that the root does not free its parameters after forward because they will be anyways immediately all-gathered in backward.\n  * Putting this together, this means that the root’s flat parameter including the embedding and final projection are all-gathered to begin forward and kept in GPU memory until the end of backward.\n  * If the embedding and final linear are not weight-tied, then we _could_ further apply FSDP to the embedding and to the final linear. For weight-tied parameters, we require them to be part of the same flat parameter (or else it would get double-counted). That would allow the embedding to be freed after its usage in forward and only all-gathered toward the end of backward.\n  * Hopefully, this gives a better sense – each FSDP module gets assigned parameters in its except those already assigned to another nested FSDP module, and the FSDP module’s defines the ‘live’ interval for its parameters. Hence, the nested structure can affect the all-gather/free schedule and hence the memory/throughput performance.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/libtorch_stable_abi.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis note will eventually contain more details on how to use the APIs in torch/csrc/stable. For the moment, it contains a table of internal representations:\n  1. type in custom extension: type used within the end user custom library.\n  2. StableIValue representation: a stable conversion of the type to liaison between the user model vs libtorch.so in an ABI-stable manner.\n  3. type in libtorch: type used within libtorch.so (or any code binary locked with libtorch).\n  4. Schema Type: type as described by the schema, which we hail as the source of truth for both ATen ops in native_functions.yaml and for user defined custom operators registered to the dispatcher via TORCH_LIBRARY or torch.library.\n\n\nOur confidently supported types are the ones in the table that have completed rows. For a limited set of use cases, we also implicitly support any literal type that is representable within 64 bits as StableIValues, as the default reinterpret_cast will succeed. You can work with StableIValue abstractions in your custom kernel for types such as c10::Device even if there is no standard defined representation of device in custom extensions. For example, a custom operator can take as argument a StableIValue device and directly pass it through to an aten operator with aoti_torch_call_dispatcher.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/hip.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nROCm™ is AMD’s open source software platform for GPU-accelerated high performance computing and machine learning. HIP is ROCm’s C++ dialect designed to ease conversion of CUDA applications to portable C++ code. HIP is used when converting existing CUDA applications like PyTorch to portable C++ and for new projects that require portability between AMD and NVIDIA.\nPyTorch for HIP intentionally reuses the existing interfaces. This helps to accelerate the porting of existing PyTorch code and models because very few code changes are necessary, if any.\nThe example from will work exactly the same for HIP:\n```\n       \n    \n    \n\n    \n\n   \n\n\n \n    \n        \n\n    \n       \n    \n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n       \n    \n\n        \n    \n\n        \n    \n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n       \n      \n      \n    \n\n```\n\nWhether you are using PyTorch for CUDA or HIP, the result of calling will be the same. If you are using a PyTorch that has been built with GPU support, it will return . If you must check which version of PyTorch you are using, refer to this example below:\n```\n   \n    \n   \n    \n\n```\n\nPyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in . You can use and to monitor memory occupied by tensors, and use and to monitor the total amount of memory managed by the caching allocator. Calling releases all cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\nFor more advanced users, we offer more comprehensive memory benchmarking via . We also offer the capability to capture a complete snapshot of the memory allocator state via , which can help you understand the underlying allocation patterns produced by your code.\nTo debug memory errors, set in your environment to disable caching. is also accepted for ease of porting.\nFor each combination of hipBLAS handle and HIP stream, a hipBLAS workspace will be allocated if that handle and stream combination executes a hipBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless is called; note that it’s the same function for CUDA or HIP. The workspace size per allocation can be specified via the environment variable with the format . As an example, the environment variable specifies a total size of or 8 MIB. The default workspace size is 32 MiB; MI300 and newer defaults to 128 MiB. To force hipBLAS to avoid using workspaces, set . For convenience, is also accepted.\nSetting the size of the cache for hipFFT/rocFFT plans is not supported.\nCurrently, only the “nccl” and “gloo” backends for torch.distributed are supported on ROCm.\nNOTE: The CUDA_VERSION macro, cudaRuntimeGetVersion and cudaDriverGetVersion APIs do not semantically map to the same values as HIP_VERSION macro, hipRuntimeGetVersion and hipDriverGetVersion APIs. Please do not use them interchangeably when doing version checks.\nuse the following to not take the code path for ROCm/HIP:\nAlternatively, if it is desired to take the code path for ROCm/HIP:\nOr if it is desired to take the code path for ROCm/HIP only for specific HIP versions:\nFor any sections not listed here, please refer to the CUDA semantics doc: \nKernel asserts are supported on ROCm, but they are disabled due to performance overhead. It can be enabled by recompiling the PyTorch from source.\nPlease add below line as an argument to cmake command parameters:\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/gradcheck.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIt will cover both forward and backward mode AD for both real and complex-valued functions as well as higher-order derivatives. This note also covers both the default behavior of gradcheck as well as the case where argument is passed (referred to as fast gradcheck below).\n  *   * \n\n  1. , , , , , , and are real-valued vectors and is a complex-valued vector that can be rewritten in terms of two real-valued vectors as .\n  2. and are two integers that we will use for the dimension of the input and output space respectively.\n  3. is our basic real-to-real function such that .\n  4. is our basic complex-to-real function such that .\n\n\nFor the simple real-to-real case, we write as the Jacobian matrix associated with of size . This matrix contains all the partial derivatives such that the entry at position contains . Backward mode AD is then computing, for a given vector of size , the quantity . Forward mode AD on the other hand is computing, for a given vector of size , the quantity .\nFor functions that contain complex values, the story is a lot more complex. We only provide the gist here and the full description can be found at .\nThe constraints to satisfy complex differentiability (Cauchy-Riemann equations) are too restrictive for all real-valued loss functions, so we instead opted to use Wirtinger calculus. In a basic setting of Wirtinger calculus, the chain rule requires access to both the Wirtinger derivative (called below) and the Conjugate Wirtinger derivative (called below). Both and need to be propagated because in general, despite their name, one is not the complex conjugate of the other.\nTo avoid having to propagate both values, for backward mode AD, we always work under the assumption that the function whose derivative is being calculated is either a real-valued function or is part of a bigger real-valued function. This assumption means that all the intermediary gradients we compute during the backward pass are also associated with real-valued functions. In practice, this assumption is not restrictive when doing optimization as such problem require real-valued objectives (as there is no natural ordering of the complex numbers).\nUnder this assumption, using and definitions, we can show that (we use to denote complex conjugation here) and so only one of the two values actually need to be “backwarded through the graph” as the other one can easily be recovered. To simplify internal computations, PyTorch uses as the value it backwards and returns when the user asks for gradients. Similarly to the real case, when the output is actually in , backward mode AD does not compute but only for a given vector .\nFor forward mode AD, we use a similar logic, in this case, assuming that the function is part of a larger function whose input is in . Under this assumption, we can make a similar claim that every intermediary result corresponds to a function whose input is in and in this case, using and definitions, we can show that for the intermediary functions. To make sure the forward and backward mode compute the same quantities in the elementary case of a one dimensional function, the forward mode also computes . Similarly to the real case, when the input is actually in , forward mode AD does not compute but only for a given vector .\nTo test a function , we reconstruct the full Jacobian matrix of size in two ways: analytically and numerically. The analytical version uses our backward mode AD while the numerical version uses finite difference. The two reconstructed Jacobian matrices are then compared elementwise for equality.\nIf we consider the elementary case of a one-dimensional function (), then we can use the basic finite difference formula from \n\\frac{\\partial y}{\\partial x} \\approx \\frac{f(x + eps) - f(x - eps)}{2 * eps} \nThis formula easily generalizes for multiple outputs () by having be a column vector of size like . In that case, the above formula can be re-used as-is and approximates the full Jacobian matrix with only two evaluations of the user function (namely and ).\nIt is more computationally expensive to handle the case with multiple inputs (). In this scenario, we loop over all the inputs one after the other and apply the perturbation for each element of one after the other. This allows us to reconstruct the matrix column by column.\nFor the analytical evaluation, we use the fact, as described above, that backward mode AD computes . For functions with a single output, we simply use to recover the full Jacobian matrix with a single backward pass.\nFor functions with more than one output, we resort to a for-loop which iterates over the outputs where each is a one-hot vector corresponding to each output one after the other. This allows to reconstruct the matrix row by row.\nTo test a function with , we reconstruct the (complex-valued) matrix that contains .\nConsider the elementary case where first. We know from (chapter 3 of) \nCW := \\frac{\\partial y}{\\partial z^*} = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b}) \nNote that and , in the above equation, are derivatives. To evaluate these numerically, we use the method described above for the real-to-real case. This allows us to compute the matrix and then multiply it by .\nNote that the code, as of time of writing, computes this value in a slightly convoluted way:\n```\n\n\n\n\n\n  \n    \n\n        \n\n        \n        \n\n# Since grad_out is always 1, and W and CW are complex conjugate of each other, the last line ends up computing exactly `conj_w_d + w_d.conj() = conj_w_d + conj_w_d = 2 * conj_w_d`.\n\n```\n\nSince backward mode AD computes exactly twice the derivative already, we simply use the same trick as for the real-to-real case here and reconstruct the matrix row by row when there are multiple real outputs.\nIn this case, the user-provided function does not follow the assumption from the autograd that the function we compute backward AD for is real-valued. This means that using autograd directly on this function is not well defined. To solve this, we will replace the test of the function (where can be either or ), with two functions: and such that:\n\\begin{aligned} hr(q) &:= real(f(q)) \\\\\\ hi(q) &:= imag(f(q)) \\end{aligned} \nwhere . We then do a basic gradcheck for both and using either the real-to-real or complex-to-real case described above, depending on .\nNote that, the code, as of time of writing, does not create these functions explicitly but perform the chain rule with the or functions manually by passing the arguments to the different functions. When , then we are considering . When , then we are considering .\nWhile the above formulation of gradcheck is great, both, to ensure correctness and debuggability, it is very slow because it reconstructs the full Jacobian matrices. This section presents a way to perform gradcheck in a faster way without affecting its correctness. The debuggability can be recovered by adding special logic when we detect an error. In that case, we can run the default version that reconstructs the full matrix to give full details to the user.\nThe high level strategy here is to find a scalar quantity that can be computed efficiently by both the numerical and analytical methods and that represents the full matrix computed by the slow gradcheck well enough to ensure that it will catch any discrepancy in the Jacobians.\nThe scalar quantity that we want to compute here is for a given random vector and a random unit norm vector .\nJ_f u \\approx \\frac{f(x + u * eps) - f(x - u * eps)}{2 * eps}. \nWe then perform the dot product between this vector and to get the scalar value of interest.\nFor the analytical version, we can use backward mode AD to compute directly. We then perform the dot product with to get the expected value.\nSimilar to the real-to-real case, we want to perform a reduction of the full matrix. But the matrix is complex-valued and so in this case, we will compare to complex scalars.\nDue to some constraints on what we can compute efficiently in the numerical case and to keep the number of numerical evaluations to a minimum, we compute the following (albeit surprising) scalar value:\ns := 2 * v^T (real(CW) ur + i * imag(CW) ui) \nWe first consider how to compute with a numerical method. To do so, keeping in mind that we’re considering with , and that CW = \\frac{1}{2} * (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b}), we rewrite it as follows:\n\\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\\ &= 2 * v^T (\\frac{1}{2} * \\frac{\\partial y}{\\partial a} ur + i * \\frac{1}{2} * \\frac{\\partial y}{\\partial b} ui) \\\\\\ &= v^T (\\frac{\\partial y}{\\partial a} ur + i * \\frac{\\partial y}{\\partial b} ui) \\\\\\ &= v^T ((\\frac{\\partial y}{\\partial a} ur) + i * (\\frac{\\partial y}{\\partial b} ui)) \\end{aligned} \nIn this formula, we can see that and can be evaluated the same way as the fast version for the real-to-real case. Once these real-valued quantities have been computed, we can reconstruct the complex vector on the right side and do a dot product with the real-valued vector.\nFor the analytical case, things are simpler and we rewrite the formula as:\n\\begin{aligned} s &= 2 * v^T (real(CW) ur + i * imag(CW) ui) \\\\\\ &= v^T real(2 * CW) ur + i * v^T imag(2 * CW) ui) \\\\\\ &= real(v^T (2 * CW)) ur + i * imag(v^T (2 * CW)) ui \\end{aligned} \nWe can thus use the fact that the backward mode AD provides us with an efficient way to compute and then perform a dot product of the real part with and the imaginary part with before reconstructing the final complex scalar .\nAt this point, you might be wondering why we did not select a complex and just performed the reduction . To dive into this, in this paragraph, we will use the complex version of noted . Using such complex , the problem is that when doing the numerical evaluation, we would need to compute:\n\\begin{aligned} 2*CW u' &= (\\frac{\\partial y}{\\partial a} + i \\frac{\\partial y}{\\partial b})(ur' + i ui') \\\\\\ &= \\frac{\\partial y}{\\partial a} ur' + i \\frac{\\partial y}{\\partial a} ui' + i \\frac{\\partial y}{\\partial b} ur' - \\frac{\\partial y}{\\partial b} ui' \\end{aligned} \nWhich would require four evaluations of real-to-real finite difference (twice as much compared to the approached proposed above). Since this approach does not have more degrees of freedom (same number of real valued variables) and we try to get the fastest possible evaluation here, we use the other formulation above.\nJust like in the slow case, we consider two real-valued functions and use the appropriate rule from above for each function.\nPyTorch also provide a utility to verify second order gradients. The goal here is to make sure that the backward implementation is also properly differentiable and computes the right thing.\nThis feature is implemented by considering the function and use the gradcheck defined above on this function. Note that in this case is just a random vector with the same type as .\nThe fast version of gradgradcheck is implemented by using the fast version of gradcheck on that same function .\n  *     *       *       *     *       * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/multiprocessing.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nWhen a is sent to another process, the data is shared. If is not , it is also shared. After a without a field is sent to the other process, it creates a standard process-specific that is not automatically shared across all processes, unlike how the ’s data has been shared.\nThis allows to implement various training methods, like Hogwild, A3C, or any others that require asynchronous operation.\nThe CUDA runtime does not support the start method; either the or start method are required to use CUDA in subprocesses.\nThe start method can be set via either creating a context with or directly using .\nUnlike CPU tensors, the sending process is required to keep the original tensor as long as the receiving process retains a copy of the tensor. It is implemented under the hood but requires users to follow the best practices for the program to run correctly. For example, the sending process must stay alive as long as the consumer process has references to the tensor, and the refcounting can not save you if the consumer process exits abnormally via a fatal signal. See .\nThere are a lot of things that can go wrong when a new process is spawned, with the most common cause of deadlocks being background threads. If there’s any thread that holds a lock or imports a module, and is called, it’s very likely that the subprocess will be in a corrupted state and will deadlock or fail in a different way. Note that even if you don’t, Python built in libraries do - no need to look further than , that doesn’t use any additional threads.\nWe’re trying our best to make it easy for you and ensure these deadlocks don’t happen but some things are out of our control. If you have any issues you can’t cope with for a while, try reaching out on forums, and we’ll see if it’s an issue we can fix.\nUsing , it is possible to train a model asynchronously, with parameters either shared all the time, or being periodically synchronized. In the first case, we recommend sending over the whole model object, while in the latter, we advise to only send the .\nWe recommend using start method, however it is very bug prone and should be used with care, and only by advanced users. Queues, even though they’re sometimes a less elegant solution, will work properly in all cases.\nYou should be careful about having global statements, that are not guarded with an . If a different start method than is used, they will be executed in all subprocesses.\n```\n   \n   \n\n \n    \n        \n        \n         \n          \n\n   \n      \n      \n    # NOTE: this is required for the ``fork`` method to work\n    \n      \n       \n           \n        \n        \n       \n        \n\n```\n\nInappropriate multiprocessing can lead to CPU oversubscription, causing different processes to compete for CPU resources, resulting in low efficiency.\nThis tutorial will explain what CPU oversubscription is and how to avoid it.\nCPU oversubscription is a technical term that refers to a situation where the total number of vCPUs allocated to a system exceeds the total number of vCPUs available on the hardware.\nThis leads to severe contention for CPU resources. In such cases, there is frequent switching between processes, which increases processes switching overhead and decreases overall system efficiency.\nSee CPU oversubscription with the code examples in the Hogwild implementation found in the \nWhen running the training example with the following command on CPU using 4 processes:\nAssuming there are N vCPUs available on the machine, executing the above command will generate 4 subprocesses. Each subprocess will allocate N vCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the machine only has N vCPUs available. Consequently, the different processes will compete for resources, leading to frequent process switching.\n  1. High CPU Utilization: By using the command, you can observe that the CPU utilization is consistently high, often reaching or exceeding its maximum capacity. This indicates that the demand for CPU resources exceeds the available physical cores, causing contention and competition among processes for CPU time.\n  2. Frequent Context Switching with Low System Efficiency: In an oversubscribed CPU scenario, processes compete for CPU time, and the operating system needs to rapidly switch between different processes to allocate resources fairly. This frequent context switching adds overhead and reduces the overall system efficiency.\n\n\nA good way to avoid CPU oversubscription is proper resource allocation. Ensure that the number of processes or threads running concurrently does not exceed the available CPU resources.\nIn this case, a solution would be to specify the appropriate number of threads in the subprocesses. This can be achieved by setting the number of threads for each process using the function in subprocess.\nAssuming there are N vCPUs on the machine and M processes will be generated, the maximum value used by each process would be . To avoid CPU oversubscription in the mnist_hogwild example, the following changes are needed for the file in \nSet for each process using . where you replace N with the number of vCPUs available and M with the chosen number of processes. The appropriate value will vary depending on the specific task at hand. However, as a general guideline, the maximum value for the should be to avoid CPU oversubscription. In the \n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n  \nIntel® Core™ Ultra Processors with Intel® Arc™ Graphics (CodeName: Meteor Lake) Intel® Core™ Ultra 200V Series with Intel® Arc™ Graphics (CodeName: Lunar Lake) Intel® Core™ Ultra Series 2 Processors with Intel® Arc™ Graphics (CodeName: Arrow Lake)  \n---  \nIntel® Core™ Ultra Processors with Intel® Arc™ Graphics (CodeName: Meteor Lake) Intel® Core™ Ultra 200V Series with Intel® Arc™ Graphics (CodeName: Lunar Lake) Intel® Core™ Ultra Series 2 Processors with Intel® Arc™ Graphics (CodeName: Arrow Lake)  \nIntel GPUs support (Prototype) is ready from PyTorch* 2.5 for Intel® Client GPUs and Intel® Data Center GPU Max Series on both Linux and Windows, which brings Intel GPUs and the SYCL* software stack into the official PyTorch stack with consistent user experience to embrace more AI application scenarios.\nTo use PyTorch on Intel GPUs, you need to install the Intel GPUs driver first. For installation guide, visit \nPlease skip the Intel® Deep Learning Essentials installation section if you install from binaries. For building from source, please refer to \nTo check if your Intel GPU is available, you would typically use the following code:\nIf the output is , double check driver installation for Intel GPUs.\nIf you are migrating code from , you would change references from to . For example:\nThe following points outline the support and limitations for PyTorch with Intel GPU:\n  1. Both eager mode and is supported. The feature is also supported on Windows from PyTorch* 2.7 with Intel GPU, refer to .\n  2. Data types such as FP32, BF16, FP16, and Automatic Mixed Precision (AMP) are all supported.\n\n\nThis section contains usage examples for both inference and training workflows.\n```\n \n   \n \n\n  \n\n     \n  \n\n  \n  \n\n   \n      \n     \n        \n        \n      \n    \n\n  \n   \n      \n     \n        \n        \n      \n    \n\n\n\n```\n\nNote: Training with requires hardware support for . is not natively supported by the Intel® Arc™ A-Series Graphics. If you run your workloads on Intel® Arc™ A-Series Graphics, please disable .\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/large_scale_deployments.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n\n\nThis note talks about several extension points and tricks that might be useful when running PyTorch within a larger system or operating multiple systems using PyTorch in a larger organization.\nIt doesn’t cover topics of deploying models to production. Check or one of the corresponding tutorials.\nThe note assumes that you either build PyTorch from source in your organization or have an ability to statically link additional code to be loaded when PyTorch is used. Therefore, many of the hooks are exposed as C++ APIs that can be triggered once in a centralized place, e.g. in static initialization code.\nPyTorch comes with capable of measuring time taken by individual operators on demand. One can use the same mechanism to do “always ON” measurements for any process running PyTorch. It might be useful for gathering information about PyTorch workloads running in a given process or across the entire set of machines.\nNew callbacks for any operator invocation can be added with . Hooks will be called with struct that describes invocation context (e.g. ). If enabled, contains arguments of the function represented as variant type. Note, that inputs logging is relatively expensive and thus has to be enabled explicitly.\nThe operator callbacks also have access to interface that returns a pointer to the struct holding the debug information. This debug information can be set earlier by using object. Debug information is propagated through the forward (including async tasks) and backward passes and can be useful for passing some extra information about execution environment (e.g. model id) from the higher layers of the application down to the operator callbacks.\nInvoking callbacks adds some overhead, so usually it’s useful to just randomly sample operator invocations. This can be enabled on per-callback basis with an optional sampling rate passed into .\nNote, that is not thread-safe and can be called only when no PyTorch operator is running. Usually, it’s a good idea to call them once during initialization.\n```\n\n\n\n\n\n\n\n\n\n\n// Note, to enable observers in the model calling thread,\n// call enableRecordFunction() in the thread before running a model\n\n\n\n\n\n\n\n\n\n\n\n```\n\nWhen running in a broader ecosystem, for example in managed job scheduler, it’s often useful to track which binaries invoke particular PyTorch APIs. There exists simple instrumentation injected at several important API points that triggers a given callback. Because usually PyTorch is invoked in one-off python scripts, the callback fires only once for a given process for each of the APIs.\ncan be used to register API usage instrumentation handler. Passed argument is going to be an “api key” identifying used point, for example for PyTorch extension import or if TorchScript compilation was triggered.\nNote for developers: new API trigger points can be added in code with in C++ or in Python.\nTorchScript modules can be saved as an archive file that bundles serialized parameters and module code as TorchScript (see ). It’s often convenient to bundle additional information together with the model, for example, description of model producer or auxiliary artifacts.\nIt can be achieved by passing the argument to and to store and retrieve arbitrary binary blobs during saving process. Since TorchScript files are regular ZIP archives, extra information gets stored as regular files inside archive’s directory.\nThere’s also a global hook allowing to attach extra files to any TorchScript archive produced in the current process. It might be useful to tag models with producer metadata, akin to JPEG metadata produced by digital cameras. Example usage might look like:\nTorchScript’s compilation needs to have access to the original python files as it uses python’s call. In certain production environments it might require explicitly deploying files along with precompiled .\nPyTorch APIs are generally loosely coupled and it’s easy to replace a component with specialized version. Common extension points include:\n  * Custom operators implemented in C++ - see .\n  * Custom data reading can be often integrated directly by invoking corresponding python library. Existing functionality of can be utilized by extending or .\n\n\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/randomness.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nCompletely reproducible results are not guaranteed across PyTorch releases, individual commits, or different platforms. Furthermore, results may not be reproducible between CPU and GPU executions, even when using identical seeds.\nHowever, there are some steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release. First, you can control sources of randomness that can cause multiple executions of your application to behave differently. Second, you can configure PyTorch to avoid using nondeterministic algorithms for some operations, so that multiple calls to those operations, given the same inputs, will produce the same result.\nDeterministic operations are often slower than nondeterministic operations, so single-run performance may decrease for your model. However, determinism may save time in development by facilitating experimentation, debugging, and regression testing.\nYou can use to seed the RNG for all devices (both CPU and CUDA):\nSome PyTorch operations may use random numbers internally. does this, for instance. Consequently, calling it multiple times back-to-back with the same input arguments may give different results. However, as long as is set to a constant at the beginning of an application and all other sources of nondeterminism have been eliminated, the same series of random numbers will be generated each time the application is run in the same environment.\nIt is also possible to obtain identical results from an operation that uses random numbers by setting to the same value between subsequent calls.\nFor custom operators, you might need to set python seed as well:\nIf you or any of the libraries you are using rely on NumPy, you can seed the global NumPy RNG with:\nHowever, some applications and libraries may use NumPy Random Generator objects, not the global RNG (\nIf you are using any other libraries that use random number generators, refer to the documentation for those libraries to see how to set consistent seeds for them.\nThe cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism across multiple executions of an application. When a cuDNN convolution is called with a new set of size parameters, an optional feature can run multiple convolution algorithms, benchmarking them to find the fastest one. Then, the fastest algorithm will be used consistently during the rest of the process for the corresponding set of size parameters. Due to benchmarking noise and different hardware, the benchmark may select different algorithms on subsequent runs, even on the same machine.\nDisabling the benchmarking feature with causes cuDNN to deterministically select an algorithm, possibly at the cost of reduced performance.\nHowever, if you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled with .\nNote that this setting is different from the setting discussed below.\nlets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones where available, and to throw an error if an operation is known to be nondeterministic (and without a deterministic alternative).\nPlease check the documentation for for a full list of affected operations. If an operation does not act correctly according to the documentation, or if you need a deterministic implementation of an operation that does not have one, please submit an issue: \nFor example, running the nondeterministic CUDA implementation of will throw an error:\nWhen is called with sparse-dense CUDA tensors it typically uses a nondeterministic algorithm, but when the deterministic flag is turned on, its alternate deterministic implementation will be used:\n```\n \n\n     \n\n\n\n\n\n```\n\nFurthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you should set the environment variable according to CUDA documentation: \nWhile disabling CUDA convolution benchmarking (discussed above) ensures that CUDA selects the same algorithm each time an application is run, that algorithm itself may be nondeterministic, unless either or is set. The latter setting controls only this behavior, unlike which will make other PyTorch operations behave deterministically, too.\nIn some versions of CUDA, RNNs and LSTM networks may have non-deterministic behavior. See and for details and workarounds.\nOperations like and can return tensors with uninitialized memory that contain undefined values. Using such a tensor as an input to another operation is invalid if determinism is required, because the output will be nondeterministic. But there is nothing to actually prevent such invalid code from being run. So for safety, is set to by default, which will fill the uninitialized memory with a known value if is set. This will prevent the possibility of this kind of nondeterministic behavior.\nHowever, filling uninitialized memory is detrimental to performance. So if your program is valid and does not use uninitialized memory as the input to an operation, then this setting can be turned off for better performance.\nDataLoader will reseed workers following algorithm. Use and to preserve reproducibility:\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/signal.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nComputes a window with a simple cosine waveform, following the same implementation as SciPy.  \n---  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/profiler.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler’s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.\nAn earlier version of the API in module is considered legacy and will be deprecated.          \n  * () – list of activity groups (CPU, CUDA) to use in profiling, supported values: , , . Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA or (when available) ProfilerActivity.XPU.\n  * () – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\n\n\nThis API is experimental and subject to change in the future.\nEnabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.     \nAdds a user defined metadata with a string key and a string value into the trace file     \nAdds a user defined metadata with a string key and a valid json value into the trace file     \nReturns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished     \nExports the collected trace in Chrome JSON format. If kineto is enabled, only last cycle in schedule is exported.     \nExport memory event information from the profiler collected tree for a given device, and export a timeline plot. There are 3 exportable files using , each controlled by the ’s suffix.\n  * For an HTML compatible plot, use the suffix , and a memory timeline plot will be embedded as a PNG file in the HTML file.\n  * For plot points consisting of , where are timestamps and are memory usage for each category. The memory timeline plot will be saved a JSON () or gzipped JSON () depending on the suffix.\n  * For raw memory points, use the suffix . Each raw memory event will consist of , where is one of , and is one of the enums from .\n\n\nOutput: Memory timeline written as gzipped JSON, JSON, or HTML.     \nAverages events, grouping them by operator name and (optionally) input shapes, stack and overload name.\nTo use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager.     \nPreset a user defined metadata when the profiler is not started and added into the trace file later. Metadata is in the format of a string key and a valid json value     \nToggle collection of activities on/off at any point of collection. Currently supports toggling Torch Ops (CPU) and CUDA activity supported in Kineto     \n() – list of activity groups to use in profiling, supported values: ,           \n  * () – list of activity groups (CPU, CUDA) to use in profiling, supported values: , , . Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA or (when available) ProfilerActivity.XPU.\n  * () – callable that takes step (int) as a single parameter and returns value that specifies the profiler action to perform at each step.\n  * () – callable that is called at each step when returns during the profiling.\n  * () – A set of experimental options used for Kineto library features. Note, backward compatibility is not guaranteed.\n\n\nUse to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.\nAfter profiling, result files can be found in the specified directory. Use the command:\nEnabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.\n```\n# Non-default profiler schedule allows user to turn profiler on and off\n\n# trace_handler is called every time a new trace becomes available\n \n    \n         \n    \n\n \n    \n        \n        \n    \n\n    \n    \n    \n    \n    \n    \n    \n\n    \n        \n        \n        \n        \n    \n    \n    \n      \n           \n            \n            # send a signal to the profiler that the next iteration has started\n            \n\n```\n\nThe following sample shows how to setup up an Execution Trace Observer ()\nYou can also refer to test_execution_trace_with_kineto() in tests/profiler/test_profiler.py. Note: One can also pass any object satisfying the _ITraceObserver interface.     \nSets a callback to be called when a new trace ID is generated.     \nSignals the profiler that the next profiling step has started.     \nProfiler actions that can be taken at the specified intervals     \nReturns a callable that can be used as profiler argument. The profiler will skip the first steps, then wait for steps, then do the warmup for the next steps, then do the active recording for the next steps and then repeat the cycle starting with steps. The optional number of cycles is specified with the parameter, the zero value means that the cycles will continue until the profiling is finished.\nThe parameter controls whether the first stage should be skipped. This can be useful if a user wants to wait longer than between cycles, but not for the first profile. For example, if is 10 and is 20, the first cycle will wait 10 + 20 = 30 steps before warmup if is zero, but will wait only 10 steps if is non-zero. All subsequent cycles will then wait 20 steps between the last active and warmup.     \nOutputs tracing files to directory of , then that directory can be directly delivered to tensorboard as logdir. should be unique for each worker in distributed scenario, it will be set to ‘[hostname]_[pid]’ by default.     \nPushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.     \nPops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/random.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nForks the RNG, so that when you return, the RNG is reset to the state that it was previously in.     \n  * () – devices for which to fork the RNG. CPU RNG state is always forked. By default, operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed\n  * (, the RNG is not forked. This is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it.\n  * (. As for custom device, see details in [Note: support the custom device with privateuse1]\n\n    \nThe returned state is for the default generator on CPU only.     \nReturns the initial seed for generating random numbers as a Python .\nThe returned seed is for the default generator on CPU only.     \nSets the seed for generating random numbers on all devices. Returns a object.     \n(. Otherwise, a RuntimeError is raised. Negative inputs are remapped to positive values with the formula .     \nSets the seed for generating random numbers to a non-deterministic random number on all devices. Returns a 64 bit number used to seed the RNG.     \nThis function only works for CPU. For CUDA, please use , which works for both CPU and CUDA.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/serialization.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis note describes how you can save and load PyTorch tensors and module states in Python, and how to serialize Python modules so they can be loaded in C++.\n  *     * \n\nBy convention, PyTorch files are typically written with a ‘.pt’ or ‘.pth’ extension.\nand use Python’s pickle by default, so you can also save multiple tensors as part of Python objects like tuples, lists, and dicts:\nCustom data structures that include PyTorch tensors can also be saved if the data structure is pickle-able.\n```\n   \n  \n  \n   \n  \n\ntensor([ 1,  4,  3,  8,  5, 12,  7, 16,  9])\n\n```\n\nBehind the scenes, these tensors share the same “storage.” See for more on views and storage.\nWhen PyTorch saves tensors it saves their storage objects and tensor metadata separately. This is an implementation detail that may change in the future, but it typically saves space and lets PyTorch easily reconstruct the view relationships between the loaded tensors. In the above snippet, for example, only a single storage is written to ‘tensors.pt’.\nIn some cases, however, saving the current storage objects may be unnecessary and create prohibitively large files. In the following snippet a storage much larger than the saved tensor is written to a file:\nInstead of saving only the five values in the tensor to ‘small.pt,’ the 999 values in the storage it shares with were saved and loaded.\nWhen saving tensors with fewer elements than their storage objects, the size of the saved file can be reduced by first cloning the tensors. Cloning a tensor produces a new tensor with a new storage object containing only the values in the tensor:\nSince the cloned tensors are independent of each other, however, they have none of the view relationships the original tensors did. If both file size and view relationships are important when saving tensors smaller than their storage objects, then care must be taken to construct new tensors that minimize the size of their storage objects but still have the desired view relationships before saving.\nIn PyTorch, a module’s state is frequently serialized using a ‘state dict.’ A module’s state dict contains all of its parameters and persistent buffers:\n```\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nInstead of saving a module directly, for compatibility reasons it is recommended to instead save only its state dict. Python modules even have a function, , to restore their states from a state dict:\nNote that the state dict is first loaded from its file with and the state then restored with .\nEven custom modules and modules containing other modules have state dicts and can use this pattern:\nSince PyTorch 1.6.0, defaults to returning an uncompressed ZIP64 archive unless the user sets .     \n  * is the result of pickling the object passed to excluding objects that it contains\n  * contains all the storages in the object, where each storage is a separate file\n  * contains a version number at save time that can be used at load time\n\n\nWhen saving, PyTorch will ensure that the local file header of each file is padded to an offset that is a multiple of 64 bytes, ensuring that the offset of each file is 64-byte aligned.\nTensors on certain devices such as XLA are serialized as pickled numpy arrays. As such, their storages are not serialized. In these cases might not exist in the checkpoint.\nAs discussed in the documentation for , restricts the unpickler used in to only executing functions/building classes required for of plain as well as some other primitive types. Further, unlike the default provided by the module, the Unpickler is not allowed to dynamically import anything during unpickling.\nAs mentioned above, saving a module’s is a best practice when using . If loading an old checkpoint that contains an , we recommend . When loading a checkpoint that contains tensor subclasses, there will likely be functions/classes that need to be allowlisted, see below for further details.\nIf the Unpickler encounters a function or class that is not allowlisted by default within the pickle file, you should see an actionable error like such\nPlease follow the steps in the error message and allowlist the functions or classes only if you trust them.\nTo get all GLOBALs (functions/classes) in the checkpoint that are not yet allowlisted you can use which will return a list of strings of the form . If you trust these functions/classes, you can import them and allowlist them per the error message either via or the context manager .\nTo access the list of user-allowlisted functions/classes you can use and to clear the current list see .\nA caveat is that analyzes the checkpoint statically, some types might be built dynamically during the unpickling process and hence will not be reported by . One such example is in numpy. In after allowlisting all the functions/classes reported by you might see an error like\nThere are two environment variables that will influence the behavior of . These can be helpful if one does not have access to the callsites.\n\n\nScriptModules can be serialized as a TorchScript program and loaded using . This serialization encodes all the modules’ methods, submodules, parameters, and attributes, and it allows the serialized program to be loaded in C++ (i.e. without Python).\nThe distinction between and may not be immediately clear. saves Python objects with pickle. This is especially useful for prototyping, researching, and training. , on the other hand, serializes ScriptModules to a format that can be loaded in Python or C++. This is useful when saving and loading C++ modules or for running modules trained in Python with C++, a common practice when deploying PyTorch models.\nTraced modules can also be saved with , with the caveat that only the traced code path is serialized. The following example demonstrates this:\nThe above module has an if statement that is not triggered by the traced inputs, and so is not part of the traced module and not serialized with it. The scripted module, however, contains the if statement and is serialized with it. See the for more on scripting and tracing.\nSee the for details about how to use PyTorch modules in C++.\nThe PyTorch Team recommends saving and loading modules with the same version of PyTorch. Older versions of PyTorch may not support newer modules, and newer versions may have removed or modified older behavior. These changes are explicitly described in PyTorch’s \nIn PyTorch 1.5 and earlier would perform floor division when given two integer inputs:\nIn PyTorch 1.7, however, will always perform a true division of its inputs, just like division in Python 3:\nThe behavior of is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see perform floor division when given two integer inputs even when loaded with newer versions of PyTorch. ScriptModules using and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.\nIn PyTorch 1.5 and earlier always returned a float tensor, regardless of the fill value it’s given:\n```\n\n    \n       \n\n```\n\nIn PyTorch 1.7, however, will infer the returned tensor’s dtype from the fill value:\nThe behavior of is preserved in serialized ScriptModules. That is, ScriptModules serialized with versions of PyTorch before 1.6 will continue to see torch.full return float tensors by default, even when given bool or integer fill values. ScriptModules using and serialized on PyTorch 1.6 and later cannot be loaded in earlier versions of PyTorch, however, since those earlier versions do not understand the new behavior.     \nRegisters callables for tagging and deserializing storage objects with an associated priority. Tagging associates a device with a storage object at save time while deserializing moves a storage object to an appropriate device at load time. and are run in the order given by their until a tagger/deserializer returns a value that is not .\nTo override the deserialization behavior for a device in the global registry, one can register a tagger with a higher priority than the existing tagger.\nThis function can also be used to register a tagger and deserializer for new devices.     \n  * () – Callable that takes in a storage object and returns its tagged device as a string or None.\n  * () – Callable that takes in storage object and a device string and returns a storage object on the appropriate device or None.\n\n         \nSetting this to may make unzipping of the output fail or warn due to corrupted CRC32. However will be able to load the file.     \nIf byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order.     \nIf byteorder mark is not present in saved checkpoint, this byte order is used as fallback. By default, it’s “native” byte order.     \nContext manager or function to set default mmap options for with to flags.\nFor now, only either or are supported. Please open an issue if you need any other option to be added here.     \nMarks the given globals as safe for load. For example, functions added to this list can be called during unpickling, classes could be instantiated and have state set.\nEach item in the list can either be a function/class or a tuple of the form (function/class, string) where string is the full path of the function/class.\nWithin the serialized format, each function is identified with its full path as . When calling this API, you can provide this full path that should match the one in the checkpoint otherwise the default will be used.\n```\n \n \n    \n   \n   \n     \n\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n    \n     \n\n#          [-0.8234,  2.0500, -0.3657]])\n\n```\n    \nReturns the list of user-added globals that are safe for load.     \nReturns a list of strings of functions/classes in a object that are not safe for .\nFor a given function or class , the corresponding string will be of the form .\nThis function will return any GLOBALs in the checkpoint that are not in the set marked safe for (either via or context or allowlisted by by default).\nThis function will statically disassemble the pickle file in the checkpoint. The implication is any classes dynamically pushed onto the stack during unpickling will not be included in the output.     \n() – File-like object or string containing the checkpoint object saved via      \nA list of strings of pickle GLOBALs in the checkpoint that are not allowlisted for .     \n```\n \n \n    \n   \n   \n     \n\n# Unsupported global: GLOBAL __main__.MyTensor was not an allowed global by default.\n# Check the code and make sure MyTensor is safe to be used when loaded from an arbitrary checkpoint.\n     \n         \n\n#          [-0.8234,  2.0500, -0.3657]])\n   \n\n```\n    \nFor the save path, storages will still be saved, but the space that their bytes would usually be written to will be empty space. The storage bytes can then be populated in a separate pass.\nFor the load path, tensors will be loaded per the checkpoint but their storages will not be populated with data.\nThe context manager is an early prototype and is subject to change.\nprovides a global config that can control the behavior of and .\n>   * : whether to compute and write the zip file checksum (Default : ). See .\n>   * : for storages that are on an accelerator when passed to , whether to move storage to pinned memory or pageable memory on CPU within . (Default: (i.e. pageable))\n>   * : alignment of storages in the checkpoint during in bytes. (Default )\n> \n\n>   * : See the documentation for argument in . This config will set the behavior of for if it is not already explicitly passed to the call (Default : ).\n>   * : If this config is set to , offsets for storages will be calculated rather than read via random reads when using . This minimizes random reads, which can be helpful when the file is being loaded over a network. (Default : )\n> \n\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/windows.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThere are two supported components for Windows PyTorch: MKL and MAGMA. Here are the steps to build with them.\n```\n\n\n\ncurl https://s3.amazonaws.com/ossci-windows/mkl_2020.2.254.7z -k -O\n7z x -aoa mkl_2020.2.254.7z -omkl\n\n\n\nREM 2.5.4 (CUDA 10.1 10.2 11.0 11.1) x (Debug Release)\n\nREM 2.5.2 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\nREM 2.5.1 (CUDA 9.2 10.0 10.1 10.2) x (Debug Release)\n cuda102\n release\ncurl -k https://s3.amazonaws.com/ossci-windows/magma_2.5.4__.7z -o magma.7z\n7z x -aoa magma.7z -omagma\n\n\n \n \n \n\n```\n\nVisual Studio doesn’t support parallel custom task currently. As an alternative, we can use to parallelize CUDA build tasks. It can be used by typing only a few lines of code.\n```\n\npip install ninja\n\n\n Ninja\n\n```\n\nThe support for CFFI Extension is very experimental. You must specify additional in object to make it build on Windows.\nThis type of extension has better support compared with the previous one. However, it still needs some manual configuration. First, you should open the . And then, you can start your compiling process.\nPyTorch doesn’t work on 32-bit system. Please use Windows and Python 64-bit version.\nThe problem is caused by the missing of the essential files. Actually, we include almost all the essential files that PyTorch need for the conda package except VC2017 redistributable and some mkl libraries. You can resolve this by typing the following command.\nAs for the wheels package, since we didn’t pack some libraries and VS2017 redistributable files in, please make sure you install them manually. The \nAnother possible cause may be you are using GPU version without NVIDIA graphics cards. Please replace your GPU package with the CPU one.\nThis is actually an upstream issue of Anaconda. When you initialize your environment with conda-forge channel, this issue will emerge. You may fix the intel-openmp libraries through this command.\nThe implementation of is different on Windows, which uses instead of . So we have to wrap the code with an if-clause to protect the code from executing multiple times. Refactor your code into the following structure.\nThis issue happens when the child process ends before the parent process finishes sending data. There may be something wrong with your code. You can debug your code by reducing the of to zero and see if the issue persists.\nPlease update your graphics driver. If this persists, this may be that your graphics card is too old or the calculation is too heavy for your card. Please update the TDR settings according to this \nThey are not supported on Windows. Something like doing multiprocessing on CUDA tensors cannot succeed, there are two alternatives for this.\n2. Share CPU tensors instead. Make sure your custom returns CPU tensors.\n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/rpc.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe distributed RPC framework provides mechanisms for multi-machine model training through a set of primitives to allow for remote communication, and a higher-level API to automatically differentiate models split across several machines.\nAPIs in the RPC package are stable. There are multiple ongoing work items to improve performance and error handling, which will ship in future releases.\nCUDA support was introduced in PyTorch 1.9 and is still a feature. Not all features of the RPC package are yet compatible with CUDA support and thus their use is discouraged. These unsupported features include: RRefs, JIT compatibility, dist autograd and dist optimizer, and profiling. These shortcomings will be addressed in future releases.\nPlease refer to for a brief introduction to all features related to distributed training.\nThe distributed RPC framework makes it easy to run functions remotely, supports referencing remote objects without copying the real data around, and provides autograd and optimizer APIs to transparently run backward and update parameters across RPC boundaries. These features can be categorized into four sets of APIs.\n  1. supports running a function on the specified destination worker with the given arguments and getting the return value back or creating a reference to the return value. There are three main RPC APIs: (synchronous), (asynchronous), and (asynchronous and returns a reference to the remote return value). Use the synchronous API if the user code cannot proceed without the return value. Otherwise, use the asynchronous API to get a future, and wait on the future when the return value is needed on the caller. The API is useful when the requirement is to create something remotely but never need to fetch it to the caller. Imagine the case that a driver process is setting up a parameter server and a trainer. The driver can create an embedding table on the parameter server and then share the reference to the embedding table with the trainer, but itself will never use the embedding table locally. In this case, and are no longer appropriate, as they always imply that the return value will be returned to the caller immediately or in the future.\n  2. serves as a distributed shared pointer to a local or remote object. It can be shared with other workers and reference counting will be handled transparently. Each RRef only has one owner and the object only lives on that owner. Non-owner workers holding RRefs can get copies of the object from the owner by explicitly requesting it. This is useful when a worker needs to access some data object, but itself is neither the creator (the caller of ) or the owner of the object. The distributed optimizer, as we will discuss below, is one example of such use cases.\n  3. stitches together local autograd engines on all the workers involved in the forward pass, and automatically reach out to them during the backward pass to compute gradients. This is especially helpful if the forward pass needs to span multiple machines when conducting, e.g., distributed model parallel training, parameter-server training, etc. With this feature, user code no longer needs to worry about how to send gradients across RPC boundaries and in which order should the local autograd engines be launched, which can become quite complicated where there are nested and inter-dependent RPC calls in the forward pass.\n  4. ’s constructor takes a (e.g., , , etc.) and a list of parameter RRefs, creates an instance on each distinct RRef owner, and updates parameters accordingly when running . When you have distributed forward and backward passes, parameters and gradients will be scattered across multiple workers, and hence it requires an optimizer on each of the involved workers. Distributed Optimizer wraps all those local optimizers into one, and provides a concise constructor and API.\n\n\nBefore using RPC and distributed autograd primitives, initialization must take place. To initialize the RPC framework we need to use which would initialize the RPC framework, RRef framework and distributed autograd.     \nInitializes RPC primitives such as the local RPC agent and distributed autograd, which immediately makes the current process ready to send and receive RPCs.     \n  * (, , , ) Name can only contain number, alphabet, underscore, colon, and/or dash, and must be shorter than 128 characters.\n  * () – The type of RPC backend implementation. Supported values is (the default). See for more information.\n  * () – The options passed to the RpcAgent constructor. It must be an agent-specific subclass of and contains agent-specific initialization configurations. By default, for all agents, it sets the default timeout to 60 seconds and performs the rendezvous with an underlying process group initialized using , meaning that environment variables and need to be set properly. See for more information and find which options are available.\n\n\nThe following APIs allow users to remotely execute functions as well as create references (RRefs) to remote data objects. In these APIs, when passing a as an argument or a return value, the destination worker will try to create a with the same meta (i.e., shape, stride, etc.). We intentionally disallow transmitting CUDA tensors because it might crash if the device lists on source and destination workers do not match. In such cases, applications can always explicitly move the input tensors to CPU on the caller and move it to the desired devices on the callee if necessary.\nTorchScript support in RPC is a prototype feature and subject to change. Since v1.5.0, supports calling TorchScript functions as RPC target functions, and this will help improve parallelism on the callee side as executing TorchScript functions does not require GIL.     \nMake a blocking RPC call to run function on worker . RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe.     \n  * () – a callable function, such as Python callables, builtin operators (e.g. ) and annotated TorchScript functions.\n  * () – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n    \nMake sure that and are set properly on both workers. Refer to API for more details. For example,\nBelow is an example of running a TorchScript function using RPC.     \nMake a non-blocking RPC call to run function on worker . RPC messages are sent and received in parallel to execution of Python code. This method is thread-safe. This method will immediately return a that can be awaited on.     \n  * () – a callable function, such as Python callables, builtin operators (e.g. ) and annotated TorchScript functions.\n  * () – timeout in seconds to use for this RPC. If the RPC does not complete in this amount of time, an exception indicating it has timed out will be raised. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n    \nReturns a object that can be waited on. When completed, the return value of on and can be retrieved from the object.\nUsing GPU tensors as arguments or return values of is not supported since we don’t support sending GPU tensors over the wire. You need to explicitly copy GPU tensors to CPU before using them as arguments or return values of .\nThe API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned completes.     \nMake sure that and are set properly on both workers. Refer to API for more details. For example,\nBelow is an example of running a TorchScript function using RPC.     \nMake a remote call to run on worker and return an to the result value immediately. Worker will be the owner of the returned , and the worker calling is a user. The owner manages the global reference count of its , and the owner is only destructed when globally there are no living references to it.     \n  * () – a callable function, such as Python callables, builtin operators (e.g. ) and annotated TorchScript functions.\n  * () – timeout in seconds for this remote call. If the creation of this on worker is not successfully processed on this worker within this timeout, then the next time there is an attempt to use the RRef (such as ), a timeout will be raised indicating this failure. A value of 0 indicates an infinite timeout, i.e. a timeout error will never be raised. If not provided, the default value set during initialization or with is used.\n\n    \nA user instance to the result value. Use the blocking API to retrieve the result value locally.\nThe API does not copy storages of argument tensors until sending them over the wire, which could be done by a different thread depending on the RPC backend type. The caller should make sure that the contents of those tensors stay intact until the returned RRef is confirmed by the owner, which can be checked using the API.\nErrors such as timeouts for the API are handled on a best-effort basis. This means that when remote calls initiated by fail, such as with a timeout error, we take a best-effort approach to error handling. This means that errors are handled and set on the resulting RRef on an asynchronous basis. If the RRef has not been used by the application before this handling (such as or fork call), then future uses of the will appropriately raise errors. However, it is possible that the user application will use the before the errors are handled. In this case, errors may not be raised as they have not yet been handled.     \nGet of a given worker name. Use this to avoid passing an expensive string on every invocation.     \nPerform a shutdown of the RPC agent, and then destroy the RPC agent. This stops the local agent from accepting outstanding requests, and shuts down the RPC framework by terminating all RPC threads. If , this will block until all local and remote RPC processes reach this method and wait for all outstanding work to complete. Otherwise, if , this is a local shutdown, and it does not wait for other RPC processes to reach this method.     \n( and delete them; 2) block until all local and remote RPC processes have reached this method and wait for all outstanding work to complete.     \nMake sure that and are set properly on both workers. Refer to API for more details. For example,\n```\n\n \n   \n  \n\n     \n\n\n\n```\n\n```\n\n   \n  \n# wait for worker 0 to finish work, and then shutdown.\n\n\n```\n    \nA structure that encapsulates information of a worker in the system. Contains the name and ID of the worker. This class is not meant to be constructed directly, rather, an instance can be retrieved through and the result can be passed in to functions such as , , to avoid copying a string on every invocation.\nThe RPC package also provides decorators which allow applications to specify how a given function should be treated on the callee side.     \nA decorator for a function indicating that the return value of the function is guaranteed to be a object and this function can run asynchronously on the RPC callee. More specifically, the callee extracts the returned by the wrapped function and installs subsequent processing steps as a callback to that . The installed callback will read the value from the when completed and send the value back as the RPC response. That also means the returned only exists on the callee side and is never sent through RPC. This decorator is useful when the wrapped function’s () execution needs to pause and resume due to, e.g., containing or waiting for other signals.\nTo enable asynchronous execution, applications must pass the function object returned by this decorator to RPC APIs. If RPC detected attributes installed by this decorator, it knows that this function returns a object and will handle that accordingly. However, this does not mean this decorator has to be outmost one when defining a function. For example, when combined with or , needs to be the inner decorator to allow the target function be recognized as a static or class function. This target function can still execute asynchronously because, when accessed, the static or class method preserves attributes installed by .     \nThe returned object can come from , , or constructor. The example below shows directly using the returned by .\n```\n   \n\n\n\n\n\n    \n    # This function runs on \"worker1\" and returns immediately when\n    # the callback is installed through the `then(cb)` API. In the\n    # mean time, the `rpc_async` to \"worker2\" can run concurrently.\n    # When the return value of that `rpc_async` arrives at\n    \n    # and set the value for the previously returned `Future`, which\n    # will then trigger RPC to send the result back to \"worker0\".\n        \n            \n    \n\n\n  \n    \n    \n       \n\n  \n\n```\n\nWhen combined with TorchScript decorators, this decorator must be the outmost one.\n```\n   \n   \n   \n\n\n\n\n\n      \n       \n\n\n\n        \n        \n\n\n  \n    \n    \n      \n\n  \n\n```\n\nWhen combined with static or class method, this decorator must be the inner one.\n```\n   \n\n\n\n\n \n\n    \n    \n        \n            \n                \n        \n\n    \n    \n         \n          \n           \n                \n        \n         \n\n    \n         \n            \n                \n        \n\n\n  \n    \n    \n       \n\n  \n\n  \n    \n    \n       \n\n  \n\n```\n\nThis decorator also works with RRef helpers, i.e., . , , and .\n```\n   \n\n\n   \n     \n  \n\n   \n     \n  \n\n   \n     \n  \n\n```\n\nThe RPC module can leverage different backends to perform the communication between the nodes. The backend to be used can be specified in the function, by passing a certain value of the enum. Regardless of what backend is used, the rest of the RPC API won’t change. Each backend also defines its own subclass of the class, an instance of which can also be passed to to configure the backend’s behavior.     \nPyTorch ships with a builtin backend. Additional ones can be registered using the function.     \nAn abstract structure encapsulating the options passed into the RPC backend. An instance of this class can be passed in to in order to initialize RPC with specific configurations, such as the RPC timeout and to be used.     \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.\nThe TensorPipe backend has been introduced in PyTorch v1.6 and is being actively developed. At the moment, it only supports CPU tensors, with GPU support coming soon. It comes with a TCP-based transport, just like Gloo. It is also able to automatically chunk and multiplex large tensors over multiple sockets and threads in order to achieve very high bandwidths. The agent will be able to pick the best transport on its own, with no intervention required.          \n  * () – The number of threads in the thread-pool used by to execute requests (default: 16).\n  * () – The default timeout, in seconds, for RPC requests (default: 60 seconds). If the RPC has not completed in this timeframe, an exception indicating so will be raised. Callers can override this timeout for individual RPCs in and if necessary.\n  * () – The URL to initialize the distributed store used for rendezvous. It takes any value accepted for the same argument of (default: ).\n  * () – Device placement mappings from this worker to the callee. Key is the callee worker name and value the dictionary ( of , , or ) that maps this worker’s devices to the callee worker’s devices. (default: )\n  * (List[int, str, or ], optional) – all local CUDA devices used by RPC agent. By Default, it will be initialized to all local devices from its own and corresponding devices from its peers’ . When processing CUDA RPC requests, the agent will properly synchronize CUDA streams for all devices in this .\n\n    \nThe number of threads in the thread-pool used by to execute requests.     \nA float indicating the timeout to use for all RPCs. If an RPC does not complete in this timeframe, it will complete with an exception indicating that it has timed out.     \nSet device mapping between each RPC caller and callee pair. This function can be called multiple times to incrementally add device placement configurations.     \n  * () – Device placement mappings from this worker to the callee. This map must be invertible.\n\n\n```\n\n  \n      \n          \n\n\n  \n    \n      \n\n\n  \n\n\n\n    \n    \n    \n    \n    \n\n\n  \n     \n# The first argument will be moved to cuda:1 on worker1. When\n# sending the return value back, it will follow the invert of\n# the device map, and hence will be moved back to cuda:0 and\n\n  \n  \n\n```\n    \nSet local devices used by the TensorPipe RPC agent. When processing CUDA RPC requests, the TensorPipe RPC agent will properly synchronize CUDA streams for all devices in this .     \n() – local devices used by the TensorPipe RPC agent.\nThe RPC framework does not automatically retry any , and calls. The reason being that there is no way the RPC framework can determine whether an operation is idempotent or not and whether it is safe to retry. As a result, it is the application’s responsibility to deal with failures and retry if necessary. RPC communication is based on TCP and as a result failures could happen due to network failures or intermittent network connectivity issues. In such scenarios, the application needs to retry appropriately with reasonable backoffs to ensure the network isn’t overwhelmed by aggressive retries.\nAn (Remote REFerence) is a reference to a value of some type (e.g. ) on a remote worker. This handle keeps the referenced remote value alive on the owner, but there is no implication that the value will be transferred to the local worker in the future. RRefs can be used in multi-machine training by holding references to that exist on other workers, and calling the appropriate functions to retrieve or modify their parameters during training. See for more details.     \nA class encapsulating a reference to a value of some type on a remote worker. This handle will keep the referenced remote value alive on the worker. A will be deleted when 1) no references to it in both the application code and in the local RRef context, or 2) the application has called a graceful shutdown. Invoking methods on a deleted RRef leads to undefined behaviors. RRef implementation only offers best-effort error detection, and applications should not use after .\nRRefs can only be serialized and deserialized by the RPC module. Serializing and deserializing RRefs without RPC (e.g., Python pickle, torch / , JIT / , etc.) will lead to errors.     \n  * () – Python type that should be passed to compiler as type hint for .\n\n    \nFollowing examples skip RPC initialization and shutdown code for simplicity. Refer to RPC docs for those details.\n```\n\n \n   \n   \n   \n# the following RPC shares the rref with worker1, reference\n\n  \n\n```\n    \n> Runs the backward pass using the RRef as the root of the backward pass. If is provided, we perform a distributed backward pass using the provided ctx_id starting from the owner of the RRef. In this case, should be used to retrieve the gradients. If is , it is assumed that this is a local autograd graph and we only perform a local backward pass. In the local case, the node calling this API has to be the owner of the RRef. The value of the RRef is expected to be a scalar Tensor.     \n  * () – The distributed autograd context id for which we should retrieve the gradients (default: -1).\n  * () – If , the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to to run backward multiple times (default: False).\n\n    \nReturns whether this has been confirmed by the owner. always returns true, while only returns true when the owner knowns about this .     \nReturns whether or not the current node is the owner of this .     \nIf the current node is the owner, returns a reference to the local value. Otherwise, throws an exception.     \nCreate a helper proxy to easily launch a using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the creation of this is not successfully completed within the timeout, then the next time there is an attempt to use the RRef (such as ), a timeout will be raised. If not provided, the default RPC timeout will be used. Please see for specific timeout semantics for .          \nCreate a helper proxy to easily launch an using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.          \nCreate a helper proxy to easily launch an using the owner of the RRef as the destination to run functions on the object referenced by this RRef. More specifically, is the same as the following:     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout will be used.          \nBlocking call that copies the value of the RRef from the owner to the local node and returns it. If the current node is the owner, returns a reference to the local value.     \n() – Timeout for . If the call does not complete within this timeframe, an exception indicating so will be raised. If this argument is not provided, the default RPC timeout (60s) will be used.\n  *     * \n\nis an easy way to create an nn.Module remotely on a different process. The actual module resides on a remote host, but the local host has a handle to this module and invoke this module similar to a regular nn.Module. The invocation however incurs RPC calls to the remote end and can be performed asynchronously if needed via additional APIs supported by RemoteModule.     \n> A RemoteModule instance can only be created after RPC initialization.\n> It creates a user-specified module on a specified remote node. It behaves like a regular except that the method is executed on the remote node. It takes care of autograd recording to ensure the backward pass propagates gradients back to the corresponding remote module.\n> It generates two methods and based on the signature of the method of . runs asynchronously and returns a Future. The arguments of and are the same as the method of the module returned by the .\n> For example, if returns an instance of , that has method signature: , the generated will have 2 methods with the signatures:     \n  * Class for the module to be created remotely. For example,\n\n    \nA remote module instance which wraps the created by the user-provided , it has a blocking method and an asynchronous method that returns a future of the call on the user-provided module on the remote side.     \nFurthermore, a more practical example that is combined with (DDP) can be found in this .     \nDistributed autograd is not currently supported when using CUDA tensors\nThis module provides an RPC-based distributed autograd framework that can be used for applications such as model parallel training. In short, applications may send and receive gradient recording tensors over RPC. In the forward pass, we record when gradient recording tensors are sent over RPC and during the backward pass we use this information to perform a distributed backward pass using RPC. For more details see .     \nKicks off the distributed backward pass using the provided roots. This currently implements the which assumes all RPC messages sent in the same distributed autograd context across workers would be part of the autograd graph during the backward pass.\nWe use the provided roots to discover the autograd graph and compute appropriate dependencies. This method blocks until the entire autograd computation is done.\nWe accumulate the gradients in the appropriate on each of the nodes. The autograd context to be used is looked up given the that is passed in when is called. If there is no valid autograd context corresponding to the given ID, we throw an error. You can retrieve the accumulated gradients using the API.     \n  * () – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Usually, you need to set this to True to run backward multiple times.\n\n    \nContext object to wrap forward and backward passes when using distributed autograd. The generated in the statement is required to uniquely identify a distributed backward pass on all workers. Each worker stores metadata associated with this , which is required to correctly execute a distributed autograd pass.     \nRetrieves a map from Tensor to the appropriate gradient for that Tensor accumulated in the provided context corresponding to the given as part of the distributed autograd backward pass.     \nA map where the key is the Tensor and the value is the associated gradient for that Tensor.\n  * \n\nThe distributed autograd design note covers the design of the RPC-based distributed autograd framework that is useful for applications such as model parallel training.\nThe RRef design note covers the design of the (Remote REFerence) protocol used to refer to values on remote workers by the framework.\nThe RPC tutorials introduce users to the RPC framework, provide several example applications using APIs, and demonstrate how to use to profile RPC-based workloads.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/optim.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nMost commonly used methods are already supported, and the interface is general enough, so that more sophisticated ones can also be easily integrated in the future.\nTo use you have to construct an optimizer object that will hold the current state and will update the parameters based on the computed gradients.\nTo construct an you have to give it an iterable containing the parameters (all should be s) or named parameters (tuples of (str, )) to optimize. Then, you can specify optimizer-specific options such as the learning rate, weight decay, etc.\ns also support specifying per-parameter options. To do this, instead of passing an iterable of s, pass in an iterable of key, containing a list of parameters belonging to it. Other keys should match the keyword arguments accepted by the optimizers, and will be used as optimization options for this group.\nFor example, this is very useful when one wants to specify per-layer learning rates:\nThis means that ’s parameters will use a learning rate of , whereas ’s parameters will stick to the default learning rate of . Finally a momentum of will be used for all parameters.\nYou can still pass options as keyword arguments. They will be used as defaults, in the groups that didn’t override them. This is useful when you only want to vary a single option, while keeping all others consistent between parameter groups.\nAlso consider the following example related to the distinct penalization of parameters. Remember that returns an iterable that contains all learnable parameters, including biases and other parameters that may prefer distinct penalization. To address this, one can specify individual penalization weights for each parameter group:\nIn this manner, bias terms are isolated from non-bias terms, and a of is set specifically for the bias terms, as to avoid any penalization for this group.\nAll optimizers implement a method, that updates the parameters. It can be used in two ways:\nThis is a simplified version supported by most optimizers. The function can be called once the gradients are computed using e.g. .\nSome optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it.     \nParameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.     \n  * () – (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them).\n\n\nRegister a load_state_dict pre-hook which will be called before is called. It should have the following signature::.  \n---  \nRegister a load_state_dict post-hook which will be called after is called. It should have the following signature::.  \nRegister a state dict pre-hook which will be called before is called.  \nRegister a state dict post-hook which will be called after is called.  \nRegister an optimizer step pre hook which will be called before optimizer step.  \nRegister an optimizer step post hook which will be called after optimizer step.  \nImplements AdamW algorithm, where weight decay does not accumulate in the momentum nor variance.  \n---  \nSparseAdam implements a masked version of the Adam algorithm suitable for sparse gradients.  \nImplements Adamax algorithm (a variant of Adam based on infinity norm).  \nMany of our algorithms have various implementations optimized for performance, readability and/or generality, so we attempt to default to the generally fastest implementation for the current device if no particular implementation has been specified by the user.\nWe have 3 major categories of implementations: for-loop, foreach (multi-tensor), and fused. The most straightforward implementations are for-loops over the parameters with big chunks of computation. For-looping is usually slower than our foreach implementations, which combine parameters into a multi-tensor and run the big chunks of computation all at once, thereby saving many sequential kernel calls. A few of our optimizers have even faster fused implementations, which fuse the big chunks of computation into one kernel. We can think of foreach implementations as fusing horizontally and fused implementations as fusing vertically on top of that.\nIn general, the performance ordering of the 3 implementations is fused > foreach > for-loop. So when applicable, we default to foreach over for-loop. Applicable means the foreach implementation is available, the user has not specified any implementation-specific kwargs (e.g., fused, foreach, differentiable), and all tensors are native. Note that while fused should be even faster than foreach, the implementations are newer and we would like to give them more bake-in time before flipping the switch everywhere. We summarize the stability status for each implementation on the second table below, you are welcome to try them out though!\nBelow is a table showing the available and default implementations of each algorithm:\nBelow table is showing the stability status for fused implementations:\nprovides several methods to adjust the learning rate based on the number of epochs. allows dynamic learning rate reducing based on some validation measurements.\nLearning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way:\nMost learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.\nIn many places in the documentation, we will use the following template to refer to schedulers algorithms.\nPrior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling ) before the optimizer’s update (calling ), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling at the wrong time.\nMultiply the learning rate of each parameter group by the factor given in the specified function.  \n---  \nDecays the learning rate of each parameter group by gamma every step_size epochs.  \nDecays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones.  \nMultiply the learning rate of each parameter group by a small constant factor.  \nDecays the learning rate of each parameter group by linearly changing small multiplicative factor.  \nDecays the learning rate of each parameter group by gamma every epoch.  \nDecays the learning rate of each parameter group using a polynomial function in the given total_iters.  \nSet the learning rate of each parameter group using a cosine annealing schedule.  \nContains a list of schedulers expected to be called sequentially during the optimization process.  \nSets the learning rate of each parameter group according to cyclical learning rate policy (CLR).  \nSets the learning rate of each parameter group according to the 1cycle learning rate policy.  \nSet the learning rate of each parameter group using a cosine annealing schedule.  \n## How to utilize named parameters to load optimizer state dict \nThe function stores the optional content from the loaded state dict if present. However, the process of loading the optimizer state is not affected, as the order of the parameters matters to maintain compatibility (in case of different ordering). To utilize the loaded parameters names from the loaded state dict, a custom needs to be implemented according to the desired behavior.\nThis can be useful, for instance, when the model architecture changes, but the weights and optimizer states need to remain unchanged. The following example demonstrates how to implement this customization.\nLet’s say that implements an expert (MoE), and we want to duplicate it and resume training for two experts, both initialized the same way as the layer. For the following we create two layers identical to and resume training by loading the model weights and optimizer states from into both and of (and adjust them accordingly):\nTo load the state dict for with the state dict of the previous optimizer such that both and will be initialized with a copy of optimizer states (to resume training for each layer from ), we can use the following hook:\n```\n  \n      \n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n        \n             \n              \n\n      \n         \n         \n         \n         \n    \n                   \n        \n            \n            \n          \n          \n          \n        \n           \n              \n\n     \n\n\n \n\n```\n\nThis ensures that the adapted state_dict with the correct states for the layers of will be used during model loading. Note that this code is designed specifically for this example (e.g., assuming a single parameter group), and other cases might require different adaptations.\nThe following example shows how to handle missing parameters in a loaded when the model structure changes. The adds a new layer, which is not present in the original . To resume training, a custom hook is used to adapt the optimizer’s , ensuring existing parameters are mapped correctly, while missing ones (like the bypass layer) remain unchanged (as initialized in this example). This approach enables smooth loading and resuming of the optimizer state despite model changes. The new bypass layer will be trained from scratch:\n```\n \n     \n        \n           \n\n      \n           \n\n\n  \n    \n\n \n\n \n     \n        \n           \n            \n        \n\n      \n           \n\n  \n    \n\n  \n      \n    # Copy setup parameters (lr, weight_decay, etc.), in case they differ in the loaded state dict.\n        \n             \n              \n\n      \n         \n         \n         \n    \n\n                   \n        \n            \n            \n          \n           \n              \n              \n            \n               \n                  \n\n     \n\n\n \n\n```\n\nAs a third example, instead of loading a state according to the order of parameters (the default approach), this hook can be used to load according to the parameters’ names:\nimplements Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA), implements the SWA learning rate scheduler and is a utility function used to update SWA/EMA batch normalization statistics at the end of training.\nEMA is a widely known technique to reduce the training time by reducing the number of weight updates needed. It is a variation of \nThe class serves to compute the weights of the SWA or EMA model.\nDecay is a parameter between 0 and 1 that controls how fast the averaged parameters are decayed. If not provided to , the default is 0.999. Decay value should be close to 1.0, as smaller values can cause optimization convergence issues.\nreturns a function that applies the following EMA equation to the weights:\nHere the model can be an arbitrary object. will keep track of the running averages of the parameters of the . To update these averages, you should use the function after the :\nFor SWA and EMA, this call is usually done right after the optimizer . In the case of SWA, this is usually skipped for some numbers of steps at the beginning of the training.\nBy default, computes a running equal average of the parameters that you provide, but you can also use custom averaging functions with the or parameters:\n  * allows defining a function operating on each parameter tuple (averaged parameter, model parameter) and should return the new averaged parameter.\n  * allows defining more efficient operations acting on a tuple of parameter lists, (averaged parameter list, model parameter list), at the same time, for example using the functions. This function must update the averaged parameters in-place.\n\n\nIn the following example computes an exponential moving average using the parameter:\nIn the following example computes an exponential moving average using the more efficient parameter:\nTypically, in SWA the learning rate is set to a high constant value. is a learning rate scheduler that anneals the learning rate to a fixed value, and then keeps it constant. For example, the following code creates a scheduler that linearly anneals the learning rate from its initial value to 0.05 in 5 epochs within each parameter group:\nYou can also use cosine annealing to a fixed value instead of linear annealing by setting .\nis a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloader at the end of training:\napplies the to every element in the dataloader and computes the activation statistics for each batch normalization layer in the model.\nassumes that each batch in the dataloader is either a tensors or a list of tensors where the first element is the tensor that the network should be applied to. If your dataloader has a different structure, you can update the batch normalization statistics of the by doing a forward pass with the on each element of the dataset.\nIn the example below, is the SWA model that accumulates the averages of the weights. We train the model for a total of 300 epochs and we switch to the SWA learning rate schedule and start to collect SWA averages of the parameters at epoch 160:\n```\n     \n  \n   \n  \n   \n\n   \n          \n          \n           \n          \n         \n          \n          \n      \n          \n\n# Update bn statistics for the swa_model at the end\n \n\n  \n\n```\n\nIn the example below, is the EMA model that accumulates the exponentially-decayed averages of the weights with a decay rate of 0.999. We train the model for a total of 300 epochs and start to collect EMA averages immediately.\n```\n     \n   \\\n            \n\n   \n          \n          \n           \n          \n          \n\n# Update bn statistics for the ema_model at the end\n \n\n  \n\n```\n\nImplements averaged model for Stochastic Weight Averaging (SWA) and Exponential Moving Average (EMA).  \n---  \nAnneals the learning rate in each parameter group to a fixed value.       \nGet the function applying exponential moving average (EMA) across multiple params.     \nIt performs one pass over data in to estimate the activation statistics for BatchNorm layers in the model.     \n  * () – dataset loader to compute the activation statistics on. Each data batch should be either a tensor, or a list/tuple whose first element is a tensor containing data.\n  * () – model for which we seek to update BatchNorm statistics.\n  * () – If set, data will be transferred to before being passed into .\n\n\nThe utility assumes that each data batch in is either a tensor or a list or tuple of tensors; in the latter case it is assumed that should be called on the first element of the list or tuple corresponding to the data batch.\n  *     *     * [How to utilize named parameters to load optimizer state dict](https://docs.pytorch.org/docs/stable/optim.html#how-to-utilize-named-parameters-to-load-optimizer-state-dict)\n    * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/package.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nadds support for creating packages containing both artifacts and arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using .\nThis document contains tutorials, how-to guides, explanations, and an API reference that will help you learn more about and how to use it.\nThis module depends on the module which is not secure. Only unpackage data you trust.\nIt is possible to construct malicious pickle data which will . Never unpackage data that could have come from an untrusted source, or that could have been tampered with.\n  *     * [See why a given module was included as a dependency?](https://docs.pytorch.org/docs/stable/package.html#see-why-a-given-module-was-included-as-a-dependency)\n    * [Include arbitrary resources with my package and access them later?](https://docs.pytorch.org/docs/stable/package.html#include-arbitrary-resources-with-my-package-and-access-them-later)\n    * [Test in my source code whether or not it is executing inside a package?](https://docs.pytorch.org/docs/stable/package.html#test-in-my-source-code-whether-or-not-it-is-executing-inside-a-package)\n  * \n\nA tutorial that guides you through packaging and unpackaging a simple model is available \nThe container format for a is ZIP, so any tools that work with standard ZIP files should work for exploring the contents. Some common ways to interact with ZIP files:\n  * will unzip the archive to disk, where you can freely inspect its contents.\n\n\n  * The Python module provides a standard way to read and write ZIP archive contents.\n\n\n  * vim has the ability to natively read ZIP archives. You can even edit files and : them back into the archive!\n\n\n```\n# add this to your .vimrc to treat `*.pt` files as zip files\n    \n\n  \n\n```\n\nprovides a method, which will return a printable and queryable object. The object is a simple directory structure that you can use to explore the current contents of a .\nThe object itself is directly printable and will print out a file tree representation. To filter what is returned, use the glob-style and filtering arguments.\n```\n   \n      \n\n  \n\n  \n \n\n```\n\n###  [See why a given module was included as a dependency?](https://docs.pytorch.org/docs/stable/package.html#id5)\nSay there is a given module , and you want to know why your is pulling in as a dependency.\nIf you would like to see how a given module depends on , the method will return a DOT-formatted graph showing all the dependency paths between and .\nIf you would just like to see the whole dependency graph of your , you can use .\n###  [Include arbitrary resources with my package and access them later?](https://docs.pytorch.org/docs/stable/package.html#id6)\nexposes three methods, , and that allow you to save Python objects, text, and binary data to a package.\n```\n   \n    # Pickles the object and saves to `my_resources/tensor.pkl` in the archive.\n      \n      \n      \n\n```\n\nexposes complementary methods named , and that allow you to load Python objects, text and binary data from a package.\nallows for the customization of how classes are packaged. This behavior is accessed through defining the method on a class and by defining a corresponding de-packaging function. This is similar to defining for Python’s normal pickling process.\n  1. Define the method on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the when it encounters an instance of the target class.\n  2. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature’s first parameter should be a instance, and the rest of the parameters are user defined.\n\n\n```\n# foo.py [Example of customizing how class Foo is packaged]\n    \n \n\n\n \n       \n        \n          \n          \n          \n\n       \n\n\n        saving an instance of this object. This method should do the work to save this\n\n\n        Returns function w/ arguments to load the object from a\n\n\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        \n          \n        \n            \n            \n              \n        \n          \n\n        \n           \n\n\n \n         \n  \n\n\n\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n\n      \n       \n      \n      \n     \n\n```\n\n```\n\n\n \n    \n \n\n  \n  \n   \n    \n      \n      \n\n  \n\n   \n\n\n\n\n```\n\n###  [Test in my source code whether or not it is executing inside a package?](https://docs.pytorch.org/docs/stable/package.html#id8)\nA will add the attribute to every module that it initializes. Your code can check for the presence of this attribute to determine whether it is executing in a packaged context or not.\n```\n\n\n     # true if the code is being loaded from a package\n     \n         \n\n      \n\n     \n         \n\n      \n\n```\n\nNow, the code will behave differently depending on whether it’s imported normally through your Python environment or imported from a .\n: in general, it’s bad practice to have code that behaves differently depending on whether it’s packaged or not. This can lead to hard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring your code so that it behaves the same way no matter how it was loaded.\noffers a method that allows one to save arbitrary Python source code to a module of your choosing.\n```\n   \n    # Save the my_module.foo available in your current Python environment.\n    \n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n     \n\n\n\n\n    \n\n    # If you want to treat my_module.bar as a package\n    \n    \n    \n                                \n                                \n\n  \n  \n\n```\n\n```\n   \n    \n      \n    \n       \n\n    \n    \n    \n\n```\n\nUsing is the recommended way to access package contents from within packaged code, since it complies with the Python standard. However, it is also possible to access the parent instance itself from within packaged code.\n```\n\n  \n\n\n \n      \n\n# You also do things that the importlib.resources API does not support, like loading\n\n \n      \n\n```\n\nTo tell if an object’s code is from a , use the function. Note: if an object is from a package but its definition is from a module marked or from , this check will return .\n```\n  \n  \n   \n   \n\n \n \n   # str is from stdlib, so this will return False\n\n```\n\nTo re-export an object that was previously imported by a , you must make the new aware of the original so that it can find source code for your object’s dependencies.\nTo package a TorchScript model, use the same and APIs as you would with any other object. Saving TorchScript objects that are attributes or submodules is supported as well with no extra work.\n```\n\n   \n      \n      \n\n  \n   \n   \n\n```\n\nA file is a ZIP archive which conventionally uses the extension. Inside the ZIP archive, there are two kinds of files:\n\n\nAs an example, this is what a fully packaged ResNet model from looks like:\nThe directory is owned by torch.package, and its contents are considered to be a private implementation detail. The format makes no guarantees about the contents of , but any changes made will be backward compatible (that is, newer version of PyTorch will always be able to load older ).\n  * : a version number for the serialized format, so that the import infrastructures knows how to load this package.\n  * : a list of modules that are considered . modules will be imported using the loading environment’s system importer.\n\n\nAll other files in the archive were put there by a user. The layout is identical to a Python \nWhen you issue a call, will pickle the object normally. Then, it uses the standard library module to parse the pickle bytecode.\nIn a pickle, an object is saved along with a opcode that describes where to find the implementation of the object’s type, like:\nThe dependency resolver will gather up all ops and mark them as dependencies of your pickled object. For more information about pickling and the pickle format, please consult \nWhen a Python module is identified as a dependency, walks the module’s python AST representation and looks for import statements with full support for the standard forms: , , , etc. When one of these import statements are encountered, registers the imported modules as dependencies that are then themselves parsed in the same AST walking way.\n: AST parsing has limited support for the syntax and does not support calls. In general, you should not expect dynamic imports to be detected by .\nautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution. For each module that the dependency resolver finds, you must specify an to take.\n  * : declare this module as an external dependency of the package.\n  * : depending on this module will raise an error during package export.\n\n\nFinally, there is one more important action that is not technically part of :\nNote that actions are only defined on entire Python modules. There is no way to package “just” a function or class from a module and leave the rest out. This is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a module, so that’s what uses.\nActions are applied to modules using patterns. Patterns can either be module names () or globs (like ). You associate a pattern with an action using methods on , e.g.\nIf a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined, and the first action will be taken.\nIf a module is -ed, it will be placed into the package.\nThis action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet from , you will need to the module torchvision.models.resnet.\nOn package import, when your packaged code tries to import an -ed module, PackageImporter will look inside your package for that module. If it can’t find that module, an error will be raised. This ensures that each is isolated from the loading environment—even if you have available in both your package and the loading environment, will only use the version in your package.\n: Only Python source modules can be -ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if you attempt to them. These kinds of modules need to be -ed or -ed.\nIf a module is -ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this list on .\nOn package import, when the packaged code tries to import an -ed module, will use the default Python importer to find that module, as if you did . If it can’t find that module, an error will be raised.\nIn this way, you can depend on third-party libraries like and from within your package without having to package them too.\n: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility for your package, try to limit your use of .\nIf a module is -ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve objects from it (so that will not error), but any use of that object will raise a .\nshould be used for code that you “know” will not be needed in the loaded package, but you still want available for use in non-packaged contents. For example, initialization/configuration code, or code only used for debugging/training.\n: In general, should be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code, which may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.\nThe best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some guidelines for writing code with clean dependencies (which are also generally good practices!):\n. Do not leave unused imports in your code. The dependency resolver is not smart enough to tell that they are indeed unused, and will try to process them.\n. For example, instead of writing import foo and later using , prefer to write . This more precisely specifies your real dependency () and lets the dependency resolver know you don’t need all of .\n**Split up large files with unrelated functionality into smaller ones**. If your module contains a hodge-podge of unrelated functionality, any module that depends on will need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define single-purpose modules that can be packaged independently of one another.\nPatterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buck \nA module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a separator string, e.g. .\nA pattern contains one or more segments. Segments can be:\n  * A string containing a wildcard (e.g. , or ). The wildcard matches any string, including the empty string.\n  * A double wildcard (). This matches against zero or more complete segments.\n\n\n\nA module will match against this action if it matches any of the patterns.\nA module will not match against this action if it matches any of the exclude patterns. In this example, we are mocking all modules except and its submodules.\nWhen a module could potentially match against multiple actions, the first action defined will be taken.\nPython makes it really easy to bind objects and run code at module-level scope. This is generally fine—after all, functions and classes are bound to names this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable global state.\nMutable global state is quite useful—it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can cause complications when used with .\nEvery creates an independent environment for its contents. This is nice because it means we load multiple packages and ensure they are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug errors.\n#### Types are not shared between packages and the loading environment \nAny class that you import from a will be a version of the class specific to that importer. For example:\nIn this example, and are . In this specific example, and have exactly the same implementation, so you might think it’s okay to consider them the same class. But consider the situation where is coming from an older package with an entirely different implementation of — in that case, it’s unsafe to consider them the same class.\nUnder the hood, each importer has a prefix that allows it to uniquely identify classes:\nThat means you should not expect checks to work when one of the arguments is from a package and the other is not. If you need this functionality, consider the following options:\n  * Doing duck typing (just using the class instead of explicitly checking that it is of a given type).\n  * Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tag and have client code check for the value of instead of checking the type directly.\n\n\nEach instance creates an independent, isolated environment for its modules and objects. Modules in a package can only import other packaged modules, or modules marked . If you use multiple instances to load a single package, you will get multiple independent environments that do not interact.\nThis is achieved by extending Python’s import infrastructure with a custom importer. provides the same core API as the importer; namely, it implements the and methods.\nWhen you invoke , will construct and return a new module, much as the system importer does. However, patches the returned module to use (i.e. that instance) to fulfill future import requests by looking in the package rather than searching the user’s Python environment.\nTo avoid confusion (“is this object the one from my package, or the one from my Python environment?”), mangles the and of all imported modules, by adding a to them.\nName mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print statements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consult in .     \nThis exception is raised when there is an issue with exporting a package. will attempt to gather up all the errors and present them to you at once.     \nThis is an exception that is thrown when a mock or extern is marked as , and is not matched with any module during packaging.     \nExporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into a self-contained package.\nImports can load this code in a hermetic way, such that code is loaded from the package rather than the normal Python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.\nThe code contained in packages is copied file-by-file from the original source when it is created, and the file format is a specially organized zip file. Future users of the package can unzip the package, and edit the code in order to perform custom modifications to it.\nThe importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external using . The file in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.\nWhen source code is added to the package, the exporter can optionally scan it for further code dependencies (). It looks for import statements, resolves relative references to qualified module names, and performs an action specified by the user (See: , , and ).          \n  * () – The location to export to. Can be a / object containing a filename or a binary I/O object.\n  * () – If a single Importer is passed, use that to search for modules. If a sequence of importers are passed, an will be constructed out of them.\n\n    \nGiven a module, add it to the dependency graph according to patterns specified by the user.          \nA dot representation containing all paths from src to dst. (     \nWrite the package to the filesystem. Any calls after are now invalid. It is preferable to use resource guard syntax instead:          \nA list containing the names of modules which will be denied in this package.     \nBlocklist modules who names match the given glob patterns from the list of modules the package can import. If a dependency on any matching packages is found, a is raised.     \n  * () – A string e.g. , or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in .\n  * () – An optional pattern that excludes some patterns that match the include string.\n\n         \nInclude in the list of external modules the package can import. This will prevent dependency discovery from saving it in the package. The importer will load an external module directly from the standard import system. Code for extern modules must also exist in the process loading the package.     \n  * () – A string e.g. , or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in .\n  * () – An optional pattern that excludes some patterns that match the include string.\n  * ( method must be matched to some module during packaging. If an extern module glob pattern is added with , and is called (either explicitly or via ) before any modules match that pattern, an exception is thrown. If , no such exception is thrown.\n\n         \nA list containing the names of modules which will be externed in this package.     \nReturn a list of all modules which depend on the module .     \nA list containing the names of modules which depend on .     \nGet an id. This id is guaranteed to only be handed out once for this package.     \nSpecify modules that should be packaged. A module must match some pattern in order to be included in the package and have its dependencies processed recursively.     \n  * () – A string e.g. “my_package.my_subpackage”, or list of strings for the names of the modules to be externed. This can also be a glob-style pattern, as described in .\n  * () – An optional pattern that excludes some patterns that match the include string.\n  * ( method must be matched to some module during packaging. If an module glob pattern is added with , and is called (either explicitly or via ) before any modules match that pattern, an exception is thrown. If , no such exception is thrown.\n\n         \nA list containing the names of modules which will be interned in this package.     \nReplace some required modules with a mock implementation. Mocked modules will return a fake object for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes find files that are imported by model files but whose functionality is never used (e.g. custom serialization code or training helpers). Use this function to mock this functionality out without having to modify the original code.     \n  * A string e.g. , or list of strings for the names of the modules to be mocked out. Strings can also be a glob-style pattern string that may match multiple modules. Any required dependencies that match this pattern string will be mocked out automatically.     \n  * () – An optional pattern that excludes some patterns that match the include string. e.g. will mock all torch packages except , Default: is .\n  * ( method must be matched to some module during packaging. If a mock is added with , and is called (either explicitly or via ) and the mock has not been matched to a module used by the package being exported, an exception is thrown. If , no such exception is thrown.\n\n         \nA list containing the names of modules which will be mocked in this package.     \nThe hook will be called each time a module matches against an pattern. It should have the following signature:     \nA handle that can be used to remove the added hook by calling .     \nThe hook will be called each time a module matches against an pattern. It should have the following signature:     \nA handle that can be used to remove the added hook by calling .     \nThe hook will be called each time a module matches against a pattern. It should have the following signature:     \nA handle that can be used to remove the added hook by calling .     \nSave the code for into the package. Code for the module is resolved using the path to find the module object, and then using its attribute to find the source code.     \n  * (, code will be saved to provide code for this package.\n\n    \nSave a python object to the archive using pickle. Equivalent to but saving into the archive rather than a stand-alone file. Standard pickle does not save the code, only the objects. If is true, this method will also scan the pickled objects for which modules are required to reconstruct them and save the relevant code.\nTo be able to save an object where is , must resolve to the class of the object according to the order. When saving objects that have previously been packaged, the importer’s method will need to be present in the list for this to work.     \n\n    \nAdds the local file system to the source package to provide the code for .     \n  * (, code will be saved to provide code for this package.\n  * (. If a file is named the code is treated as a package.\n\n         \n  * (, code will be saved to provide code for this package.\n  * () – If , this module is treated as a package. Packages are allowed to have submodules (e.g. ), and resources can be saved inside them. Defaults to .\n\n    \nImporters allow you to load code written to packages by . Code is loaded in a hermetic way, using files from the package rather than the normal python import system. This allows for the packaging of PyTorch model code and data so that it can be run on a server or used in the future for transfer learning.\nThe importer for packages ensures that code in the module can only be loaded from within the package, except for modules explicitly listed as external during export. The file in the zip archive lists all the modules that a package externally depends on. This prevents “implicit” dependencies where the package runs locally because it is importing a locally-installed package, but then fails when the package is copied to another machine.     \nOpen for importing. This checks that the imported package only requires modules allowed by      \n  * () – a file-like object (has to implement , , , and ), a string, or an object containing a filename.\n  * () – A method to determine if a externally provided module should be allowed. Can be used to ensure packages loaded do not depend on modules that the server does not support. Defaults to allowing anything.\n\n         \n  * () – An optional string e.g. , or optional list of strings for the names of the files to be included in the zipfile representation. This can also be a glob-style pattern, as described in \n  * () – An optional pattern that excludes files whose name match the pattern.\n\n         \nLoad a module from the package if it hasn’t already been loaded, and then return the module. Modules are loaded locally to the importer and will appear in rather than .     \n  * () – Unused, but present to match the signature of importlib.import_module. Defaults to .\n\n    \nUnpickles the resource from the package, loading any modules that are needed to construct the objects using .     \n  * – Passed to to determine how tensors are mapped to devices. Defaults to .\n\n         \n\n    \nReturns the version of python that was used to create this package.\nNote: this function is experimental and not Forward Compatible. The plan is to move this into a lock file later on.     \na python version e.g. 3.8.9 or None if no version was stored with this package     \nA file structure representation. Organized as Directory nodes that have lists of their Directory children. Directories for a package are created by calling .     \n  *     *       *       * [See why a given module was included as a dependency?](https://docs.pytorch.org/docs/stable/package.html#see-why-a-given-module-was-included-as-a-dependency)\n      * [Include arbitrary resources with my package and access them later?](https://docs.pytorch.org/docs/stable/package.html#include-arbitrary-resources-with-my-package-and-access-them-later)\n      * [Test in my source code whether or not it is executing inside a package?](https://docs.pytorch.org/docs/stable/package.html#test-in-my-source-code-whether-or-not-it-is-executing-inside-a-package)\n    *       *       *         * [Types are not shared between packages and the loading environment](https://docs.pytorch.org/docs/stable/package.html#types-are-not-shared-between-packages-and-the-loading-environment)\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/size.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nis the result type of a call to . It describes the size of all dimensions of the original tensor. As a subclass of                \nReturns the number of elements a with the given size would contain.\nThis function does not return the number of dimensions described by , but instead the number of elements a with that size would contain.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.overrides.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis module exposes various helper functions for the protocol. See for more details on the protocol.          \nA tuple of functions that are publicly available in the torch API but cannot be overridden with . Mostly this is because none of the arguments of these functions are tensors or tensor-likes.          \nA dictionary that maps namespaces that contain overridable functions to functions in that namespace that can be overridden.     \nGet a human readable string name for a function passed to __torch_function__     \nName of the function; if eval’ed it should give back the input function.     \nReturn a dict containing dummy overrides for all overridable functions     \nA dictionary that maps overridable functions in the PyTorch API to lambda functions that have the same signature as the real function and unconditionally return -1. These lambda functions are useful for testing API coverage for a type that defines .     \nSee torch::autograd::handle_torch_function for the equivalent of this function in the C++ implementation.     \n  * () – Function exposed by the public torch API originally called like on which arguments are now being checked.\n  * () – Iterable of arguments to check for __torch_function__ methods.\n\n    \nCheck for __torch_function__ implementations in the elements of an iterable or if a __torch_function__ mode is enabled. Considers exact s and s non-dispatchable. Use this to guard a call to ; don’t use it to test if something is Tensor-like, use instead. :param relevant_args: Iterable or arguments to check for __torch_function__ methods. :type relevant_args: iterable     \nTrue if any of the elements of relevant_args have __torch_function__ implementations, False otherwise.     \nCurrently, this occurs whenever there’s a attribute on the type of the input.     \nReturns True if the function passed in is a handler for a method or property belonging to , as passed into .\nThis may be needed, in particular, for the following reasons:\n  1. They require that the first passed-in argument is an instance of .\n\n         \n() – A callable that returns an iterable of Tensor-likes passed into the function.\nThis decorator may reduce the performance of your code. Generally, it’s enough to express your code as a series of functions that, themselves, support __torch_function__. If you find yourself in the rare situation where this is not the case, e.g. if you’re wrapping a low-level library and you also need it to work for Tensor-likes, then this function is available.\n```\n   \n     \n\n   \n       \n\n```\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.compiler.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nis a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is .\nis a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. is written in Python and it marks the transition of PyTorch from C++ to Python.\n  * is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the namespace.\n  * is the default deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through possible. For NVIDIA, AMD and Intel GPUs, it leverages OpenAI Triton as the key building block.\n  * captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor.\n\n\nIn some cases, the terms , TorchDynamo, might be used interchangeably in this documentation.\nAs mentioned above, to run your workflows faster, through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as , TorchDynamo has a list of supported backends developed by our partners, which can be see by running each of which with its optional dependencies.\nUses Torch-TensorRT for inference optimizations. Requires in the calling script to register backend.   \n---  \n\n\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch_environment_variables.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch leverages environment variables for adjusting various settings that influence its runtime behavior. These variables offer control over key functionalities, such as displaying the C++ stack trace upon encountering errors, synchronizing the execution of CUDA kernels, specifying the number of threads for parallel processing tasks and many more.\nMoreover, PyTorch leverages several high-performance libraries, such as MKL and cuDNN, which also utilize environment variables to modify their functionality. This interplay of settings allows for a highly customizable development environment that can be optimized for efficiency, debugging, and computational resource management.\nPlease note that while this documentation covers a broad spectrum of environment variables relevant to PyTorch and its associated libraries, it is not exhaustive. If you find anything in this documentation that is missing, incorrect, or could be improved, please let us know by filing an issue or opening a pull request.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/testing.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nIf and are strided, non-quantized, real-valued, and finite, they are considered close if\n\\lvert \\text{actual} - \\text{expected} \\rvert \\le \\texttt{atol} + \\texttt{rtol} \\cdot \\lvert \\text{expected} \\rvert\nNon-finite values ( and ) are only considered close if and only if they are equal. ’s are only considered equal to each other if is .\nIn addition, they are only considered close if they have the same\nIf either or is a meta tensor, only the attribute checks will be performed.\nIf and are sparse (either having COO, CSR, CSC, BSR, or BSC layout), their strided members are checked individually. Indices, namely for COO, and for CSR and BSR, or and for CSC and BSC layouts, respectively, are always checked for equality whereas the values are checked for closeness according to the definition above.\nIf and are quantized, they are considered close if they have the same and the result of is close according to the definition above.\nand can be ’s or any tensor-or-scalar-likes from which ’s can be constructed with . Except for Python scalars the input types have to be directly related. In addition, and can be \nPython scalars are an exception to the type relation requirement, because their , i.e. of a tensor-like. Thus, Python scalars of different types can be checked, but require .     \n  * ( (default) and except for Python scalars, inputs of directly related types are allowed. Otherwise type equality is required.\n  * () – Relative tolerance. If specified must also be specified. If omitted, default values based on the are selected with the below table.\n  * () – Absolute tolerance. If specified must also be specified. If omitted, default values based on the are selected with the below table.\n  * ( (default), asserts that corresponding tensors are on the same . If this check is disabled, tensors on different ’s are moved to the CPU before being compared.\n  * ( (default), asserts that corresponding tensors have the same . If this check is disabled, tensors with different ’s are promoted to a common (according to ) before being compared.\n  * ( (default), asserts that corresponding tensors have the same . If this check is disabled, tensors with different ’s are converted to strided tensors before being compared.\n  * ( and corresponding tensors are strided, asserts that they have the same stride.\n  * () – Optional error message to use in case a failure occurs during the comparison. Can also passed as callable in which case it will be called with the generated message and should return the new message.\n\n    \n  * is , but corresponding inputs are not Python scalars and have different types.\n  * is , but corresponding strided tensors do not have the same stride.\n\n\nThe following table displays the default and for different ’s. In case of mismatching ’s, the maximum of both tolerances is used.\nis highly configurable with strict default settings. Users are encouraged to that uses zero tolerances for every by default:\n```\n \n    \n \n\n\n: \n\n\n\n\n\n```\n\n```\n\n   \n# The types of the sequences do not have to match. They only have to have the same\n\n    \n  \n \n\n```\n\n```\n\n   \n   \n  \n  \n  \n# The types and a possible ordering of mappings do not have to match. They only\n# have to have the same set of keys and their elements have to match.\n       \n       \n \n\n```\n\n```\n    \n  \n\n \n# This check can be made more strict with allow_subclasses=False\n\n      \n\n\n\n: No comparison pair was able to handle inputs of type\n\n# If the inputs are not directly related, they are never considered close\n \n\n\n: No comparison pair was able to handle inputs of type <class 'numpy.ndarray'>\n\n# Exceptions to these rules are Python scalars. They can be checked regardless of\n\n  \n\n```\n\n```\n\n  \n  \n \n\n\n: \n\n\n\n\n  \n\n```\n\n```\n    \n    \n\n  \n\n\n: \n# If msg is a callable, it can be used to augment the generated message with\n\n\n        \n\n\n\n: \n\n\n\n\nGreatest absolute difference: 2.0 at index (1,) (up to 1e-05 allowed)\nGreatest relative difference: 1.0 at index (1,) (up to 1.3e-06 allowed)\n\n\n\n```\n    \nCreates a tensor with the given , , and , and filled with values uniformly drawn from .\nIf or are specified and are outside the range of the ’s representable finite values then they are clamped to the lowest or highest representable finite value, respectively. If , then the following table describes the default values for and , which depend on .     \n  * () – Single integer or a sequence of integers defining the shape of the output tensor.\n  * () – Sets the lower limit (inclusive) of the given range. If a number is provided it is clamped to the least representable finite value of the given dtype. When (default), this value is determined based on the (see the table above). Default: .\n  * Sets the upper limit (exclusive) of the given range. If a number is provided it is clamped to the greatest representable finite value of the given dtype. When (default) this value is determined based on the (see the table above). Default: .\nPassing to for floating or complex types is deprecated since 2.1 and will be removed in 2.3. Use instead.\n  * () – If autograd should record operations on the returned tensor. Default: .\n  * () – If , the returned tensor will be noncontiguous. This argument is ignored if the constructed tensor has fewer than two elements. Mutually exclusive with .\n  * () – If then zeros are replaced with the dtype’s small positive value depending on the . For bool and integer types zero is replaced with one. For floating point types it is replaced with the dtype’s smallest positive normal number (the “tiny” value of the ’s object), and for complex types it is replaced with a complex number whose real and imaginary parts are both the smallest positive normal number representable by the complex type. Default .\n  * () – The memory format of the returned tensor. Mutually exclusive with .\n\n\n```\n   \n# Creates a float tensor with values in [-1, 1)\n    \n\n\n   \n\n\n\n```\n    \nis deprecated since and will be removed in a future release. Please use instead. You can find detailed upgrade instructions \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/storage.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIn PyTorch, a regular tensor is a multi-dimensional array that is defined by the following components:\n  * Storage: The actual data of the tensor, stored as a contiguous, one-dimensional array of bytes.\n  * : The data type of the elements in the tensor, such as torch.float32 or torch.int64.\n  * : A tuple indicating the size of the tensor in each dimension.\n  * Stride: The step size needed to move from one element to the next in each dimension.\n  * Offset: The starting point in the storage from which the tensor data begins. This will usually be 0 for newly created tensors.\n\n\nThese components together define the structure and data of a tensor, with the storage holding the actual data and the rest serving as metadata.\nA is a contiguous, one-dimensional array of elements. Its length is equal to the number of bytes of the tensor. The storage serves as the underlying data container for tensors. In general, a tensor created in PyTorch using regular constructors such as , or will produce tensors where there is a one-to-one correspondence between the tensor storage and the tensor itself.\nHowever, a storage is allowed to be shared by multiple tensors. For instance, any view of a tensor (obtained through or some, but not all, kinds of indexing like integers and slices) will point to the same underlying storage as the original tensor. When serializing and deserializing tensors that share a common storage, the relationship is preserved, and the tensors continue to point to the same storage. Interestingly, deserializing multiple tensors that point to a single storage can be faster than deserializing multiple independent tensors.\nA tensor storage can be accessed through the method. This will return an object of type . Fortunately, storages have a unique identifier called accessed through the method. In regular settings, two tensors with the same data storage will have the same storage . However, tensors themselves can point to two separate storages, one for its data attribute and another for its grad attribute. Each will require a of its own. In general, there is no guarantee that a and match and this should not be assumed to be true.\nUntyped storages are somewhat independent of the tensors that are built on them. Practically, this means that tensors with different dtypes or shape can point to the same storage. It also implies that a tensor storage can be changed, as the following example shows:\n```\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n```\n\nPlease note that directly modifying a tensor’s storage as shown in this example is not a recommended practice. This low-level manipulation is illustrated solely for educational purposes, to demonstrate the relationship between tensors and their underlying storages. In general, it’s more efficient and safer to use standard methods, such as and , to achieve the same results.\nOther than , untyped storage also have other attributes such as (in case the storage points to a file on disk), or for device checks. A storage can also be manipulated in-place or out-of-place with methods like , or . FOr more information, check the API reference below. Keep in mind that modifying storages is a low-level API and comes with risks! Most of these APIs also exist on the tensor level: if present, they should be prioritized over their storage counterparts.\nWe mentioned that a tensor that has a non-None attribute has actually two pieces of data within it. In this case, will return the storage of the attribute, whereas the storage of the gradient can be obtained through .\n```\n   \n\n       \n       \n\n```\n\n\nThere are also special cases where tensors do not have a typical storage, or no storage at all:\n    \n  * Tensors on device: Tensors on the device are used for shape inference and do not hold actual data.\n  * Fake Tensors: Another internal tool used by PyTorch’s compiler is which is based on a similar idea.\n\n\nTensor subclasses or tensor-like objects can also display unusual behaviours. In general, we do not expect many use cases to require operating at the Storage level!          \nReturn a CPU copy of this storage if it’s not already on the CPU.     \nIf this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.     \n  * ( and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.\n\n    \nThe file name will be a string if the storage is on CPU and was created via with as . This attribute is otherwise.     \nIf is , then memory is shared between all processes. All changes are written to the file. If is , then the changes on the storage do not affect the file.\nis the number of elements in the storage. If is , then the file must contain at least bytes ( is the type of storage, in the case of an the file must contain at least bytes). If is the file will be created if needed.     \nIf this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.     \n  * ( and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.\n\n    \nDetermine whether the CPU storage is already pinned on device.     \n() – The device to pin memory on (default: ). This argument is discouraged and subject to deprecated.     \nReturn a MPS copy of this storage if it’s not already on the MPS.     \nCopy the CPU storage to pinned memory, if it’s not already pinned.     \n() – The device to pin memory on (default: ). This argument is discouraged and subject to deprecated.     \nThis is a no-op for storages already in shared memory and for CUDA storages, which do not need to be moved for sharing across processes. Storages in shared memory cannot be resized.\nNote that to mitigate issues like for more details.\nWhen all references to a storage in shared memory are deleted, the associated shared memory object will also be deleted. PyTorch has a special cleanup process to ensure that this happens even if the current process exits unexpectedly.\n  1. Both use an to map the file/object into the current virtual address space\n  2. will call on the object after mapping it to make sure the shared memory object is freed when no process has the object open. does not unlink the file. This file is persistent and will remain until it is deleted by the user.\n\n\nFor historical context, PyTorch previously used typed storage classes, which are now deprecated and should be avoided. The following details this API in case you should encounter it, although its usage is highly discouraged. All storage classes except for will be removed in the future, and will be used in all cases.\nis an alias for the storage class that corresponds with the default data type (). For example, if the default data type is , resolves to .\nThe and classes, like , , etc., are not actually ever instantiated. Calling their constructors creates a with the appropriate and . classes have all of the same class methods that has.\nA is a contiguous, one-dimensional array of elements of a particular . It can be given any , and the internal data will be interpreted appropriately. contains a which holds the data as an untyped array of bytes.\nEvery strided contains a , which stores all of the data that the views.          \nReturn a CPU copy of this storage if it’s not already on the CPU.     \nIf this object is already in CUDA memory and on the correct device, then no copy is performed and the original object is returned.     \n  * ( and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.\n\n    \nReturns the file name associated with this storage if the storage was memory mapped from a file. or if the storage was not created by memory mapping a file.     \nIf is , then memory is shared between all processes. All changes are written to the file. If is , then the changes on the storage do not affect the file.\nis the number of elements in the storage. If is , then the file must contain at least bytes ( is the type of storage). If is the file will be created if needed.     \nIf this object is already in HPU memory and on the correct device, then no copy is performed and the original object is returned.     \n  * ( and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.\n\n    \nDetermine whether the CPU TypedStorage is already pinned on device.     \n() – The device to pin memory on (default: ). This argument is discouraged and subject to deprecated.     \nCopy the CPU TypedStorage to pinned memory, if it’s not already pinned.     \n() – The device to pin memory on (default: ). This argument is discouraged and subject to deprecated.     \nIf this object is already on the correct device, then no copy is performed and the original object is returned.     \n  * ( and the source is in pinned memory, the copy will be asynchronous with respect to the host. Otherwise, the argument has no effect.\n\n    \nReturns the type if is not provided, else casts this object to the specified type.\nIf this is already of the correct type, no copy is performed and the original object is returned.     \n  * (, and the source is in pinned memory and destination is on the GPU or vice versa, the copy is performed asynchronously with respect to the host. Otherwise, the argument has no effect.\n  * – For compatibility, may contain the key in place of the argument. The arg is deprecated.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/sparse.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe PyTorch API of sparse tensors is in beta and may change in the near future. We highly welcome feature requests, bug reports and general suggestions as GitHub issues.\nBy default, PyTorch stores elements contiguously in physical memory. This leads to efficient implementations of various array processing algorithms that require fast access to elements.\nNow, some users might decide to represent data such as graph adjacency matrices, pruned weights or points clouds by Tensors whose . We recognize these are important applications and aim to provide performance optimizations for these use cases via sparse storage formats.\nVarious sparse storage formats such as COO, CSR/CSC, semi-structured, LIL, etc. have been developed over the years. While they differ in exact layouts, they all compress data through efficient representation of zero valued elements. We call the uncompressed values in contrast to , compressed elements.\nBy compressing repeat zeros sparse storage formats aim to save memory and computational resources on various CPUs and GPUs. Especially for high degrees of sparsity or highly structured sparsity this can have significant performance implications. As such sparse storage formats can be seen as a performance optimization.\nLike many other performance optimization sparse storage formats are not always advantageous. When trying sparse formats for your use case you might find your execution time to increase rather than decrease.\nPlease feel encouraged to open a GitHub issue if you analytically expected to see a stark increase in performance but measured a degradation instead. This helps us prioritize the implementation of efficient kernels and wider performance optimizations.\nWe make it easy to try different sparsity layouts, and convert between them, without being opinionated on what’s best for your particular application.\nWe want it to be straightforward to construct a sparse Tensor from a given dense Tensor by providing conversion routines for each layout.\nIn the next example we convert a 2D Tensor with default dense (strided) layout to a 2D Tensor backed by the COO memory layout. Only values and indices of non-zero elements are stored in this case.\nWe also have a prototype implementation to support :ref: . Please see the references for more details.\nBatching: Devices such as GPUs require batching for optimal performance and thus we support batch dimensions.\nWe currently offer a very simple version of batching where each component of a sparse format itself is batched. This also requires the same number of specified elements per batch entry. In this example we construct a 3D (batched) CSR Tensor from a 3D dense Tensor.\n```\n         \n\n\n\n\n\n\n\n\n\n\n\n```\n\nDense dimensions: On the other hand, some data such as Graph embeddings might be better viewed as sparse collections of vectors instead of scalars.\nIn this example we create a 3D Hybrid COO Tensor with 2 sparse and 1 dense dimension from a 3D strided Tensor. If an entire row in the 3D strided Tensor is zero, it is not stored. If however any of the values in the row are non-zero, they are stored entirely. This reduces the number of indices since we need one index one per row instead of one per element. But it also increases the amount of storage for the values. Since only rows that are zero can be emitted and the presence of any non-zero valued elements cause the entire row to be stored.\nFundamentally, operations on Tensor with sparse storage formats behave the same as operations on Tensor with strided (or other) storage formats. The particularities of storage, that is the physical layout of the data, influences the performance of an operation but should not influence the semantics.\nWe are actively increasing operator coverage for sparse tensors. Users should not expect support same level of support as for dense Tensors yet. See our documentation for a list.\n```\n             \n  \n\n\n  File , line , in \n: \n\n\n\n\n\n\n```\n\nAs shown in the example above, we don’t support non-zero preserving unary operators such as cos. The output of a non-zero preserving unary operation will not be able to take advantage of sparse storage formats to the same extent as the input and potentially result in a catastrophic increase in memory. We instead rely on the user to explicitly convert to a dense Tensor first and then run the operation.\nWe are aware that some users want to ignore compressed zeros for operations such as instead of preserving the exact semantics of the operation. For this we can point to torch.masked and its MaskedTensor, which is in turn also backed and powered by sparse storage formats and kernels.\nAlso note that, for now, the user doesn’t have a choice of the output layout. For example, adding a sparse Tensor to a regular strided Tensor results in a strided Tensor. Some users might prefer for this to stay a sparse layout, because they know the result will still be sufficiently sparse.\nWe acknowledge that access to kernels that can efficiently produce different output layouts can be very useful. A subsequent operation might significantly benefit from receiving a particular layout. We are working on an API to control the result layout and recognize it is an important feature to plan a more optimal path of execution for any given model.\nSparse semi-structured tensors are currently a prototype feature and subject to change. Please feel free to open an issue to report a bug or if you have feedback to share.\nSemi-Structured sparsity is a sparse data layout that was first introduced in NVIDIA’s Ampere architecture. It is also referred to as or .\nThis sparse layout stores elements out of every elements, with being determined by the width of the Tensor’s data type (dtype). The most frequently used dtype is float16, where , thus the term “2:4 structured sparsity.”\nIn PyTorch, semi-structured sparsity is implemented via a Tensor subclass. By subclassing, we can override , allowing us to use faster sparse kernels when performing matrix multiplication. We can also store the tensor in it’s compressed form inside the subclass to reduce memory overhead.\nIn this compressed form, the sparse tensor is stored by retaining only the elements and some metadata, which encodes the mask.\nThe specified elements and metadata mask of a semi-structured sparse tensor are stored together in a single flat compressed tensor. They are appended to each other to form a contiguous chunk of memory.\ncompressed tensor = [ specified elements of original tensor | metadata_mask ]\nFor an original tensor of size we expect the first elements to be the kept elements and the rest of the tensor is metadata.\nIn order to make it easier for the user to view the specified elements and mask, one can use and to access the mask and specified elements respectively.\n  * returns the specified elements in a tensor of size and with the same dtype as the dense matrix.\n  * returns the metadata_mask in a tensor of size and with element type if dtype is torch.float16 or torch.bfloat16, and element type if dtype is torch.int8.\n\n\nFor 2:4 sparse tensors, the metadata overhead is minor - just 2 bits per specified element.\nIt’s important to note that is only supported for 1:2 sparsity. Therefore, it does not follow the same formula as above.\nHere, we break down how to calculate the compression ratio ( size dense / size sparse) of a 2:4 sparse tensor.\nLet and , so for and and for .\nM_{dense} = r \\times c \\times e \\\\\\ M_{sparse} = M_{specified} + M_{metadata} = r \\times \\frac{c}{2} \\times e + r \\times \\frac{c}{2} \\times 2 = \\frac{rce}{2} + rc =rce(\\frac{1}{2} +\\frac{1}{e}) \nUsing these calculations, we can determine the total memory footprint for both the original dense and the new sparse representation.\nThis gives us a simple formula for the compression ratio, which is dependent only on the bitwidth of the tensor datatype.\nBy using this formula, we find that the compression ratio is 56.25% for or , and 62.5% for .\nYou can transform a dense tensor into a sparse semi-structured tensor by simply using the function.\nPlease also note that we only support CUDA tensors since hardware compatibility for semi-structured sparsity is limited to NVIDIA GPUs.\nThe following datatypes are supported for semi-structured sparsity. Note that each datatype has its own shape constraints and compression factor.\nTensor must be 2D and (r, c) must both be a positive multiple of 64  \n---  \nTensor must be 2D and (r, c) must both be a positive multiple of 64  \nTensor must be 2D and (r, c) must both be a positive multiple of 128  \nTo construct a semi-structured sparse tensor, start by creating a regular dense tensor that adheres to a 2:4 (or semi-structured) sparse format. To do this we tile a small 1x4 strip to create a 16x16 dense float16 tensor. Afterwards, we can call function to compress it for accelerated inference.\n```\n   \n      \n\n\n\n\n\n\n        [0., 0., 1.,  ..., 0., 1., 1.]], device='cuda:0', dtype=torch.float16)\n  \nSparseSemiStructuredTensor(shape=torch.Size([128, 128]), transposed=False, values=tensor([[1., 1., 1.,  ..., 1., 1., 1.],\n\n\n\n\n\n        [1., 1., 1.,  ..., 1., 1., 1.]], device='cuda:0', dtype=torch.float16), metadata=tensor([[-4370, -4370, -4370,  ..., -4370, -4370, -4370],\n\n\n\n\n\n\n\n\n```\n\nCurrently, the following operations are supported for semi-structured sparse tensors:\nTo use these ops, simply pass the output of instead of using once your tensor has 0s in a semi-structured sparse format, like this:\nYou can accelerate the linear layers in your model if the weights are already semi-structured sparse with just a few lines of code:\nPyTorch implements the so-called Coordinate format, or COO format, as one of the storage formats for implementing sparse tensors. In COO format, the specified elements are stored as tuples of element indices and the corresponding values. In particular,\n>   * the indices of specified elements are collected in tensor of size and with element type ,\n>   * the corresponding values are collected in tensor of size and with an arbitrary integer or floating point number element type,\n> \n\nwhere is the dimensionality of the tensor and is the number of specified elements.\nThe memory consumption of a sparse COO tensor is at least bytes (plus a constant overhead from storing other tensor data).\nThe memory consumption of a strided tensor is at least .\nFor example, the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least bytes when using COO tensor layout and bytes when using the default strided tensor layout. Notice the 200 fold memory saving from using the COO storage format.\nA sparse COO tensor can be constructed by providing the two tensors of indices and values, as well as the size of the sparse tensor (when it cannot be inferred from the indices and values tensors) to a function .\nSuppose we want to define a sparse tensor with the entry 3 at location (0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2). Unspecified elements are assumed to have the same value, fill value, which is zero by default. We would then write:\n```\n    \n\n     \n     \n\n\n\n\n\n\n\n\n\n```\n\nNote that the input is NOT a list of index tuples. If you want to write your indices this way, you should transpose before passing them to the sparse constructor:\n```\n       \n                   \n     \n\n     \n  \n\n\n\n```\n\nAn empty sparse COO tensor can be constructed by specifying its size only:\nPyTorch implements an extension of sparse tensors with scalar values to sparse tensors with (contiguous) tensor values. Such tensors are called hybrid tensors.\nPyTorch hybrid COO tensor extends the sparse COO tensor by allowing the tensor to be a multi-dimensional tensor so that we have:\n>   * the indices of specified elements are collected in tensor of size and with element type ,\n>   * the corresponding (tensor) values are collected in tensor of size and with an arbitrary integer or floating point number element type.\n> \n\nWe use (M + K)-dimensional tensor to denote a N-dimensional sparse hybrid tensor, where M and K are the numbers of sparse and dense dimensions, respectively, such that M + K == N holds.\nSuppose we want to create a (2 + 1)-dimensional tensor with the entry [3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry [7, 8] at location (1, 2). We would write\n```\n    \n\n        \n      \n\n\n\n\n\n\n\n\n```\n\nIn general, if is a sparse COO tensor and , , then we have the following invariants:\n>   * - dimensionality of a tensor is the sum of the number of sparse and dense dimensions,\n> \n\nDense dimensions always follow sparse dimensions, that is, mixing of dense and sparse dimensions is not supported.\nTo be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via keyword argument, or globally using context manager instance. By default, the sparse tensor invariants checks are disabled.\nPyTorch sparse COO tensor format permits sparse tensors, where there may be duplicate coordinates in the indices; in this case, the interpretation is that the value at that index is the sum of all duplicate value entries. For example, one can specify multiple values, and , for the same index , that leads to an 1-D uncoalesced tensor:\nwhile the coalescing process will accumulate the multi-valued elements into a single value using summation:\nIn general, the output of method is a sparse tensor with the following properties:\n\n\nFor the most part, you shouldn’t have to care whether or not a sparse tensor is coalesced or not, as most operations will work identically given a sparse coalesced or uncoalesced tensor.\nHowever, some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors.\nFor instance, addition of sparse COO tensors is implemented by simply concatenating the indices and values tensors:\nIf you repeatedly perform an operation that can produce duplicate entries (e.g., ), you should occasionally coalesce your sparse tensors to prevent them from growing too large.\nOn the other hand, the lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products.\nAs mentioned above, a sparse COO tensor is a instance and to distinguish it from the instances that use some other layout, one can use or properties:\nThe number of sparse and dense dimensions can be acquired using methods and , respectively. For instance:\nIf is a sparse COO tensor then its COO format data can be acquired using methods and .\nCurrently, one can acquire the COO format data only when the tensor instance is coalesced:\n```\n\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n\n```\n\nFor acquiring the COO format data of an uncoalesced tensor, use and :\nConstructing a new sparse COO tensor results a tensor that is not coalesced:\nbut one can construct a coalesced copy of a sparse COO tensor using the method:\nWhen working with uncoalesced sparse COO tensors, one must take into an account the additive nature of uncoalesced data: the values of the same indices are the terms of a sum that evaluation gives the value of the corresponding tensor element. For example, the scalar multiplication on a sparse uncoalesced tensor could be implemented by multiplying all the uncoalesced values with the scalar because holds. However, any nonlinear operation, say, a square root, cannot be implemented by applying the operation to uncoalesced data because does not hold in general.\nSlicing (with positive step) of a sparse COO tensor is supported only for dense dimensions. Indexing is supported for both sparse and dense dimensions:\nIn PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be zero in general. However, there exists operations that may interpret the fill value differently. For instance, computes the softmax with the assumption that the fill value is negative infinity.\nSparse Compressed Tensors represents a class of sparse tensors that have a common feature of compressing the indices of a certain dimension using an encoding that enables certain optimizations on linear algebra kernels of sparse compressed tensors. This encoding is based on the \nWe use (B + M + K)-dimensional tensor to denote a N-dimensional sparse compressed hybrid tensor, where B, M, and K are the numbers of batch, sparse, and dense dimensions, respectively, such that holds. The number of sparse dimensions for sparse compressed tensors is always two, .\nWe say that an indices tensor uses CSR encoding if the following invariants are satisfied:\n  * shape is where is the number of compressed dimensions (e.g. rows or columns)\n  * for , where is the number of plain dimensions (orthogonal to compressed dimensions, e.g. columns or rows).\n\n\nTo be sure that a constructed sparse tensor has consistent indices, values, and size, the invariant checks can be enabled per tensor creation via keyword argument, or globally using context manager instance. By default, the sparse tensor invariants checks are disabled.\nThe generalization of sparse compressed layouts to N-dimensional tensors can lead to some confusion regarding the count of specified elements. When a sparse compressed tensor contains batch dimensions the number of specified elements will correspond to the number of such elements per-batch. When a sparse compressed tensor has dense dimensions the element considered is now the K-dimensional array. Also for block sparse compressed layouts the 2-D block is considered as the element being specified. Take as an example a 3-dimensional block sparse tensor, with one batch dimension of length , and a block shape of . If this tensor has specified elements, then in fact we have blocks specified per batch. This tensor would have with shape . This interpretation of the number of specified elements comes from all sparse compressed layouts being derived from the compression of a 2-dimensional matrix. Batch dimensions are treated as stacking of sparse matrices, dense dimensions change the meaning of the element from a simple scalar value to an array with its own dimensions.\nThe primary advantage of the CSR format over the COO format is better use of storage and much faster computation operations such as sparse matrix-vector multiplication using MKL and MAGMA backends.\nIn the simplest case, a (0 + 2 + 0)-dimensional sparse CSR tensor consists of three 1-D tensors: , and :\n>   * The tensor consists of compressed row indices. This is a 1-D tensor of size (the number of rows plus 1). The last element of is the number of specified elements, . This tensor encodes the index in and depending on where the given row starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given row.\n>   * The tensor contains the column indices of each element. This is a 1-D tensor of size .\n>   * The tensor contains the values of the CSR tensor elements. This is a 1-D tensor of size .\n> \n\nThe index tensors and should have element type either (default) or . If you want to use MKL-enabled matrix operations, use . This is as a result of the default linking of pytorch being with MKL LP64, which uses 32 bit integer indexing.\nIn the general case, the (B + 2 + K)-dimensional sparse CSR tensor consists of two (B + 1)-dimensional index tensors and , and of (1 + K)-dimensional tensor such that\nThe batches of sparse CSR tensors are dependent: the number of specified elements in all batches must be the same. This somewhat artificial constraint allows efficient storage of the indices of different CSR batches.\nThe number of sparse and dense dimensions can be acquired using and methods. The batch dimensions can be computed from the tensor shape: .\nThe memory consumption of a sparse CSR tensor is at least bytes (plus a constant overhead from storing other tensor data).\nWith the same example data of , the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers is at least bytes when using CSR tensor layout. Notice the 1.6 and 310 fold savings from using CSR storage format compared to using the COO and strided formats, respectively.\nSparse CSR tensors can be directly constructed by using the function. The user must supply the row and column indices and values tensors separately where the row indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and if it is not present.\n```\n    \n     \n     \n     \n\n\n\n\n\n\n\n\n\n```\n\nThe values of sparse dimensions in deduced is computed from the size of and the maximal index value in . If the number of columns needs to be larger than in the deduced then the argument must be specified explicitly.\nThe simplest way of constructing a 2-D sparse CSR tensor from a strided or sparse COO tensor is to use method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:\n```\n              \n  \n\n\n\n\n\n```\n\nThe sparse matrix-vector multiplication can be performed with the method. This is currently the only math operation supported on CSR tensors.\nThe sparse CSC (Compressed Sparse Column) tensor format implements the CSC format for storage of 2 dimensional tensors with an extension to supporting batches of sparse CSC tensors and values being multi-dimensional tensors.\nSparse CSC tensor is essentially a transpose of the sparse CSR tensor when the transposition is about swapping the sparse dimensions.\nSimilarly to , a sparse CSC tensor consists of three tensors: , and :\n>   * The tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified elements, . This tensor encodes the index in and depending on where the given column starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements in a given column.\n>   * The tensor contains the row indices of each element. This is a (B + 1)-D tensor of shape .\n>   * The tensor contains the values of the CSC tensor elements. This is a (1 + K)-D tensor of shape .\n> \n\nSparse CSC tensors can be directly constructed by using the function. The user must supply the row and column indices and values tensors separately where the column indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\n```\n    \n     \n     \n     \n\n\n\n\n\n\n\n\n\n```\n\nThe sparse CSC tensor constructor function has the compressed column indices argument before the row indices argument.\nThe (0 + 2 + 0)-dimensional sparse CSC tensors can be constructed from any two-dimensional tensor using method. Any zeros in the (strided) tensor will be interpreted as missing values in the sparse tensor:\n```\n              \n  \n\n\n\n\n\n\n```\n\nThe sparse BSR (Block compressed Sparse Row) tensor format implements the BSR format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSR tensors and values being blocks of multi-dimensional tensors.\n>   * The tensor consists of compressed row indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified blocks, . This tensor encodes the index in and depending on where the given column block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given row.\n>   * The tensor contains the column block indices of each element. This is a (B + 1)-D tensor of shape .\n>   * The tensor contains the values of the sparse BSR tensor elements collected into two-dimensional blocks. This is a (1 + 2 + K)-D tensor of shape .\n> \n\nSparse BSR tensors can be directly constructed by using the function. The user must supply the row and column block indices and values tensors separately where the row block indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\n```\n    \n     \n       \n                            \n                            \n                            \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\ntensor([[ 0.,  1.,  2.,  3.,  4.,  5.],\n        [ 6.,  7.,  8.,  9., 10., 11.],\n\n\n\n```\n\nThe (0 + 2 + 0)-dimensional sparse BSR tensors can be constructed from any two-dimensional tensor using method that also requires the specification of the values block size:\n```\n       \n                           \n                           \n                           \n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThe sparse BSC (Block compressed Sparse Column) tensor format implements the BSC format for storage of two-dimensional tensors with an extension to supporting batches of sparse BSC tensors and values being blocks of multi-dimensional tensors.\n>   * The tensor consists of compressed column indices. This is a (B + 1)-D tensor of shape . The last element is the number of specified blocks, . This tensor encodes the index in and depending on where the given row block starts. Each successive number in the tensor subtracted by the number before it denotes the number of blocks in a given column.\n>   * The tensor contains the row block indices of each element. This is a (B + 1)-D tensor of shape .\n>   * The tensor contains the values of the sparse BSC tensor elements collected into two-dimensional blocks. This is a (1 + 2 + K)-D tensor of shape .\n> \n\nSparse BSC tensors can be directly constructed by using the function. The user must supply the row and column block indices and values tensors separately where the column block indices must be specified using the CSR compression encoding. The argument is optional and will be deduced from the and tensors if it is not present.\n```\n    \n     \n       \n                            \n                            \n                            \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nAll sparse compressed tensors — CSR, CSC, BSR, and BSC tensors — are conceptionally very similar in that their indices data is split into two parts: so-called compressed indices that use the CSR encoding, and so-called plain indices that are orthogonal to the compressed indices. This allows various tools on these tensors to share the same implementations that are parameterized by tensor layout.\nSparse CSR, CSC, BSR, and CSC tensors can be constructed by using function that have the same interface as the above discussed constructor functions , , , and , respectively, but with an extra required argument. The following example illustrates a method of constructing CSR and CSC tensors using the same input data by specifying the corresponding layout parameter to the function:\n```\n    \n     \n     \n     \n\n\n\n\n\n     \n\n\n\n\n\n   \n\n\n```\n\nThe following table summarizes supported Linear Algebra operations on sparse matrices where the operands layouts may vary. Here denotes a tensor with a given layout. Similarly, denotes a matrix (2-D PyTorch tensor), and denotes a vector (1-D PyTorch tensor). In addition, denotes a scalar (float or 0-D PyTorch tensor), is element-wise multiplication, and is matrix multiplication.\nwhere “Sparse grad?” column indicates if the PyTorch operation supports backward with respect to sparse matrix argument. All PyTorch operations, except , support backward with respect to strided matrix arguments.\nCurrently, PyTorch does not support matrix multiplication with the layout signature . However, applications can still compute this using the matrix relation .\nReturns a new with values from a strided tensor filtered by the indices of the sparse tensor .  \n---  \nConvert a tensor to a block sparse row (BSR) storage format of given blocksize.  \nConvert a tensor to a block sparse column (BSC) storage format of given blocksize.  \nCreates a strided copy of if is not a strided tensor, otherwise returns .  \nThe following Tensor methods are specific to sparse COO tensors:\nResizes to the desired size and the number of sparse and dense dimensions.  \n---  \nRemoves all specified elements from a and resizes to the desired size and the number of sparse and dense dimensions.  \nThe following methods are specific to and :\nReturns the tensor containing the compressed row indices of the tensor when is a sparse CSR tensor of layout .  \n---  \nReturns the tensor containing the column indices of the tensor when is a sparse CSR tensor of layout .  \nThe following methods are specific to and :\nConstructs a with specified values at the given .  \n---  \nConstructs a with specified values at the given and .  \nConstructs a with specified values at the given and .  \nConstructs a with specified 2-dimensional blocks at the given and .  \nConstructs a with specified 2-dimensional blocks at the given and .  \nConstructs a [sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC -](https://docs.pytorch.org/docs/stable/sparse.html#sparse-compressed-docs) with specified values at the given and .  \nReturn the sum of each row of the given sparse tensor.  \nThis function does exact same thing as in the forward, except that it supports backward for sparse COO matrix .  \nPerforms a matrix multiplication of the dense matrices and at the locations specified by the sparsity pattern of .  \nMatrix multiplies a sparse tensor with a dense tensor , then adds the sparse tensor to the result.  \nPerforms a matrix multiplication of a and a strided matrix .  \nPerforms a matrix multiplication of the sparse matrix with the dense matrix .  \nComputes the solution of a square system of linear equations with a unique solution.  \nCreates a sparse 2D tensor by placing the values from rows of along specified diagonals of the output  \nWe aim to support all ‘zero-preserving unary functions’: functions of one argument that map zero to zero.\nIf you find that we are missing a zero-preserving unary function that you need, please feel encouraged to open an issue for a feature request. As always please kindly try the search function first before opening an issue.\n  *     *     *     *       *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/type_info.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe numerical properties of a can be accessed through either the or the .\nA is an object that represents the numerical properties of a floating point , (i.e. , , , and ). This is similar to \nThe constructor of can be called without argument, in which case the class is created for the pytorch default dtype (as returned by ).\nA is an object that represents the numerical properties of a integer (i.e. , , , , and ). This is similar to \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/torch.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.\nIt has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability >= 3.0.\nReturns True if the data type of is a complex data type i.e., one of , and .  \n---  \nReturns True if the is a conjugated tensor, i.e. its conjugate bit is set to .  \nReturns True if the data type of is a floating point data type i.e., one of , , , and .  \nReturns True if the is a single element tensor which is not equal to zero after type conversions.  \nRandom sampling creation ops are listed under and include: You may also use with the methods to create s with values sampled from a broader range of distributions.\nConstructs a tensor with no autograd history (also known as a \"leaf tensor\", see ) by copying .  \n---  \nConstructs a with specified values at the given .  \nConstructs a with specified values at the given and .  \nConstructs a with specified values at the given and .  \nConstructs a with specified 2-dimensional blocks at the given and .  \nConstructs a with specified 2-dimensional blocks at the given and .  \nConverts into a tensor, sharing data and preserving autograd history if possible.  \nCreates a CPU tensor with a storage backed by a memory-mapped file.  \nCreates a 1-dimensional from an object that implements the Python buffer protocol.  \nReturns a tensor filled with the scalar value , with the shape defined by the variable argument .  \nReturns a tensor filled with the scalar value , with the same size as .  \nReturns a tensor filled with the scalar value , with the shape defined by the variable argument .  \nReturns a tensor filled with the scalar value , with the same size as .  \nReturns a 1-D tensor of size with values from the interval taken with common difference beginning from .  \nReturns a 1-D tensor of size with values from to with step .  \nCreates a one-dimensional tensor of size whose values are evenly spaced from to , inclusive.  \nCreates a one-dimensional tensor of size whose values are evenly spaced from to , inclusive, on a logarithmic scale with base .  \nReturns a 2-D tensor with ones on the diagonal and zeros elsewhere.  \nCreates a tensor with the specified and and filled with undefined data.  \nConverts a float tensor to a quantized tensor with given scale and zero point.  \nConverts a float tensor to a per-channel quantized tensor with given scales and zero points.  \nConstructs a complex tensor with its real part equal to and its imaginary part equal to .  \nConstructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value and angle .  \nReturns a view of the tensor conjugated and with the last two dimensions transposed.  \n---  \nReturns a tensor containing the indices of all non-zero elements of .  \nConcatenates the given sequence of tensors in in the given dimension.  \nAttempts to split a tensor into the specified number of chunks.  \nSplits , a tensor with three or more dimensions, into multiple tensors depthwise according to .  \nCreates a new tensor by horizontally stacking the tensors in .  \nSplits , a tensor with one or more dimensions, into multiple tensors horizontally according to .  \nReturns a new tensor which indexes the tensor along dimension using the entries in which is a .  \nReturns a new 1-D tensor which indexes the tensor according to the boolean mask which is a .  \nMoves the dimension(s) of at the position(s) in to the position(s) in .  \nReturns a new tensor that is a narrowed version of tensor.  \nSame as except this returns a copy rather than shared storage.  \nReturns a view of the original tensor with its dimensions permuted.  \nReturns a tensor with the same data and number of elements as , but with the specified shape.  \nSlices the tensor along the selected dimension at the given index.  \nEmbeds the values of the tensor into along the diagonal elements of , with respect to and .  \nExpects to be <= 2-D tensor and transposes dimensions 0 and 1.  \nReturns a new tensor with the elements of at the given indices.  \nSplits a tensor into multiple sub-tensors, all of which are views of , along dimension according to the indices or number of sections specified by .  \nConverts a tensor of flat indices into a tuple of coordinate tensors that index into an arbitrary tensor of the specified shape.  \nReturns a new tensor with a dimension of size one inserted at the specified position.  \nSplits , a tensor with two or more dimensions, into multiple tensors vertically according to .  \nReturn a tensor of elements selected from either or , depending on .  \nWithin the PyTorch repo, we define an “Accelerator” as a that is being used alongside a CPU to speed up computation. These device use an asynchronous execution scheme, using and as their main way to perform synchronization. We also assume that only one such accelerator can be available at once on a given host. This allows us to use the current accelerator as the default device for relevant concepts such as pinned memory, Stream device_type, FSDP, etc.\nAs of today, accelerator devices are (in no particular order) , , , , “HPU”, and PrivateUse1 (many device not in the PyTorch repo itself).\nMany tools in the PyTorch Ecosystem use fork to create subprocesses (for example dataloading or intra-op parallelism), it is thus important to delay as much as possible any operation that would prevent further forks. This is especially important here as most accelerator’s initialization has such effect. In practice, you should keep in mind that checking is a compile-time check by default, it is thus always fork-safe. On the contrary, passing the flag to this function or calling will usually prevent later fork.\nSome backends provide an experimental opt-in option to make the runtime availability check fork-safe. When using the CUDA device can be used for example.\nAn in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.  \n---  \nQuery and record Stream status to identify or control dependencies across Stream and measure timing.  \nCreates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.  \n---  \nSets the seed for generating random numbers to a non-deterministic random number on all devices.  \n---  \nSets the seed for generating random numbers on all devices.  \nReturns the initial seed for generating random numbers as a Python .  \nDraws binary random numbers (0 or 1) from a Bernoulli distribution.  \n---  \nReturns a tensor where each row contains indices sampled from the multinomial (a stricter definition would be multivariate, refer to for more details) probability distribution located in the corresponding row of tensor .  \nReturns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.  \nReturns a tensor of the same size as with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in i.e.,  \nReturns a tensor filled with random numbers from a uniform distribution on the interval   \nReturns a tensor with the same size as that is filled with random numbers from a uniform distribution on the interval .  \nReturns a tensor filled with random integers generated uniformly between (inclusive) and (exclusive).  \nReturns a tensor with the same shape as Tensor filled with random integers generated uniformly between (inclusive) and (exclusive).  \nReturns a tensor filled with random numbers from a normal distribution with mean and variance (also called the standard normal distribution).  \nReturns a tensor with the same size as that is filled with random numbers from a normal distribution with mean 0 and variance 1.  \nThere are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:\n\n\nReturns the number of threads used for parallelizing CPU operations  \n---  \nSets the number of threads used for intraop parallelism on CPU.  \nReturns the number of threads used for inter-op parallelism on CPU (e.g.  \nSets the number of threads used for interop parallelism (e.g.  \nThe context managers , , and are helpful for locally disabling and enabling gradient computation. See for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the module, etc.\nA floating-point “not a number” value. This value is not a legal number. Alias for .  \n---  \nReturns a new tensor with the inverse hyperbolic cosine of the elements of .  \n---  \nPerforms the element-wise division of by , multiplies the result by the scalar and adds it to .  \nPerforms the element-wise multiplication of by , multiplies the result by the scalar and adds it to .  \nReturns a new tensor with the arcsine of the elements of .  \nReturns a new tensor with the inverse hyperbolic sine of the elements of .  \nReturns a new tensor with the arctangent of the elements of .  \nReturns a new tensor with the inverse hyperbolic tangent of the elements of .  \nReturns a new tensor with the ceil of the elements of , the smallest integer greater than or equal to each element.  \nCreate a new floating-point tensor with the magnitude of and the sign of , elementwise.  \nReturns a new tensor with the cosine of the elements of .  \nReturns a new tensor with the hyperbolic cosine of the elements of .  \nReturns a new tensor with each of the elements of converted from angles in degrees to radians.  \nDivides each element of the input by the corresponding element of .  \nReturns a new tensor with the exponential of the elements of the input tensor .  \nReturns a new tensor with the data in fake quantized per channel using , , and , across the channel specified by .  \nReturns a new tensor with the data in fake quantized using , , and .  \nReturns a new tensor with the floor of the elements of , the largest integer less than or equal to each element.  \nDecomposes into mantissa and exponent tensors such that .  \nEstimates the gradient of a function in one or more dimensions using the   \nDoes a linear interpolation of two tensors (given by ) and based on a scalar or tensor and returns the resulting tensor.  \nComputes the natural logarithm of the absolute value of the gamma function on .  \nReturns a new tensor with the natural logarithm of the elements of .  \nReturns a new tensor with the logarithm to the base 10 of the elements of .  \nReturns a new tensor with the natural logarithm of (1 + ).  \nReturns a new tensor with the logarithm to the base 2 of the elements of .  \nLogarithm of the sum of exponentiations of the inputs in base-2.  \nComputes the element-wise logical AND of the given input tensors.  \nComputes the element-wise logical NOT of the given input tensor.  \nComputes the element-wise logical OR of the given input tensors.  \nComputes the element-wise logical XOR of the given input tensors.  \nGiven the legs of a right triangle, return its hypotenuse.  \nReplaces , positive infinity, and negative infinity values in with the values specified by , , and , respectively.  \nReturns a new tensor with the negative of the elements of .  \nTakes the power of each element in with and returns a tensor with the result.  \nApplies a 1D max pooling over an input quantized tensor composed of several input planes.  \nApplies a 2D max pooling over an input quantized tensor composed of several input planes.  \nReturns a new tensor with each of the elements of converted from angles in radians to degrees.  \nReturns a new tensor with the reciprocal of the elements of   \nReturns a new tensor with the reciprocal of the square-root of each of the elements of .  \nReturns a new tensor with the signs of the elements of .  \nThis function is an extension of torch.sign() to complex tensors.  \nTests if each element of has its sign bit set or not.  \nReturns a new tensor with the sine of the elements of .  \nReturns a new tensor with the hyperbolic sine of the elements of .  \nReturns a new tensor with the square-root of the elements of .  \nReturns a new tensor with the square of the elements of .  \nReturns a new tensor with the tangent of the elements of .  \nReturns a new tensor with the hyperbolic tangent of the elements of .  \nReturns a new tensor with the truncated integer values of the elements of .  \nReturns the indices of the maximum value of all elements in the tensor.  \n---  \nReturns the indices of the minimum value(s) of the flattened tensor or along a dimension  \nReturns the maximum value of each slice of the tensor in the given dimension(s) .  \nReturns the minimum value of each slice of the tensor in the given dimension(s) .  \nReturns the log of summed exponentials of each row of the tensor in the given dimension .  \nReturns a namedtuple where is the mode value of each row of the tensor in the given dimension , i.e. a value which appears most often in that row, and is the index location of each mode value found.  \nReturns the matrix norm or vector norm of a given tensor.  \nReturns the sum of all elements, treating Not a Numbers (NaNs) as zero.  \nComputes the q-th quantiles of each row of the tensor along the dimension .  \nThis is a variant of that \"ignores\" values, computing the quantiles as if values in did not exist.  \nCalculates the standard deviation and mean over the dimensions specified by .  \nEliminates all but the first element from every consecutive group of equivalent elements.  \nCalculates the variance and mean over the dimensions specified by .  \nCounts the number of non-zero values in the tensor along the given .  \nReturns the indices that sort a tensor along a given dimension in ascending order by value.  \n---  \nReturns a new tensor with boolean elements representing if each element of is \"close\" to the corresponding element of .  \nReturns a new tensor with boolean elements representing if each element is or not.  \nTests if each element of is infinite (positive or negative infinity) or not.  \nReturns a new tensor with boolean elements representing if each element of is NaN or not.  \nReturns a new tensor with boolean elements representing if each element of is real-valued or not.  \nReturns a namedtuple where is the th smallest element of each row of the tensor in the given dimension .  \nSorts the elements of the tensor along a given dimension in ascending order by value.  \nReturns the largest elements of the given tensor along a given dimension.  \nSorts the elements of the tensor along its first dimension in ascending order by value.  \nReturns a 1-dimensional view of each input tensor with zero dimensions.  \n---  \nReturns a 2-dimensional view of each input tensor with zero dimensions.  \nReturns a 3-dimensional view of each input tensor with zero dimensions.  \nCount the frequency of each value in an array of non-negative ints.  \nReturns the indices of the buckets to which each value in the belongs, where the boundaries of the buckets are set by .  \nComputes batched the p-norm distance between each pair of the two collections of row vectors.  \nEstimates the Pearson product-moment correlation coefficient matrix of the variables given by the matrix, where rows are the variables and columns are the observations.  \nEstimates the covariance matrix of the variables given by the matrix, where rows are the variables and columns are the observations.  \nReturns a namedtuple where is the cumulative maximum of elements of in the dimension .  \nReturns a namedtuple where is the cumulative minimum of elements of in the dimension .  \n  * If is a vector (1-D tensor), then returns a 2-D square tensor\n\n  \nCreates a tensor whose diagonals of certain 2D planes (specified by and ) are filled by .  \n  * If is a vector (1-D tensor), then returns a 2-D square tensor\n\n  \nReturns a partial view of with the its diagonal elements with respect to and appended as a dimension at the end of the shape.  \nSums the product of the elements of the input along dimensions specified using a notation based on the Einstein summation convention.  \nReverse the order of an n-D tensor along given axis in dims.  \nFlip tensor in the left/right direction, returning a new tensor.  \nFlip tensor in the up/down direction, returning a new tensor.  \nRotate an n-D tensor by 90 degrees in the plane specified by dims axis.  \nComputes a multi-dimensional histogram of the values in a tensor.  \nCreates grids of coordinates specified by the 1D inputs in :tensors.  \nReturns the logarithm of the cumulative summation of the exponentiation of elements of in the dimension .  \nReturns a tensor where each sub-tensor of along dimension is normalized such that the -norm of the sub-tensor is lower than the value   \nFind the indices from the dimension of such that, if the corresponding values in were inserted before the indices, when sorted, the order of the corresponding dimension within would be preserved.  \nReturns a contraction of a and b over multiple dimensions.  \nReturns the sum of the elements of the diagonal of the input 2-D matrix.  \nReturns the lower triangular part of the matrix (2-D tensor) or batch of matrices , the other elements of the result tensor are set to 0.  \nReturns the indices of the lower triangular part of a -by- matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \nReturns the upper triangular part of a matrix (2-D tensor) or batch of matrices , the other elements of the result tensor are set to 0.  \nReturns the indices of the upper triangular part of a by matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.  \nExpands a dimension of the input tensor over multiple dimensions.  \nReturns a new tensor with materialized conjugation if 's conjugate bit is set to , else returns .  \nReturns a new tensor with materialized negation if 's negative bit is set to , else returns .  \nPerforms a batch matrix-matrix product of matrices stored in and , with a reduced add step (all matrix multiplications get accumulated along the first dimension).  \n---  \nPerforms the outer-product of vectors and and adds it to the matrix .  \nComputes the Cholesky decomposition of a symmetric positive-definite matrix or for batches of symmetric positive-definite matrices.  \nComputes the inverse of a complex Hermitian or real symmetric positive-definite matrix given its Cholesky decomposition.  \nComputes the solution of a system of linear equations with complex Hermitian or real symmetric positive-definite lhs given its Cholesky decomposition.  \nThis is a low-level function for calling LAPACK's geqrf directly.  \nCalculates log determinant of a square matrix or batches of square matrices.  \nComputes the LU factorization of a matrix or batches of matrices .  \nReturns the LU solve of the linear system using the partially pivoted LU factorization of A from .  \nComputes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.  \nComputes the QR decomposition of a matrix or a batch of matrices , and returns a namedtuple (Q, R) of tensors such that with being an orthogonal matrix or batch of orthogonal matrices and being an upper triangular matrix or batch of upper triangular matrices.  \nComputes the singular value decomposition of either a matrix or batch of matrices .  \nReturn the singular value decomposition of a matrix, batches of matrices, or a sparse matrix such that .  \nPerforms linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.  \nFind the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.  \nSolves a system of equations with a square upper or lower triangular invertible matrix and multiple right-hand sides .  \nComputes the dot product of two 1D vectors along a dimension.  \nThis API is in beta and subject to future changes. Forward-mode AD is not supported.\nReturns the that would result from performing an arithmetic operation on the provided input tensors.  \n---  \nDetermines if a type conversion is allowed under PyTorch casting rules described in the type promotion .  \nReturns the with the smallest size and scalar kind that is not smaller nor of lower kind than either or .  \nReturns True if the global deterministic flag is turned on.  \nReturns True if the global deterministic flag is set to warn only.  \nReturns the current value of the debug mode for deterministic operations.  \nWhen this flag is False (default) then some PyTorch warnings may only appear once per process.  \nReturns the module associated with a given device(e.g., torch.device('cuda'), \"mtia:0\", \"xpu\", ...).  \nReturns True if the global warn_always flag is turned on.  \nvmap is the vectorizing map; returns a new function that maps over some dimension of the inputs.       \nLike an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.     \nLike an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.     \nLike an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.\nUnlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this.\nSymInt-aware utility for max which avoids branching on a < b.  \n---  \nN-ary add which is faster to compute for long lists than iterated binary addition.  \nThis feature is a prototype and may have compatibility breaking changes in the future.\nThis feature is a prototype and may have compatibility breaking changes in the future.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/special.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n                        \n() – the tensor to compute the digamma function on\nFrom PyTorch 1.8 onwards, the digamma function returns for . Previously it returned for .     \n\\begin{align} \\text{entr(x)} = \\begin{cases} -x * \\ln(x) & x > 0 \\\\\\ 0 & x = 0.0 \\\\\\ -\\infty & x < 0 \\end{cases} \\end{align}           \nComputes the error function of . The error function is defined as follows:     \nComputes the complementary error function of . The complementary error function is defined as follows:     \nComputes the scaled complementary error function for each element of . The scaled complementary error function is defined as follows:     \nComputes the inverse error function of . The inverse error function is defined in the range as:          \nComputes the expit (also known as the logistic sigmoid function) of the elements of .\n```\n  \n\n\n\n\n\n```\n    \nThis function provides greater precision than exp(x) - 1 for small values of x.     \nwhere both and are weakly positive and at least one is strictly positive. If both are zero or either is negative then . in the equation above is the gamma function,\nThe backward pass with respect to is not yet supported. Please open an issue on PyTorch’s Github to request it.     \n\n    \nwhere both and are weakly positive and at least one is strictly positive. If both are zero or either is negative then . in the equation above is the gamma function,\nThe backward pass with respect to is not yet supported. Please open an issue on PyTorch’s Github to request it.     \n\n    \nComputes the natural logarithm of the absolute value of the gamma function on .     \nComputes the zeroth order modified Bessel function of the first kind for each element of .     \nComputes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of .\n\\text{out}_{i} = \\exp(-|x|) * i0(x) = \\exp(-|x|) * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!)^2}      \nComputes the first order modified Bessel function of the first kind (as defined below) for each element of .     \nComputes the exponentially scaled first order modified Bessel function of the first kind (as defined below) for each element of .\n\\text{out}_{i} = \\exp(-|x|) * i1(x) = \\exp(-|x|) * \\frac{(\\text{input}_{i})}{2} * \\sum_{k=0}^{\\infty} \\frac{(\\text{input}_{i}^2/4)^k}{(k!) * (k+1)!}      \nComputes the log of the area under the standard Gaussian probability density function, integrated from minus infinity to , elementwise.          \nWhile mathematically equivalent to log(softmax(x)), doing these two operations separately is slower and numerically unstable. This function is computed as:     \n  * (, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to before the operation is performed. This is useful for preventing data type overflows. Default: None.\n\n    \nReturns a new tensor with the logit of the elements of . is clamped to [eps, 1 - eps] when eps is not None. When eps is None and < 0 or > 1, the function will yields NaN.\n\\begin{align} y_{i} &= \\ln(\\frac{z_{i}}{1 - z_{i}}) \\\\\\ z_{i} &= \\begin{cases} x_{i} & \\text{if eps is None} \\\\\\ \\text{eps} & \\text{if } x_{i} < \\text{eps} \\\\\\ x_{i} & \\text{if } \\text{eps} \\leq x_{i} \\leq 1 - \\text{eps} \\\\\\ 1 - \\text{eps} & \\text{if } x_{i} > 1 - \\text{eps} \\end{cases} \\end{align}      \n\n\n```\n  \n\n\n \n\n\n```\n    \n\\log(\\Gamma_{p}(a)) = C + \\displaystyle \\sum_{i=1}^{p} \\log\\left(\\Gamma\\left(a - \\frac{i - 1}{2}\\right)\\right) \nwhere and is the Gamma function.\nAll elements must be greater than , otherwise the behavior is undefiend.     \n  * () – the tensor to compute the multivariate log-gamma function\n\n    \nComputes the area under the standard Gaussian probability density function, integrated from minus infinity to , elementwise.     \nComputes the argument, x, for which the area under the Gaussian probability density function (integrated from minus infinity to x) is equal to , elementwise.     \n```\n    \ntensor([   -inf, -0.6745,  0.0000,  0.6745,     inf])\n\n```\n    \nComputes the derivative of the digamma function on . is called the order of the polygamma function.\nThis function is implemented only for nonnegative integers .          \nScaled modified Bessel function of the second kind of order .     \nScaled modified Bessel function of the second kind of order .     \n\\text{out}_{i} = \\begin{cases} 1, & \\text{if}\\ \\text{input}_{i}=0 \\\\\\ \\sin(\\pi \\text{input}_{i}) / (\\pi \\text{input}_{i}), & \\text{otherwise} \\end{cases}      \n```\n  \n\n\n\n\n\n```\n    \nIt is applied to all slices along dim, and will re-scale them so that the elements lie in the range and sum to 1.     \n  * (, optional) – the desired data type of returned tensor. If specified, the input tensor is cast to before the operation is performed. This is useful for preventing data type overflows. Default: None.\n\n         \n\\text{out}_{i} = \\begin{cases} \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\\ 0 & \\text{if } \\text{input}_{i} = 0.0 \\text{ and } \\text{other}_{i} != \\text{NaN} \\\\\\ \\text{input}_{i} * \\text{log1p}(\\text{other}_{i})& \\text{otherwise} \\end{cases} \n```\n  \n      \n \n\n    \n    \n \n\n \n\n \n\n\n```\n    \n\\text{out}_{i} = \\begin{cases} \\text{NaN} & \\text{if } \\text{other}_{i} = \\text{NaN} \\\\\\ 0 & \\text{if } \\text{input}_{i} = 0.0 \\\\\\ \\text{input}_{i} * \\log{(\\text{other}_{i})} & \\text{otherwise} \\end{cases} \n```\n  \n      \n \n\n    \n    \n \n\n \n\n \n\n\n```\n         \n\n\nThe Riemann zeta function corresponds to the case when \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\nFeatures described in this documentation are classified by release status:\n> These features will be maintained long-term and there should generally be no major performance limitations or gaps in documentation. We also expect to maintain backwards compatibility (although breaking changes can happen and notice will be given one release ahead of time).\n> These features are tagged as Beta because the API may change based on user feedback, because the performance needs to improve, or because coverage across operators is not yet complete. For Beta features, we are committing to seeing the feature through to the Stable classification. We are not, however, committing to backwards compatibility.\n> These features are typically not available as part of binary distributions like PyPI or Conda, except sometimes behind run-time flags, and are at an early stage for feedback and testing.\n\n\n\n\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/utils.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nRename the privateuse1 backend device to make it more convenient to use as a device name within PyTorch APIs.  \n---  \nAutomatically generate attributes and methods for the custom backend after rename privateuse1 backend.  \nReturn a string containing the C++ stack trace of the current thread.  \nSet the module attribute on a python object for a given object for nicer printing  \nThis function swaps the content of the two Tensor objects.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/xpu.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis package introduces support for the XPU backend, specifically tailored for Intel GPU optimization.\nThis package is lazily initialized, so you can always import it, and use to determine if your system supports XPU.\nContext-manager that changes the current device to that of given object.  \n---  \nReturn XPU AOT(ahead-of-time) build flags this library was compiled with.  \nSet the current stream.This is a wrapper API to set the stream.  \nWrap around the Context-manager StreamContext that selects a given stream.  \nWait for all kernels in all streams on a XPU device to complete.  \nReturn the random number generator state of the specified GPU as a ByteTensor.  \n---  \nReturn a list of ByteTensor representing the random number states of all devices.  \nSet the seed for generating random numbers for the current GPU.  \nSet the seed for generating random numbers on all GPUs.  \nSet the seed for generating random numbers to a random number for the current GPU.  \nSet the seed for generating random numbers to a random number on all GPUs.  \nSet the random number generator state of the specified GPU.  \nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other XPU application.  \n---  \nReturn the maximum GPU memory occupied by tensors in bytes for a given device.  \nReturn the maximum GPU memory managed by the caching allocator in bytes for a given device.  \nReturn the global free and total GPU memory for a given device.  \nReturn the current GPU memory occupied by tensors in bytes for a given device.  \nReturn the current GPU memory managed by the caching allocator in bytes for a given device.  \nReturn a dictionary of XPU memory allocator statistics for a given device.  \nReset the \"accumulated\" (historical) stats tracked by the XPU memory allocator.  \nReset the \"peak\" stats tracked by the XPU memory allocator.  \nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/autograd.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nprovides classes and functions implementing automatic differentiation of arbitrary scalar valued functions.\nIt requires minimal changes to the existing code - you only need to declare s for which gradients should be computed with the keyword. As of now, we only support autograd for floating point types ( half, float, double and bfloat16) and complex types (cfloat, cdouble).\nCompute the sum of gradients of given tensors with respect to graph leaves.  \n---  \nCompute and return the sum of gradients of outputs with respect to the inputs.  \nThis API is in beta. Even though the function signatures are very unlikely to change, improved operator coverage is planned before we consider this stable.\nPlease see the for detailed steps on how to use this API.\nContext-manager for forward AD, where all forward AD computation must occur within the context.  \n---  \nAssociate a tensor value with its tangent to create a \"dual tensor\" for forward AD gradient computation.  \nUnpack a \"dual tensor\" to get both its Tensor value and its forward AD gradient.  \nNamedtuple returned by containing the primal and tangent components of the dual tensor.  \nThis API is in beta. Even though the function signatures are very unlikely to change, major improvements to performances are planned before we consider this stable.\nThis section contains the higher level API for the autograd that builds on the basic API above and allows you to compute jacobians, hessians, etc.\nThis API works with user-provided functions that take only Tensors as input and return only Tensors. If your function takes other arguments that are not Tensors or Tensors that don’t have requires_grad set, you can use a lambda to capture them. For example, for a function that takes three inputs, a Tensor for which we want the jacobian, another tensor that should be considered constant and a boolean flag as you can use it as .\nCompute the dot product between a vector and the Jacobian of the given function at the point given by the inputs.  \n---  \nCompute the dot product between the Jacobian of the given function at the point given by the inputs and a vector .  \nCompute the dot product between vector and Hessian of a given scalar function at a specified point.  \nCompute the dot product between the scalar function's Hessian and a vector at a specified point.  \nSee for more information on the differences between no-grad and inference mode as well as other related mechanisms that may be confused with the two. Also see for a list of functions that can be used to locally disable gradients.\nWhen a non-sparse receives a non-sparse gradient during or is accumulated as follows.\n  1. If ’s memory is non-overlapping and dense, is created with strides matching (thus matching ’s layout).\n\n\n  1. If , replaces with a new tensor , which attempts (but does not guarantee) matching the preexisting ’s strides.\n\n\nThe default behavior (letting s be before the first , such that their layout is created according to 1 or 2, and retained over time according to 3 or 4) is recommended for best performance. Calls to or will not affect layouts.\nsuch that they’re recreated according to 1 or 2 every time, is a valid alternative to or that may improve performance for some networks.\nIf you need manual control over ’s strides, assign a zeroed tensor with desired strides before the first , and never reset it to . 3 guarantees your layout is preserved as long as . 4 indicates your layout is preserved even if .\nSupporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations actually lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\nAll s keep track of in-place operations applied to them, and if the implementation detects that a tensor was saved for backward in one of the functions, but it was modified in-place afterwards, an error will be raised once backward pass is started. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.\nThe Variable API has been deprecated: Variables are no longer necessary to use autograd with tensors. Autograd automatically supports Tensors with set to . Below please find a quick guide on what has changed:\n  * and still work as expected, but they return Tensors instead of Variables.\n  * Methods such as now work on tensors with the same method names.\n\n\nIn addition, one can now create tensors with using factory methods such as , , , and others like the following:\nThis attribute is by default and becomes a Tensor the first time a call to computes gradients for .  \n---  \nAll Tensors that have which is will be leaf Tensors by convention.  \nDetaches the Tensor from the graph that created it, making it a leaf.       \nTo create a custom , subclass this class and implement the and static methods. Then, to use your custom op in the forward pass, call the class method . Do not call directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on and validating your backward function using .\nSee for more details on how to use this class.\nDefine a formula for differentiating the operation with backward mode automatic differentiation.  \n---  \nDefine a formula for differentiating the operation with forward mode automatic differentiation.  \nWhen creating a new , the following methods are available to .\nCheck gradients computed via small finite differences against analytical gradients wrt tensors in that are of floating point or complex type and with .  \n---  \nCheck gradients of gradients computed via small finite differences against analytical gradients wrt tensors in and that are of floating point or complex type and with .  \nAutograd includes a profiler that lets you inspect the cost of different operators inside your model - both on the CPU and GPU. There are three modes implemented at the moment - CPU-only using . nvprof based (registers both CPU and GPU activity) using . and vtune profiler based using .     \nContext manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and exposes those events to Python. You can wrap any code into it and it will only report runtime of PyTorch functions. Note: profiler is thread local and is automatically propagated into the async tasks     \n  * () – Setting this to False makes this context manager a no-op.\n  * () – Enables timing of CUDA events as well using the cudaEvent API. (will be deprecated)\n  * () – Enables timing of device events. Adds approximately 4us of overhead to each tensor operation when use cuda. The valid devices options are ‘cuda’, ‘xpu’, ‘mtia’ and ‘privateuseone’.\n  * () – If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group by them using prof.key_averages(group_by_input_shape=True). Please note that shape recording might skew your profiling data. It is recommended to use separate runs with and without shape recording to validate the timing. Most likely the skew will be negligible for bottom most events (in a case of nested function calls). But for higher level functions the total self cpu time might be artificially increased because of the shape collection.\n  * () – If with_flops is set, the profiler will estimate the FLOPs (floating point operations) value using the operator’s input shape. This allows one to estimate the hardware performance. Currently, this option only works for the matrix multiplication and 2D convolution operators.\n  * () – record source information (file and line number) for the ops.\n  * () – profile CPU events; setting to requires and can be used to lower the overhead for GPU-only profiling.\n  * () – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.\n\n\n```\n    \n   \n         \n            \n        \n\n\n\nName                                 Self CPU total   CPU time avg     Number of Calls\n\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n\n\n```\n\nRaises an error if a key is seen more than once.  \n---  \nContext manager/function decorator that adds a label to a code block/function when running autograd profiler.       \nContext manager that makes every autograd operation emit an NVTX range.\nUnfortunately, there’s no way to force nvprof to flush the data it collected to disk, so for CUDA profiling one has to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them. Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or can load the results for inspection e.g. in Python REPL.     \n  * () – If , the nvtx range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: Non-tensor arguments will be represented by . Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of nvtx range creation. Default: \n\n\nWhen viewing a profile created using in the Nvidia Visual Profiler, correlating each backward-pass op with the corresponding forward-pass op can be difficult. To ease this task, appends sequence number information to the ranges it generates.\nDuring the forward pass, each function range is decorated with . is a running counter, incremented each time a new backward Function object is created and stashed for backward. Thus, the annotation associated with each forward function range tells you that if a backward Function object is created by this forward function, the backward object will receive sequence number N. During the backward pass, the top-level range wrapping each C++ backward Function’s call is decorated with . is the sequence number that the backward object was created with. By comparing numbers in backward with numbers in forward, you can track down which forward op created each backward Function.\nAny functions executed during the backward pass are also decorated with . During default backward (with ) this information is irrelevant, and in fact, may simply be 0 for all such functions. Only the top-level ranges associated with backward Function objects’ methods are useful, as a way to correlate these Function objects with the earlier forward pass.\nIf, on the other hand, a backward pass with is underway (in other words, if you are setting up for a double-backward), each function’s execution during backward is given a nonzero, useful . Those functions may themselves create Function objects to be executed later during double-backward, just as the original functions in the forward pass did. The relationship between backward and double-backward is conceptually the same as the relationship between forward and backward: The functions still emit current-sequence-number-tagged ranges, the Function objects they create still stash those sequence numbers, and during the eventual double-backward, the Function objects’ ranges are still tagged with numbers, which can be compared to numbers from the backward pass.     \nContext manager that makes every autograd operation emit an ITT range.\nIt is useful when running the program under Intel(R) VTune Profiler:\nThe Instrumentation and Tracing Technology (ITT) API enables your application to generate and control the collection of trace data during its execution across different Intel tools. This context manager is to annotate Intel(R) VTune Profiling trace. With help of this context manager, you will be able to see labled ranges in Intel(R) VTune Profiler GUI.     \n  * () – If , the itt range wrapping each autograd op will append information about the sizes of Tensor arguments received by that op, in the following format: Non-tensor arguments will be represented by . Arguments will be listed in the order they are received by the backend op. Please note that this order may not match the order in which those arguments were passed on the Python side. Also note that shape recording may increase the overhead of itt range creation. Default: \n\n    \n  * Running the forward pass with detection enabled will allow the backward pass to print the traceback of the forward operation that created the failing backward function.\n  * If is , any backward computation that generate “nan” value will raise an error. Default .\n\n\nThis mode should be enabled only for debugging as the different tests will slow down your program execution.\n```\n \n   \n \n    \n      \n         \n    \n      \n        \n         \n         \n \n      \n     \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n    \nContext-manager that sets the anomaly detection for the autograd engine on or off.\nwill enable or disable the autograd anomaly detection based on its argument . It can be used as a context-manager or as a function.\nAutograd exposes methods that allow one to inspect the graph and interpose behavior during the backward pass.\nThe attribute of a holds a if the tensor is the output of a operation that was recorded by autograd (i.e., grad_mode is enabled and at least one of the inputs required gradients), or otherwise.\nUpdate autograd metadata tracking whether the given Tensor was modified in place.  \n---  \nSome operations need intermediary results to be saved during the forward pass in order to execute the backward pass. These intermediary results are saved as attributes on the and can be accessed. For example:\n```\n     \n  \n \n\n\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_raw_saved_result', '_register_hook_dict', '_saved_result', 'metadata', 'name', 'next_functions', 'register_hook', 'register_prehook', 'requires_grad']\n \n\n\n```\n\nYou can also define how these saved tensors should be packed / unpacked using hooks. A common application is to trade compute for memory by saving those intermediary results to disk or to CPU instead of leaving them on the GPU. This is especially useful if you notice your model fits on GPU during evaluation, but not training. Also see .     \nContext-manager that sets a pair of pack / unpack hooks for saved tensors.\nUse this context-manager to define how intermediary results of an operation should be packed before saving, and unpacked on retrieval.\nIn that context, the function will be called everytime an operation saves a tensor for backward (this includes intermediary results saved using but also those recorded by a PyTorch-defined operation). The output of is then stored in the computation graph instead of the original tensor.\nThe is called when the saved tensor needs to be accessed, namely when executing or . It takes as argument the object returned by and should return a tensor which has the same content as the original tensor (passed as input to the corresponding ).\nIn general, you want to be equal to in terms of value, size, dtype and device.\n```\n \n     \n     \n\n \n     \n     \n\n   \n     \n  \n        \n\n\n\n\n\n\n```\n\nPerforming an inplace operation on the input to either hooks may lead to undefined behavior.\nOnly one pair of hooks is allowed at a time. When recursively nesting this context-manager, only the inner-most pair of hooks will be applied.     \nContext manager under which tensors saved by the forward pass will be stored on cpu, then retrieved for backward.\nWhen performing operations within this context manager, intermediary results saved in the graph during the forward pass will be moved to CPU, then copied back to the original device when needed for the backward pass. If the graph was already on CPU, no tensor copy is performed.\nUse this context-manager to trade compute for GPU memory usage (e.g. when your model doesn’t fit in GPU memory during training).     \n( tensors will be saved to CPU pinned memory during packing and copied to GPU asynchronously during unpacking. Defaults to . Also see .\n```\n    \n    \n    \n\n   \n                   \n     \n              \n                   \n     \n\n    \n     \n# the content of a, b, and prod_2 are still alive on GPU\n# the content of prod_1 and c only live on CPU\n  # all CPU tensors are moved back to GPU, for backward\n# all intermediary tensors are released (deleted) after the call to backward\n\n```\n    \nUseful for if you are creating a feature that does not work with saved tensors default hooks.\n```\n  \n \n    \n     \n        \n\n```\n    \nUnder the mode, the hook will be called after gradients with respect to every tensor in have been computed. If a tensor is in but is not part of the graph, or if a tensor is not needed to compute the gradients for any specified for the current or call, this tensor will be ignored and the hook will not wait for its gradient to be computed.\nAfter every non-ignored tensor’s gradient has been computed, will be called with those gradients. will be passed for tensors that did not have their gradients computed.\nUnder the mode, the hook will be called after the first gradient with respect to a tensor in has been computed. The hook will be called with that gradient as its argument.\nThis function returns a handle with a method that removes the hook.\nSee for more information on how when this hook is executed, and how its execution is ordered relative to other hooks.     \nContext manager under which mutating tensors saved for backward is allowed.\nUnder this context manager, tensors saved for backward are cloned on mutation, so the original version can still be used during backward. Normally, mutating a tensor saved for backward will result in an error raised when it’s used during backward.\nTo ensure the correct behavior, both the forward and backward should be run under the same context manager.     \nAn _AllowMutationOnSavedContext object storing the state managed by this context manager. This object can be useful for debugging purposes. The state managed by the context manager is automatically cleared upon exiting.     \nObject representing a given gradient edge within the autograd graph.\nTo get the gradient edge where a given Tensor gradient will be computed, you can do .     \nGet the gradient edge for computing the gradient of the given Tensor.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.checkpoint.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nDistributed Checkpoint (DCP) support loading and saving models from multiple ranks in parallel. It handles load-time resharding which enables saving in one cluster topology and loading into another.\n  * It produces multiple files per checkpoint, with at least one per rank.\n  * It operates in place, meaning that the model should allocate its data first and DCP uses that storage instead.\n\n\nThe entrypoints to load and save a checkpoint are the following:\n\n    \nThis function is different from as it handles , and by having each rank only save their local shards.\nThere is no guarantees of Backwards Compatibility across PyTorch versions for saved state_dicts.\nIf using the argument, make sure that only its ranks call and that all data in state_dict belong to it.\nWhen saving checkpoint for FSDP’s , only one of the shard_group should be calling and the corresponding process group needs to be passed in. \n\nIf no process group is available, this function assumes the intention is to save the\n    \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform writes. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n  * () – Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: )\n  * (, this function will assume the intent is to load a checkpoint without using cross-rank synchronization. (Default: )\n\n\nsave_state_dict uses collectives to coordinate writes across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .     \nAsynchronous version of . This code first de-stages the state_dict on to the staging storage (defaults to CPU memory), and then calls the in a separate thread.     \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform ‘stage’ and ‘save’. If this is not specified, DCP will automatically infer the writer based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n  * () – Instance of SavePlanner. If this is not specificed, the default planner will be used. (Default: )\n\n    \nLoad a checkpoint into a distributed state dict in SPMD style.\nEach rank must have the same keys in their provided to this API. Mismatched keys may result in hangs or errors. If unsure, you can use the API to check (but may incur communication costs).\nEach rank will try to read the least amount of data necessary to fulfill the requested . When loading or instances, each rank only reads data for their local shards.\nFor each object (having both a and a ), load will first call before attempting deserialization, followed by once the deserialization is complete. For each non- object, load will deserailize the object, and then replace it in the with the deserialized object.\nAll tensors in must be allocated on their destination device calling this function.\nAll non-tensor data is loaded using and modified in place on state_dict.\nUsers must call on the root module to ensure load pos-processing and non-tensor data properly propagates.     \n  * () – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )\n  * () – Instance of StorageWriter used to perform reads. If this is not specified, DCP will automatically infer the reader based on the checkpoint_id. If checkpoint_id is also None, an exception will be raised. (Default: )\n  * () – Instance of LoadPlanner. If this is not specificed, the default planner will be used. (Default: )\n  * (, this function will assume the intent is to load a checkpoint without using cross-rank synchronization. (Default: )\n\n    \n```\n\n\n\n\n\n```\n\nload_state_dict uses collectives to coordinate reads across ranks. For NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .\nThe following module is also useful for additional customization of the staging mechanisms used for asynchronous checkpointing ():     \nThis protocol is meant to provide customization and extensibility for dcp.async_save, allowing users to customize how data is staged previous to executing the usual dcp.save path in parallel. The expected order of operations (concretely defined in ) is the following:\n  1.     \nThis call gives the AsyncStager the opportunity to ‘stage’ the state_dict. The expectation and purpose of staging in this context is to create a “training-safe” representation of the state dict, meaning that any updates to module data after staging is complete should not be reflected in the state dict returned from this method. For example, in the default case a copy of the entire state dict is created on CPU RAM and returned here, allowing users to continue training without risking changes to data which is being serialized.\n  2. \n\ndcp.save is called on the state_dict returned from stage in parallel. This call is responsible\n\n  3. \n\nIf AsyncStager.should_synchronize_after_execute is True, this method will be called immediately after\n    \nthe serialization thread starts and before returning from dcp.async_save. If this is set to False, the assumption is the user has defined a custom synchronization point for the the purpose of further optimizing save latency in the training loop (for example, by overlapping staging with the forward/backward pass), and it is the respondsibility of the user to call at the appropriate time.\n\n    \nReturns a “staged” copy of . The expectation of the staged copy is that it is innoculated from any updates incurred after the stage call is complete.     \nIn the case is async in some way, this method should be called to ensure staging is complete and it is safe to begin modifying the original      \nAn implementation of AsyncStager which stages the state_dict on CPU RAM and blocks until the copy is complete. This implementation also provides an option to optimize stage latency using pinned memory.\nIn addition to the above entrypoints, objects, as described below, provide additional customization during saving/loading .. automodule:: torch.distributed.checkpoint.stateful     \nStateful protocol for objects that can be checkpointed and restored.          \nObjects should return their state_dict representation as a dictionary. The output of this function will be checkpointed, and later restored in .\nBecause of the inplace nature of restoring a checkpoint, this function is also called during .\nThe following types define the IO interface used during checkpoint:     \nOne StorageReader instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.\nA subclass should expected the following sequence of calls by :\n  1. (all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n\n    \nWhile this method can produce a completely different plan, the preferred way is to store storage specific data in LoadPlan::storage_data.     \nWhile this method can produce a completely different plan, the recommended way is to store storage specific data in LoadPlan::storage_data.     \nA subclass should call to deserialize a BytesIO object into the right place.\nA subclass should call to get access to the tensors that in should load data into.\nIt’s the StorageLayer responsibility to properly schedule any cross device copies required.     \n  * () – The planner object to use to resolve items.\n\n         \nCalls to indicates a brand new checkpoint read is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint read. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage.     \n() – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is more like a key-value store. (Default: )     \nCheck if the given checkpoint_id is supported by the stroage. This allow us to enable automatic storage selection.     \nOne StorageWriter instance acts as both the coordinator and the follower in a distributed checkpoint. As part of initialization, each instance is told its role.\n  1. (all ranks) set checkpoint_id if users pass a valid checkpoint_id.\n\n    \nWrite the metadata and marks the current checkpoint as successful.\nThe actual format/schema used for serializing is an implementation detail. The only requirement is that it’s recoverable in to the same object graph.     \n\n    \nWhile this method can produce a completely different plan, the preferred way is to store storage specific data in SavePlan::storage_data.     \nWhile this method can produce a completely different plan, the recommended way is to store storage specific data in SavePlan::storage_data.     \nCalls to indicates a brand new checkpoint write is going to happen. A checkpoint_id may be present if users set the checkpoint_id for this checkpoint write. The meaning of the checkpiont_id is storage-dependent. It can be a path to a folder/file or a key for a key-value storage.     \n() – The ID of this checkpoint instance. The meaning of the checkpoint_id depends on the storage. It can be a path to a folder or to a file. It can also be a key if the storage is a key-value store. (Default: )     \nReturn the storage-specific metadata. This is used to store additional information in a checkpoint that can be useful for providing request-level observability. StorageMeta is passed to the during save calls. Returns None by default.     \nCheck if the given checkpoint_id is supported by the stroage. This allow us to enable automatic storage selection.     \nA subclass should call on each item from the plan to get access to the underlying object to write.\nSubclasses should lazily call as it can allocate memory. In case of tensors, make following assumptions:\n  * They might be on any device, including not matching the one on \n  * They might be views or not contiguous. Only the projection needs to be saved.\n\n    \n  * () – Planner object to be used to resolve items to data.\n\n\nThe following types define the planner interface used during checkpoint:     \nAbstract class defining the protocol used by load_state_dict to plan the load process.\nLoadPlanner are stateful objects that can be used to customize the whole load process.\nLoadPlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.\nA planner subclass can expect the following sequence of calls during load_state_dict:\n  1.   2.     \nProcess the state_dict and produces a that will be sent for global planning.\n  3.     \nTakes the LoadPlan from all ranks and make any global decision.\n  4.   5. \n\nresolve_tensor and commit_tensor - called multiple times on each rank\n    \nThey are called in pair for each Tensor value in state_dict.\n\n\nUsers are recommended to extend DefaultLoadPlanner instead of this interface directly as most changes can be expressed by changes in a single method.\nRewriting state_dict. This is the simplest way to extend the load process as it doesn’t requite understanding the intrincacies of how LoadPlan works. We need to keep a reference to the original state_dict as load happens in place so we need to be able to perform it in place     \nThe provided tensor is the same one returned by the call to . This method is only needed if this LoadPlanner needs to post process prior to copying it back to the one in the state_dict.\nThe contents of tensor will follow its device synchronization model.     \nCompute the global load plan and return plans for each rank.\n. N.B. This is called on the coordinator rank only     \nCreate a LoadPlan based on state_dict and metadata provided by set_up_planner.          \nThis method is expected to modify in-place the underlying state_dict.\nThe contents of are defined by the SavePlanner used to produce the checkpoint being loaded.     \nReturn the BytesIO to be used by the StorageReader to load .\nThe BytesIO should alias with one on the underlying state_dict as StorageReader will replace its contents.     \nReturn the tensor described by to be used by the StorageReader to load .\nThe tensor should alias with one on the underlying state_dict as StorageReader will replace its contents. If, for any reason, that’s not possible, the planner can use the method to copy the data back to the one in state_dict.          \nAbstract class defining the protocol used by save_state_dict to plan the save process.\nSavePlanners are stateful objects that can be used to customize the whole save process.\nSavePlanner acts as an access proxy to the state_dict, so any transformation done to it will be visible to the whole process.\nA planner subclass can expect the following sequence of calls during save_state_dict:\n  1.   2.     \nProcess the state_dict and produces a that will be sent for global planning.\n  3.     \nTakes the SavePlan from all ranks and make any global decision.\n  4.     \nThis gives each rank a chance to adjust to global planning decisions.\n  5.     \nLookups a value on the for the storage layer to write.\n\n\nUsers are recommended to extend DefaultSavePlanner instead of this interface directly as most changes can be expressed by changes in a single method.\nRewriting state_dict. This is the simplest way to extend the save process as it doesn’t requite understanding the intrincacies of how SavePlan works:\nModifying local plan and lookup in tandem. This is useful when fine control of how data is persisted\nUsing the global planning step to make central decisions that can’t be made individually by each rank\n```\n   \n   \n \n# This uses the default local plan behavior of having all non-sharded writes in rank 0\n\n      \n            \n          \n                     \n                \n        \n          \n             \n                  \n        \n         \n\n```\n\nFinally, some planners need to save additional metadata in the checkpoint, this is accomplished by having each rank contribute their data items in the local plan and the global planner aggregate them:     \nCompute the global checkpoint plan and return the local plan of each rank.     \nThis will be aggregated and passed to create_global_plan. Planner specific data can be passed through SavePlan::planner_data.          \nLookup the object associated with in and apply any transformation (such as serialization) prior to the storage layer consuming it.\nCalled on each rank multiple times, at least once per WriteItem in the final SavePlan.\nThis method should be idempotent and thread-save. StorageWriter implementations are free to call it as frequently as they need.\nAny transformation that allocates memory should be lazily done when his method is called in order to reduce peak memory required by checkpointing.\nWhen returning tensors, they can be on any device or format, they can be views too. It’s the storage layer responsibility to figure out how to save them.     \nImplementations should save those values as they won’t be provided lated in the save process.     \nDataclass which holds information about what needs to be written to storage.     \nCalculates the storage size of the underlying tensor, or None if this is not a tensor write.     \nOptional[int] storage size, in bytes of underlying tensor if any.          \nreturn the checkpoint_id that will be used to load the checkpoint.     \n\n\nThe checkpoint consist of one file per write request plus a file with the serialized metadata.\nWe provide default implementations of and that can handle all of torch.distributed constructs such as FSDP, DDP, ShardedTensor and DistributedTensor.          \nExtension from the planner interface to make it easy to extend the default planner.     \nExtension from the planner interface to make it easy to extend the default planner.     \nflatten_state_dict: Handle state_dict with nested dicts flatten_sharded_tensors: For FSDP in 2D parallel mode allow_partial_load: If False, will raise a runtime error if a key is present in state_dict, but not in the checkpoint.     \nExtension from the planner interface to make it easy to extend the default planner.     \nExtension from the planner interface to make it easy to extend the default planner.\nDue to legacy design decisions, the state dictionaries of and may have different keys or fully qualified names (e.g., layer1.weight) even when the original unparallelized model is identical. Moreover, offers various types of model state dictionaries, such as full and sharded state dictionaries. Additionally, optimizer state dictionaries employ parameter IDs instead of fully qualified names to identify parameters, potentially causing issues when parallelisms are used (e.g., pipeline parallelism).\nTo tackle these challenges, we offer a collection of APIs for users to easily manage state_dicts. returns a model state dictionary with keys consistent with those returned by the unparallelized model state dictionary. Similarly, provides the optimizer state dictionary with keys uniform across all parallelisms applied. To achieve this consistency, converts parameter IDs to fully qualified names identical to those found in the unparallelized model state dictionary.\nNote that results returned by these APIs can be used directly with the and methods without requiring any additional conversions.\nand are provided to load the model and optimizer state_dict generated by by their respective getter APIs.\nNote that can only be called before or after is called on optimizers.\nNote that this feature is experimental, and API signatures might change in the future.     \ncan process any module that is parallelized by PyTorch FSDP/fully_shard, DDP/replicate, tensor_parallel/parallelize_module, and any combination of these parallelisms. The main functions of are: 1.) returning a model and optimizer state_dict that can be resharded with a different number of trainers and/or different parallelisms. 2.) hiding the parallelism-specific state_dict APIs. Users don’t have to call these APIs. 3.) sanity checking the result state_dict.\nThe keys of the result state dictionary are the canonical FQNs (Fully Qualified Names). A canonical FQN refers to the FQN based on a parameter’s position in an nn.Module hierarchy. More specifically, a canonical FQN to a parameter is the FQN returned by or when the module is not distributed by any parallelisms. Since the optimizer internally uses parameter IDs to represent a parameter, there will be a conversion from the parameter IDs to the canonical FQNs when calling this API.\ncan also process a module that is not parallelized. In such a case, only performs one function – converting the optimizer parameter IDs to the canonical FQNs.\n```\n\n\n   \n   \n\n```\n    \n  * () – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.\n  * () – the options to control how model state_dict and optimizer state_dict should be returned. See for the details.\n\n         \n  * () – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.\n  * () – the options to control how model state_dict and optimizer state_dict should be returned. See for the details.\n\n         \n  * () – Optional[set[nn.Module]]: only return the model parameters that belong to the submodules.\n  * () – the options to control how model state_dict and optimizer state_dict should be returned. See for the details.\n\n    \nThe counterpart of to set the state_dict to the model and optimizers. The given and do not have to be returned by but must meet the following requirements: 1) all FQNs are canonical FQNs as defined in , 2) if a tensor is sharded, it must be either a ShardedTensor or DTensor, 3) optimizer state_dict cannot contain the parameter IDs; the keys should be the canonical FQNs.     \nis called on the optimizers. Otherwise, the optimizer states won’t be initialized correctly.     \n  * () – (Union[Dict[nn.Module, Dict[str, ValueType]], Dict[str, ValueType]]): the model state_dict to load. If the key of the is nn.Module, the key is a submodule of and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.\n  * () – the options to control how model state_dict and optimizer state_dict should be loaded. See for the details.\n\n    \n  * is a list of str containing the missing keys of the model state_dict.\n  * is a list of str containing the unexpected keys of the model state_dict.\n\n    \nThe counterpart of to set the state_dict to the model. See for the detail usage.     \n  * () – (Dict[str, ValueType]): the model state_dict to load. If the key of the is nn.Module, the key is a submodule of and the value should be the state_dict of the submodule. When loading the state_dict, the prefix of the submodule will be append to the state_dict.\n  * () – the options to control how model state_dict and optimizer state_dict should be loaded. See for the details.\n\n    \n\n    \nThe counterpart of to set the state_dict to the optimizers. See for the detail usage.     \nis called on the optimizers. Otherwise, the optimizer states won’t be initialized correctly.     \n  * () – the options to control how model state_dict and optimizer state_dict should be loaded. See for the details.\n\n    \n  * : if this is set to True, all the tensors in the returned state_dict will be gathered. No ShardedTensor and DTensor will be in the returned state_dict.\n  * : offload all the tensors to cpu. To prevent CPU OOM, if is also true, then only the rank0 will get the state_dict and all other ranks will get empty state_dict.\n  * : if the value is True, the returned state_dict won’t contain any frozen parameters – the is False. The default value is False.\n  * (deprecated): when is not None, this option indicates whether to keep the submodule prefixes from the state_dict keys. or example, if the submodule is and the full FQN of the parameter is of the param. When this option is True, the parameter’s key in the returned state_dict will be . If the options is False, the key will be . Note that if is False, there may be conflicted FQNs, hence there should be only one submodule in .\n  * \n\n: when the option is True, rank0 should receive a\n    \nfull state_dict and will broadcast the tensors in the state_dict/ optim_state_dict one by one to other ranks. Other ranks will receive the tensors and shard according to the local shards in the model and optimizer. must be set to True when using this option. This option currently only supports DTensor, not the legacy ShardedTensor.\n\n\nFor users which are used to using and sharing models in the format, the following methods are provided which provide offline utilities for converting betweeing formats.     \nGiven a directory containing a DCP checkpoint, this function will convert it into a Torch save file.     \n  * () – Filename to store the converted Torch save file.\n\n\nTo avoid OOM, it’s recommended to only run this function on a single rank.     \nGiven the location of a torch save file, converts it into a DCP checkpoint.     \n\n\nTo avoid OOM, it’s recommended to only run this function on a single rank.\nThe following classes can also be utilized for online loading and resharding of models from the torch.save format.     \nStorageReader for reading a Torch Save file. This reader will read the entire checkpoint on the coordinator rank, and then broadcast and shard each tensor to all ranks.     \nReads torch save data on the coordinator rank, and broadcast afterwards this incurrs a communication cost, but avoids having to load the entire checkpoint on each rank, hopefully preventing OOM issues     \nExtends the default StorageReader to support building the metadata file     \nExtension of DefaultLoadPlanner, which creates a new Metadata object based on the passed in state dict, avoiding the need to read metadata from disk. This is useful when reading formats which don’t have a metadata file, like Torch Save files.     \nSetups of the planner, extnding default behavior by creating the Metadata object from the state dict\nThe following experimental interfaces are provided for improved observability in production environments:\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.algorithms.join.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThe generic join context manager facilitates distributed training on uneven inputs. This page outlines the API of the relevant classes: , , and . For a tutorial, see [Distributed Training with Uneven Inputs Using the Join Context Manager](https://pytorch.org/tutorials/advanced/generic_join.html).     \nThis class defines the generic join context manager, which allows custom hooks to be called after a process joins.\nThese hooks should shadow the collective communications of non-joined processes to prevent hanging and erroring and to ensure algorithmic correctness. Refer to for details about the hook definition.\nThe context manager requires each participating to call the method before its own per- iteration collective communications to ensure correctness.\nThe context manager requires that all attributes in the objects are the same. If there are multiple objects, then the of the first is used. The process group and device information is used for checking for non- joined processes and for notifying processes to throw an exception if is enabled, both of which using an all- reduce.     \n  * () – a list of the participating s; their hooks are iterated over in the given order.\n  * ( disables the context manager’s functionality and should only be set when the user knows the inputs will not be uneven (default: ).\n\n\n```\n \n \n   \n   \n   \n   \n   \n\n\n \n      \n        \n        \n    # Rank 1 gets one more input than rank 0\n            \n      \n           \n              \n            \n            \n    \n\n```\n    \nNotifies the join context manager that the calling process has not yet joined.\nThen, if , checks if uneven inputs have been detected (i.e. if one process has already joined) and throws an exception if so.\nThis method should be called from a object before its per-iteration collective communications. For example, this should be called at the beginning of the forward pass in .\nOnly the first object passed into the context manager performs the collective communications in this method, and for the others, this method is vacuous.     \nAn async work handle for the all-reduce meant to notify the context manager that the process has not yet joined if is the first one passed into the context manager; otherwise.     \nA joinable class (inheriting from ) should implement , which returns a instance, in addition to and that return device and process group information, respectively.     \nReturn the device from which to perform collective communications needed by the join context manager.          \n( instances sharing the same join context manager are forwarded the same value for .     \nReturns the process group for the collective communications needed by the join context manager itself.     \nThis defines a join hook, which provides two entry points in the join context manager.\nEntry points : a main hook, which is called repeatedly while there exists a non-joined process, and a post-hook, which is called once all processes have joined.\nTo implement a join hook for the generic join context manager, define a class that inherits from and override and as appropriate.     \nCall this hook while there exists a non-joined process to shadow collective communications in a training iteration.\nTraining iteration i.e., in one forward pass, backward pass, and optimizer step.     \nIt is passed an additional argument , which indicates if the rank is one of the last to join.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/cuda.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nIt implements the same function as CPU tensors, but they utilize GPUs for computation.\nIt is lazily initialized, so you can always import it, and use to determine if your system supports CUDA.\nReturn used global (device) memory in bytes as given by or .  \n---  \nContext-manager that changes the current device to that of given object.  \nReturn current value of debug mode for cuda synchronizing operations.  \nForce collects GPU memory after it has been released by CUDA IPC.  \nReturn a bool indicating if the current CUDA/ROCm device supports dtype tf32.  \nReturn the percent of time over the past sample period during which global (device) memory was being read or written as given by .  \nSet the current stream.This is a wrapper API to set the stream.  \nWrap around the Context-manager StreamContext that selects a given stream.  \nWait for all kernels in all streams on a CUDA device to complete.  \nReturn the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by .  \nReturn the average temperature of the GPU sensor in Degrees C (Centigrades).  \nReturn the average power draw of the GPU sensor in mW (MilliWatts)  \nReturn the clock speed of the GPU SM in MHz (megahertz) over the past sample period as given by .  \nReturn the random number generator state of the specified GPU as a ByteTensor.  \n---  \nReturn a list of ByteTensor representing the random number states of all devices.  \nSet the random number generator state of the specified GPU.  \nSet the seed for generating random numbers for the current GPU.  \nSet the seed for generating random numbers on all GPUs.  \nSet the seed for generating random numbers to a random number for the current GPU.  \nSet the seed for generating random numbers to a random number on all GPUs.  \nReturn True if CUDA graph capture is underway on the current CUDA stream, False otherwise.  \n---  \nReturn an opaque token representing the id of a graph memory pool.  \nContext-manager that captures CUDA work into a object for later replay.  \nRelease all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in .  \n---  \nReturn a human-readable printout of the running processes and their GPU memory use for a given device.  \nReturn the global free and total GPU memory for a given device using cudaMemGetInfo.  \nReturn a dictionary of CUDA memory allocator statistics for a given device.  \nReturn a dictionary of CUDA memory allocator statistics for a given device.  \nReturn a human-readable printout of the current memory allocator statistics for a given device.  \nReturn a snapshot of the CUDA memory allocator state across all devices.  \nReturn the current GPU memory occupied by tensors in bytes for a given device.  \nReturn the maximum GPU memory occupied by tensors in bytes for a given device.  \nReset the starting point in tracking maximum GPU memory occupied by tensors for a given device.  \nReturn the current GPU memory managed by the caching allocator in bytes for a given device.  \nReturn the maximum GPU memory managed by the caching allocator in bytes for a given device.  \nReset the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.  \nReset the \"peak\" stats tracked by the CUDA memory allocator.  \nReset the \"peak\" stats tracked by the host memory allocator.  \nReturn a string describing the active allocator backend as set by .  \nChange the currently used memory allocator to be the one provided.  \nMemPool represents a pool of memory in a caching allocator.  \nMemPoolContext holds the currently active pool and stashes the previous pool.       \nA context manager that routes allocations to a given pool.     \n  * () – a MemPool object to be made active so that allocations route to this pool.\n  * () – selected device. Uses MemPool on the current device, given by , if is (default).\n\n\nPush a range onto a stack of nested range span.  \n---  \nPop a range off of a stack of nested range spans.  \nContext manager / decorator that pushes an NVTX range at the beginning of its scope, and pops it at the end.  \nCreate a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.  \n---  \nSome operations could be implemented using more than one library or more than one technique. For example, a GEMM could be implemented for CUDA or ROCm using either the cublas/cublasLt libraries or hipblas/hipblasLt libraries, respectively. How does one know which implementation is the fastest and should be chosen? That’s what TunableOp provides. Certain operators have been implemented using multiple strategies as Tunable Operators. At runtime, all strategies are profiled and the fastest is selected for all subsequent operations.\nCUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the for information on how to use it.\nThe APIs in provide thin wrappers around certain cuFile APIs that allow direct memory access transfers between GPU memory and storage, avoiding a bounce buffer in the CPU. See the \nThese APIs can be used in versions greater than or equal to CUDA 12.6. In order to use these APIs, one must ensure that their system is appropriately configured to use GPUDirect Storage per the \nSee the docs for for an example of how to use these.\nRegisters a storage on a CUDA device as a cufile buffer.  \n---  \nDeregisters a previously registered storage on a CUDA device as a cufile buffer.  \n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.elastic.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/module_tracker.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis utility can be used to track the current position inside an hierarchy. It can be used within other tracking tools to be able to easily associate measured quantities to user-friendly names. This is used in particular in the FlopCounterMode today.     \nis a context manager that tracks the nn.Module hierarchy during execution so that other system can query which Module is currently being executed (or its backward is being executed).\nYou can access the attribute on this context manager to get the set of all the Modules currently being executed via their fqn (fully qualified name, also used as the key within the state_dict). You can access the attribute to know if you are currently running in backward or not.\nNote that is never empty and always contains the “Global” key. The flag will remain after the forward until another Module is executed. If you need it to be more accurate, please submit an issue requesting this. Adding a map from fqn to the module instance is possible but not done yet, please submit an issue requesting this if you need it.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/jit_utils.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/mobile_optimizer.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/executorch/stable/getting-started-setup.html) [ ](https://pytorch.org/tutorials/beginner/introyt.html)\n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n  * Compatibility with a wide variety of computing platforms, from high-end mobile phones to highly constrained embedded systems and microcontrollers.\n  * Enabling developers to use the same toolchains and Developer Tools from PyTorch model authoring and conversion, to debugging and deployment to a wide variety of platforms.\n  * Providing end users with a seamless and high-performance experience due to a lightweight runtime and utilizing full hardware capabilities such as CPUs, NPUs, and DSPs.\n\n\n  * LLMs (Large Language Models), CV (Computer Vision), ASR (Automatic Speech Recognition), TTS (Text To Speech)\n\n\n\n\n\n\n\n\n\n\n  *     *       * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/model_zoo.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n    \nIf downloaded file is a zip file, it will be automatically decompressed.\nIf the object is already present in , it’s deserialized and returned. The default value of is where is the directory returned by .     \n  * () – a function or a dict specifying how to remap storage locations (see torch.load)\n  * () – whether or not to display a progress bar to stderr. Default: True\n  * () – If True, the filename part of the URL should follow the naming convention where is the first eight or more digits of the SHA256 hash of the contents of the file. The hash is used to ensure unique names and to verify the contents of the file. Default: False\n  * () – name for the downloaded file. Filename from will be used if not set.\n  * () – If True, only weights will be loaded and no complex pickled objects. Recommended for untrusted sources. See for more details.\n\n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPyTorch FSDP2 provides a fully sharded data parallelism (FSDP) implementation targeting performant eager-mode while using per-parameter sharding for improved usability.\n  * If you are new to FSDP, we recommend that you start with FSDP2 due to improved usability.\n  * If you are currently using FSDP1, consider evaluating the following differences to see if you should switch to FSDP2:\n\n\n  * FSDP2 uses -based dim-0 per-parameter sharding for a simpler sharding representation compared to FSDP1’s flat-parameter sharding, while preserving similar throughput performance. More specifically, FSDP2 chunks each parameter on dim-0 across the data parallel workers (using ), whereas FSDP1 flattens, concatenates, and chunks a group of tensors together, making reasoning about what data is present on each worker and resharding to different parallelisms complex. Per-parameter sharding provides a more intuitive user experience, relaxes constraints around frozen parameters, and allows for communication-free (sharded) state dicts, which otherwise require all-gathers in FSDP1.\n  * FSDP2 implements a different memory management approach to handle the multi-stream usages that avoids . This ensures deterministic and expected memory usage and does not require blocking the CPU like in FSDP1’s .\n  * FSDP2 exposes APIs for manual control over prefetching and collective scheduling, allowing power users more customization. See the methods on below for details.\n  * FSDP2 simplifies some of the API surface: e.g. FSDP2 does not directly support full state dicts. Instead, users can reshard the sharded state dicts containing s to full state dicts themselves using APIs like or by using higher-level APIs like ‘s distributed state dict APIs. Also, some other args have been removed; see \n\n\nIf you are onboarding FSDP for the first time or if any of the above appeals to your use case, we recommend that you consider using FSDP2.\nis currently in prototype state and under development. The core API will likely not change, but we may make some API changes if necessary.     \nApply fully sharded data parallelism (FSDP) to , where FSDP shards module parameters, gradients, and optimizer states across data parallel workers to save memory at the cost of communication.\nAt initialization, FSDP shards the module’s parameters across the data parallel workers given by . Before forward, FSDP all-gathers the sharded parameters across the data-parallel workers to get the unsharded parameters for forward computation. If is , then FSDP frees the unsharded parameters after forward and re-all-gathers them in backward before gradient computation. After gradient computation, FSDP frees the unsharded parameters and reduce-scatters the unsharded gradients across data-parallel workers.\nThis implementation represents the sharded parameters as s sharded on dim-0, while the unsharded parameters will be like the original parameters on (e.g. if originally ). A module on all-gathers the parameters, and a module on frees them (if needed). Similar backward hooks all-gather parameters and later free parameters and reduce-scatter gradients.\nSince grouping multiple tensors together for one collective is critical for communication efficiency, this implementation makes this grouping first class. Calling on constructs one group that includes the parameters in except those already assigned to a group from an earlier call on a submodule. This means that should be called bottom-up on your model. Each group’s parameters are all-gathered in one collective, and its gradients are reduce-scattered in one collective. Partitioning the model into multiple groups (“layer by layer”) allows for peak memory savings and communication/computation overlap. Users generally should call only on the topmost root module.     \n  * () – The module or modules to shard with FSDP and group together for communication.\n  * () – This data parallel mesh defines the sharding and device. If 1D, then parameters are fully sharded across the 1D mesh (FSDP) with placement. If 2D, then parameters are sharded across the 1st dim and replicated across the 0th dim (HSDP) with placement. The mesh’s device type gives the device type used for communication; if a CUDA or CUDA-like device type, then we use the current device.\n  * This controls the parameter behavior after forward and can trade off memory and communication:\n    * If , then this reshards parameters after forward and re-all-gathers in backward.\n    * If , then this keeps the unsharded parameters in memory after forward and avoids the all-gather in backward.\n    * If an , then this represents the world size to reshard to after forward. It should be a non-trivial divisor of the shard dim size (i.e. excluding 1 and the dim size itself). A choice may be the intra-node size (e.g. ). This allows the all-gather in backward to be over a smaller world size at the cost of higher memory usage than setting to .\n    * The root FSDP state has its value specially set to as a heuristic since its parameters would typically be immediately all-gathered for backward.\n    * After forward, the parameters registered to the module depend on to this: The registered parameters are the sharded parameters if ; unsharded parameters if ; and the paramters resharded to the smaller mesh otherwise. To modify the parameters between forward and backward, the registered parameters must be the sharded parameters. For or an , this can be done by manually resharding via .\n  * () – This callable can be used to override the sharding placement for a parameter to shard a parameter on a dimension other than dim-0. If this callable returns a placement (not ), then FSDP will shard according to that placement (e.g. ). If sharding on a nonzero dim, we currently require even sharding, i.e. the tensor dim size on that dim must be divisible by the FSDP shard mesh size.\n  * () – This controls the mixed precision policy, which offers parameter/reduction mixed precision for this module. See for details.\n  * () – This controls the offloading policy, which offers parameter/gradient/optimizer state offloading. See and its subclasses for details.\n  * () – Optional(Set[nn.Parameter]): The set of parameters that we don’t want to shard with FSDP.\n\n\nCalling dynamically constructs a new class that subclasses and an FSDP class . For example, if we call on a module , then FSDP constructs a new class and changes ‘s type to this. Otherwise, does not change the module structure and parameter fully-qualified names. The class allows providing some FSDP-specific methods on the module.          \nReshards the module’s parameters, freeing the unsharded parameters if they are allocated and registering the sharded parameters to the module. This method is recursive.          \n  * () – User-defined all-reduce hook with expected signature where is the reduce-scatter output if only using FSDP or the all-reduce output if using native HSDP.\n  * () – Stream to run the all-reduce hook in. This should only be set if not using native HSDP. If using native HSDP, the hook will run in the internally defined all-reduce stream used by the native HSDP all-reduce.\n\n    \nSets whether the next backward is the last one. On the last backward, FSDP waits on pending gradient reduction and clears internal data data structures for backward prefetching. This can be useful for microbatching.     \nSets the FSDP modules for which this FSDP module should explicitly prefetch all-gathers in backward. This overrides the default backward pretching implementation that prefetches the next FSDP module based on the reverse post-forward order.\nPassing a singleton list containing the previous FSDP module gives the same all-gather overlap behavior as the default overlap behavior. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory.     \nSets the FSDP modules for which this FSDP module should explicitly prefetch all-gathers in forward. The prefetching runs after this module’s all-gather copy-out.\nPassing a singleton list containing the next FSDP module gives the same all-gather overlap behavior as the default overlap behavior, except the prefetched all-gather is issued earlier from the CPU. Passing a list with at least length two is required for more aggressive overlap and will use more reserved memory.     \nSets a post-optimizer-step event for the root FSDP module to wait the all-gather streams on.\nBy default, the root FSDP module waits the all-gather streams on the current stream to ensure that the optimizer step has finished before all-gathering. However, this may introduce false dependencies if there is unrelated computation after the optimizer step. This API allows the user to provide their own event to wait on. After the root waits on the event, the event is discarded, so this API should be called with a new event each iteration.     \n() – Event recorded after the optimizer step to wait all-gather streams on.     \nSets a custom divide factor for the reduce-scatter. This becomes a custom reduce op using NCCL’s PreMulSum, which allows multiplying by the factor before reduction.     \nSets if the module should all-reduce gradients. This can be used to implement gradient accumulation with only reduce-scatter but not all-reduce for HSDP.     \nSets if the module should sync gradients. This can be used to implement gradient accumulation . For HSDP, this controls both reduce-scatter and all-reduce together. This is the equivalence of in FSDP1.     \nSets if the module should reshard parameters after backward. This can be used during gradient accumulation to trade off higher memory for reduced communication since the unsharded parameters do not need to be re-all-gathered before the next forward.     \nSets whether the FSDP module’s parameters need to be unsharded in backward. This can be used in expert cases when the user knows that all parameters in this FSDP module’s parameter group are not needed for backward computation (e.g. embedding).     \nUnshards the module’s parameters by allocating memory and all-gathering the parameters. This method is recursive. The unshard follows the , so it will all-gather following if set.     \n(, then returns a that has a method to wait on the unshard op. If , then returns and waits on the handle inside this function.\nIf , then FSDP will wait on the pending unshard in the module’s pre-forward for the user. The user only needs to call explicitly if the wait should happen before pre-forward.          \nWaits on the unshard op. This ensures that the current stream can use the unsharded parameters, which are now registered to the module.     \nRegisters a method on to be considered a forward method for FSDP.\nFSDP all-gathers parameters pre-forward and optionally frees parameters post-forward (depending on ). FSDP only knows to do this for by default. This function patches a user-specified method to run the pre/post-forward hooks before/after the method, respectively. If is not an , then this is a no-op.     \nThis configures FSDP’s mixed precision. Unlike autocast, this applies mixed precision at the module level, not op level, which means low-precision activations are saved for backward and high-to-low-precision casts are incurred only at module boundaries.\nFSDP works well with module-level mixed precision since it keeps the high-precision sharded parameters in memory anyway. In other words, FSDP does not require any extra memory to keep a high-precision copy of the parameters for the optimizer step.     \n  * () – This specifies the dtype for the unsharded parameter and hence the dtype for forward/backward computation and the parameter all-gather. If this is , then the unsharded parameter uses the original dtype. The optimizer step uses the sharded parameter in the original dtype. (Default: )\n  * () – This specifies the dtype for gradient reduction (i.e. reduce-scatter or all-reduce). If this is but is not , then the reduction uses the compute dtype. This can be used to run gradient reduction in full precision while using low precision for compute. If also gradient reduction is disabled via , then FSDP will accumulate gradients using . (Default: )\n  * () – This specifies the dtype for casting floating-point forward outputs. This can be used to help implement cases where different modules have different mixed precision policies. (Default: )\n\n    \nThis base class represents the policy of no offloading and is only used as the default value for the arg.     \nThis offload policy offloads parameters, gradients, and optimizer states to CPU. Sharded parameters are copied host-to-device before all-gather. The all-gathered parameters are freed according to . Sharded gradients are copied device-to-host in backward, and the optimizer step runs on CPU with CPU optimizer states.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.optim.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nDistributed optimizer is not currently supported when using CUDA tensors\nexposes DistributedOptimizer, which takes a list of remote parameters () and runs the optimizer locally on the workers where the parameters live. The distributed optimizer can use any of the local optimizer to apply the gradients on each worker.     \nDistributedOptimizer takes remote references to parameters scattered across workers and applies the given optimizer locally for each parameter.\nThis class uses in order to retrieve the gradients for specific parameters.\nConcurrent calls to , either from the same or different clients, will be serialized on each worker – as each worker’s optimizer can only work on one set of gradients at a time. However, there is no guarantee that the full forward-backward-optimizer sequence will execute for one client at a time. This means that the gradients being applied may not correspond to the latest forward pass executed on a given worker. Also, there is no guaranteed ordering across workers.\ncreates the local optimizer with TorchScript enabled by default, so that optimizer updates are not blocked by the Python Global Interpreter Lock (GIL) in the case of multithreaded training (e.g. Distributed Model Parallel). This feature is currently enabled for most optimizers. You can also follow      \n  * () – the class of optimizer to instantiate on each worker.\n  * () – list of RRefs to local or remote parameters to optimize.\n  * – arguments to pass to the optimizer constructor on each worker.\n  * – arguments to pass to the optimizer constructor on each worker.\n\n    \nThis will call on each worker containing parameters to be optimized, and will block until all workers return. The provided will be used to retrieve the corresponding that contains the gradients that should be applied to the parameters.     \n– the autograd context id for which we should run the optimizer step.          \n  * () – A model averager instance to run post-localSGD algorithm.\n\n\n```\n \n   \n   \n   \n   \n   \n  \n  \n\n\n  \n     \n\n\n\n    \n \n\n# Create a post-localSGD optimizer that wraps a local optimizer.\n# Note that ``warmup_steps`` used in ``PostLocalSGDOptimizer`` must be the same as\n\n   \n  \n    \n     \n\n\n# In the first 100 steps, DDP runs global gradient averaging at every step.\n# After 100 steps, DDP runs gradient averaging within each subgroup (intra-node by default),\n# and post-localSGD optimizer runs global model averaging every 4 steps after applying the local optimizer.\n    \n   \n      \n   \n   \n\n```\n    \nThis is the same as , but also restores model averager’s step value to the one saved in the provided .\nIf there is no entry in , it will raise a warning and initialize the model averager’s step to 0.     \nThis is the same as , but adds an extra entry to record model averager’s step to the checkpoint to ensure reload does not cause unnecessary warm up again.     \nWrap an arbitrary and shards its states across ranks in the group.\nThe local optimizer instance in each rank is only responsible for updating approximately parameters and hence only needs to keep optimizer states. After parameters are updated locally, each rank will broadcast its parameters to all other peers to keep all model replicas in the same state. can be used in conjunction with to reduce per-rank peak memory consumption.\nuses a sorted-greedy algorithm to pack a number of parameters at each rank. Each parameter belongs to a single rank and is not divided among ranks. The partition is arbitrary and might not match the the parameter registration or usage order.     \n  * () – if , parameters are packed into buckets to speed up communication, and fields point to bucket views at different offsets; if , each individual parameter is communicated separately, and each stays intact (default: ).\n  * () – if , is overlapped with ‘s gradient synchronization; this requires (1) either a functional optimizer for the argument or one with a functional equivalent and (2) registering a DDP communication hook constructed from one of the functions in ; parameters are packed into buckets matching those in , meaning that the argument is ignored. If , runs disjointly after the backward pass (per normal). (default: )\n  * – any trailing arguments, which are forwarded to the local optimizer.\n\n\nCurrently, requires that all of the passed-in parameters are the same dense type.\nIf you pass , be wary of the following: Given the way that overlapping with is currently implemented, the first two or three training iterations do not perform parameter updates in the optimizer step, depending on if or , respectively. This is because it needs information about the gradient bucketing strategy used by , which is not finalized until the second forward pass if or until the third forward pass if . To adjust for this, one option is to prepend dummy inputs.     \nThis can be useful when fine tuning a pre-trained network, as frozen layers can be made trainable and added to the as training progresses.\nThis method handles updating the shards on all partitions but needs to be called on all ranks. Calling this on a subset of the ranks will cause the training to hang because communication primitives are called depending on the managed parameters and expect all the ranks to participate on the same set of parameters.     \nConsolidate a list of s (one per rank) on the target rank.     \nand this method is called before this instance has been fully initialized, which happens once gradient buckets have been rebuilt.     \nIt enables training on uneven inputs by shadowing the collective communications in the optimizer step.\nGradients must be properly set before this hook is called.     \n( instances sharing the same join context manager are forwarded the same value for .     \nLoad the state pertaining to the given rank from the input , updating the local optimizer as needed.     \nand this method is called before this instance has been fully initialized, which happens once gradient buckets have been rebuilt.     \nReturn the last global optimizer state known to this rank.     \nand this method is called before this instance has been fully initialized, which happens once gradient buckets have been rebuilt; or if this method is called without a preceding call to .     \nPerform a single optimizer step and syncs parameters across all ranks.     \n() – a closure that re-evaluates the model and returns the loss; optional for most optimizers.\nAny extra parameters are passed to the base optimizer as-is.\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.pipelining.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nis currently in alpha state and under development. API changes may be possible. It was migrated from the \nPipeline Parallelism is one of the parallelism for deep learning. It allows the of a model to be partitioned such that multiple can execute different parts of the model code concurrently. Pipeline parallelism can be an effective technique for:\nThe above scenarios share a commonality that the computation per device cannot hide the communication of conventional parallelism, for example, the weight all-gather of FSDP.\nWhile promising for scaling, pipelining is often difficult to implement because it needs to of a model in addition to model weights. The partitioning of execution often requires intrusive code changes to your model. Another aspect of complexity comes from , with considered.\nThe package provides a toolkit that does said things which allows easy implementation of pipeline parallelism on models.\nIt consists of two parts: a and a . The splitting frontend takes your model code as-is, splits it up into “model partitions”, and captures the data-flow relationship. The distributed runtime executes the pipeline stages on different devices in parallel, handling things like micro-batch splitting, scheduling, communication, and gradient propagation, etc.\n  * Rich support for pipeline schedules, including GPipe, 1F1B, Interleaved 1F1B and Looped BFS, and providing the infrastructure for writing customized schedules.\n  * First-class support for cross-host pipeline parallelism, as this is where PP is typically used (over slower interconnects).\n  * Composability with other PyTorch parallel techniques such as data parallel (DDP, FSDP) or tensor parallel. The \n\n\nBefore we can use a , we need to create objects that wrap the part of the model running in that stage. The is responsible for allocating communication buffers and creating send/recv ops to communicate with its peers. It manages intermediate buffers e.g. for the outputs of forward that have not been consumed yet, and it provides a utility for running the backwards for the stage model.\nA needs to know the input and output shapes for the stage model, so that it can correctly allocate communication buffers. The shapes must be static, e.g. at runtime the shapes can not change from step to step. A class will be raised if runtime shapes do not match the expected shapes. When composing with other paralleisms or applying mixed precision, these techniques must be taken into account so the knows the correct shape (and dtype) for the output of the stage module at runtime.\nUsers may construct a instance directly, by passing in an representing the portion of the model that should run on the stage. This may require changes to the original model code. See the example in .\nAlternatively, the splitting frontend can use graph partitioning to split your model into a series of automatically. This technique requires the model is traceable with . Composability of the resulting with other parallelism techniques is experimental, and may require some workarounds. Usage of this frontend may be more appealing if the user cannot easily change the model code. See for more information.\nWe can now attach the to a pipeline schedule, and run the schedule with input data. Here is a GPipe example:\n```\n   \n\n\n   \n\n\n    \n\n\n\n   \n    \n\n      \n\n```\n\nNote that the above code needs to be launched for each worker, thus we use a launcher service to launch multiple processes:\nTo directly construct a , the user is responsible for providing a single instance that owns the relevant and , and defines a method that executes the operations relevant for that stage. For example, a condensed version of the Transformer class defined in Torchtitan shows a pattern of building an easily partitionable model.\n```\n \n       \n        \n\n          \n\n        # Using a ModuleDict lets us delete layers without affecting names,\n        \n          \n           \n              \n\n          \n\n       \n        # Handling layers being 'None' at runtime enables easy pipeline splitting\n              \n\n           \n               \n\n              \n              \n         \n\n```\n\nA model defined in this manner can be easily configured per stage by first initializing the whole model (using meta-device to avoid OOM errors), deleting undesired layers for that stage, and then creating a PipelineStage that wraps the model. For example:\n```\n \n        \n\n    # we construct the entire model, then delete the parts we do not need for this stage\n    # in practice, this can be done using a helper function that automatically divides up layers across stages.\n      \n\n       \n        \n         \n          \n          \n\n       \n        \n          \n         \n\n       \n      \n        \n        \n        \n        \n    \n\n```\n\nWhen composing with other Data or Model parallelism techniques, may also be required, if the output shape/dtype of the model chunk will be affected.\nIf you have a full model and do not want to spend time on modifying it into a sequence of “model partitions”, the API is here to help. Here is a brief example:\nIf we print the model, we can see multiple hierarchies, which makes it hard to split by hand:\nThe API splits your model given a , where stands for adding a split point execution of certain submodule in the function, and similarly, for split point such.\nThe “model partitions” are represented by submodules (, ), each of which is reconstructed with original model operations, weights and hierarchies. In addition, a “root-level” function is reconstructed to capture the data flow between those partitions. Such data flow will be replayed by the pipeline runtime later, in a distributed fashion.\nThe returned is a , with which you can create an optimizer, save or load checkpoints, or apply other parallelisms.\nalso allows you to create a distributed stage runtime on a device given a :\nAlternatively, if you would like to build the stage runtime later after some modification to the , you can use a functional version of the API. For example:\nThe frontend uses a tracer () to capture your model into a single graph. If your model is not full-graph’able, you can use our manual frontend below.\nFirst, the API turns our model into a directed acyclic graph (DAG) by tracing the model. It traces the model using – a PyTorch 2 full-graph capturing tool.\nThen, it groups together the needed by a stage into a reconstructed submodule: , , …\nDifferent from conventional submodule access methods like , the API does not only cut the module structure of your model, but also the function of your model.\nThis is necessary because model structure like merely captures information during , and does not capture any information about . Said differently, lacks information about the following aspects key to pipelininig:\n  * Whether there are any functional operators between child modules (for example, or operations will not be captured by ).\n\n\nThe API, on the contrary, makes sure that the behavior is truly preserved. It also captures the activation flow between the partitions, helping the distributed runtime to make correct send/receive calls without human intervention.\nAnother flexibility of the API is that split points can be at arbitrary levels within your model hierarchy. In the split partitions, the original model hierarchy related to that partition will be reconstructed at no cost to you. At a result, fully-qualified names (FQNs) pointing to a submodule or parameter would be still valid, and services that relies on FQNs (such as FSDP, TP or checkpointing) can still run with your partitioned modules with almost zero code change.\nYou can implement your own pipeline schedule by extending one of the following two class:\nis for schedules that assigns stage per rank. is for schedules that assigns multiple stages per rank.\nYou can turn on additional logging using the environment variable from :\n\n\nThe following set of APIs transform your model into a pipeline representation.     \nEnum representing the points at which a split can occur in the execution of a submodule. :ivar BEGINNING: Represents adding a split point the execution of a certain submodule in the function. :ivar END: Represents adding a split point the execution of a certain submodule in the function.          \n  * () – A dictionary using submodule names as split marker. (default: )\n  * () – The policy to use for splitting the module. (default: )\n\n    \npipe_split is a special operator that is used to mark the boundary between stages in a module. It is used to split the module into stages. It is a no-op if your annotated module is run eagerly.     \nGiven a sequence of args and kwargs, split them into a number of chunks according to their respective chunking specs.     \n  * () – chunking specs for args, in same shape as args\n  * () – chunking specs for kwargs, in same shape as kwargs\n\n    \nGiven a list of chunks, merge them into a single value according to the chunk spec.     \n\n    \nA class representing a pipeline stage in a pipeline parallelism setup.\nPipelineStage assumes sequential partitioning of the model, i.e. the model is split into chunks where outputs from one chunk feed into inputs of the next chunk, with no skip connections.\nPipelineStage performs runtime shape/dtype inference automatically by propagating the outputs from stage0 to stage1 and so forth, in linear order. To bypass shape inference, pass the and to each PipelineStage instance.     \n  * () – The process group for distributed training. If None, default group.\n  * () – If provided, dw_builder will build a new dw_runner function that will the W action (input weights) for F, I, W (Fwd, Input, Weight) zero bubble schedules.\n\n    \nCreate a pipeline stage given a stage_module to be wrapped by this stage and pipeline information.     \n  * () – the module to be wrapped by this stage\n  * () – information about the pipeline, can be retrieved by \n  * () – the device to be used by this stage\n  * () – the process group to be used by this stage\n\n    \nThe GPipe schedule. Will go through all the microbatches in a fill-drain manner.     \nThe 1F1B schedule. Will perform one forward and one backward on the microbatches in steady state.     \nThis schedule is mostly similar to the original paper. It differs by being relaxing the requirement of num_microbatch % pp_size == 0. Using the flex_pp schedule, we will have num_rounds = max(1, n_microbatches // pp_group_size) and it works as long as n_microbatches % num_rounds is 0. As a few examples, support\n  1. pp_group_size = 4, n_microbatches = 10. We will have num_rounds = 2 and n_microbatches % 2 is 0.\n  2. pp_group_size = 4, n_microbatches = 3. We will have num_rounds = 1 and n_microbatches % 1 is 0.\n\n    \nIn particular this is implementing the ZB1P schedule in the paper.     \nThis schedule will perform one forward and one backward on inputs for the microbatches in steady state and supports multiple stages per rank. Uses backward with respect to weights to fill in the pipeline bubble.\nThis ZB-V schedule would have the “zero bubble” property only if time forward == time backward input == time backward weights. In practice, this is not likely true for real models so alternatively a greedy scheduler could be implemented for unequal/unbalanced time.     \nGradients are scaled by num_microbatches depending on the argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False).     \nRun one iteration of the pipeline schedule with input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch.     \nGradients are scaled by num_microbatches depending on the argument, defaulting to True. This setting should match the configuration of your loss_fn, which may either average losses (scale_grads=True) or sum losses (scale_grads=False).     \nRun one iteration of the pipeline schedule with input. Will chunk the input into microbatches automatically, and go through the microbatches according to the schedule implementation.\nargs: positional arguments to the model (as in non-pipeline case). kwargs: keyword arguments to the model (as in non-pipeline case). target: target for the loss function. losses: a list to store the losses for each microbatch.\n  *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/jit.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n  * \n\nTorchScript is a way to create serializable and optimizable models from PyTorch code. Any TorchScript program can be saved from a Python process and loaded in a process where there is no Python dependency.\nWe provide tools to incrementally transition a model from a pure Python program to a TorchScript program that can be run independently from Python, such as in a standalone C++ program. This makes it possible to train models in PyTorch using familiar tools in Python and then export the model via TorchScript to a production environment where Python programs may be disadvantageous for performance and multi-threading reasons.\nFor a gentle introduction to TorchScript, see the tutorial.\nFor an end-to-end example of converting a PyTorch model to TorchScript and running it in C++, see the tutorial.\nTrace a function and return an executable or that will be optimized using just-in-time compilation.  \n---  \nTrace a module and return an executable that will be optimized using just-in-time compilation.  \nCreate an asynchronous task executing and a reference to the value of the result of this execution.  \nForce completion of a asynchronous task, returning the result of the task.  \nFunctionally equivalent to a , but represents a single function and does not have any attributes or Parameters.  \nPerform a set of optimization passes to optimize a model for the purposes of inference.  \nEnable or disables onednn JIT fusion based on the parameter .  \nSet the type and number of specializations that can occur during fusion.  \nGive errors if not all nodes have been fused in inference, or symbolically differentiated in training.  \nSave an offline version of this module for use in a separate process.  \nThis decorator indicates to the compiler that a function or method should be ignored and left as a Python function.  \nThis decorator indicates to the compiler that a function or method should be ignored and replaced with the raising of an exception.  \nThis method is a pass-through function that returns , mostly used to indicate to the TorchScript compiler that the left-hand side expression is a class instance attribute with type of .  \nIn many cases either tracing or scripting is an easier approach for converting a model to TorchScript. Tracing and scripting can be composed to suit the particular requirements of a part of a model.\nScripted functions can call traced functions. This is particularly useful when you need to use control-flow around a simple feed-forward model. For instance the beam search of a sequence to sequence model will typically be written in script but can call an encoder module generated using tracing.\nTraced functions can call script functions. This is useful when a small part of a model requires some control-flow even though most of the model is just a feed-forward network. Control-flow inside of a script function called by a traced function is preserved correctly.\nThis composition also works for s as well, where it can be used to generate a submodule using tracing that can be called from the methods of a script module.\nTorchScript is a statically typed subset of Python, so many Python features apply directly to TorchScript. See the full for details.\nTorchScript supports the use of most PyTorch functions and many Python built-ins. See for a full reference of supported functions.\nTorchScript supports a subset of the tensor and neural network functions that PyTorch provides. Most methods on Tensor as well as functions in the namespace, all functions in and most modules from are supported in TorchScript.\nSee for a list of unsupported PyTorch functions and modules.\nMany of Python’s for details), but no other Python modules (built-in or third party) are supported.\nFor a full listing of supported Python features, see .\nSetting the environment variable will disable all script and tracing annotations. If there is hard-to-debug error in one of your TorchScript models, you can use this flag to force everything to run using native Python. Since TorchScript (scripting and tracing) is disabled with this flag, you can use tools like to debug the model code. For example:\nDebugging this script with works except for when we invoke the function. We can globally disable JIT, so that we can call the function as a normal Python function and not compile it. If the above script is called , we can invoke it like so:\nand we will be able to step into the function as a normal Python function. To disable the TorchScript compiler for a specific function, see .\nTorchScript provides a code pretty-printer for all instances. This pretty-printer gives an interpretation of the script method’s code as valid Python syntax. For example:\nA with a single method will have an attribute , which you can use to inspect the ’s code. If the has more than one method, you will need to access on the method itself and not the module. We can inspect the code of a method named on a by accessing . The example above produces this output:\nThis is TorchScript’s compilation of the code for the method. You can use this to ensure TorchScript (tracing or scripting) has captured your model code correctly.\nTorchScript also has a representation at a lower level than the code pretty- printer, in the form of IR graphs.\nTorchScript uses a static single assignment (SSA) intermediate representation (IR) to represent computation. The instructions in this format consist of ATen (the C++ backend of PyTorch) operators and other primitive operators, including control flow operators for loops and conditionals. As an example:\nfollows the same rules described in the section with regard to method lookup.\n  * means we assign the output to a (unique) value named , that value is of type and that we do not know its concrete shape.\n  * is the operator (equivalent to ) and the input list specifies which values in scope should be passed as inputs. The schema for built-in functions like can be found at .\n  * is the location in the original source file that generated this instruction. In this case, it is a file named , on line 9, and at character 10.\n\n\nNotice that operators can also have associated , namely the and operators. In the graph print-out, these operators are formatted to reflect their equivalent source code forms to facilitate easy debugging.\nGraphs can be inspected as shown to confirm that the computation described by a is correct, in both automated and manual fashion, as described below.\nThere are some edge cases that exist where the trace of a given Python function/module will not be representative of the underlying code. These cases can include:\n  * Tracing of control flow that is dependent on inputs (e.g. tensor shapes)\n  * Tracing of in-place operations of tensor views (e.g. indexing on the left-hand side of an assignment)\n\n\nNote that these cases may in fact be traceable in the future.\nOne way to automatically catch many errors in traces is by using on the API. takes a list of tuples of inputs that will be used to re-trace the computation and verify the results. For example:\nThis message indicates to us that the computation differed between when we first traced it and when we traced it with the . Indeed, the loop within the body of depends on the shape of the input , and thus when we try another with a different shape, the trace differs.\nIn this case, data-dependent control flow like this can be captured using instead:\nThe tracer produces warnings for several problematic patterns in traced computation. As an example, take a trace of a function that contains an in-place assignment on a slice (a view) of a Tensor:\nProduces several warnings and a graph which simply returns the input:\nWe can fix this by modifying the code to not use the in-place update, but rather build up the result tensor out-of-place with :\nQ: I would like to train a model on GPU and do inference on CPU. What are the best practices?\n> First convert your model from GPU to CPU and then save it, like so:\n> This is recommended because the tracer may witness tensor creation on a specific device, so casting an already-loaded model may have unexpected effects. Casting the model saving it ensures that the tracer has the correct device information.\n> If is instantiated it will result in a compilation error since the compiler doesn’t know about . There are 4 ways to inform the compiler of attributes on :\n> 2. - Values wrapped in will work as they do on s. This is equivalent to an attribute (see 4) of type .\n> 3. Constants - Annotating a class member as (or adding it to a list called at the class definition level) will mark the contained names as constants. Constants are saved directly in the code of the model. See for details.\n> 4. Attributes - Values that are a can be added as mutable attributes. Most types can be inferred but some may need to be specified, see for details.\nQ: I would like to trace module’s method but I keep getting this error:\n> This error usually means that the method you are tracing uses a module’s parameters and you are passing the module’s method instead of the module instance (e.g. vs ).\n>>   * Invoking with a module’s method captures module parameters (which may require gradients) as .\n>>   * On the other hand, invoking with module’s instance (e.g. ) creates a new module and correctly copies parameters into the new module, so they can accumulate gradients if required.\n>> \n\nIf you’re using with TorchScript, the inputs of some of the submodules may be falsely inferred to be , even if they’re annotated otherwise. The canonical solution is to subclass and redeclare with the input typed correctly.\nThis section details the changes to TorchScript in PyTorch 1.2. If you are new to TorchScript you can skip this section. There are two main changes to the TorchScript API with PyTorch 1.2.\n1. will now attempt to recursively compile functions, methods, and classes that it encounters. Once you call , compilation is “opt-out”, rather than “opt-in”.\n2. is now the preferred way to create s, instead of inheriting from . These changes combine to provide a simpler, easier-to-use API for converting your s into s, ready to be optimized and executed in a non-Python environment.\n  * The module’s is compiled by default. Methods called from are lazily compiled in the order they are used in .\n  * To compile a method other than that is not called from , add .\n  * To stop the compiler from compiling a method, add or . leaves the\n  * method as a call to python, and replaces it with an exception. cannot be exported; can.\n  * Most attribute types can be inferred, so is not necessary. For empty container types, annotate their types using \n  * Constants can be marked with a class annotation instead of adding the name of the member to .\n  * Python 3 type hints can be used in place of \n\n\n\nAs a result of these changes, the following items are considered deprecated and should not appear in new code:\n\nThe annotation’s behavior changes in PyTorch 1.2. Before PyTorch 1.2 the @ignore decorator was used to make a function or method callable from code that is exported. To get this functionality back, use . is now equivalent to . See and for details.\nWhen passed to the function, a 's data is copied to a and the TorchScript compiler compiles the module. The module’s is compiled by default. Methods called from are lazily compiled in the order they are used in , as well as any methods.     \nThis decorator indicates that a method on an is used as an entry point into a and should be compiled.\nimplicitly is assumed to be an entry point, so it does not need this decorator. Functions and methods called from are compiled as they are seen by the compiler, so they do not need this decorator either.\n```\n \n   \n\n \n      \n           \n\n    \n    \n      \n           \n\n    \n      \n        # When the compiler sees this call, it will compile\n        \n         \n\n      \n           \n\n\n\n\n\n# `unused_method` will not be compiled since it was not called from\n\n  \n\n```\n\nFunctions don’t change much, they can be decorated with or if needed.\n```\n\n\n \n     \n\n\n\n\n \n     \n\n# As with ignore, if nothing calls it then it has no effect.\n# If it is called in script it is replaced with an exception.\n\n \n    \n   \n\n\n\n\n \n     \n\n```\n\nTorchScript class support is experimental. Currently it is best suited for simple record-like types (think a with methods attached).\nEverything in a user defined is exported by default, functions can be decorated with if needed.\nThe TorchScript compiler needs to know the types of . Most types can be inferred from the value of the member. Empty lists and dicts cannot have their types inferred and must have their types annotated with \n```\n   \n\n \n      \n\n     \n        \n        # This type cannot be inferred and must be specified\n          \n\n        # The attribute type here is inferred to be `int`\n          \n\n     \n        \n\n  \n\n```\n\nThe type constructor can be used to mark members as . If members are not marked constant, they will be copied to the resulting as an attribute. Using opens opportunities for optimization if the value is known to be fixed and gives additional type safety.\nContainers are assumed to have type and be non-optional (see for more information). Previously, was used to tell the TorchScript compiler what the type should be. Python 3 style type hints are now supported.\nThere are a couple of fusion backends available to optimize TorchScript execution. The default fuser on CPUs is NNC, which can perform fusions for both CPUs and GPUs. The default fuser on GPUs is NVFuser, which supports a wider range of operators and has demonstrated generated kernels with improved throughput. See the \n  *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/distributed.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nPlease refer to for a brief introduction to all features related to distributed training.\nsupports three built-in backends, each with different capabilities. The table below shows which functions are available for use with CPU / CUDA tensors. MPI supports CUDA only if the implementation used to build PyTorch supports it.\nPyTorch distributed package supports Linux (stable), MacOS (stable), and Windows (prototype). By default for Linux, the Gloo and NCCL backends are built and included in PyTorch distributed (NCCL only when building with CUDA). MPI is an optional backend that can only be included if you build PyTorch from source. (e.g. building PyTorch on a host that has MPI installed.)\nAs of PyTorch v1.8, Windows supports all collective communications backend but NCCL, If the argument of points to a file it must adhere to the following schema:\nSame as on Linux platform, you can enable TcpStore by setting environment variables, MASTER_ADDR and MASTER_PORT.\nIn the past, we were often asked: “which backend should I use?”.\n  *   *     * Use NCCL, since it’s the only backend that currently supports InfiniBand and GPUDirect.\n  *     * Use NCCL, since it currently provides the best distributed GPU training performance, especially for multiprocess single-node or multi-node distributed training. If you encounter any problem with NCCL, use Gloo as the fallback option. (Note that Gloo currently runs slower than NCCL for GPUs.)\n  *     * If your InfiniBand has enabled IP over IB, use Gloo, otherwise, use MPI instead. We are planning on adding InfiniBand support for Gloo in the upcoming releases.\n  *     * Use Gloo, unless you have specific reasons to use MPI.\n\n\nBy default, both the NCCL and Gloo backends will try to find the right network interface to use. If the automatically detected interface is not correct, you can override it using the following environment variables (applicable to the respective backend):\nIf you’re using the Gloo backend, you can specify multiple interfaces by separating them by a comma, like this: . The backend will dispatch operations in a round-robin fashion across these interfaces. It is imperative that all processes specify the same number of interfaces in this variable.\n- in case of NCCL failure, you can set to print an explicit warning message as well as basic NCCL initialization information.\nYou may also use to get more details about a specific aspect of NCCL. For example, would print logs of collective calls, which may be helpful when debugging hangs, especially those caused by collective type or message size mismatch. In case of topology detection failure, it would be helpful to set to inspect the detailed detection result and save as reference if further help from NCCL team is needed.\n- NCCL performs automatic tuning based on its topology detection to save users’ tuning effort. On some socket-based systems, users may still try tuning and to increase socket network bandwidth. These two environment variables have been pre-tuned by NCCL for some cloud providers, such as AWS or GCP.\nFor a full list of NCCL environment variables, please refer to \nThe package provides PyTorch support and communication primitives for multiprocess parallelism across several computation nodes running on one or more machines. The class builds on this functionality to provide synchronous distributed training as a wrapper around any PyTorch model. This differs from the kinds of parallelism provided by and in that it supports multiple network-connected machines and in that the user must explicitly launch a separate copy of the main training script for each process.\nIn the single-machine synchronous case, or the wrapper may still have advantages over other approaches to data-parallelism, including :\n  * Each process maintains its own optimizer and performs a complete optimization step with each iteration. While this may appear redundant, since the gradients have already been gathered together and averaged across processes and are thus the same for every process, this means that no parameter broadcast step is needed, reducing time spent transferring tensors between nodes.\n  * Each process contains an independent Python interpreter, eliminating the extra interpreter overhead and “GIL-thrashing” that comes from driving several execution threads, model replicas, or GPUs from a single Python process. This is especially important for models that make heavy use of the Python runtime, including models with recurrent layers or many small components.\n\n\nThe package needs to be initialized using the or function before calling any other methods. Both block until all processes have joined.\nInitialization is not thread-safe. Process group creation should be performed from a single thread, to prevent inconsistent ‘UUID’ assignment across ranks, and to prevent races during initialization that can lead to hangs.     \nOtherwise, does not expose any other APIs. Currently, is available on Linux, MacOS and Windows. Set to enable it when building PyTorch from source. Currently, the default value is for Linux and Windows, for MacOS.      \n\nThere are 2 main ways to initialize a process group:\n    \n  1. Specify (a URL string) which indicates where/how to discover peers. Optionally specify and , or encode all required parameters in the URL and omit them.\n\n    \n  * () – The backend to use. Depending on build-time configurations, valid values include , , , , or one that is registered by a third-party plugin. Since 2.6, if is not provided, c10d will use a backend registered for the device type indicated by the kwarg (if provided). The known default registrations today are: for , for . If neither nor is provided, c10d will detect the accelerator on the run-time machine and use a backend registered for that detected accelerator (or ). This field can be given as a lowercase string (e.g., ), which can also be accessed via attributes (e.g., ). If using multiple processes per machine with backend, each process must have exclusive access to every GPU it uses, as sharing GPUs between processes can result in deadlock or NCCL invalid usage. backend is experimental.\n  * () – URL specifying how to initialize the process group. Default is “env://” if no or is specified. Mutually exclusive with .\n  * () – Number of processes participating in the job. Required if is specified.\n  * () – Rank of the current process (it should be a number between 0 and -1). Required if is specified.\n  * () – Key/value store accessible to all workers, used to exchange connection/address information. Mutually exclusive with .\n  * () – Timeout for operations executed against the process group. Default value is 10 minutes for NCCL and 30 minutes for other backends. This is the duration after which collectives will be aborted asynchronously and the process will crash. This is done since CUDA execution is async and it is no longer safe to continue executing user code since failed async NCCL operations might result in subsequent CUDA operations running on corrupted data. When TORCH_NCCL_BLOCKING_WAIT is set, the process will block and wait for this timeout.\n  * () – process group options specifying what additional options need to be passed in during the construction of specific process groups. As of now, the only options we support is for the backend, can be specified so that the nccl backend can pick up high priority cuda streams when there’re compute kernels waiting. For other availble options to config nccl, See \n  * () – a single, specific device to “bind” this process to, allowing for backend-specific optimizations. Currently this has two effects, only under NCCL: the communicator is immediately formed (calling immediately rather than the normal lazy call) and sub-groups will use when possible to avoid unnecessary overhead of group creation. If you want to know NCCL initialization error early, you can also use this field.\n\n\nTo enable , PyTorch needs to be built from source on a system that supports MPI.\nSupport for multiple backends is experimental. Currently when no backend is specified, both and backends will be created. The backend will be used for collectives with CPU tensors and the backend will be used for collectives with CUDA tensors. A custom backend can be specified by passing in a string with format “<device_type>:<backend_name>,<device_type>:<backend_name>”, e.g. “cpu:gloo,cuda:custom_backend”.     \nThis creates a DeviceMesh with an n-dimensional array layout, where is the length of . If is provided, each dimension is labeled as .\nfollows SPMD programming model, meaning the same PyTorch Python program runs on all processes/ranks in the cluster. Ensure (the dimensions of the nD array describing device layout) is identical across all ranks. Inconsistent may lead to hanging.\nIf no process group is found, init_device_mesh will initialize distributed process group/groups required for distributed communications behind the scene.     \n  * () – A tuple defining the dimensions of the multi-dimensional array describing the layout of devices.\n  * () – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of . Each string in must be unique.\n\n         \nThe existence of environment variable is used as a proxy to determine whether the current process was launched with torchelastic. This is a reasonable proxy since maps to the rendezvous id which is always a non-null value indicating the job id for peer discovery purposes..\nThere are two ways to initialize using TCP, both requiring a network address reachable from all processes and a desired . The first way requires specifying an address that belongs to the rank 0 process. This initialization method requires that all processes have manually specified ranks.\nNote that multicast address is not supported anymore in the latest distributed package. is deprecated as well.\nAnother initialization method makes use of a file system that is shared and visible from all machines in a group, along with a desired . The URL should start with and contain a path to a non-existent file (in an existing directory) on a shared file system. File-system initialization will automatically create that file if it doesn’t exist, but will not delete the file. Therefore, it is your responsibility to make sure that the file is cleaned up before the next call on the same file path/name.\nNote that automatic rank assignment is not supported anymore in the latest distributed package and is deprecated as well.\nThis method assumes that the file system supports locking using - most local systems and NFS support it.\nThis method will always create the file and try its best to clean up and remove the file at the end of the program. In other words, each initialization with the file init method will need a brand new empty file in order for the initialization to succeed. If the same file used by the previous initialization (which happens not to get cleaned up) is used again, this is unexpected behavior and can often cause deadlocks and failures. Therefore, even though this method will try its best to clean up the file, if the auto-delete happens to be unsuccessful, it is your responsibility to ensure that the file is removed at the end of the training to prevent the same file to be reused again during the next time. This is especially important if you plan to call multiple times on the same file name. In other words, if the file is not removed/cleaned up and you call again on that file, failures are expected. The rule of thumb here is that, make sure that the file is non-existent or empty every time is called.\nThis method will read the configuration from environment variables, allowing one to fully customize how the information is obtained. The variables to be set are:\n  * - required; has to be a free port on machine with rank 0\n  * - required (except for rank 0); address of rank 0 node\n  * - required; can be set either here, or in a call to init function\n  * - required; can be set either here, or in a call to init function\n\n\nThe machine with rank 0 will be used to set up all connections.\nThis is the default method, meaning that does not have to be specified (or can be ).\nOnce was run, the following functions can be used. To check whether the process group has already been initialized use .     \nAvailable backends: GLOO, NCCL, UCC, MPI, XCCL, and other registered backends.\nThe values of this class are lowercase strings, e.g., . They can be accessed as attributes, e.g., .\nThis class can be directly called to parse the string, e.g., will check if is valid, and return the parsed lowercase string if so. It also accepts uppercase strings, e.g., returns .\nThe entry is present but only used as initial value of some fields. Users should neither use it directly nor assume its existence.     \nRegister a new backend with the given name and instantiating function.\nThis class method is used by 3rd party extension to register new backends.     \n  * () – Function handler that instantiates the backend. The function should be implemented in the backend extension and takes four arguments, including , , , and .\n  * () – Whether the backend supports extended argument structure. Default: . If set to , the backend will get an instance of , and a process group options object as defined by the backend implementation.\n  * () – device type this backend supports, e.g. “cpu”, “cuda”, etc. If , assuming both “cpu” and “cuda”\n\n\nThis support of 3rd party backend is experimental and subject to change.          \n() – The process group to work on. The default is the general main process group. If another specific group is specified, the calling process must be part of .     \nThe backend of the given process group as a lower case string.     \nReturn the rank of the current process in the provided , default otherwise.\nRank is a unique identifier assigned to each process within a distributed process group. They are always consecutive integers ranging from 0 to .     \n() – The process group to work on. If None, the default process group will be used.     \nThe rank of the process group -1, if not part of the group     \nReturn the number of processes in the current process group.     \n() – The process group to work on. If None, the default process group will be used.     \nThe world size of the process group -1, if not part of the group\nIt is important to clean up resources on exit by calling .\nThe simplest pattern to follow is to destroy every process group and backend by calling with the default value of None for the argument, at a point in the training script where communications are no longer needed, usually near the end of main(). The call should be made once per trainer-process, not at the outer process-launcher level.\nif is not called by all ranks in a pg within the timeout duration, especially when there are multiple process-groups in the application e.g. for N-D parallelism, hangs on exit are possible. This is because the destructor for ProcessGroupNCCL calls ncclCommAbort, which must be called collectively, but the order of calling ProcessGroupNCCL’s destructor if called by python’s GC is not deterministic. Calling helps by ensuring ncclCommAbort is called in a consistent order across ranks, and avoids calling ncclCommAbort during ProcessGroupNCCL’s destructor.\ncan also be used to destroy individual process groups. One use case could be fault tolerant training, where a process group may be destroyed and then a new one initialized during runtime. In this case, it’s critical to synchronize the trainer processes using some means other than torch.distributed primitives _after_ calling destroy and before subsequently initializing. This behavior is currently unsupported/untested, due to the difficulty of achieving this synchronization, and is considered a known issue. Please file a github issue or RFC if this is a use case that’s blocking you.\nBy default collectives operate on the default group (also called the world) and require all processes to enter the distributed function call. However, some workloads can benefit from more fine-grained communication. This is where distributed groups come into play. function can be used to create new groups, with arbitrary subsets of all processes. It returns an opaque group handle that can be given as a argument to all collectives (collectives are distributed functions to exchange information in certain well-known programming patterns).     \nThis function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\nSafe concurrent usage: When using multiple process groups with the backend, the user must ensure a globally consistent execution order of collectives across ranks.\nIf multiple threads within a process issue collectives, explicit synchronization is necessary to ensure consistent ordering.\nWhen using async variants of torch.distributed communication APIs, a work object is returned and the communication kernel is enqueued on a separate CUDA stream, allowing overlap of communication and computation. Once one or more async ops have been issued on one process group, they must be synchronized with other cuda streams by calling before using another process group.     \n  * () – List of ranks of group members. If , will be set to all ranks. Default is .\n  * () – The backend to use. Depending on build-time configurations, valid values are and . By default uses the same backend as the global group. This field should be given as a lowercase string (e.g., ), which can also be accessed via attributes (e.g., ). If is passed in, the backend corresponding to the default process group will be used. Default is .\n  * () – process group options specifying what additional options need to be passed in during the construction of specific process groups. i.e. for the backend, can be specified so that process group can pick up high priority cuda streams. For other availble options to config nccl, See \n  * () – perform a group-local barrier at the end of the process group creation. This is different in that non-member ranks don’t need to call into API and don’t join the barrier.\n  * () – a single, specific device to “bind” this process to, The call will try to initialize a communication backend immediately for the device if this field is given.\n\n    \nA handle of distributed group that can be given to collective calls or GroupMember.NON_GROUP_MEMBER if the rank is not part of .\nN.B. While use_local_synchronization=True can be significantly faster with larger clusters and small process groups, care must be taken since it changes cluster behavior as non-member ranks don’t join the group barrier().\nN.B. use_local_synchronization=True can lead to deadlocks when each rank creates multiple overlaping process groups. To avoid that, make sure all ranks follow the same global creation order.     \nN.B. calling this function on the default process group returns identity     \nN.B. calling this function on the default process group returns identity     \nDeviceMesh is a higher level abstraction that manages process groups (or NCCL communicators). It allows user to easily create inter node and intra node process groups without worrying about how to set up the ranks correctly for different sub process groups, and it helps manage those distributed process group easily. function can be used to create new DeviceMesh, with a mesh shape describing the device topology.     \nDeviceMesh represents a mesh of devices, where layout of devices could be represented as a n-d dimension array, and each value of the n-d dimensional array is the global id of the default process group ranks.\nDeviceMesh could be used to describe the layout of devices across the cluster, and serves as a proxy for communication among the device lists within the cluster.\nDeviceMesh follows SPMD programming model, which means the same PyTorch Python program is running on all processes/ranks in the cluster. Therefore, users need to make sure the array (which describes the layout of devices) should be identical across all ranks. Inconsistent will lead to silent hang.     \n  * () – A multi-dimensional array or an integer tensor describing the layout of devices, where the IDs are global IDs of the default process group.\n\n\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. A reduction over the first dimension of mesh will reduce across columns (0, 4), .. and (3, 7), a reduction over the second dimension of mesh reduces across rows (0, 1, 2, 3) and (4, 5, 6, 7).     \n```\n   \n\n# Initialize device mesh as (2, 4) to represent the topology\n\n         \n\n```\n    \nThe constructed device mesh has number of dimensions equal to the number of groups passed. For example, if a single process group is passed in, the resulted DeviceMesh is a 1D mesh. If a list of 2 process groups is passed in, the resulted DeviceMesh is a 2D mesh.\nIf more than one group is passed, then the and arguments are required. The order of the process groups passed in determines the topology of the mesh. For example, the first process group will be the 0th dimension of the DeviceMesh. The tensor passed in must have the same number of dimensions as the number of process groups passed in, and the order of the dimensions in the tensor must match the order in the process groups passed in.     \n  * () – the existing ProcessGroup or a list of existing ProcessGroups.\n  * () – A multi-dimensional array or an integer tensor describing the layout of devices, where the IDs are global IDs of the default process group. Default is None.\n  * () – A tuple of mesh dimension names to assign to each dimension of the multi-dimensional array describing the layout of devices. Its length must match the length of . Each string in must be unique. Default is None.\n\n         \nReturn the relative indices of this rank relative to all dimensions of the mesh. If this rank is not part of the mesh, return None.     \nReturns the single ProcessGroup specified by mesh_dim, or, if mesh_dim is not specified and the DeviceMesh is 1-dimensional, returns the only ProcessGroup in the mesh.     \n  * () – it can be the name of the mesh dimension or the index\n\n    \nReturns the local rank of the given mesh_dim of the DeviceMesh.     \n  * () – it can be the name of the mesh dimension or the index\n\n\nThe following program runs on each process/rank in an SPMD manner. In this example, we have 2 hosts with 4 GPUs each. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 0, 1, 2, 3 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=0) on rank 4, 5, 6, 7 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 0, 4 would return 0. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 1, 5 would return 1. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 2, 6 would return 2. Calling mesh_2d.get_local_rank(mesh_dim=1) on rank 3, 7 would return 3.     \n```\n   \n\n# Initialize device mesh as (2, 4) to represent the topology\n\n         \n\n```\n         \n  * ( argument). Destination rank should not be the same as the rank of the current process.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Destination rank on . Invalid to specify both and .\n\n         \n  * () – Source rank on global process group (regardless of argument). Will receive from any process if unspecified.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Destination rank on . Invalid to specify both and .\n\n\nand return distributed request objects when used. In general, the type of this object is unspecified as they should never be created manually, but they are guaranteed to support two methods:\n  * - will block the process until the operation is finished. is guaranteed to return True once it returns.\n\n    \nUnlike send, which is blocking, isend allows src == dst rank, i.e. send to self.     \n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Destination rank on . Invalid to specify both and \n\n    \nA distributed request object. None, if not part of the group     \nUnlike recv, which is blocking, irecv allows src == dst rank, i.e. recv from self.     \n  * () – Source rank on global process group (regardless of argument). Will receive from any process if unspecified.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Destination rank on . Invalid to specify both and .\n\n    \nA distributed request object. None, if not part of the group     \nSimilar to , but Python objects can be passed in. Note that all objects in must be picklable in order to be sent.     \n  * () – List of input objects to sent. Each object must be picklable. Receiver must provide lists of equal sizes.\n  * ( to. Destination rank is based on global process group (regardless of argument)\n  * () – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is .\n  * (, optional) – If not None, the objects are serialized and converted to tensors which are moved to the before sending. Default is .\n  * () – Destination rank on . Must specify one of and but not both\n\n\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n\n  \n   \n    \n          \n      \n\n        \n      \n\n\n\n```\n         \n  * () – List of objects to receive into. Must provide a list of sizes equal to the size of the list being sent.\n  * () – Source rank from which to recv . Source rank is based on global process group (regardless of argument) Will receive from any rank if set to None. Default is .\n  * () – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is .\n  * (, optional) – If not None, receives on this device. Default is .\n  * () – Destination rank on . Invalid to specify both and .\n\n    \nSender rank. -1 if rank is not part of the group. If rank is part of the group, will contain the sent objects from rank.\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n\n  \n   \n    \n          \n      \n\n        \n      \n\n\n\n```\n    \nSend or Receive a batch of tensors asynchronously and return a list of requests.\nProcess each of the operations in and return the corresponding requests. NCCL, Gloo, and UCC backend are currently supported.     \n() – A list of point-to-point operations(type of each operator is ). The order of the isend/irecv in the list matters and it needs to match with corresponding isend/irecv on the remote end.     \nA list of distributed request objects returned by calling the corresponding op in the op_list.\n```\n       \n   \n        \n  \n            \n\n   \n   \n    \n\n\n\n\n```\n\nNote that when this API is used with the NCCL PG backend, users must set the current GPU device with , otherwise it will lead to unexpected hang issues.\nIn addition, if this API is the first collective call in the passed to , all ranks of the must participate in this API call; otherwise, the behavior is undefined. If this API call is not the first collective call in the , batched P2P operations involving only a subset of ranks of the are allowed.     \nThis class builds the type of P2P operation, communication buffer, peer rank, Process Group, and tag. Instances of this class will be passed to for point-to-point communications.     \n  * () – A function to send data to or receive data from a peer process. The type of is either or .\n  * () – The process group to work on. If None, the default process group will be used.\n\n\nEvery collective operation function supports the following two kinds of operations, depending on the setting of the flag passed into the collective:\n- the default mode, when is set to . When the function returns, it is guaranteed that the collective operation is performed. In the case of CUDA operations, it is not guaranteed that the CUDA operation is completed, since CUDA operations are asynchronous. For CPU collectives, any further function calls utilizing the output of the collective call will behave as expected. For CUDA collectives, function calls utilizing the output on the same CUDA stream will behave as expected. Users must take care of synchronization under the scenario of running under different streams. For details on CUDA semantics such as stream synchronization, see . See the below script to see examples of differences in these semantics for CPU and CUDA operations.\n- when is set to True. The collective operation function returns a distributed request object. In general, you don’t need to create it manually and it is guaranteed to support two methods:\n  * - in the case of CPU collectives, returns if completed. In the case of CUDA operations, returns if the operation has been successfully enqueued onto a CUDA stream and the output can be utilized on the default stream without further synchronization.\n  * - in the case of CPU collectives, will block the process until the operation is completed. In the case of CUDA collectives, will block the currently active CUDA stream until the operation is completed (but will not block the CPU).\n  * - returns object. Supported for NCCL, also supported for most operations on GLOO and MPI, except for peer to peer operations. Note: as we continue adopting Futures and merging APIs, call might become redundant.\n\n\nThe following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives. It shows the explicit need to synchronize when using collective outputs on different CUDA streams:\n```\n\n  \n  \n  \n   \n# Wait ensures the operation is enqueued, but not necessarily complete.\n\n\n \n    \n    \n   \n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    \n    \n\n```\n    \nmust have the same number of elements in all processes participating in the collective.     \n  * () – Data to be sent if is the rank of current process, and tensor to be used to save received data otherwise.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group     \nSimilar to , but Python objects can be passed in. Note that all objects in must be picklable in order to be broadcasted.     \n  * () – List of input objects to broadcast. Each object must be picklable. Only objects on the rank will be broadcast, but each rank must provide lists of equal sizes.\n  * (. Source rank is based on global process group (regardless of argument)\n  * () – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is .\n  * (, optional) – If not None, the objects are serialized and converted to tensors which are moved to the before broadcasting. Default is .\n\n    \n. If rank is part of the group, will contain the broadcasted objects from rank.\nFor NCCL-based process groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsibility to ensure that this is set so that each rank has an individual GPU, via .\nNote that this API differs slightly from the collective since it does not provide an handle and thus will be a blocking call.\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n   \n    \n          \n\n        \n\n  \n  \n\n\n\n```\n    \nReduces the tensor data across all machines in a way that all get the final result.\nAfter the call is going to be bitwise identical in all processes.     \n  * () – Input and output of the collective. The function operates in-place.\n  * () – One of the values from enum. Specifies an operation used for element-wise reductions.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n```\n\n\n  \n          \n\n\n\n \n\n\n\n\n```\n\n```\n\n\n  \n           \n        \n\n\n\n \n\n\n\n\n```\n    \nOnly the process with rank is going to receive the final result.     \n  * () – Input and output of the collective. The function operates in-place.\n  * () – One of the values from enum. Specifies an operation used for element-wise reductions.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group          \n  * () – Output list. It should contain correctly-sized tensors to be used for output of the collective. Uneven sized tensors are supported.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n```\n\n\n  \n  \n          \n\n\n\n\n          \n\n\n\n \n\n\n\n\n```\n\n```\n\n\n  \n          \n\n\n\n\n  \n           \n        \n\n\n\n \n\n\n\n\n```\n    \nGather tensors from all ranks and put them in a single output tensor.\nThis function requires all tensors to be the same size on each process.     \n  * () – Output tensor to accommodate tensor elements from all ranks. It must be correctly sized to have one of the following forms: (i) a concatenation of all the input tensors along the primary dimension; for definition of “concatenation”, see ; (ii) a stack of all the input tensors along the primary dimension; for definition of “stack”, see . Examples below may better explain the supported output forms.\n  * () – Tensor to be gathered from current rank. Different from the API, the input tensors in this API must have the same size across all ranks.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\n```\n# All tensors below are of torch.int64 dtype and on CUDA devices.\n\n  \n          \n\n\n\n\n      \n \n\n\n\n\n     \n \n\n\n\n\n\n\n```\n    \nGathers picklable objects from the whole group into a list.\nSimilar to , but Python objects can be passed in. Note that the object must be picklable in order to be gathered.     \n  * () – Output list. It should be correctly sized as the size of the group for this collective and will contain the output.\n  * () – Pickable Python object to be broadcast from current process.\n  * () – The process group to work on. If None, the default process group will be used. Default is .\n\n    \nNone. If the calling rank is part of this group, the output of the collective will be populated into the input . If the calling rank is not part of the group, the passed in will be unmodified.\nNote that this API differs slightly from the collective since it does not provide an handle and thus will be a blocking call.\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via .\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n\n      \n      \n \n\n\n\n```\n    \nThis function requires all tensors to be the same size on each process.     \n  * () – List of appropriately, same-sized tensors to use for gathered data (default is None, must be specified on the destination rank)\n  * () – Destination rank on global process group (regardless of argument). (If both and are None, default is global rank 0)\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n  * () – Destination rank on . Invalid to specify both and \n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\nNote that all Tensors in gather_list must have the same size.     \n```\n\n  \n  \n     \n   \n           \n\n      \n  \n\n\n\nNone                                                                   # Rank 1\n\n```\n    \nGathers picklable objects from the whole group in a single process.\nSimilar to , but Python objects can be passed in. Note that the object must be picklable in order to be gathered.     \n  * () – Output list. On the rank, it should be correctly sized as the size of the group for this collective and will contain the output. Must be on non-dst ranks. (default is )\n  * () – Destination rank on global process group (regardless of argument). (If both and are None, default is global rank 0)\n  * () – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is .\n  * () – Destination rank on . Invalid to specify both and \n\n\nNote that this API differs slightly from the gather collective since it does not provide an async_op handle and thus will be a blocking call.\nFor NCCL-based processed groups, internal tensor representations of objects must be moved to the GPU device before communication takes place. In this case, the device used is given by and it is the user’s responsiblity to ensure that this is set so that each rank has an individual GPU, via .\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n\n      \n      \n\n    \n          \n    \n\n\n\n\n\n```\n    \nScatters a list of tensors to all processes in a group.\nEach process will receive exactly one tensor and store its data in the argument.     \n  * () – List of tensors to scatter (default is None, must be specified on the source rank)\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n  * () – Source rank on . Invalid to specify both and \n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\nNote that all Tensors in scatter_list must have the same size.     \n```\n\n   \n  \n  \n   \n   \n    \n    # Only tensors, all of which must be the same size.\n       \n         \n       \n\n      \n  \n\n\n\n\n\n```\n    \nSimilar to , but Python objects can be passed in. On each rank, the scattered object will be stored as the first element of . Note that all objects in must be picklable in order to be scattered.     \n  * () – Non-empty list whose first element will store the object scattered to this rank.\n  * () – List of input objects to scatter. Each object must be picklable. Only objects on the rank will be scattered, and the argument can be for non-src ranks.\n  * (. Source rank is based on global process group (regardless of argument). (If both and are None, default is global rank 0)\n  * () – (ProcessGroup, optional): The process group to work on. If None, the default process group will be used. Default is .\n  * () – Source rank on . Invalid to specify both and \n\n    \n. If rank is part of the group, will have its first element set to the scattered object for this rank.\nNote that this API differs slightly from the scatter collective since it does not provide an handle and thus will be a blocking call.\nuses module implicitly, which is known to be insecure. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling. Only call this function with data you trust.\nCalling with GPU tensors is not well supported and inefficient as it incurs GPU -> CPU transfer since tensors would be pickled. Please consider using instead.     \n```\n\n   \n   \n    \n          \n\n    # Can be any list on non-src ranks, elements are not used.\n        \n  \n  \n# Rank i gets objects[i]. For example, on rank 2:\n\n\n\n```\n    \nReduces, then scatters a list of tensors to all processes in a group.     \n  * () – One of the values from enum. Specifies an operation used for element-wise reductions.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op.\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.     \nReduces, then scatters a tensor to all ranks in a group.     \n  * () – Output tensor. It should have the same size across all ranks.\n  * () – Input tensor to be reduced and scattered. Its size should be output tensor size times the world size. The input tensor can have one of the following shapes: (i) a concatenation of the output tensors along the primary dimension, or (ii) a stack of the output tensors along the primary dimension. For definition of “concatenation”, see . For definition of “stack”, see .\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op.\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n```\n# All tensors below are of torch.int64 dtype and on CUDA devices.\n\n  \n    \n\n      \n\n\n\n \n\n\n\n\n    \n\n\n\n\n\n \n\n\n\n\n```\n    \nSplit input tensor and then scatter the split list to all processes in a group.\nLater the received tensors are concatenated from all the processes in the group and returned as a single output tensor.     \n  * – (list[Int], optional): Output split sizes for dim 0 if specified None or empty, dim 0 of tensor must divide equally by .\n  * – (list[Int], optional): Input split sizes for dim 0 if specified None or empty, dim 0 of tensor must divide equally by .\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op.\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n```\n      \n\ntensor([0, 1, 2, 3])     # Rank 0\ntensor([4, 5, 6, 7])     # Rank 1\n\n\n   \n \n\ntensor([0, 4, 8, 12])    # Rank 0\ntensor([1, 5, 9, 13])    # Rank 1\n\n\n\n```\n\n```\n\n\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n  \n   \n\ntensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0\ntensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1\ntensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2\ntensor([ 5, 17, 18, 24, 36])                                     # Rank 3\n\n```\n\n```\n\n  \n                \n        \n\ntensor([1+1j, 2+2j, 3+3j, 4+4j])                                # Rank 0\ntensor([5+5j, 6+6j, 7+7j, 8+8j])                                # Rank 1\ntensor([9+9j, 10+10j, 11+11j, 12+12j])                          # Rank 2\ntensor([13+13j, 14+14j, 15+15j, 16+16j])                        # Rank 3\n   \n \n\ntensor([1+1j, 5+5j, 9+9j, 13+13j])                              # Rank 0\ntensor([2+2j, 6+6j, 10+10j, 14+14j])                            # Rank 1\ntensor([3+3j, 7+7j, 11+11j, 15+15j])                            # Rank 2\ntensor([4+4j, 8+8j, 12+12j, 16+16j])                            # Rank 3\n\n```\n    \nScatters list of input tensors to all processes in a group and return gathered list of tensors in output list.     \n  * () – List of tensors to be gathered one per rank.\n  * () – List of tensors to scatter one per rank.\n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op.\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group.\n```\n      \n  \n\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n\n\n   \n \n\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n\n\n\n```\n\n```\n\ntensor([0, 1, 2, 3, 4, 5])                                       # Rank 0\ntensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1\ntensor([20, 21, 22, 23, 24])                                     # Rank 2\ntensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3\n\n[2, 2, 1, 1]                                                     # Rank 0\n[3, 2, 2, 2]                                                     # Rank 1\n[2, 1, 1, 1]                                                     # Rank 2\n[2, 2, 2, 1]                                                     # Rank 3\n\n[2, 3, 2, 2]                                                     # Rank 0\n[2, 2, 1, 2]                                                     # Rank 1\n[1, 2, 1, 2]                                                     # Rank 2\n[1, 2, 1, 1]                                                     # Rank 3\n  \n\n[tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0\n[tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1\n[tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2\n[tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3\n  \n \n\n[tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0\n[tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1\n[tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2\n[tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3\n\n```\n\n```\n\n  \n                \n        \n  \n\n[tensor([1+1j]), tensor([2+2j]), tensor([3+3j]), tensor([4+4j])]            # Rank 0\n[tensor([5+5j]), tensor([6+6j]), tensor([7+7j]), tensor([8+8j])]            # Rank 1\n[tensor([9+9j]), tensor([10+10j]), tensor([11+11j]), tensor([12+12j])]      # Rank 2\n[tensor([13+13j]), tensor([14+14j]), tensor([15+15j]), tensor([16+16j])]    # Rank 3\n   \n \n\n[tensor([1+1j]), tensor([5+5j]), tensor([9+9j]), tensor([13+13j])]          # Rank 0\n[tensor([2+2j]), tensor([6+6j]), tensor([10+10j]), tensor([14+14j])]        # Rank 1\n[tensor([3+3j]), tensor([7+7j]), tensor([11+11j]), tensor([15+15j])]        # Rank 2\n[tensor([4+4j]), tensor([8+8j]), tensor([12+12j]), tensor([16+16j])]        # Rank 3\n\n```\n    \nThis collective blocks processes until the whole group enters this function, if async_op is False, or if async work handle is called on wait().     \n  * () – The process group to work on. If None, the default process group will be used.\n  * () – Whether this op should be an async op\n\n    \nAsync work handle, if async_op is set to True. None, if not async_op or if not part of the group\nnow blocks the cpu thread till the completion of the barrier collective.     \nIt is able to report ranks that did not pass this barrier within the provided timeout. Specifically, for non-zero ranks, will block until a send/recv is processed from rank 0. Rank 0 will block until all send /recv from other ranks are processed, and will report failures for ranks that failed to respond in time. Note that if one rank does not reach the monitored_barrier (for example due to a hang), all other ranks would fail in monitored_barrier.\nThis collective will block all processes/ranks in the group, until the whole group exits the function successfully, making it useful for debugging and synchronizing. However, it can have a performance impact and should only be used for debugging or scenarios that require full synchronization points on the host-side. For debugging purposes, this barrier can be inserted before the application’s collective calls to check if any ranks are desynchronized.\nNote that this collective is only supported with the GLOO backend.     \n  * () – The process group to work on. If , the default process group will be used.\n  * () – Timeout for monitored_barrier. If , the default process group timeout will be used.\n  * () – Whether to collect all failed ranks or not. By default, this is and on rank 0 will throw on the first failed rank it encounters in order to fail fast. By setting will collect all failed ranks and throw an error containing information about all failed ranks.\n\n    \n```\n\n   \n   \n     \n\n\n   \n     \n# indicating that ranks 1, 2, ... world_size - 1 did not call into\n\n\n```\n    \nA object represents the handle to a pending asynchronous operation in PyTorch’s distributed package. It is returned by non-blocking collective operations, such as .          \nA object which is associated with the completion of the . As an example, a future object can be retrieved by .     \nBelow is an example of a simple allreduce DDP communication hook that uses .\nAPI supports NCCL, and partially GLOO and MPI backends (no support for peer-to-peer operations like send/recv) and will return a .\nIn the example above, work will be done on GPU using NCCL backend, will return after synchronizing the appropriate NCCL streams with PyTorch’s current device streams to ensure we can have asynchronous CUDA execution and it does not wait for the entire operation to complete on GPU. Note that does not support flag or NCCL’s . In addition, if a callback function was added by , it will wait until ’s NCCL streams synchronize with ’s dedicated callback stream and invoke the callback inline after running the callback on the callback stream. will return another that holds the return value of the callback and a that recorded the callback stream.\n>   1. For CPU work, returns true when work has been completed and value() tensors are ready.\n>   2. For GPU work, returns true only whether the operation has been enqueued.\n>   3. For mixed CPU-GPU work (e.g. sending GPU tensors with GLOO), returns true when tensors have arrived on respective nodes, but not yet necessarily synched on respective GPUs (similarly to GPU work).\n> \n         \nA object of int type which maps to the enum type of WorkResult As an example, a future object can be retrieved by .     \nusers can use to blocking wait for the completion of the work and get the WorkResult by . Also, users can use to register a callback function to be called when the work is completed, without blocking the current thread.     \nIn normal cases, users do not need to set the timeout. calling wait() is the same as calling synchronize(): Letting the current stream block on the completion of the NCCL work. However, if timeout is set, it will block the CPU thread until the NCCL work is completed or timed out. If timeout, exception will be thrown.     \ndivides values by the world size before summing across ranks. is only available with the backend, and only for NCCL versions 2.10 or later.\nmultiplies inputs by a given scalar locally before reduction. is only available with the backend, and only available for NCCL versions 2.11 or later. Users are supposed to use .\nThe values of this class can be accessed as attributes, e.g., . They are used in specifying strategies for reduction collectives, e.g., .     \nThe distributed package comes with a distributed key-value store, which can be used to share information between processes in the group as well as to initialize the distributed package in (by explicitly creating the store as an alternative to specifying .) There are 3 choices for Key-Value Stores: , , and .     \nBase class for all store implementations, such as the 3 provided by PyTorch distributed: (, , and ).     \nThe first call to add for a given creates a counter associated with in the store, initialized to . Subsequent calls to add with the same increment the counter by the specified . Calling with a key that has already been set in the store by will result in an exception.     \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n \n \n\n\n\n```\n    \nAppend the key-value pair into the store based on the supplied and . If does not exists in the store, it will be created.     \nThe call to check whether a given list of have value stored in the store. This call immediately returns in normal cases but still suffers from some edge deadlock cases, e.g, calling check after TCPStore has been destroyed. Calling with a list of keys that one wants to check whether stored in the store or not.     \n() – The keys to query whether stored in the store.     \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n \n\n\n\n```\n    \nInserts the key-value pair into the store based on the supplied and performs comparison between and before inserting. will only be set if for the already exists in the store or if is an empty string.     \n\n    \nDeletes the key-value pair associated with from the store. Returns if the key was successfully deleted, and if it was not.\nThe API is only supported by the and . Using this API with the will result in an exception.     \n```\n   \n   \n# Using TCPStore as an example, HashStore can also be used\n      \n\n\n\n\n\n\n```\n    \nRetrieves the value associated with the given in the store. If is not present in the store, the function will wait for , which is defined when initializing the store, before throwing an exception.     \nRetrieve all values in . If any key in is not present in the store, the function will wait for      \n() – The keys to be retrieved from the store.     \nInserts a list key-value pair into the store based on the supplied and      \n\n    \nReturns the number of keys set in the store. Note that this number will typically be one greater than the number of keys added by and since one key is used to coordinate all the workers using the store.\nWhen used with the , returns the number of keys written to the underlying file. If the store is destructed and another store is created with the same file, the original keys will be retained.     \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n \n\n\n\n```\n    \nInserts the key-value pair into the store based on the supplied and . If already exists in the store, it will overwrite the old value with the new supplied .     \nSets the store’s default timeout. This timeout is used during initialization and in and .     \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n\n\n\n\n```\n    \nWaits for each key in to be added to the store. If not all keys are set before the (set during store initialization), then will throw an exception.     \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n\n\n\n```\n\nWaits for each key in to be added to the store, and throws an exception if the keys have not been set by the supplied .     \n  * () – Time to wait for the keys to be added before throwing an exception.\n\n    \n```\n   \n   \n# Using TCPStore as an example, other store types can also be used\n      \n\n \n\n```\n    \nA TCP-based distributed key-value store implementation. The server store holds the data, while the client stores can connect to the server store over TCP and perform actions such as to insert a key-value pair, to retrieve a key-value pair, etc. There should always be one server store initialized because the client store(s) will wait for the server to establish a connection.     \n  * () – The total number of store users (number of clients + 1 for the server). Default is None (None indicates a non-fixed number of store users).\n  * () – True when initializing the server store and False for client stores. Default is False.\n  * () – Timeout used by the store during initialization and for methods such as and . Default is timedelta(seconds=300)\n  * () – Whether to wait for all the workers to connect with the server store. This is only applicable when world_size is a fixed value. Default is True.\n  * () – If True, all instances in the current process with the same host/port will use the same underlying . Default is False.\n  * () – If specified, the underlying will listen on this file descriptor, which must be a socket already bound to . Useful to avoid port assignment races in some scenarios. Default is None (meaning the server creates a new socket and attempts to bind it to ).\n  * () – If True, use libuv for backend. Default is True.\n\n    \n```\n   \n   \n\n      \n\n     \n# Use any of the store methods from either the client or server after initialization\n \n\n\n```\n    \nGets the hostname on which the store listens for requests.     \nGets the port number on which the store listens for requests.     \nA thread-safe store implementation based on an underlying hashmap. This store can be used within the same process (for example, by other threads), but cannot be used across processes.     \n```\n   \n  \n\n\n \n\n```\n    \nA store implementation that uses a file to store the underlying key-value pairs.     \n  * () – The total number of processes using the store. Default is -1 (a negative value indicates a non-fixed number of store users).\n\n    \n```\n   \n   \n   \n# Use any of the store methods from either the client or server after initialization\n \n\n\n```\n    \nGets the path of the file used by FileStore to store key-value pairs.     \nA wrapper around any of the 3 key-value stores (, , and ) that adds a prefix to each key inserted to the store.     \n  * () – A store object that forms the underlying key-value store.\n\n\nNote that you can use (recommended, only available after 1.8.1) or to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (, , ) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:\nPlease refer to the for a full overview of profiler features.\nThe multi-GPU functions (which stand for multiple GPUs per CPU thread) are deprecated. As of today, PyTorch Distributed’s preferred programming model is one device per thread, as exemplified by the APIs in this document. If you are a backend developer and want to support multiple devices per thread, please contact PyTorch Distributed’s maintainers.\nBesides the builtin GLOO/MPI/NCCL backends, PyTorch distributed supports third-party backends through a run-time register mechanism. For references on how to develop a third-party backend through C++ Extension, please refer to and . The capability of third-party backends are decided by their own implementations.\nThe new backend derives from and registers the backend name and the instantiating interface through when imported.\nWhen manually importing this backend and invoking with the corresponding backend name, the package runs on the new backend.\nThe support of third-party backend is experimental and subject to change.\nThe package also provides a launch utility in . This helper utility can be used to launch multiple processes per node for distributed training.\nis a module that spawns up multiple distributed training processes on each of the training nodes.\nThis module is going to be deprecated in favor of .\nThe utility can be used for single-node distributed training, in which one or more processes per node will be spawned. The utility can be used for either CPU training or GPU training. If the utility is used for GPU training, each distributed process will be operating on a single GPU. This can achieve well-improved single-node training performance. It can also be used in multi-node distributed training, by spawning up multiple processes on each node for well-improved multi-node distributed training performance as well. This will especially be beneficial for systems with multiple Infiniband interfaces that have direct-GPU support, since all of them can be utilized for aggregated communication bandwidth.\nIn both cases of single-node distributed training or multi-node distributed training, this utility will launch the given number of processes per node (). If used for GPU training, this number needs to be less or equal to the number of GPUs on the current system (), and each process will be operating on a single GPU from .\n1. This utility and multi-process distributed (single-node or multi-node) GPU training currently only achieves the best performance using the NCCL distributed backend. Thus NCCL backend is the recommended backend to use for GPU training.\n2. In your training program, you must parse the command-line argument: , which will be provided by this module. If your training program uses GPUs, you should ensure that your code only runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:\nThe launcher will passes the argument to your script. From PyTorch 2.0.0 onwards, the dashed is preferred over the previously used underscored .\nFor backward compatibility, it may be necessary for users to handle both cases in their argument parsing code. This means including both and in the argument parser. If only is provided, the launcher will trigger an error: “error: unrecognized arguments: –local-rank=<rank>”. For training code that only supports PyTorch 2.0.0+, including should be sufficient.\n3. In your training program, you are supposed to call the following function at the beginning to start the distributed backend. It is strongly recommended that . Other init methods (e.g. ) may work, but is the one that is officially supported by this module.\n4. In your training program, you can either use regular distributed functions or use module. If your training program uses GPUs for training and you would like to use module, here is how to configure it.\nPlease ensure that argument is set to be the only GPU device id that your code will be operating on. This is generally the local rank of the process. In other words, the needs to be , and needs to be in order to use this utility\n5. Another way to pass to the subprocesses via environment variable . This behavior is enabled when you launch the script with . You must adjust the subprocess example above to replace with ; the launcher will not pass when you specify this flag.\nis NOT globally unique: it is only unique per process on a machine. Thus, don’t use it to decide if you should, e.g., write to a networked filesystem. See \nThe package also provides a function in . This helper function can be used to spawn multiple processes. It works by passing in the function that you want to run and spawns N processes to run it. This can be used for multiprocess distributed training as well.\nFor references on how to use it, please refer to \nDebugging distributed applications can be challenging due to hard to understand hangs, crashes, or inconsistent behavior across ranks. provides a suite of tools to help debug training applications in a self-serve fashion:\nIt is extremely convenient to use python’s debugger in a distributed environment, but because it does not work out of the box many people do not use it at all. PyTorch offers a customized wrapper around pdb that streamlines the process.\nmakes this process easy. Internally, it customizes ’s breakpoint behavior in two ways but otherwise behaves as normal . 1. Attaches the debugger only on one rank (specified by the user). 2. Ensures all other ranks stop, by using a that will release once the debugged rank issues a 3. Reroutes stdin from the child process such that it connects to your terminal.\nTo use it, simply issue on all ranks, using the same value for in each case.\nAs of v1.10, exists as an alternative to which fails with helpful information about which rank may be faulty when crashing, i.e. not all ranks calling into within the provided timeout. implements a host-side barrier using / communication primitives in a process similar to acknowledgements, allowing rank 0 to report which rank(s) failed to acknowledge the barrier in time. As an example, consider the following function where rank 1 fails to call into (in practice this could be due to an application bug or hang in a previous collective):\n```\n \n   \n\n \n   \n   \n\n\n \n      \n    # monitored barrier requires gloo process group to perform host-side sync.\n      \n        \n         \n\n\n   \n      \n      \n      \n\n```\n\nThe following error message is produced on rank 0, allowing the user to determine which rank(s) may be faulty and investigate further:\nWith , the environment variable can be used to trigger additional useful logging and collective synchronization checks to ensure all ranks are synchronized appropriately. can be set to either (default), , or depending on the debugging level required. Please note that the most verbose option, may impact the application performance and thus should only be used when debugging issues.\nSetting will result in additional debug logging when models trained with are initialized, and will additionally log runtime performance statistics a select number of iterations. These runtime statistics include data such as forward time, backward time, gradient communication time, etc. As an example, given the following application:\nIn addition, enhances crash logging in due to unused parameters in the model. Currently, must be passed into initialization if there are parameters that may be unused in the forward pass, and as of v1.10, all model outputs are required to be used in loss computation as does not support unused parameters in the backwards pass. These constraints are challenging especially for larger models, thus when crashing with an error, will log the fully qualified name of all parameters that went unused. For example, in the above application, if we modify to be instead computed as , then does not receive a gradient in the backwards pass, and thus results in failing. On a crash, the user is passed information about parameters which went unused, which may be challenging to manually find for large models:\nSetting will trigger additional consistency and synchronization checks on every collective call issued by the user either directly or indirectly (such as DDP ). This is done by creating a wrapper process group that wraps all process groups returned by and APIs. As a result, these APIs will return a wrapper process group that can be used exactly like a regular process group, but performs consistency checks before dispatching the collective to an underlying process group. Currently, these checks include a , which ensures all ranks complete their outstanding collective calls and reports ranks which are stuck. Next, the collective itself is checked for consistency by ensuring all collective functions match and are called with consistent tensor shapes. If this is not the case, a detailed error report is included when the application crashes, rather than a hang or uninformative error message. As an example, consider the following function which has mismatched input shapes into :\nWith the backend, such an application would likely result in a hang which can be challenging to root-cause in nontrivial scenarios. If the user enables and reruns the application, the following error message reveals the root cause:\nFor fine-grained control of the debug level during runtime the functions , , and can also be used.\nIn addition, can be used in conjunction with to log the entire callstack when a collective desynchronization is detected. These collective desynchronization checks will work for all applications that use collective calls backed by process groups created with the and APIs.\nIn addition to explicit debugging support via and , the underlying C++ library of also outputs log messages at various levels. These messages can be helpful to understand the execution state of a distributed training job and to troubleshoot problems such as network connection failures. The following matrix shows how the log level can be adjusted via the combination of and environment variables.\n  * : This is the base type of all distributed exceptions.\n  * : This exception is thrown when a backend-specific error occurs. For example, if the backend is used and the user attempts to use a GPU that is not available to the library.\n  * : This exception is thrown when networking libraries encounter errors (ex: Connection reset by peer)\n  * : This exception is thrown when the Store encounters an error (ex: TCPStore timeout)\n\n    \nException raised when an error occurs in the distributed library     \nException raised when an error occurs in the distributed store\nIf you are running single node training, it may be convenient to interactively breakpoint your script. We offer a way to conveniently breakpoint a single rank:     \nSet a breakpoint, but only on a single rank. All other ranks will wait for you to be done with the breakpoint before continuing.\n  *     *       * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/name_inference.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis document is a reference for , a process that defines how named tensors:\n\n\nBelow is a list of all operations that are supported with named tensors and their associated name inference rules.\nIf you don’t see an operation listed here, but it would help your use case, please \nThe named tensor API is experimental and subject to change.\nAll pointwise unary functions follow this rule as well as some other unary functions.\n  * Propagate names: input tensor’s names are propagated to the output.\n\n\nAll reduction ops like remove dimensions by reducing over the desired dimensions. Other operations like and remove dimensions.\nWherever one can pass an integer dimension index to an operator, one can also pass a dimension name. Functions that take lists of dimension indices can also take in a list of dimension names.\n  * Check names: If or is passed in as a list of names, check that those names exist in .\n  * Propagate names: If the dimensions of the input tensor specified by or are not present in the output tensor, then the corresponding names of those dimensions do not appear in .\n\n\n```\n         \n\n\n\n         \n \n\n\n\n         \n  \n\n\n```\n\nAll binary arithmetic ops follow this rule. Operations that broadcast still broadcast positionally from the right to preserve compatibility with unnamed tensors. To perform explicit broadcasting by names, use .\n  * Check names: All names must match positionally from the right. i.e., in , must be true for all in .\n  * Check names: Furthermore, all named dimensions must be aligned from the right. During matching, if we match a named dimension with an unnamed dimension , then must not appear in the tensor with the unnamed dimension.\n  * Propagate names: unify pairs of names from the right from both tensors to produce output names.\n\n\n```\n\n\n      \n      \n   \n \n\n```\n\n  * Because we matched in with , check to make sure doesn’t exist in (it does not).\n\n\n```\n\n\n\n      \n    \n   \n          \n               \n \n\n\n\n# other:  Tensor[      N]\n      \n    \n   \n         \n            \n  \n\n```\n\nIn both of the last examples, it is possible to align the tensors by names and then perform the addition. Use to align tensors by name or to align tensors to a custom dimension ordering.\nSome operations, like , permute the order of dimensions. Dimension names are attached to individual dimensions so they get permuted as well.\nIf the operator takes in positional index , it is also able to take a dimension name as .\n  * Check names: If is passed as a name, check that it exists in the tensor.\n  * Propagate names: Permute dimension names in the same way as the dimensions that are being permuted.\n\n\nMatrix multiply functions follow some variant of this. Let’s go through first and then generalize the rule for batch matrix multiplication.\nInherently, a matrix multiplication performs a dot product over two dimensions, collapsing them. When two tensors are matrix-multiplied, the contracted dimensions disappear and do not show up in the output tensor.\n, work in a similar way: name inference does not check input names and removes the dimensions that are involved in the dot product:\n  * Check names: Check that the batch dimensions of the inputs are aligned and broadcastable. See for what it means for the inputs to be aligned.\n  * Propagate names: result names are obtained by unifying the batch dimensions and removing the contracted dimensions: .\n\n\n```\n# Batch matrix multiply of matrices Tensor['C', 'D'] and Tensor['E', 'F'].\n\n          \n        \n  \n   \n\n```\n\nFinally, there are fused versions of many matmul functions. i.e., and . These are treated as composing name inference for i.e. and name inference for .\nFactory functions now take a new argument that associates a name with each dimension.\n  * If it has no named dimensions, then the names computed from the operation get propagated to it.\n  * If it has any named dimensions, then the names computed from the operation must be exactly equal to the existing names. Otherwise, the operation errors.\n\n\nAll in-place methods modify inputs to have names equal to the computed names from name inference. For example:\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/named_tensor.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nNamed Tensors allow users to give explicit names to tensor dimensions. In most cases, operations that take dimension parameters will accept dimension names, avoiding the need to track dimensions by position. In addition, named tensors use names to automatically check that APIs are being used correctly at runtime, providing extra safety. Names can also be used to rearrange dimensions, for example, to support “broadcasting by name” rather than “broadcasting by position”.\nThe named tensor API is a prototype feature and subject to change.\nFactory functions now take a new argument that associates a name with each dimension.\nNamed dimensions, like regular Tensor dimensions, are ordered. is the name of dimension of .\nUse to access the dimension names of a tensor and to rename named dimensions.\nNamed tensors can coexist with unnamed tensors; named tensors are instances of . Unnamed tensors have -named dimensions. Named tensors do not require all dimensions to be named.\nNamed tensors use names to automatically check that APIs are being called correctly at runtime. This occurs in a process called . More formally, name inference consists of the following two steps:\n  * : an operator may perform automatic checks at runtime that check that certain dimension names must match.\n\n\nTwo names if they are equal (string equality) or if at least one is . Nones are essentially a special “wildcard” name.\ndetermines which of the names and to propagate to the outputs. It returns the more of the two names, if they match. If the names do not match, then it errors.\nIn practice, when working with named tensors, one should avoid having unnamed dimensions because their handling can be complicated. It is recommended to lift all unnamed dimensions to be named dimensions by using .\nLet’s see how and are used in name inference in the case of adding two one-dim tensors with no broadcasting.\n: check that the names of the two tensors .\n```\n# x + y  # match('X', None) is True\n# x + z  # match('X', 'Z') is False\n# x + x  # match('X', 'X') is True\n\n  \nError when attempting to broadcast dims ['X'] and dims ['Z']: dim 'X' and dim 'Z' are at the same position from the right but do not match.\n\n```\n\n: the names to select which one to propagate. In the case of , because is more specific than .\nFor a comprehensive list of name inference rules, see . Here are two common operations that may be useful to go over:\n\n\nUse or to align tensor dimensions by name to a specified ordering. This is useful for performing “broadcasting by names”.\n```\n# This function is agnostic to the dimension ordering of `input`,\n# as long as it has a `C` dimension somewhere.\n  \n      \n       \n\n   \n    \n          \n          \n            \n\n  \n  \n  \n\n```\n\nUse to permute large amounts of dimensions without mentioning all of them as in required by .\n```\n       \n       \n\n# Move the F (dim 5) and E dimension (dim 4) to the front while keeping\n\n     \n  \n\n```\n\nUse and to flatten and unflatten dimensions, respectively. These methods are more verbose than and , but have more semantic meaning to someone reading the code.\nAutograd currently supports named tensors in a limited manner: autograd ignores names on all tensors. Gradient computation is still correct but we lose the safety that names give us.\n```\n   \n    \n    \n  \n\n  # Unnamed for now. Will be named in the future\n\n\n\n  \n    \n# Ideally we'd check that the names of loss and grad_loss match but we don't yet.\n\n\n\n\n```\n\nSee for a full list of the supported torch and tensor operations. We do not yet support the following that is not covered by the link:\nAutograd is supported, see . Because gradients are currently unnamed, optimizers may work but are untested.\nNN modules are currently unsupported. This can lead to the following when calling modules with named tensor inputs:\n  * NN module parameters are unnamed, so outputs may be partially named.\n  * NN module forward passes have code that don’t support named tensors and will error out appropriately.\n\n\nWe also do not support the following subsystems, though some may work out of the box:\nIf any of these would help your use case, please \nIn this section please find the documentation for named tensor specific APIs. For a comprehensive reference for how names are propagated through other PyTorch operators, see .          \ncorresponds to the name of tensor dimension . Names are either a string if the dimension is named or if the dimension is unnamed.\nDimension names may contain characters or underscore. Furthermore, a dimension name must be a valid Python variable name (i.e., does not start with underscore).\nTensors may not have two named dimensions with the same name.\nThe named tensor API is experimental and subject to change.     \nreturns a view on tensor that has dims renamed as specified in the mapping .\nreturns a view on tensor, renaming all dimensions positionally using . Use to drop names on a tensor.\n```\n         \n   \n\n\n\n  \n\n\n\n     \n\n\n\n```\n\nThe named tensor API is experimental and subject to change.     \nRefining is a special case of renaming that “lifts” unnamed dimensions. A dim can be refined to have any name; a named dim can only be refined to have the same name.\nBecause named tensors can coexist with unnamed tensors, refining names gives a nice way to write named-tensor-aware code that works with both named and unnamed tensors.\nmay contain up to one Ellipsis (). The Ellipsis is expanded greedily; it is expanded in-place to fill to the same length as using names from the corresponding indices of .\nPython 2 does not support Ellipsis but one may use a string literal instead ().\nThe named tensor API is experimental and subject to change.     \nPermutes the dimensions of the tensor to match the dimension order in the tensor, adding size-one dims for any new names.\nThis operation is useful for explicit broadcasting by names (see examples).\nAll of the dims of must be named in order to use this method. The resulting tensor is a view on the original tensor.\nAll dimension names of must be present in . may contain named dimensions that are not in ; the output tensor has a size-one dimension for each of those new names.\n```\n\n       \n          \n  \n\n\n\n   \n      \n       \n\n   \n    \n          \n          \n            \n\n# scale_channels is agnostic to the dimension order of the input\n  \n  \n  \n\n```\n\nThe named tensor API is experimental and subject to change.     \nPermutes the dimensions of the tensor to match the order specified in , adding size-one dims for any new names.\nAll of the dims of must be named in order to use this method. The resulting tensor is a view on the original tensor.\nAll dimension names of must be present in . may contain additional names that are not in ; the output tensor has a size-one dimension for each of those new names.\nmay contain up to one Ellipsis (). The Ellipsis is expanded to be equal to all dimension names of that are not mentioned in , in the order that they appear in .\nPython 2 does not support Ellipsis but one may use a string literal instead ().\n```\n       \n       \n\n# Move the F and E dims to the front while keeping the rest in order\n  \n\n```\n\nThe named tensor API is experimental and subject to change.     \nAll of must be consecutive in order in the tensor, but not necessary contiguous in memory.\nThe named tensor API is experimental and subject to change.\n  * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/executorch": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/executorch/stable/getting-started-setup.html) [ ](https://pytorch.org/tutorials/beginner/introyt.html)\n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\n  * Compatibility with a wide variety of computing platforms, from high-end mobile phones to highly constrained embedded systems and microcontrollers.\n  * Enabling developers to use the same toolchains and Developer Tools from PyTorch model authoring and conversion, to debugging and deployment to a wide variety of platforms.\n  * Providing end users with a seamless and high-performance experience due to a lightweight runtime and utilizing full hardware capabilities such as CPUs, NPUs, and DSPs.\n\n\n  * LLMs (Large Language Models), CV (Computer Vision), ASR (Automatic Speech Recognition), TTS (Text To Speech)\n\n\n\n\n\n\n\n\n\n\n  *     *       * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/cuda.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nis used to set up and run CUDA operations. It keeps track of the currently selected GPU, and all CUDA tensors you allocate will by default be created on that device. The selected device can be changed with a context manager.\nHowever, once a tensor is allocated, you can do operations on it irrespective of the selected device, and the results will be always placed on the same device as the tensor.\nCross-GPU operations are not allowed by default, with the exception of and other methods with copy-like functionality such as and . Unless you enable peer-to-peer memory access, any attempts to launch ops on tensors spread across different devices will raise an error.\n```\n       \n  \n    \n\n    \n\n   \n\n\n \n    \n        \n\n    \n       \n    \n\n    # You can also use ``Tensor.to`` to transfer a tensor:\n       \n    \n\n        \n    \n\n        \n    \n\n    # even within a context, you can specify the device\n    # (or give a GPU index to the .cuda call)\n       \n      \n      \n    \n\n```\n\nStarting in PyTorch 1.7, there is a new flag called . This flag defaults to True in PyTorch 1.7 to PyTorch 1.11, and False in PyTorch 1.12 and later. This flag controls whether PyTorch is allowed to use the TensorFloat32 (TF32) tensor cores, available on NVIDIA GPUs since Ampere, internally to compute matmul (matrix multiplies and batched matrix multiplies) and convolutions.\nTF32 tensor cores are designed to achieve better performance on matmul and convolutions on tensors by rounding input data to have 10 bits of mantissa, and accumulating results with FP32 precision, maintaining FP32 dynamic range.\nmatmuls and convolutions are controlled separately, and their corresponding flags can be accessed at:\n```\n# The flag below controls whether to allow TF32 on matmul. This flag defaults to False\n\n  \n\n# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n  \n\n```\n\nThe precision of matmuls can also be set more broadly (limited not just to CUDA) via . Note that besides matmuls and convolutions themselves, functions and nn modules that internally uses matmuls or convolutions are also affected. These include , , cdist, tensordot, affine grid and grid sample, adaptive log softmax, GRU and LSTM.\nTo get an idea of the precision and speed, see the example code and benchmark data (on A100) below:\n```\n     \n     \n    \n    \n\n  \n  \n\n\n  \n      \n      \n      \n\n\n  \n      \n      \n      \n\n```\n\nFrom the above example, we can see that with TF32 enabled, the speed is ~7x faster on A100, and that relative error compared to double precision is approximately 2 orders of magnitude larger. Note that the exact ratio of TF32 to single precision speed depends on the hardware generation, as properties such as the ratio of memory bandwidth to compute as well as the ratio of TF32 to FP32 matmul throughput may vary from generation to generation or model to model. If full FP32 precision is needed, users can disable TF32 by:\nTo toggle the TF32 flags off in C++, you can do\n(Distinct from full FP16 accumulation that is intended for hardware that has higher throughput with FP16 accumulation than FP32 accumulation, see )\nfp16 GEMMs are potentially done with some intermediate reduced precision reductions (e.g., in fp16 rather than fp32). These selective reductions in precision can allow for higher performance on certain workloads (particularly those with a large dimension) and GPU architectures at the cost of numerical precision and potential for overflow.\nIf full precision reductions are needed, users can disable reduced precision reductions in fp16 GEMMs with:\nTo toggle the reduced precision reduction flags in C++, one can do\nA similar flag (as above) exists for BFloat16 GEMMs. Note that this switch is set to by default for BF16, if you observe numerical instability in your workload, you may wish to set it to .\nIf reduced precision reductions are not desired, users can disable reduced precision reductions in bf16 GEMMs with:\nTo toggle the reduced precision reduction flags in C++, one can do\nCertain GPUs have increased performance when doing _all_ FP16 GEMM accumulation in FP16, at the cost of numerical precision and greater likelihood of overflow. Note that this setting only has an effect on GPUs of compute capability 7.0 (Volta) or newer.\nTo toggle the reduced precision reduction flags in C++, one can do\nBy default, GPU operations are asynchronous. When you call a function that uses the GPU, the operations are to the particular device, but not necessarily executed until later. This allows us to execute more computations in parallel, including operations on CPU or other GPUs.\nIn general, the effect of asynchronous computation is invisible to the caller, because (1) each device executes operations in the order they are queued, and (2) PyTorch automatically performs necessary synchronization when copying data between CPU and GPU or between two GPUs. Hence, computation will proceed as if every operation was executed synchronously.\nYou can force synchronous computation by setting environment variable . This can be handy when an error occurs on the GPU. (With asynchronous execution, such an error isn’t reported until after the operation is actually executed, so the stack trace does not show where it was requested.)\nA consequence of the asynchronous computation is that time measurements without synchronizations are not accurate. To get precise measurements, one should either call before measuring, or use to record times as following:\n```\n  \n  \n\n\n\n\n\n  \n  \n\n```\n\nAs an exception, several functions such as and admit an explicit argument, which lets the caller bypass synchronization when it is unnecessary. Another exception is CUDA streams, explained below.\nOperations inside each stream are serialized in the order they are created, but operations from different streams can execute concurrently in any relative order, unless explicit synchronization functions (such as or ) are used. For example, the following code is incorrect:\n```\n  \n    \n     \n \n    \n      \n\n```\n\nWhen the “current stream” is the default stream, PyTorch automatically performs necessary synchronization when data is moved around, as explained above. However, when using non-default streams, it is the user’s responsibility to ensure proper synchronization. The fixed version of this example is:\nThere are two new additions. The call ensures that the execution has finished before we start running on a side stream. The (see for more details) ensures that we do not deallocate A before has completed. You can also manually wait on the stream at some later point in time with (note that it is pointless to wait immediately, since that will prevent the stream execution from running in parallel with other work on the default stream.) See the documentation for on more details on when to use one or another.\nNote that this synchronization is necessary even when there is no read dependency, e.g., as seen in this example:\nDespite the computation on not reading the contents of and no other uses of , it is still necessary to synchronize, because may correspond to memory reallocated by the CUDA caching allocator, with pending operations from the old (deallocated) memory.\nEach backward CUDA op runs on the same stream that was used for its corresponding forward op. If your forward pass runs independent ops in parallel on different streams, this helps the backward pass exploit that same parallelism.\nThe stream semantics of a backward call with respect to surrounding ops are the same as for any other call. The backward pass inserts internal syncs to ensure this even when backward ops run on multiple streams as described in the previous paragraph. More concretely, when calling , , or , and optionally supplying CUDA tensor(s) as the initial gradient(s) (e.g., , , or ), the acts of\n\n\nhave the same stream-semantics relationship as any group of ops:\n```\n  \n\n# Safe, grads are used in the same stream context as backward()\n \n    \n     \n\n\n \n    \n \n\n\n \n    \n\n \n\n# Safe, populating initial grad and invoking backward are in the same stream context\n \n    \n\n# Unsafe, populating initial_grad and invoking backward are in different stream contexts,\n\n  \n \n    \n\n\n  \n\n \n    \n    \n\n```\n\nIn prior versions of PyTorch (1.9 and earlier), the autograd engine always synced the default stream with all backward ops, so the following pattern:\nwas safe as long as happened on the default stream. In present PyTorch, that pattern is no longer safe. If and are in different stream contexts, you must sync the streams:\nPyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in . You can use and to monitor memory occupied by tensors, and use and to monitor the total amount of memory managed by the caching allocator. Calling releases all cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\nTo better understand how CUDA memory is being used over time, describes tools for capturing and visualizing traces of memory use.\nFor more advanced users, we offer more comprehensive memory benchmarking via . We also offer the capability to capture a complete snapshot of the memory allocator state via , which can help you understand the underlying allocation patterns produced by your code.\nUse of a caching allocator can interfere with memory checking tools such as . To debug memory errors using , set in your environment to disable caching.\nThe behavior of the caching allocator can be controlled via the environment variable . The format is Available options:\n  * allows selecting the underlying allocator implementation. Currently, valid options are , which uses PyTorch’s native implementation, and , which uses requires CUDA 11.4 or newer. The default is . applies to all devices used by the process, and can’t be specified on a per-device basis.\n  * prevents the native allocator from splitting blocks larger than this size (in MB). This can reduce fragmentation and may allow some borderline workloads to complete without running out of memory. Performance cost can range from ‘zero’ to ‘substantial’ depending on allocation patterns. Default value is unlimited, i.e. all blocks can be split. The and methods are useful for tuning. This option should be used as a last resort for a workload that is aborting due to ‘out of memory’ and showing a large amount of inactive split blocks. is only meaningful with . With , is ignored.\n  * helps with rounding the requested allocation size to nearest power-2 division and making better use of the blocks. In the native CUDACachingAllocator, the sizes are rounded up in multiple of blocks size of 512, so this works fine for smaller sizes. However, this can be inefficient for large near-by allocations as each will go to different size of blocks and re-use of those blocks are minimized. This might create lots of unused blocks and will waste GPU memory capacity. This option enables the rounding of allocation size to nearest power-2 division. For example, if we need to round-up size of 1200 and if number of divisions is 4, the size 1200 lies between 1024 and 2048 and if we do 4 divisions between them, the values are 1024, 1280, 1536, and 1792. So, allocation size of 1200 will be rounded to 1280 as the nearest ceiling of power-2 division. Specify a single value to apply for all allocation sizes or specify an array of key value pairs to set power-2 division individually for each power of two interval. For example to set 1 division for all allocations under 256MB, 2 division for allocations between 256MB and 512MB, 4 divisions for allocations between 512MB and 1GB and 8 divisions for any larger allocations, set the knob value to: [256:1,512:2,1024:4,>:8]. is only meaningful with . With , is ignored.\n  *     \na 1024MB cached block can be re-used for a 512MB allocation request. In the default case, we only allow up to 20MB of rounding of non-split blocks, so a 512MB block can only be served with between 512-532 MB size block. If we set the value of this option to 1024, it will alow 512-1536 MB size blocks to be used for a 512MB block which increases reuse of larger blocks. This will also help in reducing the stalls in avoiding expensive cudaMalloc calls.\n  * helps actively reclaiming unused GPU memory to avoid triggering expensive sync-and-reclaim-all operation (release_cached_blocks), which can be unfavorable to latency-critical GPU applications (e.g., servers). Upon setting this threshold (e.g., 0.8), the allocator will start reclaiming GPU memory blocks if the GPU memory capacity usage exceeds the threshold (i.e., 80% of the total memory allocated to the GPU application). The algorithm prefers to free old & unused blocks first to avoid freeing blocks that are actively being reused. The threshold value should be between greater than 0.0 and less than 1.0. is only meaningful with . With , is ignored.\n  * (experimental, default: ) If set to , this setting instructs the allocator to create CUDA allocations that can later be expanded to better handle cases where a job changing allocation sizes frequently, such as having a changing batch size. Normally for large (>2MB) allocations, the allocator calls cudaMalloc to get allocations that are the same size as what the user requests. In the future, parts of these allocations can be reused for other requests if they are free. This works well when the program makes many requests of exactly the same size or of sizes that even multiples of that size. Many deep learning models follow this behavior. However, one common exception is when the batch size changes slightly from one iteration to the next, e.g. in batched inference. When the program runs initially with batch size , it will make allocations appropriate for that size. If in the future, it runs at size , the existing allocations will still be big enough. However, if it runs at size , then it will have to make new allocations that are slightly larger. Not all the tensors are the same size. Some might be and others where and are some non-batch dimensions in the model. Because the allocator reuses existing allocations when they are big enough, some number of allocations will actually fit in the already existing segments, though not perfectly. As the model runs it will partially fill up all of these segments leaving unusable free slices of memory at the end of these segments. The allocator at some point will need to a new segment. If there is not enough memory, there is now no way to recover the slices of memory that are free at the end of existing segments. With models 50+ layers deep, this pattern might repeat 50+ times creating many slivers.\nallows the allocator to create a segment initially and then expand its size later when more memory is needed. Instead of making one segment per allocation, it tries to make one segment (per stream) that grows as necessary. Now when the case runs, the allocations will tile nicely into the one large segment until it fills up. Then more memory is requested and appended to the end of the segment. This process does not create as many slivers of unusable memory, so it is more likely to succeed at finding this memory.\noption is a boolean flag that determines whether to use the CUDA API’s cudaHostRegister function for allocating pinned memory instead of the default cudaHostAlloc. When set to True, the memory is allocated using regular malloc and then pages are mapped to the memory before calling cudaHostRegister. This pre-mapping of pages helps reduce the lock time during the execution of cudaHostRegister.\noption is only valid when pinned_use_cuda_host_register is set to True. By default, one thread is used to map the pages. This option allows using more threads to parallelize the page mapping operations to reduce the overall allocation time of pinned memory. A good value for this option is 8 based on benchmarking results.\noption is a boolean flag to enable background thread for processing events. This avoids any slow path associated with querying/processing of events in the fast allocation path. This feature is disabled by default.\n\n\nSome stats reported by the are specific to , and are not meaningful with . See each function’s docstring for details.\nIt is possible to define allocators as simple functions in C/C++ and compile them as a shared library, the code below shows a basic allocator that just traces all the memory operations.\n```\n\n\n\n// Compile with g++ alloc.cc -o alloc.so -I/usr/local/cuda/include -shared -fPIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n```\n\nThis can be used in python through the . The user is responsible for supplying the path to the file and the name of the alloc/free functions that match the signatures specified above.\n```\n \n\n\n  \n      \n\n\n# This will allocate memory in the device using the new allocator\n   \n\n```\n\n```\n \n\n\n   \n\n  \n      \n# This will error since the current allocator was already instantiated\n\n\n```\n\nDepending on your use case, may not be what you want to use, since it swaps the CUDA allocator for the entire program (similar to ). For instance, if the swapped allocator doesn’t have caching mechanism, you will lose all the benefits of PyTorch’s CUDACachingAllocator. Instead, you can selectively mark a region of PyTorch code to use a custom allocator using . This will let you use multiple CUDA system allocators in the same PyTorch program, along with most of the benefits of the CUDACachingAllocator (e.g. caching). Using , you can utilize custom allocators that enable several features, such as:\n  * Allocating output buffers for an all-reduce using allocator can enable NVLink Switch Reductions (NVLS). This can reduce contention between overlapping compute and communication kernels on GPU resources (SMs, and Copy Engines), especially on tensor-parallel workloads.\n  * For Grace CPU based systems, allocating host outputs buffers for an all-gather using and specifying can enable Extended GPU Memory (EGM) based memory transfers from source GPUs to the destination CPU. This accelerates the all-gather since the transfer happens over NVLinks, which otherwise would have happened over bandwidth-limited, Network Interface Card (NIC) links. Such an accelerated all-gather can in turn speed up model checkpointing.\n  * If you are crafting a model and don’t want to think about the optimal memory placements of a memory intensive module at first (e.g. an embedding table), or perhaps you have a module which is not performance sensitive and doesn’t fit in the GPU, then you could just allocate that module with with preferred CPU location and get your model working first.\n\n\nWhile offers convenient automatic memory management using CUDA Unified Virtual Memory (UVM), it is not recommended for DL workloads. For DL workloads that fit in GPU memory, explicit placement consistently outperforms UVM, since there are no page faults and access patterns remain predictable. When GPU memory gets saturated, UVM has to perform costly double transfers, evicting pages to CPU before bringing in new ones.\n```\n \n\n \n   \n   \n   \n   \n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\nvoid nccl_free_plug(void* ptr, size_t size, int device, void* stream) {\n\n\n\n\n\n\n  \n  \n    \n    \n    \n    \n    \n    \n    \n\n\n  \n      \n\n\n\n  \n  \n  \n\n\n  \n  \n  \n\n\n\n  \n\n```\n\nYou can now define a new memory pool by passing this allocator to :\nThe pool can then be used with the context manager to allocate tensors into that pool:\n```\n \n    # tensor gets allocated with ncclMemAlloc passed in the pool\n           \n    \n\n# register user buffers using ncclCommRegister (called under the hood)\n\n\n\n\n\n\n\n```\n\nNote the usage of in the above example. This is an extra step for NVLS reductions, where the user buffers need to be registered with NCCL. A user can de-register the buffers with a similar call.\nTo reclaim memory, users will first need to ensure nothing is using the pool. When none of the tensors are holding a reference to the pool, will be called internally on deletion of the pool, hence returning all the memory to the system.\n```\n  \n\n# pool's use count should be 1 at this point as MemPool object\n\n   \n\n      \n\n \n       \n\n    # pool's use count should be 2 at this point as use_mem_pool\n    \n       \n\n# pool's use count should be back to 1 at this point as use_mem_pool\n\n   \n\n \n    # pool should have 1 segment since we made a small allocation (1 MB)\n    # above and so the CUDACachingAllocator packed it into a 2 MB buffer\n       \n\n       \n\n    # pool should still have 1 segment since we made another small allocation\n    # (1 MB) that got packed into the existing 2 MB buffer\n       \n\n       \n\n    # pool now should have 2 segments since the CUDACachingAllocator had\n    # to make a new 2 MB buffer to accomodate out_2\n       \n\n```\n\n  * holds a reference to the pool. When you use the context manager, it will also acquire another reference to the pool. On exit of the context manager, it will release its reference. After that, ideally it should only be tensors holding references to the pool. Once the tensors release their references, the use count of the pool will be 1, reflecting that only the object is holding a reference. Only at that point, can the memory held by the pool be returned to the system when the pool’s destructor is called using .\n  * Allocators like can use more memory than requested, due to alignment requirements (, ), and can cause your workload to run out of memory.\n\n\nFor each combination of cuBLAS handle and CUDA stream, a cuBLAS workspace will be allocated if that handle and stream combination executes a cuBLAS kernel that requires a workspace. In order to avoid repeatedly allocating workspaces, these workspaces are not deallocated unless is called. The workspace size per allocation can be specified via the environment variable with the format . As an example, the default workspace size per allocation is which specifies a total size of . To force cuBLAS to avoid using workspaces, set .\nFor each CUDA device, an LRU cache of cuFFT plans is used to speed up repeatedly running FFT methods (e.g., ) on CUDA tensors of same geometry with same configuration. Because some cuFFT plans may allocate GPU memory, these caches have a maximum capacity.\nYou may control and query the properties of the cache of current device with the following APIs:\n  * gives the capacity of the cache (default is 4096 on CUDA 10 and newer, and 1023 on older CUDA versions). Setting this value directly modifies the capacity.\n\n\nTo control and query plan caches of a non-default device, you can index the object with either a object or a device index, and access one of the above attributes. E.g., to set the capacity of the cache for device , one can write .\nPyTorch just-in-time compiles some operations, like torch.special.zeta, when performed on CUDA tensors. This compilation can be time consuming (up to a few seconds depending on your hardware and software) and may occur multiple times for a single operator since many PyTorch operators actually select from a variety of kernels, each of which must be compiled once, depending on their input. This compilation occurs once per process, or just once if a kernel cache is used.\nBy default, PyTorch creates a kernel cache in $XDG_CACHE_HOME/torch/kernels if XDG_CACHE_HOME is defined and $HOME/.cache/torch/kernels if it’s not (except on Windows, where the kernel cache is not yet supported). The caching behavior can be directly controlled with two environment variables. If USE_PYTORCH_KERNEL_CACHE is set to 0 then no cache will be used, and if PYTORCH_KERNEL_CACHE_PATH is set then that path will be used as a kernel cache instead of the default location.\nDue to the structure of PyTorch, you may need to explicitly write device-agnostic (CPU or GPU) code; an example may be creating a new tensor as the initial hidden state of a recurrent neural network.\nThe first step is to determine whether the GPU should be used or not. A common pattern is to use Python’s module to read in user arguments, and have a flag that can be used to disable CUDA, in combination with . In the following, results in a object that can be used to move tensors to CPU or CUDA.\nWhen assessing the availability of CUDA in a given environment (), PyTorch’s default behavior is to call the CUDA Runtime API method will fail with a CUDA initialization error.\nOne can set in your environment before importing PyTorch modules that execute (or before executing it directly) in order to direct to attempt an NVML-based assessment ( calls will not poison subsequent forks.\nIf NVML discovery/initialization fails, will fallback to the standard CUDA Runtime API assessment and the aforementioned fork constraint will apply.\nNote that the above NVML-based CUDA availability assessment provides a weaker guarantee than the default CUDA Runtime API approach (which requires CUDA initialization to succeed). In some circumstances, the NVML-based check may succeed while later CUDA initialization fails.\nNow that we have , we can use it to create a Tensor on the desired device.\nThis can be used in a number of cases to produce device agnostic code. Below is an example when using a dataloader:\nWhen working with multiple GPUs on a system, you can use the environment flag to manage which GPUs are available to PyTorch. As mentioned above, to manually control which GPU a tensor is created on, the best practice is to use a context manager.\n```\n  \n \n      \n  \n\n```\n\nIf you have a tensor and would like to create a new tensor of the same type on the same device, then you can use a method (see ). Whilst the previously mentioned factory functions () depend on the current GPU context and the attributes arguments you pass in, methods preserve the device and other attributes of the tensor.\nThis is the recommended practice when creating modules in which new tensors need to be created internally during the forward pass.\nIf you want to create a tensor of the same type and size of another tensor, and fill it with either ones or zeros, or are provided as convenient helper functions (which also preserve and of a Tensor).\nThis is an advanced tip. If you overuse pinned memory, it can cause serious problems when running low on RAM, and you should be aware that pinning is often an expensive operation.\nHost to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a method, that returns a copy of the object, with data put in a pinned region.\nAlso, once you pin a tensor or storage, you can use asynchronous GPU copies. Just pass an additional argument to a or a call. This can be used to overlap data transfers with computation.\nYou can make the return batches placed in pinned memory by passing to its constructor.\nMost use cases involving batched inputs and multiple GPUs should default to using to utilize more than one GPU.\nThere are significant caveats to using CUDA models with ; unless care is taken to meet the data handling requirements exactly, it is likely that your program will have incorrect or undefined behavior.\nIt is recommended to use , instead of to do multi-GPU training, even if there is only a single node.\nThe difference between and is: uses multiprocessing where a process is created for each GPU, while uses multithreading. By using multiprocessing, each GPU has its dedicated process, this avoids the performance overhead caused by GIL of Python interpreter.\nIf you use , you could use utility to launch your program, see .\nA CUDA graph is a record of the work (mostly kernels and their arguments) that a CUDA stream and its dependent streams perform. For general principles and details on the underlying CUDA API, see \nPyTorch supports the construction of CUDA graphs using . CUDA work issued to a capturing stream doesn’t actually run on the GPU. Instead, the work is recorded in a graph.\nAfter capture, the graph can be to run the GPU work as many times as needed. Each replay runs the same kernels with the same arguments. For pointer arguments this means the same memory addresses are used. By filling input memory with new data (e.g., from a new batch) before each replay, you can rerun the same work on new data.\nReplaying a graph sacrifices the dynamic flexibility of typical eager execution in exchange for . A graph’s arguments and kernels are fixed, so a graph replay skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver overheads. Under the hood, a replay submits the entire graph’s work to the GPU with a single call to \nYou should try CUDA graphs if all or part of your network is graph-safe (usually this means static shapes and static control flow, but see the other ) and you suspect its runtime is at least somewhat CPU-limited.\nThis API is in beta and may change in future releases.\nis a simple, versatile context manager that captures CUDA work in its context. Before capture, warm up the workload to be captured by running a few eager iterations. Warmup must occur on a side stream. Because the graph reads from and writes to the same memory addresses in every replay, you must maintain long-lived references to tensors that hold input and output data during capture. To run the graph on new input data, copy new data to the capture’s input tensor(s), replay the graph, then read the new output from the capture’s output tensor(s). Example:\n```\n  \n\n\n   \n\n\n  \n\n \n       \n            \n\n\n\n# To allow capture, automatically sets a side stream as the current stream in the context\n \n        \n\n# Fills the graph's input memory with new data to compute on\n  \n\n\n  \n\n# Fills the graph's input memory with more data to compute on\n  \n\n  \n\n```\n\nSee , , and for realistic and advanced patterns.\nis more sophisticated. accepts Python functions and s. For each passed function or Module, it creates separate graphs of the forward-pass and backward-pass work. See .\nA set of ops is if it doesn’t violate any of the following constraints.\nConstraints apply to all work in a context and all work in the forward and backward passes of any callable you pass to .\nViolating any of these will likely cause a runtime error:\n  * Capture must occur on a non-default stream. (This is only a concern if you use the raw and calls. and set a side stream for you.)\n  * Ops that synchronize the CPU with the GPU (e.g., calls) are prohibited.\n  * CUDA RNG operations are permitted, and when using multiple instances within a graph, they must be registered using before graph capture. Avoid using and during capture; instead, utilize and for managing generator states safely within the graph context. This ensures proper RNG operation and generator management within CUDA graphs.\n\n\nViolating any of these will likely cause silent numerical errors or undefined behavior:\n  * Within a process, only one capture may be underway at a time.\n  * No non-captured CUDA work may run in this process (on any thread) while capture is underway.\n  * CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.\n  * Every replay reads from and writes to the same (virtual) memory addresses.\n  * Dynamic control flow (based on CPU or GPU data) is prohibited.\n  * Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence has the same size and layout in every replay.\n  * Using multiple streams in a capture is allowed, but there are .\n\n\n  * Once captured, the graph may be replayed on any stream.\n\n\nIf your entire network is capturable, you can capture and replay an entire iteration:\n```\n        \n   \n                            \n                             \n                            \n  \n   \n\n\n    \n    \n\n\n\n# but in a real setting, because the warmup includes optimizer.step()\n# you must use a few batches of real data.\n  \n\n \n       \n        \n          \n           \n        \n        \n\n\n\n  \n# Sets grads to None before capture, so backward() will create\n# .grad attributes with allocations from the graph's private pool\n\n \n      \n       \n    \n    \n\n      \n      \n\n     \n    # Fills the graph's input memory with new data to compute on\n    \n    \n    \n    # You don't even need to call optimizer.zero_grad() between iterations\n    # because the captured backward refills static .grad tensors in place.\n    \n    \n    # attributes hold values from computing on this iteration's data.\n\n```\n\nIf some of your network is unsafe to capture (e.g., due to dynamic control flow, dynamic shapes, CPU syncs, or essential CPU-side logic), you can run the unsafe part(s) eagerly and use to graph only the capture-safe part(s).\nBy default, callables returned by are autograd-aware, and can be used in the training loop as direct replacements for the functions or s you passed.\ninternally creates objects, runs warmup iterations, and maintains static inputs and outputs as needed. Therefore (unlike with ) you don’t need to handle those manually.\nIn the following example, data-dependent dynamic control flow means the network isn’t capturable end-to-end, but lets us capture and run graph-safe sections as graphs regardless:\n```\n        \n\n   \n   \n   \n\n  \n  \n                                  \n                                  \n                            \n\n\n\n# requires_grad state of real inputs each callable will see.\n    \n     \n\n   \n   \n   \n\n      \n        \n\n     \n    \n\n        \n\n       \n            \n    \n            \n\n       \n    \n    # as well as module1's backward ops, run as graphs\n    \n    \n\n```\n\nFor typical optimizers, syncs the CPU with the GPU, which is prohibited during capture. To avoid errors, either use , or (if forward, loss, and backward are capture-safe) capture forward, loss, and backward but not the optimizer step:\n```\n\n# In a real setting, use a few batches of real data.\n  \n\n \n       \n        \n         \n              \n               \n        \n        \n        \n\n\n\n  \n\n \n     \n          \n           \n    \n    \n\n      \n      \n\n     \n    \n    \n    \n    \n    \n    \n\n```\n\nCapture mode automatically propagates to any streams that sync with a capturing stream. Within capture, you may expose parallelism by issuing calls to different streams, but the overall stream dependency DAG must branch out from the initial capturing stream after capture begins and rejoin the initial stream before capture ends:\n```\n \n    \n    \n\n    # INCORRECT (does not branch out from or rejoin initial stream)\n     \n        \n\n    \n    \n    \n     \n        \n    \n    \n\n```\n\nTo avoid confusion for power users looking at replays in nsight systems or nvprof: Unlike eager execution, the graph interprets a nontrivial stream DAG in capture as a hint, not a command. During replay, the graph may reorganize independent ops onto different streams or enqueue them in a different order (while respecting your original DAG’s overall dependencies).\nNCCL versions earlier than 2.9.6 don’t allow collectives to be captured. You must use , which defers allreduces to happen outside graphed sections of backward.\nNCCL versions 2.9.6 or later allow collectives in the graph. Approaches that capture an are a viable option, but need three setup steps.\n  1. Before full-backward capture, DDP must be constructed in a side-stream context:\n  2. Your warmup must run at least 11 DDP-enabled eager iterations before capture.\n\n\nA captured graph acts on the same virtual addresses every time it replays. If PyTorch frees the memory, a later replay can hit an illegal memory access. If PyTorch reassigns the memory to new tensors, the replay can corrupt the values seen by those tensors. Therefore, the virtual addresses used by the graph must be reserved for the graph across replays. The PyTorch caching allocator achieves this by detecting when capture is underway and satisfying the capture’s allocations from a graph-private memory pool. The private pool stays alive until its object and all tensors created during capture go out of scope.\nPrivate pools are maintained automatically. By default, the allocator creates a separate private pool for each capture. If you capture multiple graphs, this conservative approach ensures graph replays never corrupt each other’s values, but sometimes needlessly wastes memory.\nTo economize the memory stashed in private pools, and optionally allow different captures to share the same private pool. It’s safe for a set of graphs to share a private pool if you know they’ll always be replayed in the same order they were captured, and never be replayed concurrently.\n’s argument is a hint to use a particular private pool, and can be used to share memory across graphs as shown:\n```\n  \n  \n\n# (create static inputs for g1 and g2, run warmups of their workloads...)\n\n\n \n      \n\n# Captures g2, hinting that g2 may share a memory pool with g1\n  \n      \n\n\n\n\n\n\n```\n\nWith , if you want to graph several callables and you know they’ll always run in the same order (and never concurrently) pass them as a tuple in the same order they’ll run in the live workload, and will capture their graphs using a shared private pool.\nIf, in the live workload, your callables will run in an order that occasionally changes, or if they’ll run concurrently, passing them as a tuple to a single invocation of is not allowed. Instead, you must call separately for each one.\n  *     *       *     *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n",
  "https://docs.pytorch.org/docs/stable/notes/autograd.html": "  * [ Run PyTorch locally or get started quickly with one of the supported cloud platforms ](https://pytorch.org/get-started) [ ](https://pytorch.org/tutorials/beginner/introyt.html)  \n  * [ Learn about the tools and frameworks in the PyTorch Ecosystem ](https://pytorch.org/ecosystem) [ Join the PyTorch developer community to contribute, learn, and get your questions answered ](https://pytorch.org/#community-module) [ ](https://pytorch.org/ecosystem/contributor-awards-2024)\n  * [ ](https://pytorch.org/edge) [ End-to-end solution for enabling on-device inference capabilities across mobile and edge devices ](https://pytorch.org/executorch-overview)\n  * [ Explore the documentation for comprehensive guidance on how to use PyTorch ](https://pytorch.org/docs/stable/index.html) [ Read the PyTorch Domains documentation to learn more about domain-specific libraries ](https://pytorch.org/pytorch-domains)\n  * [ ](https://pytorch.org/blog/) [ Learn how our community solves real, everyday machine learning problems with PyTorch ](https://pytorch.org/community-stories)\n  * \n\nThis note will present an overview of how autograd works and records the operations. It’s not strictly necessary to understand all this, but we recommend getting familiar with it, as it will help you write more efficient, cleaner programs, and can aid you in debugging.\nAutograd is a reverse automatic differentiation system. Conceptually, autograd records a graph recording all of the operations that created the data as you execute operations, giving you a directed acyclic graph whose leaves are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\nInternally, autograd represents this graph as a graph of objects (really expressions), which can be ed to compute the result of evaluating the graph. When computing the forward pass, autograd simultaneously performs the requested computations and builds up a graph representing the function that computes the gradient (the attribute of each is an entry point into this graph). When the forward pass is completed, we evaluate this graph in the backwards pass to compute the gradients.\nAn important thing to note is that the graph is recreated from scratch at every iteration, and this is exactly what allows for using arbitrary Python control flow statements, that can change the overall shape and size of the graph at every iteration. You don’t have to encode all possible paths before you launch the training - what you run is what you differentiate.\nSome operations need intermediary results to be saved during the forward pass in order to execute the backward pass. For example, the function saves the input to compute the gradient.\nWhen defining a custom Python , you can use to save tensors during the forward pass and to retrieve them during the backward pass. See for more information.\nFor operations that PyTorch defines (e.g. ), tensors are automatically saved as needed. You can explore (for educational or debugging purposes) which tensors are saved by a certain by looking for its attributes starting with the prefix .\nIn the previous code, refers to the same Tensor object as . But that may not always be the case. For instance:\nUnder the hood, to prevent reference cycles, PyTorch has the tensor upon saving and it into a different tensor for reading. Here, the tensor you get from accessing is a different tensor object than (but they still share the same storage).\nWhether a tensor will be packed into a different tensor object depends on whether it is an output of its own , which is an implementation detail subject to change and that users should not rely on.\nYou can control how PyTorch does packing / unpacking with .\nThe gradient computation using Automatic Differentiation is only valid when each elementary function being used is differentiable. Unfortunately many of the functions we use in practice do not have this property ( or at , for example). To try and reduce the impact of functions that are non-differentiable, we define the gradients of the elementary operations by applying the following rules in order:\n  1. If the function is differentiable and thus a gradient exists at the current point, use it.\n  2. If the function is convex (at least locally), use the sub-gradient of minimum norm (it is the steepest descent direction).\n  3. If the function is concave (at least locally), use the super-gradient of minimum norm (consider and apply the previous point).\n  4. If the function is defined, define the gradient at the current point by continuity (note that is possible here, for example for ). If multiple values are possible, pick one arbitrarily.\n  5. If the function is not defined (, or most functions when the input is , for example) then the value used as the gradient is arbitrary (we might also raise an error but that is not guaranteed). Most functions will use as the gradient, but for performance reasons, some functions will use other values (, for example).\n  6. If the function is not a deterministic mapping (i.e. it is not a environment.\n\n\nThere are several mechanisms available from Python to locally disable gradient computation:\nTo disable gradients across entire blocks of code, there are context managers like no-grad mode and inference mode. For more fine-grained exclusion of subgraphs from gradient computation, there is setting the field of a tensor.\nBelow, in addition to discussing the mechanisms above, we also describe evaluation mode (), a method that is not used to disable gradient computation but, because of its name, is often mixed up with the three.\nis a flag, defaulting to false , that allows for fine-grained exclusion of subgraphs from gradient computation. It takes effect in both the forward and backward passes:\nDuring the forward pass, an operation is only recorded in the backward graph if at least one of its input tensors require grad. During the backward pass (), only leaf tensors with will have gradients accumulated into their fields.\nIt is important to note that even though every tensor has this flag, it only makes sense for leaf tensors (tensors that do not have a , e.g., a ’s parameters). Non-leaf tensors (tensors that do have ) are tensors that have a backward graph associated with them. Thus their gradients will be needed as an intermediary result to compute the gradient for a leaf tensor that requires grad. From this definition, it is clear that all non-leaf tensors will automatically have .\nSetting should be the main way you control which parts of the model are part of the gradient computation, for example, if you need to freeze parts of your pretrained model during model fine-tuning.\nTo freeze parts of your model, simply apply to the parameters that you don’t want updated. And as described above, since computations that use these parameters as inputs would not be recorded in the forward pass, they won’t have their fields updated in the backward pass because they won’t be part of the backward graph in the first place, as desired.\nBecause this is such a common pattern, can also be set at the module level with . When applied to a module, takes effect on all of the module’s parameters (which have by default).\nApart from setting there are also three grad modes that can be selected from Python that can affect how computations in PyTorch are processed by autograd internally: default mode (grad mode), no-grad mode, and inference mode, all of which can be togglable via context managers and decorators.\nTensors created while the mode is enabled can be used in grad-mode later  \n---  \nThe “default mode” is the mode we are implicitly in when no other modes like no-grad and inference mode are enabled. To be contrasted with “no-grad mode” the default mode is also sometimes called “grad mode”.\nThe most important thing to know about the default mode is that it is the only mode in which takes effect. is always overridden to be in both the two other modes.\nComputations in no-grad mode behave as if none of the inputs require grad. In other words, computations in no-grad mode are never recorded in the backward graph even if there are inputs that have .\nEnable no-grad mode when you need to perform operations that should not be recorded by autograd, but you’d still like to use the outputs of these computations in grad mode later. This context manager makes it convenient to disable gradients for a block of code or function without having to temporarily set tensors to have , and then back to .\nFor example, no-grad mode might be useful when writing an optimizer: when performing the training update you’d like to update parameters in-place without the update being recorded by autograd. You also intend to use the updated parameters for computations in grad mode in the next forward pass.\nThe implementations in also rely on no-grad mode when initializing the parameters as to avoid autograd tracking when updating the initialized parameters in-place.\nInference mode is the extreme version of no-grad mode. Just like in no-grad mode, computations in inference mode are not recorded in the backward graph, but enabling inference mode will allow PyTorch to speed up your model even more. This better runtime comes with a drawback: tensors created in inference mode will not be able to be used in computations to be recorded by autograd after exiting inference mode.\nEnable inference mode when you are performing computations that do not have interactions with autograd, AND you don’t plan on using the tensors created in inference mode in any computation that is to be recorded by autograd later.\nIt is recommended that you try out inference mode in the parts of your code that do not require autograd tracking (e.g., data processing and model evaluation). If it works out of the box for your use case it’s a free performance win. If you run into errors after enabling inference mode, check that you are not using tensors created in inference mode in computations that are recorded by autograd after exiting inference mode. If you cannot avoid such use in your case, you can always switch back to no-grad mode.\nEvaluation mode is not a mechanism to locally disable gradient computation. It is included here anyway because it is sometimes confused to be such a mechanism.\nFunctionally, (or equivalently ) are completely orthogonal to no-grad mode and inference mode. How affects your model depends entirely on the specific modules used in your model and whether they define any training-mode specific behavior.\nYou are responsible for calling and if your model relies on modules such as and that may behave differently depending on training mode, for example, to avoid updating your BatchNorm running statistics on validation data.\nIt is recommended that you always use when training and when evaluating your model (validation/testing) even if you aren’t sure your model has training-mode specific behavior, because a module you are using might be updated to behave differently in training and eval modes.\nSupporting in-place operations in autograd is a hard matter, and we discourage their use in most cases. Autograd’s aggressive buffer freeing and reuse makes it very efficient and there are very few occasions when in-place operations lower memory usage by any significant amount. Unless you’re operating under heavy memory pressure, you might never need to use them.\nThere are two main reasons that limit the applicability of in-place operations:\n  1. Every in-place operation requires the implementation to rewrite the computational graph. Out-of-place versions simply allocate new objects and keep references to the old graph, while in-place operations, require changing the creator of all inputs to the representing this operation. This can be tricky, especially if there are many Tensors that reference the same storage (e.g. created by indexing or transposing), and in-place functions will raise an error if the storage of modified inputs is referenced by any other .\n\n\nEvery tensor keeps a version counter, that is incremented every time it is marked dirty in any operation. When a Function saves any tensors for backward, a version counter of their containing Tensor is saved as well. Once you access it is checked, and if it is greater than the saved value an error is raised. This ensures that if you’re using in-place functions and not seeing any errors, you can be sure that the computed gradients are correct.\nThe autograd engine is responsible for running all the backward operations necessary to compute the backward pass. This section will describe all the details that can help you make the best use of it in a multithreaded environment. (This is relevant only for PyTorch 1.6+ as the behavior in previous version was different.)\nUser could train their model with multithreading code (e.g. Hogwild training), and does not block on the concurrent backward computations, example code could be:\n```\n# Define a train function to be used in different threads\n \n        \n    \n              \n    \n    \n    \n\n\n# User write their own threading code to drive the train_fn\n  \n   \n       \n    \n    \n\n   \n    \n\n```\n\nNote that some behaviors that user should be aware of:\nWhen you run or via python or C++ API in multiple threads on CPU, you are expecting to see extra concurrency instead of serializing all the backward calls in a specific order during execution (behavior before PyTorch 1.6).\nIf you are calling from multiple threads concurrently and have shared inputs (i.e. Hogwild CPU training), then non-determinism should be expected. This can occur because parameters are automatically shared across threads, as such, multiple threads may access and try to accumulate the same attribute during gradient accumulation. This is technically not safe, and it might result in race condition and the result might be invalid to use.\nUsers developing multithreaded models featuring shared parameters should have the threading model in mind and should understand the issues described above.\nThe functional API may be used to calculate the gradients instead of to avoid non-determinism.\nIf part of the autograd graph is shared between threads, i.e. run first part of forward single thread, then run second part in multiple threads, then the first part of graph is shared. In this case different threads execute or on the same graph might have issue of destroying the graph on the fly of one thread, and the other thread will crash in this case. Autograd will error out to the user similar to what call twice with out , and let the user know they should use .\nSince Autograd allows the caller thread to drive its backward execution for potential parallelism, it’s important that we ensure thread safety on CPU with parallel calls that share part/whole of the GraphTask.\nCustom Python s are automatically thread safe because of GIL. For built-in C++ Autograd Nodes (e.g. AccumulateGrad, CopySlices) and custom s, the Autograd Engine uses thread mutex locking to ensure thread safety on autograd Nodes that might have state write/read.\nAutograd relies on the user to write thread safe C++ hooks. If you want the hook to be correctly applied in multithreading environment, you will need to write proper thread locking code to ensure the hooks are thread safe.\n  * When you use PyTorch to differentiate any function with complex domain and/or codomain, the gradients are computed under the assumption that the function is a part of a larger real-valued loss function . The gradient computed is (note the conjugation of z), the negative of which is precisely the direction of steepest descent used in Gradient Descent algorithm. Thus, there is a viable path in making the existing optimizers work out of the box with complex parameters.\n  * This convention matches TensorFlow’s convention for complex differentiation, but is different from JAX (which computes ).\n  * If you have a real-to-real function which internally uses complex operations, the convention here doesn’t matter: you will always get the same result that you would have gotten if it had been implemented with only real operations.\n\n\nIf you are curious about the mathematical details, or want to know how to define complex derivatives in PyTorch, read on.\nThe mathematical definition of complex-differentiability takes the limit definition of a derivative and generalizes it to operate on complex numbers. Consider a function ,\n> f'(z) = \\lim_{h \\to 0, h \\in C} \\frac{f(z+h) - f(z)}{h} \nIn order for this limit to exist, not only must and must be real differentiable, but must also satisfy the Cauchy-Riemann ) must be equal. This is a more restrictive condition.\nThe complex differentiable functions are commonly known as holomorphic functions. They are well behaved, have all the nice properties that you’ve seen from real differentiable functions, but are practically of no use in the optimization world. For optimization problems, only real valued objective functions are used in the research community since complex numbers are not part of any ordered field and so having complex valued loss does not make much sense.\nIt also turns out that no interesting real-valued objective fulfill the Cauchy-Riemann equations. So the theory with holomorphic function cannot be used for optimization and most people therefore use the Wirtinger calculus.\nSo, we have this great theory of complex differentiability and holomorphic functions, and we can’t use any of it at all, because many of the commonly used functions are not holomorphic. What’s a poor mathematician to do? Well, Wirtinger observed that even if isn’t holomorphic, one could rewrite it as a two variable function which is always holomorphic. This is because real and imaginary of the components of can be expressed in terms of and as:\n> \\begin{aligned} \\mathrm{Re}(z) &= \\frac {z + z^*}{2} \\\\\\ \\mathrm{Im}(z) &= \\frac {z - z^*}{2j} \\end{aligned} \nWirtinger calculus suggests to study instead, which is guaranteed to be holomorphic if was real differentiable (another way to think of it is as a change of coordinate system, from to .) This function has partial derivatives and . We can use the chain rule to establish a relationship between these partial derivatives and the partial derivatives w.r.t., the real and imaginary components of .\n> \\begin{aligned} \\frac{\\partial }{\\partial x} &= \\frac{\\partial z}{\\partial x} * \\frac{\\partial }{\\partial z} + \\frac{\\partial z^*}{\\partial x} * \\frac{\\partial }{\\partial z^*} \\\\\\ &= \\frac{\\partial }{\\partial z} + \\frac{\\partial }{\\partial z^*} \\\\\\ \\\\\\ \\frac{\\partial }{\\partial y} &= \\frac{\\partial z}{\\partial y} * \\frac{\\partial }{\\partial z} + \\frac{\\partial z^*}{\\partial y} * \\frac{\\partial }{\\partial z^*} \\\\\\ &= 1j * \\left(\\frac{\\partial }{\\partial z} - \\frac{\\partial }{\\partial z^*}\\right) \\end{aligned} \n> \\begin{aligned} \\frac{\\partial }{\\partial z} &= 1/2 * \\left(\\frac{\\partial }{\\partial x} - 1j * \\frac{\\partial }{\\partial y}\\right) \\\\\\ \\frac{\\partial }{\\partial z^*} &= 1/2 * \\left(\\frac{\\partial }{\\partial x} + 1j * \\frac{\\partial }{\\partial y}\\right) \\end{aligned} \nwhich is the classic definition of Wirtinger calculus that you would find on \nThere are a lot of beautiful consequences of this change.\n  * For one, the Cauchy-Riemann equations translate into simply saying that (that is to say, the function can be written entirely in terms of , without making reference to ).\n  * Another important (and somewhat counterintuitive) result, as we’ll see later, is that when we do optimization on a real-valued loss, the step we should take while making variable update is given by (not ).\n\n\nResearchers in audio and other fields, more commonly, use gradient descent to optimize real valued loss functions with complex variables. Typically, these people treat the real and imaginary values as separate channels that can be updated. For a step size and loss , we can write the following equations in :\n> \\begin{aligned} x_{n+1} &= x_n - (\\alpha/2) * \\frac{\\partial L}{\\partial x} \\\\\\ y_{n+1} &= y_n - (\\alpha/2) * \\frac{\\partial L}{\\partial y} \\end{aligned} \n> \\begin{aligned} z_{n+1} &= x_n - (\\alpha/2) * \\frac{\\partial L}{\\partial x} + 1j * (y_n - (\\alpha/2) * \\frac{\\partial L}{\\partial y}) \\\\\\ &= z_n - \\alpha * 1/2 * \\left(\\frac{\\partial L}{\\partial x} + j \\frac{\\partial L}{\\partial y}\\right) \\\\\\ &= z_n - \\alpha * \\frac{\\partial L}{\\partial z^*} \\end{aligned} \nSomething very interesting has happened: Wirtinger calculus tells us that we can simplify the complex variable update formula above to only refer to the conjugate Wirtinger derivative , giving us exactly the step we take in optimization.\nBecause the conjugate Wirtinger derivative gives us exactly the correct step for a real valued loss function, PyTorch gives you this derivative when you differentiate a function with a real valued loss.\nTypically, our derivative formulas take in as an input, representing the incoming Vector-Jacobian product that we’ve already computed, aka, , where is the loss of the entire computation (producing a real loss) and is the output of our function. The goal here is to compute , where is the input of the function. It turns out that in the case of real loss, we can get away with calculating , even though the chain rule implies that we also need to have access to . If you want to skip this derivation, look at the last equation in this section and then skip to the next section.\nLet’s continue working with defined as . As discussed above, autograd’s gradient convention is centered around optimization for real valued loss functions, so let’s assume is a part of larger real valued loss function . Using chain rule, we can write:\n> \\frac{\\partial L}{\\partial z^*} = \\frac{\\partial L}{\\partial u} * \\frac{\\partial u}{\\partial z^*} + \\frac{\\partial L}{\\partial v} * \\frac{\\partial v}{\\partial z^*} \n> \\begin{aligned} \\frac{\\partial L}{\\partial s} = 1/2 * \\left(\\frac{\\partial L}{\\partial u} - \\frac{\\partial L}{\\partial v} j\\right) \\\\\\ \\frac{\\partial L}{\\partial s^*} = 1/2 * \\left(\\frac{\\partial L}{\\partial u} + \\frac{\\partial L}{\\partial v} j\\right) \\end{aligned} \nIt should be noted here that since and are real functions, and is real by our assumption that is a part of a real valued function, we have:\nSolving the above equations for and , we get:\n> \\begin{aligned} \\frac{\\partial L}{\\partial u} = \\frac{\\partial L}{\\partial s} + \\frac{\\partial L}{\\partial s^*} \\\\\\ \\frac{\\partial L}{\\partial v} = 1j * \\left(\\frac{\\partial L}{\\partial s} - \\frac{\\partial L}{\\partial s^*}\\right) \\end{aligned} \n> \\begin{aligned} \\frac{\\partial L}{\\partial z^*} &= \\left(\\frac{\\partial L}{\\partial s} + \\frac{\\partial L}{\\partial s^*}\\right) * \\frac{\\partial u}{\\partial z^*} + 1j * \\left(\\frac{\\partial L}{\\partial s} - \\frac{\\partial L}{\\partial s^*}\\right) * \\frac{\\partial v}{\\partial z^*} \\\\\\ &= \\frac{\\partial L}{\\partial s} * \\left(\\frac{\\partial u}{\\partial z^*} + \\frac{\\partial v}{\\partial z^*} j\\right) + \\frac{\\partial L}{\\partial s^*} * \\left(\\frac{\\partial u}{\\partial z^*} - \\frac{\\partial v}{\\partial z^*} j\\right) \\\\\\ &= \\frac{\\partial L}{\\partial s} * \\frac{\\partial (u + vj)}{\\partial z^*} + \\frac{\\partial L}{\\partial s^*} * \\frac{\\partial (u + vj)^*}{\\partial z^*} \\\\\\ &= \\frac{\\partial L}{\\partial s} * \\frac{\\partial s}{\\partial z^*} + \\frac{\\partial L}{\\partial s^*} * \\frac{\\partial s^*}{\\partial z^*} \\\\\\ \\end{aligned} \n> \\begin{aligned} \\frac{\\partial L}{\\partial z^*} &= \\left(\\frac{\\partial L}{\\partial s^*}\\right)^* * \\frac{\\partial s}{\\partial z^*} + \\frac{\\partial L}{\\partial s^*} * \\left(\\frac{\\partial s}{\\partial z}\\right)^* \\\\\\ &= \\boxed{ (grad\\\\_output)^* * \\frac{\\partial s}{\\partial z^*} + grad\\\\_output * \\left(\\frac{\\partial s}{\\partial z}\\right)^* } \\\\\\ \\end{aligned} \nThis last equation is the important one for writing your own gradients, as it decomposes our derivative formula into a simpler one that is easy to compute by hand.\n### How can I write my own derivative formula for a complex function? \nThe above boxed equation gives us the general formula for all derivatives on complex functions. However, we still need to compute and . There are two ways you could do this:\n>   * The first way is to just use the definition of Wirtinger derivatives directly and calculate and by using and (which you can compute in the normal way).\n>   * The second way is to use the change of variables trick and rewrite as a two variable function , and compute the conjugate Wirtinger derivatives by treating and as independent variables. This is often easier; for example, if the function in question is holomorphic, only will be used (and will be zero).\n> \n\nLet’s consider the function f(z = x + yj) = c * z = c * (x+yj) as an example, where .\nUsing the first way to compute the Wirtinger derivatives, we have.\n\\begin{aligned} \\frac{\\partial s}{\\partial z} &= 1/2 * \\left(\\frac{\\partial s}{\\partial x} - \\frac{\\partial s}{\\partial y} j\\right) \\\\\\ &= 1/2 * (c - (c * 1j) * 1j) \\\\\\ &= c \\\\\\ \\\\\\ \\\\\\ \\frac{\\partial s}{\\partial z^*} &= 1/2 * \\left(\\frac{\\partial s}{\\partial x} + \\frac{\\partial s}{\\partial y} j\\right) \\\\\\ &= 1/2 * (c + (c * 1j) * 1j) \\\\\\ &= 0 \\\\\\ \\end{aligned} \nUsing , and (which is the default grad output value used when is called on a scalar output in PyTorch), we get:\n> \\frac{\\partial L}{\\partial z^*} = 1 * 0 + 1 * c = c \nUsing the second way to compute Wirtinger derivatives, we directly get:\n> \\begin{aligned} \\frac{\\partial s}{\\partial z} &= \\frac{\\partial (c*z)}{\\partial z} \\\\\\ &= c \\\\\\ \\frac{\\partial s}{\\partial z^*} &= \\frac{\\partial (c*z)}{\\partial z^*} \\\\\\ &= 0 \\end{aligned} \nAnd using again, we get . As you can see, the second way involves lesser calculations, and comes in more handy for faster calculations.\nSome functions map from complex inputs to real outputs, or vice versa. These functions form a special case of , which we can derive using the chain rule:\n>   * > \\frac{\\partial L}{\\partial z^*} = 2 * grad\\\\_output * \\frac{\\partial s}{\\partial z^{*}} \n>   * > \\frac{\\partial L}{\\partial z^*} = 2 * \\mathrm{Re}(grad\\\\_output^* * \\frac{\\partial s}{\\partial z^{*}}) \n> \n\nYou can control by defining a pair of / hooks. The function should take a tensor as its single argument but can return any python object (e.g. another tensor, a tuple, or even a string containing a filename). The function takes as its single argument the output of and should return a tensor to be used in the backward pass. The tensor returned by only needs to have the same content as the tensor passed as input to . In particular, any autograd-related metadata can be ignored as they will be overwritten during unpacking.\nNotice that the should not delete the temporary file because it might be called multiple times: the temporary file should be alive for as long as the returned object is alive. In the above example, we prevent leaking the temporary file by closing it when it is no longer needed (on deletion of the object).\nWe guarantee that will only be called once but can be called as many times as the backward pass requires it and we expect it to return the same data each time.\nPerforming inplace operations on the input of any of the functions is forbidden as they may lead to unexpected side-effects. PyTorch will throw an error if the input to a pack hook is modified inplace but does not catch the case where the input to an unpack hook is modified inplace.\nYou can register a pair of hooks on a saved tensor by calling the method on a object. Those objects are exposed as attributes of a and start with the prefix.\nThe method is called as soon as the pair is registered. The method is called each time the saved tensor needs to be accessed, either by means of or during the backward pass.\nIf you maintain a reference to a after the saved tensors have been released (i.e. after backward has been called), calling its is forbidden. PyTorch will throw an error most of the time but it may fail to do so in some cases and undefined behavior may arise.\nAlternatively, you can use the context-manager to register a pair of hooks which will be applied to saved tensors that are created in that context.\n```\n# Only save on disk tensors that have size >= 1000\n  \n\n \n       \n         \n      \n     \n     \n\n \n      \n         \n     \n\n \n      \n          \n          \n            \n         \n\n  \n  \n\n```\n\nThe hooks defined with this context manager are thread-local. Hence, the following code will not produce the desired effects because the hooks do not go through .\nNote that using those hooks disables all the optimization in place to reduce Tensor object creation. For example:\nWithout the hooks, , and all refer to the same tensor object. With the hooks, PyTorch will pack and unpack into two new tensor objects that share the same storage with the original (no copy performed).\nThis section will discuss when different hooks fire or don’t fire. Then it will discuss the order in which they are fired. The hooks that will be covered are: backward hooks registered to Tensor via , post-accumulate-grad hooks registered to Tensor via , post-hooks registered to Node via , and pre-hooks registered to Node via .\nHooks registered to a Tensor via are executed when gradients are being computed for that Tensor. (Note that this does not require the Tensor’s grad_fn to be executed. For example, if the Tensor is passed as part of the argument to , the Tensor’s grad_fn may not be executed, but the hook register to that Tensor will always be executed.)\nHooks registered to a Tensor via are executed after the gradients have been accumulated for that Tensor, meaning the Tensor’s grad field has been set. Whereas hooks registered via are run as gradients are being computed, hooks registered via are only triggered once the Tensor’s grad field is updated by autograd at the end of the backward pass. Thus, post-accumulate-grad hooks can only be registered for leaf Tensors. Registering a hook via on a non-leaf Tensor will error, even if you call .\nHooks registered to using or are only fired if the Node it was registered to is executed.\nWhether a particular Node is executed may depend on whether the backward pass was called with or . Specifically, you should be aware of these differences when you register a hook on a Node corresponding to a Tensor that you are passing to or as part of the argument.\nIf you are using , all of the above mentioned hooks will be executed, whether or not you specified the argument. This is because executes all Nodes, even if they correspond to a Tensor specified as an input. (Note that the execution of this additional Node corresponding to Tensors passed as is usually unnecessary, but done anyway. This behavior is subject to change; you should not depend on it.)\nOn the other hand, if you are using , the backward hooks registered to Nodes that correspond to the Tensors passed to may not be executed, because those Nodes will not be executed unless there is another input that depends on the gradient result of this Node.\n  1. pre-hooks registered to Node are executed (if Node is executed).\n  2. post-hooks registered to Node are executed (if Node is executed)\n\n\nIf multiple hooks of the same type are registered on the same Tensor or Node they are executed in the order in which they are registered. Hooks that are executed later can observe the modifications to the gradient made by earlier hooks.\nis implemented using hooks registered to Tensors. Each individual Tensor hook is fired following the Tensor hook ordering defined above and the registered multi-grad hook is called when the last Tensor gradient is computed.\nis implemented using hooks registered to Node. As the forward is computed, hooks are registered to grad_fn corresponding to the inputs and outputs of the module. Because a module may take multiple inputs and return multiple outputs, a dummy custom autograd Function is first applied to the inputs of the module before forward and the outputs of the module before the output of forward is returned to ensure that those Tensors share a single grad_fn, which we can then attach our hooks to.\nUsually hooks registered to a Tensor receive the gradient of the outputs with respect to that Tensor, where the value of the Tensor is taken to be its value at the time backward is computed.\nHowever, if you register hooks to a Tensor, and then modify that Tensor in-place, hooks registered before in-place modification similarly receive gradients of the outputs with respect to the Tensor, but the value of the Tensor is taken to be its value before in-place modification.\nIf you prefer the behavior in the former case, you should register them to the Tensor after all in-place modifications to it have been made. For example:\nFurthermore, it can be helpful to know that under the hood, when hooks are registered to a Tensor, they actually become permanently bound to the grad_fn of that Tensor, so if that Tensor is then modified in-place, even though the Tensor now has a new grad_fn, hooks registered before it was modified in-place will continue to be associated with the old grad_fn, e.g. they will fire when that Tensor’s old grad_fn is reached in the graph by the autograd engine.\n  *     *     *     *       * [How can I write my own derivative formula for a complex function?](https://docs.pytorch.org/docs/stable/notes/autograd.html#how-can-i-write-my-own-derivative-formula-for-a-complex-function)\n    *     * \n\nTo analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: \n"
}